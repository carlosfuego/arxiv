[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando Garc√≠a-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David G√ºera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria M√∂nchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Berm√∫dez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v1",
                "updated": "2025-08-27T10:11:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v3",
                "updated": "2025-08-26T03:23:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    23,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinst√§dtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17445v1",
                "updated": "2025-08-24T16:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling"
                },
                "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO."
                },
                "authors": [
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17434v1",
                "updated": "2025-08-24T16:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yuhang Yu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v1",
                "updated": "2025-08-24T13:30:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17219v1",
                "updated": "2025-08-24T05:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T05:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving"
                },
                "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v2",
                "updated": "2025-08-23T20:28:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    45,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17137v1",
                "updated": "2025-08-23T20:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T20:28:32Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "title": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices"
                },
                "summary": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines."
                },
                "authors": [
                    {
                        "name": "Nishant Gavhane"
                    },
                    {
                        "name": "Arush Mehrotra"
                    },
                    {
                        "name": "Rohit Chawla"
                    },
                    {
                        "name": "Peter Proenca"
                    }
                ],
                "author_detail": {
                    "name": "Peter Proenca"
                },
                "author": "Peter Proenca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17125v1",
                "updated": "2025-08-23T19:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T19:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "title": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling"
                },
                "summary": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Yongxiang Tang"
                    },
                    {
                        "name": "Yanhua Cheng"
                    },
                    {
                        "name": "Yong Bai"
                    },
                    {
                        "name": "Yanxiang Zeng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xialong Liu"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17032v1",
                "updated": "2025-08-23T14:20:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "title": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations"
                },
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling."
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16984v1",
                "updated": "2025-08-23T10:35:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T10:35:16Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching"
                },
                "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v2",
                "updated": "2025-08-23T08:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    40,
                    52,
                    5,
                    235,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16211v1",
                "updated": "2025-08-22T08:34:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:34:03Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16184v1",
                "updated": "2025-08-22T07:57:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:57:28Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "title": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach"
                },
                "summary": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost."
                },
                "authors": [
                    {
                        "name": "Yuhao Zheng"
                    },
                    {
                        "name": "Ting You"
                    },
                    {
                        "name": "Kejia Peng"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16134v1",
                "updated": "2025-08-22T06:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"
                },
                "summary": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoyu Qiao"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16121v1",
                "updated": "2025-08-22T06:28:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:28:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables"
                },
                "summary": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance."
                },
                "authors": [
                    {
                        "name": "Wontae Kim"
                    },
                    {
                        "name": "Keuntek Lee"
                    },
                    {
                        "name": "Nam Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Nam Ik Cho"
                },
                "author": "Nam Ik Cho",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v2",
                "updated": "2025-08-22T03:36:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    3,
                    36,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v2",
                "updated": "2025-08-21T22:45:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    22,
                    45,
                    6,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "Added link to code repository. Fixed typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v2",
                "updated": "2025-08-21T20:13:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    20,
                    13,
                    40,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/"
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v1",
                "updated": "2025-08-21T18:40:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15717v1",
                "updated": "2025-08-21T16:56:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:56:29Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches."
                },
                "authors": [
                    {
                        "name": "Yanlai Yang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15694v1",
                "updated": "2025-08-21T16:21:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:21:46Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "title": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search"
                },
                "summary": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems."
                },
                "authors": [
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Shengyuan Lin"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shuhao Fan"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "arxiv_comment": "12 pages, 12 figures, this paper is the English version of our\n  Chinese paper accepted for publication in Journal of Software, Vol. 37, No.\n  3, 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15647v1",
                "updated": "2025-08-21T15:25:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T15:25:30Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "title": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing"
                },
                "summary": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals"
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Shuai Mu"
                    },
                    {
                        "name": "Sebastian Angel"
                    },
                    {
                        "name": "Vincent Liu"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Liu"
                },
                "author": "Vincent Liu",
                "arxiv_doi": "10.14778/3704965.3704969",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3704965.3704969",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.15647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version from PVLDB Volume 17, Issue 13, 2024. This version\n  includes full proofs and formal verification in Dafny and fixes some small\n  bugs",
                "arxiv_journal_ref": "PVLDB Volume 17, Issue 13, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v2",
                "updated": "2025-08-21T14:58:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    58,
                    12,
                    3,
                    233,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15601v1",
                "updated": "2025-08-21T14:24:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T14:24:52Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "title": "Efficient Mixed-Precision Large Language Model Inference with TurboMind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Mixed-Precision Large Language Model Inference with TurboMind"
                },
                "summary": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy."
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Qian Yao"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15583v1",
                "updated": "2025-08-21T13:57:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:57:09Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "title": "Time-Optimal Directed q-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Optimal Directed q-Analysis"
                },
                "summary": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Florian Unger"
                    }
                ],
                "author_detail": {
                    "name": "Florian Unger"
                },
                "author": "Florian Unger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15545v1",
                "updated": "2025-08-21T13:24:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:24:13Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "title": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation"
                },
                "summary": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value."
                },
                "authors": [
                    {
                        "name": "Mingyang Yu"
                    },
                    {
                        "name": "Haorui Yang"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Desheng Kong"
                    },
                    {
                        "name": "Ji Du"
                    },
                    {
                        "name": "Yulong Fu"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v2",
                "updated": "2025-08-21T12:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    12,
                    52,
                    11,
                    3,
                    233,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas H√∂llein"
                    },
                    {
                        "name": "Alja≈æ Bo≈æiƒç"
                    },
                    {
                        "name": "Michael Zollh√∂fer"
                    },
                    {
                        "name": "Matthias Nie√üner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nie√üner"
                },
                "author": "Matthias Nie√üner",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  https://lukashoel.github.io/3DGS-LM, Video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, Code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14204v3",
                "updated": "2025-08-21T11:43:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    11,
                    43,
                    48,
                    3,
                    233,
                    0
                ],
                "published": "2024-04-22T14:13:36Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    14,
                    13,
                    36,
                    0,
                    113,
                    0
                ],
                "title": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading"
                },
                "summary": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "18 pages, 13 figures. Part of this work has been accepted by ICDCS\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15290v1",
                "updated": "2025-08-21T06:26:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T06:26:18Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "title": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search"
                },
                "summary": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%."
                },
                "authors": [
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Xiaolu Li"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Meiling Wang"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v1",
                "updated": "2025-08-21T03:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15036v1",
                "updated": "2025-08-20T20:02:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T20:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs"
                },
                "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."
                },
                "authors": [
                    {
                        "name": "Ruyi Ding"
                    },
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Xinyi Shen"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "This paper will appear in CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15033v1",
                "updated": "2025-08-20T19:54:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T19:54:41Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "title": "Rethinking the Potential of Layer Freezing for Efficient DNN Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Potential of Layer Freezing for Efficient DNN Training"
                },
                "summary": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training."
                },
                "authors": [
                    {
                        "name": "Chence Yang"
                    },
                    {
                        "name": "Ci Zhang"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Xulong Tang"
                    },
                    {
                        "name": "Shaoyi Huang"
                    },
                    {
                        "name": "Jinzhen Wang"
                    },
                    {
                        "name": "Guoming Li"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19263v1",
                "updated": "2025-08-20T12:46:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    46,
                    50,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T12:46:50Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    46,
                    50,
                    2,
                    232,
                    0
                ],
                "title": "Lossless Compression of Neural Network Components: Weights, Checkpoints,\n  and K/V Caches in Low-Precision Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless Compression of Neural Network Components: Weights, Checkpoints,\n  and K/V Caches in Low-Precision Formats"
                },
                "summary": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment."
                },
                "authors": [
                    {
                        "name": "Anat Heilper"
                    },
                    {
                        "name": "Doron Singer"
                    }
                ],
                "author_detail": {
                    "name": "Doron Singer"
                },
                "author": "Doron Singer",
                "arxiv_comment": "16 pages 9 images",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14468v1",
                "updated": "2025-08-20T06:48:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:48:54Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "title": "Diverse Negative Sampling for Implicit Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Negative Sampling for Implicit Collaborative Filtering"
                },
                "summary": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yueqing Xuan"
                    },
                    {
                        "name": "Kacper Sokol"
                    },
                    {
                        "name": "Mark Sanderson"
                    },
                    {
                        "name": "Jeffrey Chan"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Chan"
                },
                "author": "Jeffrey Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14420v1",
                "updated": "2025-08-20T04:36:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:36:25Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "title": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan"
                },
                "summary": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform."
                },
                "authors": [
                    {
                        "name": "Shuli Wang"
                    },
                    {
                        "name": "Yinqiu Huang"
                    },
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yonggang Liu"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Yinhua Zhu"
                    },
                    {
                        "name": "Haitao Wang"
                    },
                    {
                        "name": "Xingxing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wang"
                },
                "author": "Xingxing Wang",
                "arxiv_doi": "10.1145/3746252.3761539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.14420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16653v1",
                "updated": "2025-08-20T03:42:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T03:42:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "title": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Zizhuo Fu"
                    },
                    {
                        "name": "Xiaotian Guo"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Peiyu Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "International Conference on Computer-Aided Design (ICCAD) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13935v1",
                "updated": "2025-08-19T15:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/TC.2025.3587513",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TC.2025.3587513",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Computers",
                "arxiv_journal_ref": "Year 2025, pp. 1-14,",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13909v1",
                "updated": "2025-08-19T15:08:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:08:39Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB)."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/ICDE60146.2024.00312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE60146.2024.00312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, accepted by 2024 IEEE 40st International Conference on Data\n  Engineering (ICDE)",
                "arxiv_journal_ref": "Year: 2024, Pages: 4072-4085",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v1",
                "updated": "2025-08-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13859v1",
                "updated": "2025-08-19T14:18:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:18:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "Zobrist Hash-based Duplicate Detection in Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zobrist Hash-based Duplicate Detection in Symbolic Regression"
                },
                "summary": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information."
                },
                "authors": [
                    {
                        "name": "Bogdan Burlacu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Burlacu"
                },
                "author": "Bogdan Burlacu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13756v1",
                "updated": "2025-08-19T11:54:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:54:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video"
                },
                "summary": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems."
                },
                "authors": [
                    {
                        "name": "Ruonan Chai"
                    },
                    {
                        "name": "Yixiang Zhu"
                    },
                    {
                        "name": "Xinjiao Li"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Zili Meng"
                    },
                    {
                        "name": "Dirk Kutscher"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Kutscher"
                },
                "author": "Dirk Kutscher",
                "arxiv_comment": "9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM\n  International Conference on Multimedia (MM '25), October 27--31, 2025,\n  Dublin, Ireland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13716v1",
                "updated": "2025-08-19T10:21:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:21:33Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."
                },
                "authors": [
                    {
                        "name": "Xianfeng Song"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Zheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Shi"
                },
                "author": "Zheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v3",
                "updated": "2025-08-19T09:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v1",
                "updated": "2025-08-19T05:27:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v2",
                "updated": "2025-08-19T03:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    3,
                    13,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v2",
                "updated": "2025-08-19T01:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    1,
                    38,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13382v1",
                "updated": "2025-08-18T21:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T21:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis"
                },
                "summary": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution."
                },
                "authors": [
                    {
                        "name": "Ayoub Ben Chaliah"
                    },
                    {
                        "name": "Hela Dellagi"
                    }
                ],
                "author_detail": {
                    "name": "Hela Dellagi"
                },
                "author": "Hela Dellagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v3",
                "updated": "2025-08-18T16:52:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    52,
                    22,
                    0,
                    230,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up"
                },
                "summary": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v2",
                "updated": "2025-08-18T16:06:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    6,
                    9,
                    0,
                    230,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12767v1",
                "updated": "2025-08-18T09:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "title": "Some optimization possibilities in data plane programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some optimization possibilities in data plane programming"
                },
                "summary": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality."
                },
                "authors": [
                    {
                        "name": "Altangerel Gereltsetseg"
                    },
                    {
                        "name": "Tejfel M√°t√©"
                    }
                ],
                "author_detail": {
                    "name": "Tejfel M√°t√©"
                },
                "author": "Tejfel M√°t√©",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12743v1",
                "updated": "2025-08-18T09:06:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:06:49Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs"
                },
                "summary": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Ruimin Shi"
                    },
                    {
                        "name": "Edgar A. Le√≥n"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "arxiv_comment": "To be published in IISWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12691v1",
                "updated": "2025-08-18T07:49:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T07:49:33Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration"
                },
                "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Yuanxin Wei"
                    },
                    {
                        "name": "Lansong Diao"
                    },
                    {
                        "name": "Bujiao Chen"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Jiangsu Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiangsu Du"
                },
                "author": "Jiangsu Du",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12485v1",
                "updated": "2025-08-17T20:01:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T20:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX"
                },
                "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs."
                },
                "authors": [
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "Arpit Bhayani"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Bhayani"
                },
                "author": "Arpit Bhayani",
                "arxiv_comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; C.4; D.4.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v1",
                "updated": "2025-08-17T19:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12407v1",
                "updated": "2025-08-17T15:48:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T15:48:50Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads"
                },
                "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance."
                },
                "authors": [
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12357v1",
                "updated": "2025-08-17T13:05:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T13:05:52Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "title": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method"
                },
                "summary": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn."
                },
                "authors": [
                    {
                        "name": "S. Khardazi"
                    },
                    {
                        "name": "Z. Gargar"
                    },
                    {
                        "name": "A. Lyubchyk"
                    },
                    {
                        "name": "O. Zakir"
                    },
                    {
                        "name": "D. Mezzane"
                    },
                    {
                        "name": "M. Amjoud"
                    },
                    {
                        "name": "A. Alimoussa"
                    },
                    {
                        "name": "Z. Kutnjak"
                    }
                ],
                "author_detail": {
                    "name": "Z. Kutnjak"
                },
                "author": "Z. Kutnjak",
                "arxiv_doi": "10.1016/j.jssc.2025.125547",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jssc.2025.125547",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.12357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v5",
                "updated": "2025-08-16T23:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    23,
                    41,
                    48,
                    5,
                    228,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10824v2",
                "updated": "2025-08-16T03:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    3,
                    17,
                    35,
                    5,
                    228,
                    0
                ],
                "published": "2025-08-14T16:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures"
                },
                "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Parsa Omidi"
                    },
                    {
                        "name": "Xingshuai Huang"
                    },
                    {
                        "name": "Axel Laborieux"
                    },
                    {
                        "name": "Bahareh Nikpour"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Armaghan Eshaghi"
                    }
                ],
                "author_detail": {
                    "name": "Armaghan Eshaghi"
                },
                "author": "Armaghan Eshaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11495v1",
                "updated": "2025-08-15T14:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation"
                },
                "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators."
                },
                "authors": [
                    {
                        "name": "Jingnan Xu"
                    },
                    {
                        "name": "Leixia Wang"
                    },
                    {
                        "name": "Xiaofeng Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Meng"
                },
                "author": "Xiaofeng Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19257v1",
                "updated": "2025-08-15T12:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "Manuscript submitted to AAAI 2026, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v2",
                "updated": "2025-08-15T04:27:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    27,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10875v1",
                "updated": "2025-08-14T17:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2405.14862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14862v2",
                "updated": "2025-08-28T17:59:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    59,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2024-05-23T17:59:22Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    59,
                    22,
                    3,
                    144,
                    0
                ],
                "title": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs"
                },
                "summary": "Decoder-only large language models typically rely solely on masked causal\nattention, which limits their expressiveness by restricting information flow to\none direction. We propose Bitune, a method that enhances pretrained\ndecoder-only LLMs by incorporating bidirectional attention into prompt\nprocessing. We evaluate Bitune in instruction-tuning and question-answering\nsettings, showing significant improvements in performance on commonsense\nreasoning, arithmetic, and language understanding tasks. Furthermore, extensive\nablation studies validate the role of each component of the method, and\ndemonstrate that Bitune is compatible with various parameter-efficient\nfinetuning techniques and full model finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only large language models typically rely solely on masked causal\nattention, which limits their expressiveness by restricting information flow to\none direction. We propose Bitune, a method that enhances pretrained\ndecoder-only LLMs by incorporating bidirectional attention into prompt\nprocessing. We evaluate Bitune in instruction-tuning and question-answering\nsettings, showing significant improvements in performance on commonsense\nreasoning, arithmetic, and language understanding tasks. Furthermore, extensive\nablation studies validate the role of each component of the method, and\ndemonstrate that Bitune is compatible with various parameter-efficient\nfinetuning techniques and full model finetuning."
                },
                "authors": [
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21061v1",
                "updated": "2025-08-28T17:58:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    58,
                    29,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:58:29Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    58,
                    29,
                    3,
                    240,
                    0
                ],
                "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn\n  Dialogue with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn\n  Dialogue with Large Language Models"
                },
                "summary": "As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance."
                },
                "authors": [
                    {
                        "name": "Adam Coscia"
                    },
                    {
                        "name": "Shunan Guo"
                    },
                    {
                        "name": "Eunyee Koh"
                    },
                    {
                        "name": "Alex Endert"
                    }
                ],
                "author_detail": {
                    "name": "Alex Endert"
                },
                "author": "Alex Endert",
                "arxiv_doi": "10.1145/3746059.3747746",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746059.3747746",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.21061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to UIST 2025. 18 pages, 9 figures, 2 tables. For a demo\n  video, see https://youtu.be/uobhmxo6EIE",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21051v1",
                "updated": "2025-08-28T17:55:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    55,
                    7,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:55:07Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    55,
                    7,
                    3,
                    240,
                    0
                ],
                "title": "Enabling Equitable Access to Trustworthy Financial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Equitable Access to Trustworthy Financial Reasoning"
                },
                "summary": "According to the United States Internal Revenue Service, ''the average\nAmerican spends $\\$270$ and 13 hours filing their taxes''. Even beyond the\nU.S., tax filing requires complex reasoning, combining application of\noverlapping rules with numerical calculations. Because errors can incur costly\npenalties, any automated system must deliver high accuracy and auditability,\nmaking modern large language models (LLMs) poorly suited for this task. We\npropose an approach that integrates LLMs with a symbolic solver to calculate\ntax obligations. We evaluate variants of this system on the challenging\nStAtutory Reasoning Assessment (SARA) dataset, and include a novel method for\nestimating the cost of deploying such a system based on real-world penalties\nfor tax errors. We further show how combining up-front translation of\nplain-text rules into formal logic programs, combined with intelligently\nretrieved exemplars for formal case representations, can dramatically improve\nperformance on this task and reduce costs to well below real-world averages.\nOur results demonstrate the promise and economic feasibility of neuro-symbolic\narchitectures for increasing equitable access to reliable tax assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "According to the United States Internal Revenue Service, ''the average\nAmerican spends $\\$270$ and 13 hours filing their taxes''. Even beyond the\nU.S., tax filing requires complex reasoning, combining application of\noverlapping rules with numerical calculations. Because errors can incur costly\npenalties, any automated system must deliver high accuracy and auditability,\nmaking modern large language models (LLMs) poorly suited for this task. We\npropose an approach that integrates LLMs with a symbolic solver to calculate\ntax obligations. We evaluate variants of this system on the challenging\nStAtutory Reasoning Assessment (SARA) dataset, and include a novel method for\nestimating the cost of deploying such a system based on real-world penalties\nfor tax errors. We further show how combining up-front translation of\nplain-text rules into formal logic programs, combined with intelligently\nretrieved exemplars for formal case representations, can dramatically improve\nperformance on this task and reduce costs to well below real-world averages.\nOur results demonstrate the promise and economic feasibility of neuro-symbolic\narchitectures for increasing equitable access to reliable tax assistance."
                },
                "authors": [
                    {
                        "name": "William Jurayj"
                    },
                    {
                        "name": "Nils Holzenberger"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10175v2",
                "updated": "2025-08-28T17:54:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    54,
                    36,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-13T20:22:58Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    20,
                    22,
                    58,
                    2,
                    225,
                    0
                ],
                "title": "Estimating Machine Translation Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Machine Translation Difficulty"
                },
                "summary": "Machine translation quality has steadily improved over the years, achieving\nnear-perfect translations in recent benchmarks. These high-quality outputs make\nit difficult to distinguish between state-of-the-art models and to identify\nareas for future improvement. In this context, automatically identifying texts\nwhere machine translation systems struggle holds promise for developing more\ndiscriminative evaluations and guiding future research.\n  In this work, we address this gap by formalizing the task of translation\ndifficulty estimation, defining a text's difficulty based on the expected\nquality of its translations. We introduce a new metric to evaluate difficulty\nestimators and use it to assess both baselines and novel approaches. Finally,\nwe demonstrate the practical utility of difficulty estimators by using them to\nconstruct more challenging benchmarks for machine translation. Our results show\nthat dedicated models outperform both heuristic-based methods and\nLLM-as-a-judge approaches, with Sentinel-src achieving the best performance.\nThus, we release two improved models for difficulty estimation, Sentinel-src-24\nand Sentinel-src-25, which can be used to scan large collections of texts and\nselect those most likely to challenge contemporary machine translation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation quality has steadily improved over the years, achieving\nnear-perfect translations in recent benchmarks. These high-quality outputs make\nit difficult to distinguish between state-of-the-art models and to identify\nareas for future improvement. In this context, automatically identifying texts\nwhere machine translation systems struggle holds promise for developing more\ndiscriminative evaluations and guiding future research.\n  In this work, we address this gap by formalizing the task of translation\ndifficulty estimation, defining a text's difficulty based on the expected\nquality of its translations. We introduce a new metric to evaluate difficulty\nestimators and use it to assess both baselines and novel approaches. Finally,\nwe demonstrate the practical utility of difficulty estimators by using them to\nconstruct more challenging benchmarks for machine translation. Our results show\nthat dedicated models outperform both heuristic-based methods and\nLLM-as-a-judge approaches, with Sentinel-src achieving the best performance.\nThus, we release two improved models for difficulty estimation, Sentinel-src-24\nand Sentinel-src-25, which can be used to scan large collections of texts and\nselect those most likely to challenge contemporary machine translation systems."
                },
                "authors": [
                    {
                        "name": "Lorenzo Proietti"
                    },
                    {
                        "name": "Stefano Perrella"
                    },
                    {
                        "name": "Vil√©m Zouhar"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Tom Kocmi"
                    }
                ],
                "author_detail": {
                    "name": "Tom Kocmi"
                },
                "author": "Tom Kocmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21046v1",
                "updated": "2025-08-28T17:50:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    58,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:50:58Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    58,
                    3,
                    240,
                    0
                ],
                "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification"
                },
                "summary": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Renshan Zhang"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "23 pages, 8 figures, Project Page:\n  https://jiutian-vl.github.io/CogVLA-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21044v1",
                "updated": "2025-08-28T17:50:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    3,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:50:03Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    3,
                    3,
                    240,
                    0
                ],
                "title": "MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for\n  Efficient Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for\n  Efficient Video LLMs"
                },
                "summary": "Video Large Language Models (VLLMs) excel in video understanding, but their\nexcessive visual tokens pose a significant computational challenge for\nreal-world applications. Current methods aim to enhance inference efficiency by\nvisual token pruning. However, they do not consider the dynamic characteristics\nand temporal dependencies of video frames, as they perceive video understanding\nas a multi-frame task. To address these challenges, we propose MMG-Vid, a novel\ntraining-free visual token pruning framework that removes redundancy by\nMaximizing Marginal Gains at both segment-level and token-level. Specifically,\nwe first divide the video into segments based on frame similarity, and then\ndynamically allocate the token budget for each segment to maximize the marginal\ngain of each segment. Subsequently, we propose a temporal-guided DPC algorithm\nthat jointly models inter-frame uniqueness and intra-frame diversity, thereby\nmaximizing the marginal gain of each token. By combining both stages, MMG-Vid\ncan maximize the utilization of the limited token budget, significantly\nimproving efficiency while maintaining strong performance. Extensive\nexperiments demonstrate that MMG-Vid can maintain over 99.5% of the original\nperformance, while effectively reducing 75% visual tokens and accelerating the\nprefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VLLMs) excel in video understanding, but their\nexcessive visual tokens pose a significant computational challenge for\nreal-world applications. Current methods aim to enhance inference efficiency by\nvisual token pruning. However, they do not consider the dynamic characteristics\nand temporal dependencies of video frames, as they perceive video understanding\nas a multi-frame task. To address these challenges, we propose MMG-Vid, a novel\ntraining-free visual token pruning framework that removes redundancy by\nMaximizing Marginal Gains at both segment-level and token-level. Specifically,\nwe first divide the video into segments based on frame similarity, and then\ndynamically allocate the token budget for each segment to maximize the marginal\ngain of each segment. Subsequently, we propose a temporal-guided DPC algorithm\nthat jointly models inter-frame uniqueness and intra-frame diversity, thereby\nmaximizing the marginal gain of each token. By combining both stages, MMG-Vid\ncan maximize the utilization of the limited token budget, significantly\nimproving efficiency while maintaining strong performance. Extensive\nexperiments demonstrate that MMG-Vid can maintain over 99.5% of the original\nperformance, while effectively reducing 75% visual tokens and accelerating the\nprefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon."
                },
                "authors": [
                    {
                        "name": "Junpeng Ma"
                    },
                    {
                        "name": "Qizhe Zhang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Jun Song"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "arxiv_comment": "10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10950v2",
                "updated": "2025-08-28T17:44:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    44,
                    54,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-13T17:56:29Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    56,
                    29,
                    2,
                    225,
                    0
                ],
                "title": "From Promise to Practical Reality: Transforming Diffusion MRI Analysis\n  with Fast Deep Learning Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Promise to Practical Reality: Transforming Diffusion MRI Analysis\n  with Fast Deep Learning Enhancement"
                },
                "summary": "Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling\ntechnique that represents complex white matter fiber configurations, and a key\nstep for subsequent brain tractography and connectome analysis. Its reliability\nand accuracy, however, heavily rely on the quality of the MRI acquisition and\nthe subsequent estimation of the FODs at each voxel. Generating reliable FODs\nfrom widely available clinical protocols with single-shell and\nlow-angular-resolution acquisitions remains challenging but could potentially\nbe addressed with recent advances in deep learning-based enhancement\ntechniques. Despite advancements, existing methods have predominantly been\nassessed on healthy subjects, which have proved to be a major hurdle for their\nclinical adoption. In this work, we validate a newly optimized enhancement\nframework, FastFOD-Net, across healthy controls and six neurological disorders.\nThis accelerated end-to-end deep learning framework enhancing FODs with\nsuperior performance and delivering training/inference efficiency for clinical\nuse ($60\\times$ faster comparing to its predecessor). With the most\ncomprehensive clinical evaluation to date, our work demonstrates the potential\nof FastFOD-Net in accelerating clinical neuroscience research, empowering\ndiffusion MRI analysis for disease differentiation, improving interpretability\nin connectome applications, and reducing measurement errors to lower sample\nsize requirements. Critically, this work will facilitate the more widespread\nadoption of, and build clinical trust in, deep learning based methods for\ndiffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of\nreal-world, clinical diffusion MRI data, comparable to that achievable with\nhigh-quality research acquisitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling\ntechnique that represents complex white matter fiber configurations, and a key\nstep for subsequent brain tractography and connectome analysis. Its reliability\nand accuracy, however, heavily rely on the quality of the MRI acquisition and\nthe subsequent estimation of the FODs at each voxel. Generating reliable FODs\nfrom widely available clinical protocols with single-shell and\nlow-angular-resolution acquisitions remains challenging but could potentially\nbe addressed with recent advances in deep learning-based enhancement\ntechniques. Despite advancements, existing methods have predominantly been\nassessed on healthy subjects, which have proved to be a major hurdle for their\nclinical adoption. In this work, we validate a newly optimized enhancement\nframework, FastFOD-Net, across healthy controls and six neurological disorders.\nThis accelerated end-to-end deep learning framework enhancing FODs with\nsuperior performance and delivering training/inference efficiency for clinical\nuse ($60\\times$ faster comparing to its predecessor). With the most\ncomprehensive clinical evaluation to date, our work demonstrates the potential\nof FastFOD-Net in accelerating clinical neuroscience research, empowering\ndiffusion MRI analysis for disease differentiation, improving interpretability\nin connectome applications, and reducing measurement errors to lower sample\nsize requirements. Critically, this work will facilitate the more widespread\nadoption of, and build clinical trust in, deep learning based methods for\ndiffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of\nreal-world, clinical diffusion MRI data, comparable to that achievable with\nhigh-quality research acquisitions."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Michael Barnett"
                    },
                    {
                        "name": "Frederique Boonstra"
                    },
                    {
                        "name": "Yael Barnett"
                    },
                    {
                        "name": "Mariano Cabezas"
                    },
                    {
                        "name": "Arkiev D'Souza"
                    },
                    {
                        "name": "Matthew C. Kiernan"
                    },
                    {
                        "name": "Kain Kyle"
                    },
                    {
                        "name": "Meng Law"
                    },
                    {
                        "name": "Lynette Masters"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Stephen Tisch"
                    },
                    {
                        "name": "Sicong Tu"
                    },
                    {
                        "name": "Anneke Van Der Walt"
                    },
                    {
                        "name": "Dongang Wang"
                    },
                    {
                        "name": "Fernando Calamante"
                    },
                    {
                        "name": "Weidong Cai"
                    },
                    {
                        "name": "Chenyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu Wang"
                },
                "author": "Chenyu Wang",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21032v1",
                "updated": "2025-08-28T17:35:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    35,
                    3,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:35:03Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    35,
                    3,
                    3,
                    240,
                    0
                ],
                "title": "Reusing Computation in Text-to-Image Diffusion for Efficient Generation\n  of Image Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reusing Computation in Text-to-Image Diffusion for Efficient Generation\n  of Image Sets"
                },
                "summary": "Text-to-image diffusion models enable high-quality image generation but are\ncomputationally expensive. While prior work optimizes per-inference efficiency,\nwe explore an orthogonal approach: reducing redundancy across correlated\nprompts. Our method leverages the coarse-to-fine nature of diffusion models,\nwhere early denoising steps capture shared structures among similar prompts. We\npropose a training-free approach that clusters prompts based on semantic\nsimilarity and shares computation in early diffusion steps. Experiments show\nthat for models trained conditioned on image embeddings, our approach\nsignificantly reduces compute cost while improving image quality. By leveraging\nUnClip's text-to-image prior, we enhance diffusion step allocation for greater\nefficiency. Our method seamlessly integrates with existing pipelines, scales\nwith prompt sets, and reduces the environmental and financial burden of\nlarge-scale text-to-image generation. Project page:\nhttps://ddecatur.github.io/hierarchical-diffusion/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion models enable high-quality image generation but are\ncomputationally expensive. While prior work optimizes per-inference efficiency,\nwe explore an orthogonal approach: reducing redundancy across correlated\nprompts. Our method leverages the coarse-to-fine nature of diffusion models,\nwhere early denoising steps capture shared structures among similar prompts. We\npropose a training-free approach that clusters prompts based on semantic\nsimilarity and shares computation in early diffusion steps. Experiments show\nthat for models trained conditioned on image embeddings, our approach\nsignificantly reduces compute cost while improving image quality. By leveraging\nUnClip's text-to-image prior, we enhance diffusion step allocation for greater\nefficiency. Our method seamlessly integrates with existing pipelines, scales\nwith prompt sets, and reduces the environmental and financial burden of\nlarge-scale text-to-image generation. Project page:\nhttps://ddecatur.github.io/hierarchical-diffusion/"
                },
                "authors": [
                    {
                        "name": "Dale Decatur"
                    },
                    {
                        "name": "Thibault Groueix"
                    },
                    {
                        "name": "Wang Yifan"
                    },
                    {
                        "name": "Rana Hanocka"
                    },
                    {
                        "name": "Vladimir Kim"
                    },
                    {
                        "name": "Matheus Gadelha"
                    }
                ],
                "author_detail": {
                    "name": "Matheus Gadelha"
                },
                "author": "Matheus Gadelha",
                "arxiv_comment": "ICCV 2025. Project page:\n  https://ddecatur.github.io/hierarchical-diffusion/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19945v2",
                "updated": "2025-08-28T17:30:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    30,
                    11,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-27T15:01:09Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    15,
                    1,
                    9,
                    2,
                    239,
                    0
                ],
                "title": "Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of\n  Local Nash Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of\n  Local Nash Interactions"
                },
                "summary": "We present an inverse dynamic game-based algorithm to learn parametric\nconstraints from a given dataset of local generalized Nash equilibrium\ninteractions between multiple agents. Specifically, we introduce mixed-integer\nlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the\ninteracting agents, which recover constraints consistent with the Nash\nstationarity of the interaction demonstrations. We establish theoretical\nguarantees that our method learns inner approximations of the true safe and\nunsafe sets, as well as limitations of constraint learnability from\ndemonstrations of Nash equilibrium interactions. We also use the interaction\nconstraints recovered by our method to design motion plans that robustly\nsatisfy the underlying constraints. Across simulations and hardware\nexperiments, our methods proved capable of inferring constraints and designing\ninteractive motion plans for various classes of constraints, both convex and\nnon-convex, from interaction demonstrations of agents with nonlinear dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an inverse dynamic game-based algorithm to learn parametric\nconstraints from a given dataset of local generalized Nash equilibrium\ninteractions between multiple agents. Specifically, we introduce mixed-integer\nlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the\ninteracting agents, which recover constraints consistent with the Nash\nstationarity of the interaction demonstrations. We establish theoretical\nguarantees that our method learns inner approximations of the true safe and\nunsafe sets, as well as limitations of constraint learnability from\ndemonstrations of Nash equilibrium interactions. We also use the interaction\nconstraints recovered by our method to design motion plans that robustly\nsatisfy the underlying constraints. Across simulations and hardware\nexperiments, our methods proved capable of inferring constraints and designing\ninteractive motion plans for various classes of constraints, both convex and\nnon-convex, from interaction demonstrations of agents with nonlinear dynamics."
                },
                "authors": [
                    {
                        "name": "Zhouyu Zhang"
                    },
                    {
                        "name": "Chih-Yuan Chiu"
                    },
                    {
                        "name": "Glen Chou"
                    }
                ],
                "author_detail": {
                    "name": "Glen Chou"
                },
                "author": "Glen Chou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19200v2",
                "updated": "2025-08-28T17:29:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    29,
                    36,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-26T17:03:43Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    43,
                    1,
                    238,
                    0
                ],
                "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ramon Llull's Thinking Machine for Automated Ideation"
                },
                "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI."
                },
                "authors": [
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Runlong Zhou"
                    },
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21025v1",
                "updated": "2025-08-28T17:28:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    28,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:28:39Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    28,
                    39,
                    3,
                    240,
                    0
                ],
                "title": "Pivotal inference for linear predictions in stationary processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pivotal inference for linear predictions in stationary processes"
                },
                "summary": "In this paper we develop pivotal inference for the final (FPE) and relative\nfinal prediction error (RFPE) of linear forecasts in stationary processes. Our\napproach is based on a novel self-normalizing technique and avoids the\nestimation of the asymptotic variances of the empirical autocovariances. We\nprovide pivotal confidence intervals for the (R)FPE, develop estimates for the\nminimal order of a linear prediction that is required to obtain a prespecified\nforecasting accuracy and also propose (pivotal) statistical tests for the\nhypotheses that the (R)FPE exceeds a given threshold. Additionally, we provide\nnew (pivotal) inference tools for the partial autocorrelation, which do not\nrequire the assumption of an autoregressive process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we develop pivotal inference for the final (FPE) and relative\nfinal prediction error (RFPE) of linear forecasts in stationary processes. Our\napproach is based on a novel self-normalizing technique and avoids the\nestimation of the asymptotic variances of the empirical autocovariances. We\nprovide pivotal confidence intervals for the (R)FPE, develop estimates for the\nminimal order of a linear prediction that is required to obtain a prespecified\nforecasting accuracy and also propose (pivotal) statistical tests for the\nhypotheses that the (R)FPE exceeds a given threshold. Additionally, we provide\nnew (pivotal) inference tools for the partial autocorrelation, which do not\nrequire the assumption of an autoregressive process."
                },
                "authors": [
                    {
                        "name": "Holger Dette"
                    },
                    {
                        "name": "Sebastian K√ºhnert"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian K√ºhnert"
                },
                "author": "Sebastian K√ºhnert",
                "arxiv_comment": "31, pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M10, 62M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21024v1",
                "updated": "2025-08-28T17:27:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    27,
                    9,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:27:09Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    27,
                    9,
                    3,
                    240,
                    0
                ],
                "title": "An Agile Method for Implementing Retrieval Augmented Generation Tools in\n  Industrial SMEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agile Method for Implementing Retrieval Augmented Generation Tools in\n  Industrial SMEs"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to\nmitigate the limitations of Large Language Models (LLMs), such as\nhallucinations and outdated knowledge. However, deploying RAG-based tools in\nSmall and Medium Enterprises (SMEs) remains a challenge due to their limited\nresources and lack of expertise in natural language processing (NLP). This\npaper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a\nstructured, agile method designed to facilitate the deployment of RAG systems\nin industrial SME contexts. EASI-RAG is based on method engineering principles\nand comprises well-defined roles, activities, and techniques. The method was\nvalidated through a real-world case study in an environmental testing\nlaboratory, where a RAG tool was implemented to answer operators queries using\ndata extracted from operational procedures. The system was deployed in under a\nmonth by a team with no prior RAG experience and was later iteratively improved\nbased on user feedback. Results demonstrate that EASI-RAG supports fast\nimplementation, high user adoption, delivers accurate answers, and enhances the\nreliability of underlying data. This work highlights the potential of RAG\ndeployment in industrial SMEs. Future works include the need for generalization\nacross diverse use cases and further integration with fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to\nmitigate the limitations of Large Language Models (LLMs), such as\nhallucinations and outdated knowledge. However, deploying RAG-based tools in\nSmall and Medium Enterprises (SMEs) remains a challenge due to their limited\nresources and lack of expertise in natural language processing (NLP). This\npaper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a\nstructured, agile method designed to facilitate the deployment of RAG systems\nin industrial SME contexts. EASI-RAG is based on method engineering principles\nand comprises well-defined roles, activities, and techniques. The method was\nvalidated through a real-world case study in an environmental testing\nlaboratory, where a RAG tool was implemented to answer operators queries using\ndata extracted from operational procedures. The system was deployed in under a\nmonth by a team with no prior RAG experience and was later iteratively improved\nbased on user feedback. Results demonstrate that EASI-RAG supports fast\nimplementation, high user adoption, delivers accurate answers, and enhances the\nreliability of underlying data. This work highlights the potential of RAG\ndeployment in industrial SMEs. Future works include the need for generalization\nacross diverse use cases and further integration with fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Mathieu Bourdin"
                    },
                    {
                        "name": "Anas Neumann"
                    },
                    {
                        "name": "Thomas Paviot"
                    },
                    {
                        "name": "Robert Pellerin"
                    },
                    {
                        "name": "Samir Lamouri"
                    }
                ],
                "author_detail": {
                    "name": "Samir Lamouri"
                },
                "author": "Samir Lamouri",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21016v1",
                "updated": "2025-08-28T17:18:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    18,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:18:31Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    18,
                    31,
                    3,
                    240,
                    0
                ],
                "title": "Inference-Time Alignment Control for Diffusion Models with Reinforcement\n  Learning Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Alignment Control for Diffusion Models with Reinforcement\n  Learning Guidance"
                },
                "summary": "Denoising-based generative models, particularly diffusion and flow matching\nalgorithms, have achieved remarkable success. However, aligning their output\ndistributions with complex downstream objectives, such as human preferences,\ncompositional accuracy, or data compressibility, remains challenging. While\nreinforcement learning (RL) fine-tuning methods, inspired by advances in RL\nfrom human feedback (RLHF) for large language models, have been adapted to\nthese generative frameworks, current RL approaches are suboptimal for diffusion\nmodels and offer limited flexibility in controlling alignment strength after\nfine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models\nthrough the lens of stochastic differential equations and implicit reward\nconditioning. We introduce Reinforcement Learning Guidance (RLG), an\ninference-time method that adapts Classifier-Free Guidance (CFG) by combining\nthe outputs of the base and RL fine-tuned models via a geometric average. Our\ntheoretical analysis shows that RLG's guidance scale is mathematically\nequivalent to adjusting the KL-regularization coefficient in standard RL\nobjectives, enabling dynamic control over the alignment-quality trade-off\nwithout further training. Extensive experiments demonstrate that RLG\nconsistently improves the performance of RL fine-tuned models across various\narchitectures, RL algorithms, and downstream tasks, including human\npreferences, compositional control, compressibility, and text rendering.\nFurthermore, RLG supports both interpolation and extrapolation, thereby\noffering unprecedented flexibility in controlling generative alignment. Our\napproach provides a practical and theoretically sound solution for enhancing\nand controlling diffusion model alignment at inference. The source code for RLG\nis publicly available at the Github:\nhttps://github.com/jinluo12345/Reinforcement-learning-guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising-based generative models, particularly diffusion and flow matching\nalgorithms, have achieved remarkable success. However, aligning their output\ndistributions with complex downstream objectives, such as human preferences,\ncompositional accuracy, or data compressibility, remains challenging. While\nreinforcement learning (RL) fine-tuning methods, inspired by advances in RL\nfrom human feedback (RLHF) for large language models, have been adapted to\nthese generative frameworks, current RL approaches are suboptimal for diffusion\nmodels and offer limited flexibility in controlling alignment strength after\nfine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models\nthrough the lens of stochastic differential equations and implicit reward\nconditioning. We introduce Reinforcement Learning Guidance (RLG), an\ninference-time method that adapts Classifier-Free Guidance (CFG) by combining\nthe outputs of the base and RL fine-tuned models via a geometric average. Our\ntheoretical analysis shows that RLG's guidance scale is mathematically\nequivalent to adjusting the KL-regularization coefficient in standard RL\nobjectives, enabling dynamic control over the alignment-quality trade-off\nwithout further training. Extensive experiments demonstrate that RLG\nconsistently improves the performance of RL fine-tuned models across various\narchitectures, RL algorithms, and downstream tasks, including human\npreferences, compositional control, compressibility, and text rendering.\nFurthermore, RLG supports both interpolation and extrapolation, thereby\noffering unprecedented flexibility in controlling generative alignment. Our\napproach provides a practical and theoretically sound solution for enhancing\nand controlling diffusion model alignment at inference. The source code for RLG\nis publicly available at the Github:\nhttps://github.com/jinluo12345/Reinforcement-learning-guidance."
                },
                "authors": [
                    {
                        "name": "Luozhijie Jin"
                    },
                    {
                        "name": "Zijie Qiu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Zijie Diao"
                    },
                    {
                        "name": "Lifeng Qiao"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Alex Lamb"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17009v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17009v3",
                "updated": "2025-08-28T17:13:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    13,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2024-12-22T13:16:28Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    13,
                    16,
                    28,
                    6,
                    357,
                    0
                ],
                "title": "Expert Routing with Synthetic Data for Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert Routing with Synthetic Data for Continual Learning"
                },
                "summary": "In many real-world settings, regulations and economic incentives permit the\nsharing of models but not data across institutional boundaries. In such\nscenarios, practitioners might hope to adapt models to new domains, without\nlosing performance on previous domains (so-called catastrophic forgetting).\nWhile any single model may struggle to achieve this goal, learning an ensemble\nof domain-specific experts offers the potential to adapt more closely to each\nindividual institution. However, a core challenge in this context is\ndetermining which expert to deploy at test time. In this paper, we propose\nGenerate to Discriminate (G2D), a domain-incremental continual learning method\nthat leverages synthetic data to train a domain-discriminator that routes\nsamples at inference time to the appropriate expert. Surprisingly, we find that\nleveraging synthetic data in this capacity is more effective than using the\nsamples to \\textit{directly} train the downstream classifier (the more common\napproach to leveraging synthetic data in the lifelong learning literature). We\nobserve that G2D outperforms competitive domain-incremental learning methods on\ntasks in both vision and language modalities, providing a new perspective on\nthe use of synthetic data in the lifelong learning literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many real-world settings, regulations and economic incentives permit the\nsharing of models but not data across institutional boundaries. In such\nscenarios, practitioners might hope to adapt models to new domains, without\nlosing performance on previous domains (so-called catastrophic forgetting).\nWhile any single model may struggle to achieve this goal, learning an ensemble\nof domain-specific experts offers the potential to adapt more closely to each\nindividual institution. However, a core challenge in this context is\ndetermining which expert to deploy at test time. In this paper, we propose\nGenerate to Discriminate (G2D), a domain-incremental continual learning method\nthat leverages synthetic data to train a domain-discriminator that routes\nsamples at inference time to the appropriate expert. Surprisingly, we find that\nleveraging synthetic data in this capacity is more effective than using the\nsamples to \\textit{directly} train the downstream classifier (the more common\napproach to leveraging synthetic data in the lifelong learning literature). We\nobserve that G2D outperforms competitive domain-incremental learning methods on\ntasks in both vision and language modalities, providing a new perspective on\nthe use of synthetic data in the lifelong learning literature."
                },
                "authors": [
                    {
                        "name": "Yewon Byun"
                    },
                    {
                        "name": "Sanket Vaibhav Mehta"
                    },
                    {
                        "name": "Saurabh Garg"
                    },
                    {
                        "name": "Emma Strubell"
                    },
                    {
                        "name": "Michael Oberst"
                    },
                    {
                        "name": "Bryan Wilder"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    }
                ],
                "author_detail": {
                    "name": "Zachary C. Lipton"
                },
                "author": "Zachary C. Lipton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17009v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17009v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21010v1",
                "updated": "2025-08-28T17:10:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    10,
                    53,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:10:53Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    10,
                    53,
                    3,
                    240,
                    0
                ],
                "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate\n  Representations for Improved and Explainable Causal Video Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChainReaction! Structured Approach with Causal Chains as Intermediate\n  Representations for Improved and Explainable Causal Video Question Answering"
                },
                "summary": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle\nwith higher-order reasoning, relying on opaque, monolithic pipelines that\nentangle video understanding, causal inference, and answer generation. These\nblack-box approaches offer limited interpretability and tend to depend on\nshallow heuristics. We propose a novel, modular framework that explicitly\ndecouples causal reasoning from answer generation, introducing natural language\ncausal chains as interpretable intermediate representations. Inspired by human\ncognitive models, these structured cause-effect sequences bridge low-level\nvideo content with high-level causal reasoning, enabling transparent and\nlogically coherent inference. Our two-stage architecture comprises a Causal\nChain Extractor (CCE) that generates causal chains from video-question pairs,\nand a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in\nthese chains. To address the lack of annotated reasoning traces, we introduce a\nscalable method for generating high-quality causal chains from existing\ndatasets using large language models. We also propose CauCo, a new evaluation\nmetric for causality-oriented captioning. Experiments on three large-scale\nbenchmarks demonstrate that our approach not only outperforms state-of-the-art\nmodels, but also yields substantial gains in explainability, user trust, and\ngeneralization -- positioning the CCE as a reusable causal reasoning engine\nacross diverse domains. Project page:\nhttps://paritoshparmar.github.io/chainreaction/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle\nwith higher-order reasoning, relying on opaque, monolithic pipelines that\nentangle video understanding, causal inference, and answer generation. These\nblack-box approaches offer limited interpretability and tend to depend on\nshallow heuristics. We propose a novel, modular framework that explicitly\ndecouples causal reasoning from answer generation, introducing natural language\ncausal chains as interpretable intermediate representations. Inspired by human\ncognitive models, these structured cause-effect sequences bridge low-level\nvideo content with high-level causal reasoning, enabling transparent and\nlogically coherent inference. Our two-stage architecture comprises a Causal\nChain Extractor (CCE) that generates causal chains from video-question pairs,\nand a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in\nthese chains. To address the lack of annotated reasoning traces, we introduce a\nscalable method for generating high-quality causal chains from existing\ndatasets using large language models. We also propose CauCo, a new evaluation\nmetric for causality-oriented captioning. Experiments on three large-scale\nbenchmarks demonstrate that our approach not only outperforms state-of-the-art\nmodels, but also yields substantial gains in explainability, user trust, and\ngeneralization -- positioning the CCE as a reusable causal reasoning engine\nacross diverse domains. Project page:\nhttps://paritoshparmar.github.io/chainreaction/"
                },
                "authors": [
                    {
                        "name": "Paritosh Parmar"
                    },
                    {
                        "name": "Eric Peh"
                    },
                    {
                        "name": "Basura Fernando"
                    }
                ],
                "author_detail": {
                    "name": "Basura Fernando"
                },
                "author": "Basura Fernando",
                "arxiv_comment": "Project page: https://paritoshparmar.github.io/chainreaction/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21007v1",
                "updated": "2025-08-28T17:09:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    9,
                    5,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:09:05Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    9,
                    5,
                    3,
                    240,
                    0
                ],
                "title": "Rapid Mismatch Estimation via Neural Network Informed Variational\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Mismatch Estimation via Neural Network Informed Variational\n  Inference"
                },
                "summary": "With robots increasingly operating in human-centric environments, ensuring\nsoft and safe physical interactions, whether with humans, surroundings, or\nother machines, is essential. While compliant hardware can facilitate such\ninteractions, this work focuses on impedance controllers that allow\ntorque-controlled robots to safely and passively respond to contact while\naccurately executing tasks. From inverse dynamics to quadratic\nprogramming-based controllers, the effectiveness of these methods relies on\naccurate dynamics models of the robot and the object it manipulates. Any model\nmismatch results in task failures and unsafe behaviors. Thus, we introduce\nRapid Mismatch Estimation (RME), an adaptive, controller-agnostic,\nprobabilistic framework that estimates end-effector dynamics mismatches online,\nwithout relying on external force-torque sensors. From the robot's\nproprioceptive feedback, a Neural Network Model Mismatch Estimator generates a\nprior for a Variational Inference solver, which rapidly converges to the\nunknown parameters while quantifying uncertainty. With a real 7-DoF manipulator\ndriven by a state-of-the-art passive impedance controller, RME adapts to sudden\nchanges in mass and center of mass at the end-effector in $\\sim400$ ms, in\nstatic and dynamic settings. We demonstrate RME in a collaborative scenario\nwhere a human attaches an unknown basket to the robot's end-effector and\ndynamically adds/removes heavy items, showcasing fast and safe adaptation to\nchanging dynamics during physical interaction without any external sensory\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With robots increasingly operating in human-centric environments, ensuring\nsoft and safe physical interactions, whether with humans, surroundings, or\nother machines, is essential. While compliant hardware can facilitate such\ninteractions, this work focuses on impedance controllers that allow\ntorque-controlled robots to safely and passively respond to contact while\naccurately executing tasks. From inverse dynamics to quadratic\nprogramming-based controllers, the effectiveness of these methods relies on\naccurate dynamics models of the robot and the object it manipulates. Any model\nmismatch results in task failures and unsafe behaviors. Thus, we introduce\nRapid Mismatch Estimation (RME), an adaptive, controller-agnostic,\nprobabilistic framework that estimates end-effector dynamics mismatches online,\nwithout relying on external force-torque sensors. From the robot's\nproprioceptive feedback, a Neural Network Model Mismatch Estimator generates a\nprior for a Variational Inference solver, which rapidly converges to the\nunknown parameters while quantifying uncertainty. With a real 7-DoF manipulator\ndriven by a state-of-the-art passive impedance controller, RME adapts to sudden\nchanges in mass and center of mass at the end-effector in $\\sim400$ ms, in\nstatic and dynamic settings. We demonstrate RME in a collaborative scenario\nwhere a human attaches an unknown basket to the robot's end-effector and\ndynamically adds/removes heavy items, showcasing fast and safe adaptation to\nchanging dynamics during physical interaction without any external sensory\nsystem."
                },
                "authors": [
                    {
                        "name": "Mateusz Jaszczuk"
                    },
                    {
                        "name": "Nadia Figueroa"
                    }
                ],
                "author_detail": {
                    "name": "Nadia Figueroa"
                },
                "author": "Nadia Figueroa",
                "arxiv_comment": "Accepted at 9th Annual Conference on Robot Learning. Project Website\n  - https://mateusz-jaszczuk.github.io/rme/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21006v1",
                "updated": "2025-08-28T17:08:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    8,
                    11,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:08:11Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    8,
                    11,
                    3,
                    240,
                    0
                ],
                "title": "Practical indistinguishability in a gene regulatory network inference\n  problem, a case study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical indistinguishability in a gene regulatory network inference\n  problem, a case study"
                },
                "summary": "Computationally inferring mechanistic insights from typical biological data\nis a challenging pursuit. Even the highest-quality experimental data come with\nchallenges. There are always sources of noise, a limit to how often we can\nmeasure the system, and we can rarely measure all the relevant states that\nparticipate in the underlying complexity. There are usually sources of\nuncertainty in model development, which give rise to multiple competing model\nstructures. To underscore the need for further analysis of structural\nuncertainty in modeling, we use a meta-analysis across six journals covering\nmathematical biology and show that a huge number of models for biological\nsystems are developed each year, but model selection and comparison across\nmodel structures appear to be less common. We walk through a case study\ninvolving inference of regulatory network structure involved in a developmental\ndecision in the nematode, \\textit{Pristonchus pacificus}. We use real\nbiological data and compare across 13,824 models--each corresponding to a\ndifferent regulatory network structure, to determine which regulatory features\nare supported by the data across three experimental conditions. We find that\nthe best-fitting models for each experimental condition share a combination of\nfeatures and identify a regulatory network that is common across the model sets\nfor each condition. This model can describe the data across the experimental\nconditions we considered and exhibits a high degree of positive regulation and\ninterconnectivity between the key regulators, \\textit{eud-1}, $textit{sult-1},\nand \\textit{nhr-40}. While the biological results are specific to the molecular\nbiology of development in \\textit{Pristonchus pacificus}, the general modeling\nframework and underlying challenges we faced doing this analysis are widespread\nacross biology, chemistry, physics, and many other scientific disciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computationally inferring mechanistic insights from typical biological data\nis a challenging pursuit. Even the highest-quality experimental data come with\nchallenges. There are always sources of noise, a limit to how often we can\nmeasure the system, and we can rarely measure all the relevant states that\nparticipate in the underlying complexity. There are usually sources of\nuncertainty in model development, which give rise to multiple competing model\nstructures. To underscore the need for further analysis of structural\nuncertainty in modeling, we use a meta-analysis across six journals covering\nmathematical biology and show that a huge number of models for biological\nsystems are developed each year, but model selection and comparison across\nmodel structures appear to be less common. We walk through a case study\ninvolving inference of regulatory network structure involved in a developmental\ndecision in the nematode, \\textit{Pristonchus pacificus}. We use real\nbiological data and compare across 13,824 models--each corresponding to a\ndifferent regulatory network structure, to determine which regulatory features\nare supported by the data across three experimental conditions. We find that\nthe best-fitting models for each experimental condition share a combination of\nfeatures and identify a regulatory network that is common across the model sets\nfor each condition. This model can describe the data across the experimental\nconditions we considered and exhibits a high degree of positive regulation and\ninterconnectivity between the key regulators, \\textit{eud-1}, $textit{sult-1},\nand \\textit{nhr-40}. While the biological results are specific to the molecular\nbiology of development in \\textit{Pristonchus pacificus}, the general modeling\nframework and underlying challenges we faced doing this analysis are widespread\nacross biology, chemistry, physics, and many other scientific disciplines."
                },
                "authors": [
                    {
                        "name": "Cody E. FitzGerald"
                    },
                    {
                        "name": "Shelley Reich"
                    },
                    {
                        "name": "Victor Agaba"
                    },
                    {
                        "name": "Arjun Mathur"
                    },
                    {
                        "name": "Michael S. Werner"
                    },
                    {
                        "name": "Niall M. Mangan"
                    }
                ],
                "author_detail": {
                    "name": "Niall M. Mangan"
                },
                "author": "Niall M. Mangan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21004v1",
                "updated": "2025-08-28T17:05:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    5,
                    18,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    5,
                    18,
                    3,
                    240,
                    0
                ],
                "title": "Lethe: Purifying Backdoored Large Language Models with Knowledge\n  Dilution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lethe: Purifying Backdoored Large Language Models with Knowledge\n  Dilution"
                },
                "summary": "Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks.\nHowever, they remain vulnerable to backdoor attacks, where models behave\nnormally for standard queries but generate harmful responses or unintended\noutput when specific triggers are activated. Existing backdoor defenses either\nlack comprehensiveness, focusing on narrow trigger settings, detection-only\nmechanisms, and limited domains, or fail to withstand advanced scenarios like\nmodel-editing-based, multi-trigger, and triggerless attacks. In this paper, we\npresent LETHE, a novel method to eliminate backdoor behaviors from LLMs through\nknowledge dilution using both internal and external mechanisms. Internally,\nLETHE leverages a lightweight dataset to train a clean model, which is then\nmerged with the backdoored model to neutralize malicious behaviors by diluting\nthe backdoor impact within the model's parametric memory. Externally, LETHE\nincorporates benign and semantically relevant evidence into the prompt to\ndistract LLM's attention from backdoor features. Experimental results on\nclassification and generation domains across 5 widely used LLMs demonstrate\nthat LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor\nattacks. LETHE reduces the attack success rate of advanced backdoor attacks by\nup to 98% while maintaining model utility. Furthermore, LETHE has proven to be\ncost-efficient and robust against adaptive backdoor attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks.\nHowever, they remain vulnerable to backdoor attacks, where models behave\nnormally for standard queries but generate harmful responses or unintended\noutput when specific triggers are activated. Existing backdoor defenses either\nlack comprehensiveness, focusing on narrow trigger settings, detection-only\nmechanisms, and limited domains, or fail to withstand advanced scenarios like\nmodel-editing-based, multi-trigger, and triggerless attacks. In this paper, we\npresent LETHE, a novel method to eliminate backdoor behaviors from LLMs through\nknowledge dilution using both internal and external mechanisms. Internally,\nLETHE leverages a lightweight dataset to train a clean model, which is then\nmerged with the backdoored model to neutralize malicious behaviors by diluting\nthe backdoor impact within the model's parametric memory. Externally, LETHE\nincorporates benign and semantically relevant evidence into the prompt to\ndistract LLM's attention from backdoor features. Experimental results on\nclassification and generation domains across 5 widely used LLMs demonstrate\nthat LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor\nattacks. LETHE reduces the attack success rate of advanced backdoor attacks by\nup to 98% while maintaining model utility. Furthermore, LETHE has proven to be\ncost-efficient and robust against adaptive backdoor attacks."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuchen Sun"
                    },
                    {
                        "name": "Jiaxin Gao"
                    },
                    {
                        "name": "Xueluan Gong"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yongsen Zheng"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok-Yan Lam"
                },
                "author": "Kwok-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20996v1",
                "updated": "2025-08-28T16:57:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:57:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic\n  Support in Addiction Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic\n  Support in Addiction Recovery"
                },
                "summary": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation."
                },
                "authors": [
                    {
                        "name": "Junda Wang"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Lingxi Li"
                    },
                    {
                        "name": "Junhui Qian"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15161v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15161v3",
                "updated": "2025-08-28T16:44:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    44,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2024-07-21T13:33:08Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    13,
                    33,
                    8,
                    6,
                    203,
                    0
                ],
                "title": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via\n  Flow Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via\n  Flow Variational Inference"
                },
                "summary": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from\npartial observations remains a critical challenge in robot learning. Prior\ngenerative methods struggle to model the intricate grasp distribution of\ndexterous hands and often fail to reason about shape uncertainty inherent in\npartial point clouds, leading to unreliable or overly conservative grasps. We\npropose FFHFlow, a flow-based variational framework that generates diverse,\nrobust multi-finger grasps while explicitly quantifying perceptual uncertainty\nin the partial point clouds. Our approach leverages a normalizing flow-based\ndeep latent variable model to learn a hierarchical grasp manifold, overcoming\nthe mode collapse and rigid prior limitations of conditional Variational\nAutoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of\nflows, FFHFlow introspects shape uncertainty in partial observations and\nidentifies novel object structures, enabling risk-aware grasp synthesis. To\nfurther enhance reliability, we integrate a discriminative grasp evaluator with\nthe flow likelihoods, formulating an uncertainty-aware ranking strategy that\nprioritizes grasps robust to shape ambiguity. Extensive experiments in\nsimulation and real-world setups demonstrate that FFHFlow outperforms\nstate-of-the-art baselines (including diffusion models) in grasp diversity and\nsuccess rate, while achieving run-time efficient sampling. We also showcase its\npractical value in cluttered and confined environments, where diversity-driven\nsampling excels by mitigating collisions (Project Page:\nhttps://sites.google.com/view/ffhflow/home/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from\npartial observations remains a critical challenge in robot learning. Prior\ngenerative methods struggle to model the intricate grasp distribution of\ndexterous hands and often fail to reason about shape uncertainty inherent in\npartial point clouds, leading to unreliable or overly conservative grasps. We\npropose FFHFlow, a flow-based variational framework that generates diverse,\nrobust multi-finger grasps while explicitly quantifying perceptual uncertainty\nin the partial point clouds. Our approach leverages a normalizing flow-based\ndeep latent variable model to learn a hierarchical grasp manifold, overcoming\nthe mode collapse and rigid prior limitations of conditional Variational\nAutoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of\nflows, FFHFlow introspects shape uncertainty in partial observations and\nidentifies novel object structures, enabling risk-aware grasp synthesis. To\nfurther enhance reliability, we integrate a discriminative grasp evaluator with\nthe flow likelihoods, formulating an uncertainty-aware ranking strategy that\nprioritizes grasps robust to shape ambiguity. Extensive experiments in\nsimulation and real-world setups demonstrate that FFHFlow outperforms\nstate-of-the-art baselines (including diffusion models) in grasp diversity and\nsuccess rate, while achieving run-time efficient sampling. We also showcase its\npractical value in cluttered and confined environments, where diversity-driven\nsampling excels by mitigating collisions (Project Page:\nhttps://sites.google.com/view/ffhflow/home/)."
                },
                "authors": [
                    {
                        "name": "Qian Feng"
                    },
                    {
                        "name": "Jianxiang Feng"
                    },
                    {
                        "name": "Zhaopeng Chen"
                    },
                    {
                        "name": "Rudolph Triebel"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "First two authors contributed equally, whose ordering decided via\n  coin-tossing. Accepted for CoRL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15161v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15161v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22931v2",
                "updated": "2025-08-28T16:42:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    42,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-07-24T13:46:51Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    46,
                    51,
                    3,
                    205,
                    0
                ],
                "title": "Dynamic Context Compression for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Compression for Efficient RAG"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy."
                },
                "authors": [
                    {
                        "name": "Shuyu Guo"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03818v2",
                "updated": "2025-08-28T16:38:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    38,
                    13,
                    3,
                    240,
                    0
                ],
                "published": "2025-05-02T20:03:35Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    20,
                    3,
                    35,
                    4,
                    122,
                    0
                ],
                "title": "Program Semantic Inequivalence Game with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Program Semantic Inequivalence Game with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can achieve strong performance on everyday\ncoding tasks, but they can fail on complex tasks that require non-trivial\nreasoning about program semantics. Finding training examples to teach LLMs to\nsolve these tasks can be challenging.\n  In this work, we explore a method to synthetically generate code reasoning\ntraining data based on a semantic inequivalence game SInQ: a generator agent\ncreates program variants that are semantically distinct, derived from a dataset\nof real-world programming tasks, while an evaluator agent has to identify input\nexamples that cause the original programs and the generated variants to diverge\nin their behaviour, with the agents training each other semi-adversarially. We\nprove that this setup enables theoretically unlimited improvement through\nself-play in the limit of infinite computational resources.\n  We evaluated our approach on multiple code generation and understanding\nbenchmarks, including cross-language vulnerability detection (Lu et al., 2021),\nwhere our method improves vulnerability detection in C/C++ code despite being\ntrained exclusively on Python code, and the challenging Python builtin\nidentifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas\nmodern LLMs still struggle with this benchmark, our approach yields substantial\nimprovements.\n  We release the code needed to replicate the experiments, as well as the\ngenerated synthetic data, which can be used to fine-tune LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can achieve strong performance on everyday\ncoding tasks, but they can fail on complex tasks that require non-trivial\nreasoning about program semantics. Finding training examples to teach LLMs to\nsolve these tasks can be challenging.\n  In this work, we explore a method to synthetically generate code reasoning\ntraining data based on a semantic inequivalence game SInQ: a generator agent\ncreates program variants that are semantically distinct, derived from a dataset\nof real-world programming tasks, while an evaluator agent has to identify input\nexamples that cause the original programs and the generated variants to diverge\nin their behaviour, with the agents training each other semi-adversarially. We\nprove that this setup enables theoretically unlimited improvement through\nself-play in the limit of infinite computational resources.\n  We evaluated our approach on multiple code generation and understanding\nbenchmarks, including cross-language vulnerability detection (Lu et al., 2021),\nwhere our method improves vulnerability detection in C/C++ code despite being\ntrained exclusively on Python code, and the challenging Python builtin\nidentifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas\nmodern LLMs still struggle with this benchmark, our approach yields substantial\nimprovements.\n  We release the code needed to replicate the experiments, as well as the\ngenerated synthetic data, which can be used to fine-tune LLMs."
                },
                "authors": [
                    {
                        "name": "Antonio Valerio Miceli-Barone"
                    },
                    {
                        "name": "Vaishak Belle"
                    },
                    {
                        "name": "Ali Payani"
                    }
                ],
                "author_detail": {
                    "name": "Ali Payani"
                },
                "author": "Ali Payani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20978v1",
                "updated": "2025-08-28T16:33:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    33,
                    27,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:33:27Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    33,
                    27,
                    3,
                    240,
                    0
                ],
                "title": "Efficient Neuro-Symbolic Learning of Constraints and Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Neuro-Symbolic Learning of Constraints and Objective"
                },
                "summary": "In the ongoing quest for hybridizing discrete reasoning with neural nets,\nthere is an increasing interest in neural architectures that can learn how to\nsolve discrete reasoning or optimization problems from natural inputs, a task\nthat Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a\nloss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints\nand the objective, thus delivering a complete model that can be scrutinized and\ncompleted with side constraints. By pushing the combinatorial solver out of the\ntraining loop, our architecture also offers scalable training while exact\ninference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve\nNP-hard reasoning problems from natural inputs. On three variants of the Sudoku\nbenchmark -- symbolic, visual, and many-solution --, our approach requires a\nfraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut\ntask, it optimizes the regret better than a Decision-Focused-Learning\nregret-dedicated loss. Finally, it efficiently learns the energy optimization\nformulation of the large real-world problem of designing proteins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the ongoing quest for hybridizing discrete reasoning with neural nets,\nthere is an increasing interest in neural architectures that can learn how to\nsolve discrete reasoning or optimization problems from natural inputs, a task\nthat Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a\nloss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints\nand the objective, thus delivering a complete model that can be scrutinized and\ncompleted with side constraints. By pushing the combinatorial solver out of the\ntraining loop, our architecture also offers scalable training while exact\ninference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve\nNP-hard reasoning problems from natural inputs. On three variants of the Sudoku\nbenchmark -- symbolic, visual, and many-solution --, our approach requires a\nfraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut\ntask, it optimizes the regret better than a Decision-Focused-Learning\nregret-dedicated loss. Finally, it efficiently learns the energy optimization\nformulation of the large real-world problem of designing proteins."
                },
                "authors": [
                    {
                        "name": "Marianne Defresne"
                    },
                    {
                        "name": "Romain Gambardella"
                    },
                    {
                        "name": "Sophie Barbe"
                    },
                    {
                        "name": "Thomas Schiex"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Schiex"
                },
                "author": "Thomas Schiex",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20977v2",
                "updated": "2025-08-29T01:33:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    1,
                    33,
                    26,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T16:31:08Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    31,
                    8,
                    3,
                    240,
                    0
                ],
                "title": "ConfLogger: Enhance Systems' Configuration Diagnosability through\n  Configuration Logging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfLogger: Enhance Systems' Configuration Diagnosability through\n  Configuration Logging"
                },
                "summary": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%."
                },
                "authors": [
                    {
                        "name": "Shiwen Shan"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Yuxin Su"
                    },
                    {
                        "name": "Zhining Wang"
                    },
                    {
                        "name": "Dan Li"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764570",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764570",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 6 figures, accepted by ICSE '26 (The 48th IEEE/ACM\n  International Conference on Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20973v1",
                "updated": "2025-08-28T16:26:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    26,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:26:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    26,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue\n  Agents"
                },
                "summary": "Proactive dialogue has emerged as a critical and challenging research problem\nin advancing large language models (LLMs). Existing works predominantly focus\non domain-specific or task-oriented scenarios, which leads to fragmented\nevaluations and limits the comprehensive exploration of models' proactive\nconversation abilities. In this work, we propose ProactiveEval, a unified\nframework designed for evaluating proactive dialogue capabilities of LLMs. This\nframework decomposes proactive dialogue into target planning and dialogue\nguidance, establishing evaluation metrics across various domains. Moreover, it\nalso enables the automatic generation of diverse and challenging evaluation\ndata. Based on the proposed framework, we develop 328 evaluation environments\nspanning 6 distinct domains. Through experiments with 22 different types of\nLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional\nperformance on target planning and dialogue guidance tasks, respectively.\nFinally, we investigate how reasoning capabilities influence proactive\nbehaviors and discuss their implications for future model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive dialogue has emerged as a critical and challenging research problem\nin advancing large language models (LLMs). Existing works predominantly focus\non domain-specific or task-oriented scenarios, which leads to fragmented\nevaluations and limits the comprehensive exploration of models' proactive\nconversation abilities. In this work, we propose ProactiveEval, a unified\nframework designed for evaluating proactive dialogue capabilities of LLMs. This\nframework decomposes proactive dialogue into target planning and dialogue\nguidance, establishing evaluation metrics across various domains. Moreover, it\nalso enables the automatic generation of diverse and challenging evaluation\ndata. Based on the proposed framework, we develop 328 evaluation environments\nspanning 6 distinct domains. Through experiments with 22 different types of\nLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional\nperformance on target planning and dialogue guidance tasks, respectively.\nFinally, we investigate how reasoning capabilities influence proactive\nbehaviors and discuss their implications for future model development."
                },
                "authors": [
                    {
                        "name": "Tianjian Liu"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Jiajian Guo"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "arxiv_comment": "21 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20971v1",
                "updated": "2025-08-28T16:25:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    25,
                    22,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:25:22Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    25,
                    22,
                    3,
                    240,
                    0
                ],
                "title": "Cosmo-Learn: code for learning cosmology using different methods and\n  mock data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmo-Learn: code for learning cosmology using different methods and\n  mock data"
                },
                "summary": "We present cosmo_learn, an open-source python-based software package designed\nto simulate cosmological data and perform data-driven inference using a range\nof modern statistical and machine learning techniques. Motivated by the growing\ncomplexity of cosmological models and the emergence of observational tensions,\ncosmo_learn provides a standardized and flexible framework for benchmarking\ncosmological inference methods. The package supports realistic noise modeling\nfor key observables in the late Universe, including cosmic chronometers,\nsupernovae Ia, baryon acoustic oscillations, redshift space distortions, and\ngravitational wave bright sirens. We demonstrate the internal consistency of\nthe simulated data with the input cosmology via residuals and parameter\nrecovery using a fiducial $w$CDM model. Built-in learning and inference modules\ninclude traditional Markov Chain Monte Carlo, as well as more recent approaches\nsuch as genetic algorithms, Gaussian processes, Bayesian ridge regression, and\nartificial neural networks. These methods are implemented in a modular and\nextensible architecture designed to facilitate comparisons across inference\nstrategies in a common pipeline. By providing a flexible and transparent\nsimulation and learning environment, cosmo_learn supports both educational and\nresearch efforts at the intersection of cosmology, statistics, and machine\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present cosmo_learn, an open-source python-based software package designed\nto simulate cosmological data and perform data-driven inference using a range\nof modern statistical and machine learning techniques. Motivated by the growing\ncomplexity of cosmological models and the emergence of observational tensions,\ncosmo_learn provides a standardized and flexible framework for benchmarking\ncosmological inference methods. The package supports realistic noise modeling\nfor key observables in the late Universe, including cosmic chronometers,\nsupernovae Ia, baryon acoustic oscillations, redshift space distortions, and\ngravitational wave bright sirens. We demonstrate the internal consistency of\nthe simulated data with the input cosmology via residuals and parameter\nrecovery using a fiducial $w$CDM model. Built-in learning and inference modules\ninclude traditional Markov Chain Monte Carlo, as well as more recent approaches\nsuch as genetic algorithms, Gaussian processes, Bayesian ridge regression, and\nartificial neural networks. These methods are implemented in a modular and\nextensible architecture designed to facilitate comparisons across inference\nstrategies in a common pipeline. By providing a flexible and transparent\nsimulation and learning environment, cosmo_learn supports both educational and\nresearch efforts at the intersection of cosmology, statistics, and machine\nlearning."
                },
                "authors": [
                    {
                        "name": "Reginald Christian Bernardo"
                    },
                    {
                        "name": "Daniela Grand√≥n"
                    },
                    {
                        "name": "Jackson Levi Said"
                    },
                    {
                        "name": "V√≠ctor H. C√°rdenas"
                    },
                    {
                        "name": "Gene Carlo Belinario"
                    },
                    {
                        "name": "Reinabelle Reyes"
                    }
                ],
                "author_detail": {
                    "name": "Reinabelle Reyes"
                },
                "author": "Reinabelle Reyes",
                "arxiv_comment": "25 pages + refs, 10 figures, comments welcome, our code at\n  https://github.com/reggiebernardo/cosmo_learn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20965v1",
                "updated": "2025-08-28T16:22:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    22,
                    54,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:22:54Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    22,
                    54,
                    3,
                    240,
                    0
                ],
                "title": "DrivingGaussian++: Towards Realistic Reconstruction and Editable\n  Simulation for Surrounding Dynamic Driving Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrivingGaussian++: Towards Realistic Reconstruction and Editable\n  Simulation for Surrounding Dynamic Driving Scenes"
                },
                "summary": "We present DrivingGaussian++, an efficient and effective framework for\nrealistic reconstructing and controllable editing of surrounding dynamic\nautonomous driving scenes. DrivingGaussian++ models the static background using\nincremental 3D Gaussians and reconstructs moving objects with a composite\ndynamic Gaussian graph, ensuring accurate positions and occlusions. By\nintegrating a LiDAR prior, it achieves detailed and consistent scene\nreconstruction, outperforming existing methods in dynamic scene reconstruction\nand photorealistic surround-view synthesis. DrivingGaussian++ supports\ntraining-free controllable editing for dynamic driving scenes, including\ntexture modification, weather simulation, and object manipulation, leveraging\nmulti-view images and depth priors. By integrating large language models (LLMs)\nand controllable editing, our method can automatically generate dynamic object\nmotion trajectories and enhance their realism during the optimization process.\nDrivingGaussian++ demonstrates consistent and realistic editing results and\ngenerates dynamic multi-view driving scenarios, while significantly enhancing\nscene diversity. More results and code can be found at the project site:\nhttps://xiong-creator.github.io/DrivingGaussian_plus.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DrivingGaussian++, an efficient and effective framework for\nrealistic reconstructing and controllable editing of surrounding dynamic\nautonomous driving scenes. DrivingGaussian++ models the static background using\nincremental 3D Gaussians and reconstructs moving objects with a composite\ndynamic Gaussian graph, ensuring accurate positions and occlusions. By\nintegrating a LiDAR prior, it achieves detailed and consistent scene\nreconstruction, outperforming existing methods in dynamic scene reconstruction\nand photorealistic surround-view synthesis. DrivingGaussian++ supports\ntraining-free controllable editing for dynamic driving scenes, including\ntexture modification, weather simulation, and object manipulation, leveraging\nmulti-view images and depth priors. By integrating large language models (LLMs)\nand controllable editing, our method can automatically generate dynamic object\nmotion trajectories and enhance their realism during the optimization process.\nDrivingGaussian++ demonstrates consistent and realistic editing results and\ngenerates dynamic multi-view driving scenarios, while significantly enhancing\nscene diversity. More results and code can be found at the project site:\nhttps://xiong-creator.github.io/DrivingGaussian_plus.github.io"
                },
                "authors": [
                    {
                        "name": "Yajiao Xiong"
                    },
                    {
                        "name": "Xiaoyu Zhou"
                    },
                    {
                        "name": "Yongtao Wan"
                    },
                    {
                        "name": "Deqing Sun"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07612v2",
                "updated": "2025-08-28T16:22:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    22,
                    13,
                    3,
                    240,
                    0
                ],
                "published": "2025-04-10T10:03:29Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    3,
                    29,
                    3,
                    100,
                    0
                ],
                "title": "SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline\n  Dataset"
                },
                "summary": "The primary goal of a news headline is to summarize an event in as few words\nas possible. Depending on the media outlet, a headline can serve as a means to\nobjectively deliver a summary or improve its visibility. For the latter,\nspecific publications may employ stylistic approaches that incorporate the use\nof sarcasm, irony, and exaggeration, key elements of a satirical approach. As\nsuch, even the headline must reflect the tone of the satirical main content.\nCurrent approaches for the Romanian language tend to detect the\nnon-conventional tone (i.e., satire and clickbait) of the news content by\ncombining both the main article and the headline. Because we consider a\nheadline to be merely a brief summary of the main article, we investigate in\nthis paper the presence of satirical tone in headlines alone, testing multiple\nbaselines ranging from standard machine learning algorithms to deep learning\nmodels. Our experiments show that Bidirectional Transformer models outperform\nboth standard machine-learning approaches and Large Language Models (LLMs),\nparticularly when the meta-learning Reptile approach is employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary goal of a news headline is to summarize an event in as few words\nas possible. Depending on the media outlet, a headline can serve as a means to\nobjectively deliver a summary or improve its visibility. For the latter,\nspecific publications may employ stylistic approaches that incorporate the use\nof sarcasm, irony, and exaggeration, key elements of a satirical approach. As\nsuch, even the headline must reflect the tone of the satirical main content.\nCurrent approaches for the Romanian language tend to detect the\nnon-conventional tone (i.e., satire and clickbait) of the news content by\ncombining both the main article and the headline. Because we consider a\nheadline to be merely a brief summary of the main article, we investigate in\nthis paper the presence of satirical tone in headlines alone, testing multiple\nbaselines ranging from standard machine learning algorithms to deep learning\nmodels. Our experiments show that Bidirectional Transformer models outperform\nboth standard machine-learning approaches and Large Language Models (LLMs),\nparticularly when the meta-learning Reptile approach is employed."
                },
                "authors": [
                    {
                        "name": "Mihnea-Alexandru V√Ærlan"
                    },
                    {
                        "name": "RƒÉzvan-Alexandru SmƒÉdu"
                    },
                    {
                        "name": "Dumitru-Clementin Cercel"
                    },
                    {
                        "name": "Florin Pop"
                    },
                    {
                        "name": "Mihaela-Claudia Cercel"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela-Claudia Cercel"
                },
                "author": "Mihaela-Claudia Cercel",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20961v1",
                "updated": "2025-08-28T16:19:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    19,
                    58,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:19:58Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    19,
                    58,
                    3,
                    240,
                    0
                ],
                "title": "Distinct weak asymmetric interactions shape human brain functions as\n  probability fluxes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinct weak asymmetric interactions shape human brain functions as\n  probability fluxes"
                },
                "summary": "The functional computation of the human brain arises from the collective\nbehaviour of the underlying neural network. The emerging technology enables the\nrecording of population activity in neurons, and the theory of neural networks\nis expected to explain and extract functional computations from the data.\nThermodynamically, a large proportion of the whole-body energy is consumed by\nthe brain, and functional computation of the human brain seems to involve high\nenergy consumption. The human brain, however, does not increase its energy\nconsumption with its function, and most of its energy consumption is not\ninvolved in specific brain function: how can the human brain perform its wide\nrepertoire of functional computations without drastically changing its energy\nconsumption? Here, we present a mechanism to perform functional computation by\nsubtle modification of the interaction network among the brain regions. We\nfirst show that, by analyzing the data of spontaneous and task-induced\nwhole-cerebral-cortex activity, the probability fluxes, which are the\nmicroscopic irreversible measure of state transitions, exhibit unique patterns\ndepending on the task being performed, indicating that the human brain function\nis a distinct sequence of the brain state transitions. We then fit the\nparameters of Ising spin systems with asymmetric interactions, where we reveal\nthat the symmetric interactions among the brain regions are strong and\ntask-independent, but the antisymmetric interactions are subtle and\ntask-dependent, and the inferred model reproduces most of the observed\nprobability flux patterns. Our results indicate that the human brain performs\nits functional computation by subtly modifying the antisymmetric interaction\namong the brain regions, which might be possible with a small amount of energy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The functional computation of the human brain arises from the collective\nbehaviour of the underlying neural network. The emerging technology enables the\nrecording of population activity in neurons, and the theory of neural networks\nis expected to explain and extract functional computations from the data.\nThermodynamically, a large proportion of the whole-body energy is consumed by\nthe brain, and functional computation of the human brain seems to involve high\nenergy consumption. The human brain, however, does not increase its energy\nconsumption with its function, and most of its energy consumption is not\ninvolved in specific brain function: how can the human brain perform its wide\nrepertoire of functional computations without drastically changing its energy\nconsumption? Here, we present a mechanism to perform functional computation by\nsubtle modification of the interaction network among the brain regions. We\nfirst show that, by analyzing the data of spontaneous and task-induced\nwhole-cerebral-cortex activity, the probability fluxes, which are the\nmicroscopic irreversible measure of state transitions, exhibit unique patterns\ndepending on the task being performed, indicating that the human brain function\nis a distinct sequence of the brain state transitions. We then fit the\nparameters of Ising spin systems with asymmetric interactions, where we reveal\nthat the symmetric interactions among the brain regions are strong and\ntask-independent, but the antisymmetric interactions are subtle and\ntask-dependent, and the inferred model reproduces most of the observed\nprobability flux patterns. Our results indicate that the human brain performs\nits functional computation by subtly modifying the antisymmetric interaction\namong the brain regions, which might be possible with a small amount of energy."
                },
                "authors": [
                    {
                        "name": "Yoshiaki Horiike"
                    },
                    {
                        "name": "Shin Fujishiro"
                    }
                ],
                "author_detail": {
                    "name": "Shin Fujishiro"
                },
                "author": "Shin Fujishiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20947v1",
                "updated": "2025-08-28T16:09:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    9,
                    8,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:09:08Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    9,
                    8,
                    3,
                    240,
                    0
                ],
                "title": "Nonparametric Inference for Noise Covariance Kernels in Parabolic SPDEs\n  using Space-Time Infill-Asymptotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Inference for Noise Covariance Kernels in Parabolic SPDEs\n  using Space-Time Infill-Asymptotics"
                },
                "summary": "We develop an asymptotic limit theory for nonparametric estimation of the\nnoise covariance kernel in linear parabolic stochastic partial differential\nequations (SPDEs) with additive colored noise, using space-time infill\nasymptotics. The method employs discretized infinite-dimensional realized\ncovariations and requires only mild regularity assumptions on the kernel to\nensure consistent estimation and asymptotic normality of the estimator. On this\nbasis, we construct omnibus goodness-of-fit tests for the noise covariance that\nare independent of the SPDE's differential operator. Our framework accommodates\na variety of spatial sampling schemes and allows for reliable inference even\nwhen spatial resolution is coarser than temporal resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop an asymptotic limit theory for nonparametric estimation of the\nnoise covariance kernel in linear parabolic stochastic partial differential\nequations (SPDEs) with additive colored noise, using space-time infill\nasymptotics. The method employs discretized infinite-dimensional realized\ncovariations and requires only mild regularity assumptions on the kernel to\nensure consistent estimation and asymptotic normality of the estimator. On this\nbasis, we construct omnibus goodness-of-fit tests for the noise covariance that\nare independent of the SPDE's differential operator. Our framework accommodates\na variety of spatial sampling schemes and allows for reliable inference even\nwhen spatial resolution is coarser than temporal resolution."
                },
                "authors": [
                    {
                        "name": "Andreas Petersson"
                    },
                    {
                        "name": "Dennis Schroers"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Schroers"
                },
                "author": "Dennis Schroers",
                "arxiv_comment": "55 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60H15, 62M20, 62G05, 62G10, 35R60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20944v1",
                "updated": "2025-08-28T16:04:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    4,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:04:39Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    4,
                    39,
                    3,
                    240,
                    0
                ],
                "title": "STARE at the Structure: Steering ICL Exemplar Selection with Structural\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STARE at the Structure: Steering ICL Exemplar Selection with Structural\n  Alignment"
                },
                "summary": "In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to\nperform a wide range of tasks without task-specific fine-tuning. However, the\neffectiveness of ICL heavily depends on the quality of exemplar selection. In\nparticular, for structured prediction tasks such as semantic parsing, existing\nICL selection strategies often overlook structural alignment, leading to\nsuboptimal performance and poor generalization. To address this issue, we\npropose a novel two-stage exemplar selection strategy that achieves a strong\nbalance between efficiency, generalizability, and performance. First, we\nfine-tune a BERT-based retriever using structure-aware supervision, guiding it\nto select exemplars that are both semantically relevant and structurally\naligned. Then, we enhance the retriever with a plug-in module, which amplifies\nsyntactically meaningful information in the hidden representations. This\nplug-in is model-agnostic, requires minimal overhead, and can be seamlessly\nintegrated into existing pipelines. Experiments on four benchmarks spanning\nthree semantic parsing tasks demonstrate that our method consistently\noutperforms existing baselines with multiple recent LLMs as inference-time\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to\nperform a wide range of tasks without task-specific fine-tuning. However, the\neffectiveness of ICL heavily depends on the quality of exemplar selection. In\nparticular, for structured prediction tasks such as semantic parsing, existing\nICL selection strategies often overlook structural alignment, leading to\nsuboptimal performance and poor generalization. To address this issue, we\npropose a novel two-stage exemplar selection strategy that achieves a strong\nbalance between efficiency, generalizability, and performance. First, we\nfine-tune a BERT-based retriever using structure-aware supervision, guiding it\nto select exemplars that are both semantically relevant and structurally\naligned. Then, we enhance the retriever with a plug-in module, which amplifies\nsyntactically meaningful information in the hidden representations. This\nplug-in is model-agnostic, requires minimal overhead, and can be seamlessly\nintegrated into existing pipelines. Experiments on four benchmarks spanning\nthree semantic parsing tasks demonstrate that our method consistently\noutperforms existing baselines with multiple recent LLMs as inference-time\nmodels."
                },
                "authors": [
                    {
                        "name": "Jiaqian Li"
                    },
                    {
                        "name": "Qisheng Hu"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Wenya Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenya Wang"
                },
                "author": "Wenya Wang",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20941v1",
                "updated": "2025-08-28T16:02:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    2,
                    37,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:02:37Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    2,
                    37,
                    3,
                    240,
                    0
                ],
                "title": "AI Reasoning Models for Problem Solving in Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Reasoning Models for Problem Solving in Physics"
                },
                "summary": "Reasoning models are the new generation of Large Language Models (LLMs)\ncapable of complex problem solving. Their reliability in solving introductory\nphysics problems was tested by evaluating a sample of n = 5 solutions generated\nby one such model -- OpenAI's o3-mini -- per each problem from 20 chapters of a\nstandard undergraduate textbook. In total, N = 408 problems were given to the\nmodel and N x n = 2,040 generated solutions examined. The model successfully\nsolved 94% of the problems posed, excelling at the beginning topics in\nmechanics but struggling with the later ones such as waves and thermodynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models are the new generation of Large Language Models (LLMs)\ncapable of complex problem solving. Their reliability in solving introductory\nphysics problems was tested by evaluating a sample of n = 5 solutions generated\nby one such model -- OpenAI's o3-mini -- per each problem from 20 chapters of a\nstandard undergraduate textbook. In total, N = 408 problems were given to the\nmodel and N x n = 2,040 generated solutions examined. The model successfully\nsolved 94% of the problems posed, excelling at the beginning topics in\nmechanics but struggling with the later ones such as waves and thermodynamics."
                },
                "authors": [
                    {
                        "name": "Amir Bralin"
                    },
                    {
                        "name": "N. Sanjay Rebello"
                    }
                ],
                "author_detail": {
                    "name": "N. Sanjay Rebello"
                },
                "author": "N. Sanjay Rebello",
                "arxiv_comment": "6 pages, 1 table; Physics Education Research Conference (PERC) 2025\n  Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20931v1",
                "updated": "2025-08-28T15:57:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:57:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on $œÑ$-bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on $œÑ$-bench"
                },
                "summary": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Venkatesh Mishra"
                    },
                    {
                        "name": "Amir Saeidi"
                    },
                    {
                        "name": "Satyam Raj"
                    },
                    {
                        "name": "Mutsumi Nakamura"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Chitta Baral"
                    }
                ],
                "author_detail": {
                    "name": "Chitta Baral"
                },
                "author": "Chitta Baral",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06312v2",
                "updated": "2025-08-28T15:52:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    52,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-08T13:39:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    39,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha\n  Mining in Quantitative Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha\n  Mining in Quantitative Trading"
                },
                "summary": "Alpha factor mining is a fundamental task in quantitative trading, aimed at\ndiscovering interpretable signals that can predict asset returns beyond\nsystematic market risk. While traditional methods rely on manual formula design\nor heuristic search with machine learning, recent advances have leveraged Large\nLanguage Models (LLMs) for automated factor discovery. However, existing\nLLM-based alpha mining approaches remain limited in terms of automation,\ngenerality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,\nsimple, yet effective and efficient LLM-based framework for fully automated\nformulaic alpha mining. Our method features a dual-chain architecture,\nconsisting of a Factor Generation Chain and a Factor Optimization Chain, which\niteratively generate, evaluate, and refine candidate alpha factors using only\nmarket data, while leveraging backtest feedback and prior optimization\nknowledge. The two chains work synergistically to enable high-quality alpha\ndiscovery without human intervention and offer strong scalability. Extensive\nexperiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha\noutperforms existing baselines across multiple metrics, presenting a promising\ndirection for LLM-driven quantitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha factor mining is a fundamental task in quantitative trading, aimed at\ndiscovering interpretable signals that can predict asset returns beyond\nsystematic market risk. While traditional methods rely on manual formula design\nor heuristic search with machine learning, recent advances have leveraged Large\nLanguage Models (LLMs) for automated factor discovery. However, existing\nLLM-based alpha mining approaches remain limited in terms of automation,\ngenerality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,\nsimple, yet effective and efficient LLM-based framework for fully automated\nformulaic alpha mining. Our method features a dual-chain architecture,\nconsisting of a Factor Generation Chain and a Factor Optimization Chain, which\niteratively generate, evaluate, and refine candidate alpha factors using only\nmarket data, while leveraging backtest feedback and prior optimization\nknowledge. The two chains work synergistically to enable high-quality alpha\ndiscovery without human intervention and offer strong scalability. Extensive\nexperiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha\noutperforms existing baselines across multiple metrics, presenting a promising\ndirection for LLM-driven quantitative research."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Long Liao"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20924v1",
                "updated": "2025-08-28T15:52:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    52,
                    14,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:52:14Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    52,
                    14,
                    3,
                    240,
                    0
                ],
                "title": "Palm distributions of superposed point processes for statistical\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palm distributions of superposed point processes for statistical\n  inference"
                },
                "summary": "Palm distributions play a central role in the study of point processes and\ntheir associated summary statistics. In this paper, we characterize the Palm\ndistributions of the superposition of two independent point processes,\nestablishing a simple mixture representation depending on the point processes'\nPalm distributions and moment measures. We explore two statistical applications\nenabled by our main result. First, we consider minimum contrast estimation for\nsuperposed point processes based on Ripley's $K$ function. Second, we focus on\nthe class of shot-noise Cox processes and obtain a tractable expression for the\nJanossy density which leads to maximum likelihood estimation via a novel\nexpectation-maximization algorithm. Both approaches are validated through\nnumerical simulations. Extensions to the superposition of multiple point\nprocesses, and higher-order Palm distributions, are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palm distributions play a central role in the study of point processes and\ntheir associated summary statistics. In this paper, we characterize the Palm\ndistributions of the superposition of two independent point processes,\nestablishing a simple mixture representation depending on the point processes'\nPalm distributions and moment measures. We explore two statistical applications\nenabled by our main result. First, we consider minimum contrast estimation for\nsuperposed point processes based on Ripley's $K$ function. Second, we focus on\nthe class of shot-noise Cox processes and obtain a tractable expression for the\nJanossy density which leads to maximum likelihood estimation via a novel\nexpectation-maximization algorithm. Both approaches are validated through\nnumerical simulations. Extensions to the superposition of multiple point\nprocesses, and higher-order Palm distributions, are also discussed."
                },
                "authors": [
                    {
                        "name": "Mario Beraha"
                    },
                    {
                        "name": "Federico Camerlenghi"
                    },
                    {
                        "name": "Lorenzo Ghilotti"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Ghilotti"
                },
                "author": "Lorenzo Ghilotti",
                "arxiv_comment": "This submission replaces arXiv:2409.14753",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11017v2",
                "updated": "2025-08-28T15:51:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    51,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-14T18:44:13Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    18,
                    44,
                    13,
                    3,
                    226,
                    0
                ],
                "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics"
                },
                "summary": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs."
                },
                "authors": [
                    {
                        "name": "Carter Blum"
                    },
                    {
                        "name": "Katja Filippova"
                    },
                    {
                        "name": "Ann Yuan"
                    },
                    {
                        "name": "Asma Ghandeharioun"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Jessica Hoffmann"
                    },
                    {
                        "name": "Tal Linzen"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Lucas Dixon"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20922v1",
                "updated": "2025-08-28T15:51:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    51,
                    16,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:51:16Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    51,
                    16,
                    3,
                    240,
                    0
                ],
                "title": "Static Factorisation of Probabilistic Programs With User-Labelled Sample\n  Statements and While Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Factorisation of Probabilistic Programs With User-Labelled Sample\n  Statements and While Loops"
                },
                "summary": "It is commonly known that any Bayesian network can be implemented as a\nprobabilistic program, but the reverse direction is not so clear. In this work,\nwe address the open question to what extent a probabilistic program with\nuser-labelled sample statements and while loops - features found in languages\nlike Gen, Turing, and Pyro - can be represented graphically. To this end, we\nextend existing operational semantics to support these language features. By\ntranslating a program to its control-flow graph, we define a sound static\nanalysis that approximates the dependency structure of the random variables in\nthe program. As a result, we obtain a static factorisation of the implicitly\ndefined program density, which is equivalent to the known Bayesian network\nfactorisation for programs without loops and constant labels, but constitutes a\nnovel graphical representation for programs that define an unbounded number of\nrandom variables via loops or dynamic labels. We further develop a sound\nprogram slicing technique to leverage this structure to statically enable three\nwell-known optimisations for the considered program class: we reduce the\nvariance of gradient estimates in variational inference and we speed up both\nsingle-site Metropolis Hastings and sequential Monte Carlo. These optimisations\nare proven correct and empirically shown to match or outperform existing\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is commonly known that any Bayesian network can be implemented as a\nprobabilistic program, but the reverse direction is not so clear. In this work,\nwe address the open question to what extent a probabilistic program with\nuser-labelled sample statements and while loops - features found in languages\nlike Gen, Turing, and Pyro - can be represented graphically. To this end, we\nextend existing operational semantics to support these language features. By\ntranslating a program to its control-flow graph, we define a sound static\nanalysis that approximates the dependency structure of the random variables in\nthe program. As a result, we obtain a static factorisation of the implicitly\ndefined program density, which is equivalent to the known Bayesian network\nfactorisation for programs without loops and constant labels, but constitutes a\nnovel graphical representation for programs that define an unbounded number of\nrandom variables via loops or dynamic labels. We further develop a sound\nprogram slicing technique to leverage this structure to statically enable three\nwell-known optimisations for the considered program class: we reduce the\nvariance of gradient estimates in variational inference and we speed up both\nsingle-site Metropolis Hastings and sequential Monte Carlo. These optimisations\nare proven correct and empirically shown to match or outperform existing\ntechniques."
                },
                "authors": [
                    {
                        "name": "Markus B√∂ck"
                    },
                    {
                        "name": "J√ºrgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "J√ºrgen Cito"
                },
                "author": "J√ºrgen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20916v1",
                "updated": "2025-08-28T15:47:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    47,
                    37,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:47:37Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    47,
                    37,
                    3,
                    240,
                    0
                ],
                "title": "SageLM: A Multi-aspect and Explainable Large Language Model for Speech\n  Judgement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SageLM: A Multi-aspect and Explainable Large Language Model for Speech\n  Judgement"
                },
                "summary": "Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to\nnatural human-computer interaction, enabling end-to-end spoken dialogue\nsystems. However, evaluating these models remains a fundamental challenge. We\npropose \\texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech\nLLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches\nthat disregard acoustic features, SageLM jointly assesses both semantic and\nacoustic dimensions. Second, it leverages rationale-based supervision to\nenhance explainability and guide model learning, achieving superior alignment\nwith evaluation outcomes compared to rule-based reinforcement learning methods.\nThird, we introduce \\textit{SpeechFeedback}, a synthetic preference dataset,\nand employ a two-stage training paradigm to mitigate the scarcity of speech\npreference data. Trained on both semantic and acoustic dimensions, SageLM\nachieves an 82.79\\% agreement rate with human evaluators, outperforming\ncascaded and SLM-based baselines by at least 7.42\\% and 26.20\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to\nnatural human-computer interaction, enabling end-to-end spoken dialogue\nsystems. However, evaluating these models remains a fundamental challenge. We\npropose \\texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech\nLLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches\nthat disregard acoustic features, SageLM jointly assesses both semantic and\nacoustic dimensions. Second, it leverages rationale-based supervision to\nenhance explainability and guide model learning, achieving superior alignment\nwith evaluation outcomes compared to rule-based reinforcement learning methods.\nThird, we introduce \\textit{SpeechFeedback}, a synthetic preference dataset,\nand employ a two-stage training paradigm to mitigate the scarcity of speech\npreference data. Trained on both semantic and acoustic dimensions, SageLM\nachieves an 82.79\\% agreement rate with human evaluators, outperforming\ncascaded and SLM-based baselines by at least 7.42\\% and 26.20\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Yuan Ge"
                    },
                    {
                        "name": "Junxiang Zhang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Xiangnan Ma"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Kaiyang Ye"
                    },
                    {
                        "name": "Yangfan Du"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "JingBo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "JingBo Zhu"
                },
                "author": "JingBo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20912v1",
                "updated": "2025-08-28T15:41:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    41,
                    49,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:41:49Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    41,
                    49,
                    3,
                    240,
                    0
                ],
                "title": "Research Challenges in Relational Database Management Systems for LLM\n  Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research Challenges in Relational Database Management Systems for LLM\n  Queries"
                },
                "summary": "Large language models (LLMs) have become essential for applications such as\ntext summarization, sentiment analysis, and automated question-answering.\nRecently, LLMs have also been integrated into relational database management\nsystems to enhance querying and support advanced data processing. Companies\nsuch as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly\nwithin SQL, denoted as LLM queries, to boost data insights. However,\nopen-source solutions currently have limited functionality and poor\nperformance. In this work, we present an early exploration of two open-source\nsystems and one enterprise platform, using five representative queries to\nexpose functional, performance, and scalability limits in today's SQL-invoked\nLLM integrations. We identify three main issues: enforcing structured outputs,\noptimizing resource utilization, and improving query planning. We implemented\ninitial solutions and observed improvements in accommodating LLM powered SQL\nqueries. These early gains demonstrate that tighter integration of LLM+DBMS is\nthe key to scalable and efficient processing of LLM queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become essential for applications such as\ntext summarization, sentiment analysis, and automated question-answering.\nRecently, LLMs have also been integrated into relational database management\nsystems to enhance querying and support advanced data processing. Companies\nsuch as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly\nwithin SQL, denoted as LLM queries, to boost data insights. However,\nopen-source solutions currently have limited functionality and poor\nperformance. In this work, we present an early exploration of two open-source\nsystems and one enterprise platform, using five representative queries to\nexpose functional, performance, and scalability limits in today's SQL-invoked\nLLM integrations. We identify three main issues: enforcing structured outputs,\noptimizing resource utilization, and improving query planning. We implemented\ninitial solutions and observed improvements in accommodating LLM powered SQL\nqueries. These early gains demonstrate that tighter integration of LLM+DBMS is\nthe key to scalable and efficient processing of LLM queries."
                },
                "authors": [
                    {
                        "name": "Kerem Akillioglu"
                    },
                    {
                        "name": "Anurag Chakraborty"
                    },
                    {
                        "name": "Sairaj Voruganti"
                    },
                    {
                        "name": "M. Tamer √ñzsu"
                    }
                ],
                "author_detail": {
                    "name": "M. Tamer √ñzsu"
                },
                "author": "M. Tamer √ñzsu",
                "arxiv_comment": "This paper will appear in the 6th International Workshop on Applied\n  AI for Database Systems and Applications, AIDB Workshop at VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20907v1",
                "updated": "2025-08-28T15:37:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    37,
                    40,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:37:40Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    37,
                    40,
                    3,
                    240,
                    0
                ],
                "title": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant"
                },
                "summary": "Qiskit is an open-source quantum computing framework that allows users to\ndesign, simulate, and run quantum circuits on real quantum hardware. We explore\npost-training techniques for LLMs to assist in writing Qiskit code. We\nintroduce quantum verification as an effective method for ensuring code quality\nand executability on quantum hardware. To support this, we developed a\nsynthetic data pipeline that generates quantum problem-unit test pairs and used\nit to create preference data for aligning LLMs with DPO. Additionally, we\ntrained models using GRPO, leveraging quantum-verifiable rewards provided by\nthe quantum hardware. Our best-performing model, combining DPO and GRPO,\nsurpasses the strongest open-source baselines on the challenging\nQiskit-HumanEval-hard benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qiskit is an open-source quantum computing framework that allows users to\ndesign, simulate, and run quantum circuits on real quantum hardware. We explore\npost-training techniques for LLMs to assist in writing Qiskit code. We\nintroduce quantum verification as an effective method for ensuring code quality\nand executability on quantum hardware. To support this, we developed a\nsynthetic data pipeline that generates quantum problem-unit test pairs and used\nit to create preference data for aligning LLMs with DPO. Additionally, we\ntrained models using GRPO, leveraging quantum-verifiable rewards provided by\nthe quantum hardware. Our best-performing model, combining DPO and GRPO,\nsurpasses the strongest open-source baselines on the challenging\nQiskit-HumanEval-hard benchmark."
                },
                "authors": [
                    {
                        "name": "Nicolas Dupuis"
                    },
                    {
                        "name": "Adarsh Tiwari"
                    },
                    {
                        "name": "Youssef Mroueh"
                    },
                    {
                        "name": "David Kremer"
                    },
                    {
                        "name": "Ismael Faro"
                    },
                    {
                        "name": "Juan Cruz-Benito"
                    }
                ],
                "author_detail": {
                    "name": "Juan Cruz-Benito"
                },
                "author": "Juan Cruz-Benito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01228v2",
                "updated": "2025-08-28T15:37:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    37,
                    0,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-02T06:46:37Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    46,
                    37,
                    5,
                    214,
                    0
                ],
                "title": "Inferring processes within dynamic forest models using hybrid modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring processes within dynamic forest models using hybrid modeling"
                },
                "summary": "Modeling forest dynamics under novel climatic conditions requires a careful\nbalance between process-based understanding and empirical flexibility. Dynamic\nVegetation Models (DVM) represent ecological processes mechanistically, but\ntheir performance is prone to misspecified assumptions about functional forms.\nInferring the structure of these processes and their functional forms correctly\nfrom data remains a major challenge because current approaches, such as plug-in\nestimators, have proven ineffective. We introduce Forest Informed Neural\nNetworks (FINN), a hybrid modeling approach that combines a forest gap model\nwith deep neural networks (DNN). FINN replaces processes with DNNs, which are\nthen calibrated alongside the other mechanistic components in one unified step.\nIn a case study on the Barro Colorado Island 50-ha plot we demonstrate that\nreplacing the growth process with a DNN improves predictive performance and\nsuccession trajectories compared to a mechanistic version of FINN. Furthermore,\nwe discovered that the DNN learned an ecologically plausible, improved\nfunctional form of the growth process, which we extracted from the DNN using\nexplainable AI. In conclusion, our new hybrid modeling approach offers a\nversatile opportunity to infer forest dynamics from data and to improve\nforecasts of ecosystem trajectories under unprecedented environmental change.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling forest dynamics under novel climatic conditions requires a careful\nbalance between process-based understanding and empirical flexibility. Dynamic\nVegetation Models (DVM) represent ecological processes mechanistically, but\ntheir performance is prone to misspecified assumptions about functional forms.\nInferring the structure of these processes and their functional forms correctly\nfrom data remains a major challenge because current approaches, such as plug-in\nestimators, have proven ineffective. We introduce Forest Informed Neural\nNetworks (FINN), a hybrid modeling approach that combines a forest gap model\nwith deep neural networks (DNN). FINN replaces processes with DNNs, which are\nthen calibrated alongside the other mechanistic components in one unified step.\nIn a case study on the Barro Colorado Island 50-ha plot we demonstrate that\nreplacing the growth process with a DNN improves predictive performance and\nsuccession trajectories compared to a mechanistic version of FINN. Furthermore,\nwe discovered that the DNN learned an ecologically plausible, improved\nfunctional form of the growth process, which we extracted from the DNN using\nexplainable AI. In conclusion, our new hybrid modeling approach offers a\nversatile opportunity to infer forest dynamics from data and to improve\nforecasts of ecosystem trajectories under unprecedented environmental change."
                },
                "authors": [
                    {
                        "name": "Maximilian Pichler"
                    },
                    {
                        "name": "Yannek K√§ber"
                    }
                ],
                "author_detail": {
                    "name": "Yannek K√§ber"
                },
                "author": "Yannek K√§ber",
                "arxiv_comment": "29 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20904v1",
                "updated": "2025-08-28T15:36:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    36,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:36:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    36,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Clustering of DESI galaxies split by thermal Sunyaev-Zeldovich effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering of DESI galaxies split by thermal Sunyaev-Zeldovich effect"
                },
                "summary": "The thermal Sunyaev-Zeldovich (tSZ) effect is associated with galaxy clusters\n- extremely large and dense structures tracing the dark matter with a higher\nbias than isolated galaxies. We propose to use the tSZ data to separate\ngalaxies from redshift surveys into distinct subpopulations corresponding to\ndifferent densities and biases independently of the redshift survey\nsystematics. Leveraging the information from different environments, as in\ndensity-split and density-marked clustering, is known to tighten the\nconstraints on cosmological parameters, like $\\Omega_m$, $\\sigma_8$ and\nneutrino mass. We use data from the Dark Energy Spectroscopic Instrument (DESI)\nand the Atacama Cosmology Telescope (ACT) in their region of overlap to\ndemonstrate informative tSZ splitting of Luminous Red Galaxies (LRGs). We\ndiscover a significant increase in the large-scale clustering of DESI LRGs\ncorresponding to detections starting from 1-2 sigma in the ACT DR6 + Planck tSZ\nCompton-$y$ map, below the cluster candidate threshold (4 sigma). We also find\nthat such galaxies have higher line-of-sight coordinate (and velocity)\ndispersions and a higher number of close neighbors than both the full sample\nand near-zero tSZ regions. We produce simple simulations of tSZ maps that are\nintrinsically consistent with galaxy catalogs and do not include systematic\neffects, and find a similar pattern of large-scale clustering enhancement with\ntSZ effect significance. Moreover, we observe that this relative bias pattern\nremains largely unchanged with variations in the galaxy-halo connection model\nin our simulations. This is promising for future cosmological inference from\ntSZ-split clustering with semi-analytical models. Thus, we demonstrate that\nvaluable cosmological information is present in the lower signal-to-noise\nregions of the thermal Sunyaev-Zeldovich map, extending far beyond the\nindividual cluster candidates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The thermal Sunyaev-Zeldovich (tSZ) effect is associated with galaxy clusters\n- extremely large and dense structures tracing the dark matter with a higher\nbias than isolated galaxies. We propose to use the tSZ data to separate\ngalaxies from redshift surveys into distinct subpopulations corresponding to\ndifferent densities and biases independently of the redshift survey\nsystematics. Leveraging the information from different environments, as in\ndensity-split and density-marked clustering, is known to tighten the\nconstraints on cosmological parameters, like $\\Omega_m$, $\\sigma_8$ and\nneutrino mass. We use data from the Dark Energy Spectroscopic Instrument (DESI)\nand the Atacama Cosmology Telescope (ACT) in their region of overlap to\ndemonstrate informative tSZ splitting of Luminous Red Galaxies (LRGs). We\ndiscover a significant increase in the large-scale clustering of DESI LRGs\ncorresponding to detections starting from 1-2 sigma in the ACT DR6 + Planck tSZ\nCompton-$y$ map, below the cluster candidate threshold (4 sigma). We also find\nthat such galaxies have higher line-of-sight coordinate (and velocity)\ndispersions and a higher number of close neighbors than both the full sample\nand near-zero tSZ regions. We produce simple simulations of tSZ maps that are\nintrinsically consistent with galaxy catalogs and do not include systematic\neffects, and find a similar pattern of large-scale clustering enhancement with\ntSZ effect significance. Moreover, we observe that this relative bias pattern\nremains largely unchanged with variations in the galaxy-halo connection model\nin our simulations. This is promising for future cosmological inference from\ntSZ-split clustering with semi-analytical models. Thus, we demonstrate that\nvaluable cosmological information is present in the lower signal-to-noise\nregions of the thermal Sunyaev-Zeldovich map, extending far beyond the\nindividual cluster candidates."
                },
                "authors": [
                    {
                        "name": "M. Rashkovetskyi"
                    },
                    {
                        "name": "D. J. Eisenstein"
                    },
                    {
                        "name": "J. Aguilar"
                    },
                    {
                        "name": "S. Ahlen"
                    },
                    {
                        "name": "A. Anand"
                    },
                    {
                        "name": "D. Bianchi"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "T. Claybaugh"
                    },
                    {
                        "name": "A. Cuceu"
                    },
                    {
                        "name": "K. S. Dawson"
                    },
                    {
                        "name": "A. de la Macorra"
                    },
                    {
                        "name": "Arjun Dey"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "S. Ferraro"
                    },
                    {
                        "name": "A. Font-Ribera"
                    },
                    {
                        "name": "J. E. Forero-Romero"
                    },
                    {
                        "name": "E. Gazta√±aga"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "H. K. Herrera-Alcantar"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "C. Howlett"
                    },
                    {
                        "name": "M. Ishak"
                    },
                    {
                        "name": "R. Joyce"
                    },
                    {
                        "name": "R. Kehoe"
                    },
                    {
                        "name": "T. Kisner"
                    },
                    {
                        "name": "A. Kremin"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "A. Lambert"
                    },
                    {
                        "name": "M. Landriau"
                    },
                    {
                        "name": "M. Manera"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "E. Mueller"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "N. Palanque-Delabrouille"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "F. Prada"
                    },
                    {
                        "name": "I. P√©rez-R√†fols"
                    },
                    {
                        "name": "A. J. Ross"
                    },
                    {
                        "name": "G. Rossi"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "D. Schlegel"
                    },
                    {
                        "name": "M. Schubnell"
                    },
                    {
                        "name": "J. Silber"
                    },
                    {
                        "name": "D. Sprayberry"
                    },
                    {
                        "name": "G. Tarl√©"
                    },
                    {
                        "name": "B. A. Weaver"
                    },
                    {
                        "name": "R. Zhou"
                    },
                    {
                        "name": "H. Zou"
                    }
                ],
                "author_detail": {
                    "name": "H. Zou"
                },
                "author": "H. Zou",
                "arxiv_comment": "15 pages, 12 figures. The data (including all the points from the\n  plots) and source code are available at https://zenodo.org/records/16943122",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17513v2",
                "updated": "2025-08-28T15:33:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    33,
                    2,
                    3,
                    240,
                    0
                ],
                "published": "2025-03-21T19:56:59Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    56,
                    59,
                    4,
                    80,
                    0
                ],
                "title": "Improving Quantization with Post-Training Model Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Quantization with Post-Training Model Expansion"
                },
                "summary": "The size of a model has been a strong predictor of its quality, as well as\nits cost. As such, the trade-off between model cost and quality has been\nwell-studied. Post-training optimizations like quantization and pruning have\ntypically focused on reducing the overall volume of pre-trained models to\nreduce inference costs while maintaining model quality. However, recent\nadvancements have introduced optimization techniques that, interestingly,\nexpand models post-training, increasing model size to improve quality when\nreducing volume. For instance, to enable 4-bit weight and activation\nquantization, incoherence processing often necessitates inserting online\nHadamard rotations in the compute graph, and preserving highly sensitive\nweights often calls for additional higher precision computations. However, if\napplication requirements cannot be met, the prevailing solution is to relax\nquantization constraints. In contrast, we demonstrate post-training model\nexpansion is a viable strategy to improve model quality within a quantization\nco-design space, and provide theoretical justification. We show it is possible\nto progressively and selectively expand the size of a pre-trained large\nlanguage model (LLM) to improve model quality without end-to-end retraining. In\nparticular, when quantizing the weights and activations to 4 bits for Llama3\n1B, we reduce the gap to full-precision perplexity by an average of 9% relative\nto both QuaRot and SpinQuant with only 5% more parameters, which is still a\n3.8% reduction in volume relative to a BF16 reference model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of a model has been a strong predictor of its quality, as well as\nits cost. As such, the trade-off between model cost and quality has been\nwell-studied. Post-training optimizations like quantization and pruning have\ntypically focused on reducing the overall volume of pre-trained models to\nreduce inference costs while maintaining model quality. However, recent\nadvancements have introduced optimization techniques that, interestingly,\nexpand models post-training, increasing model size to improve quality when\nreducing volume. For instance, to enable 4-bit weight and activation\nquantization, incoherence processing often necessitates inserting online\nHadamard rotations in the compute graph, and preserving highly sensitive\nweights often calls for additional higher precision computations. However, if\napplication requirements cannot be met, the prevailing solution is to relax\nquantization constraints. In contrast, we demonstrate post-training model\nexpansion is a viable strategy to improve model quality within a quantization\nco-design space, and provide theoretical justification. We show it is possible\nto progressively and selectively expand the size of a pre-trained large\nlanguage model (LLM) to improve model quality without end-to-end retraining. In\nparticular, when quantizing the weights and activations to 4 bits for Llama3\n1B, we reduce the gap to full-precision perplexity by an average of 9% relative\nto both QuaRot and SpinQuant with only 5% more parameters, which is still a\n3.8% reduction in volume relative to a BF16 reference model."
                },
                "authors": [
                    {
                        "name": "Giuseppe Franco"
                    },
                    {
                        "name": "Pablo Monteagudo-Lago"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Nicholas Fraser"
                    },
                    {
                        "name": "Michaela Blott"
                    }
                ],
                "author_detail": {
                    "name": "Michaela Blott"
                },
                "author": "Michaela Blott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20899v1",
                "updated": "2025-08-28T15:27:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    27,
                    35,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:27:35Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    27,
                    35,
                    3,
                    240,
                    0
                ],
                "title": "Language-Enhanced Mobile Manipulation for Efficient Object Search in\n  Indoor Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Enhanced Mobile Manipulation for Efficient Object Search in\n  Indoor Environments"
                },
                "summary": "Enabling robots to efficiently search for and identify objects in complex,\nunstructured environments is critical for diverse applications ranging from\nhousehold assistance to industrial automation. However, traditional scene\nrepresentations typically capture only static semantics and lack interpretable\ncontextual reasoning, limiting their ability to guide object search in\ncompletely unfamiliar settings. To address this challenge, we propose a\nlanguage-enhanced hierarchical navigation framework that tightly integrates\nsemantic perception and spatial reasoning. Our method, Goal-Oriented\nDynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large\nlanguage models (LLMs) to infer scene semantics and guide the search process\nthrough a multi-level decision hierarchy. Reliability in reasoning is achieved\nthrough the use of structured prompts and logical constraints applied at each\nstage of the hierarchy. For the specific challenges of mobile manipulation, we\nintroduce a heuristic-based motion planner that combines polar angle sorting\nwith distance prioritization to efficiently generate exploration paths.\nComprehensive evaluations in Isaac Sim demonstrate the feasibility of our\nframework, showing that GODHS can locate target objects with higher search\nefficiency compared to conventional, non-semantic search strategies. Website\nand Video are available at: https://drapandiger.github.io/GODHS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling robots to efficiently search for and identify objects in complex,\nunstructured environments is critical for diverse applications ranging from\nhousehold assistance to industrial automation. However, traditional scene\nrepresentations typically capture only static semantics and lack interpretable\ncontextual reasoning, limiting their ability to guide object search in\ncompletely unfamiliar settings. To address this challenge, we propose a\nlanguage-enhanced hierarchical navigation framework that tightly integrates\nsemantic perception and spatial reasoning. Our method, Goal-Oriented\nDynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large\nlanguage models (LLMs) to infer scene semantics and guide the search process\nthrough a multi-level decision hierarchy. Reliability in reasoning is achieved\nthrough the use of structured prompts and logical constraints applied at each\nstage of the hierarchy. For the specific challenges of mobile manipulation, we\nintroduce a heuristic-based motion planner that combines polar angle sorting\nwith distance prioritization to efficiently generate exploration paths.\nComprehensive evaluations in Isaac Sim demonstrate the feasibility of our\nframework, showing that GODHS can locate target objects with higher search\nefficiency compared to conventional, non-semantic search strategies. Website\nand Video are available at: https://drapandiger.github.io/GODHS"
                },
                "authors": [
                    {
                        "name": "Liding Zhang"
                    },
                    {
                        "name": "Zeqi Li"
                    },
                    {
                        "name": "Kuanqi Cai"
                    },
                    {
                        "name": "Qian Huang"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_journal_ref": "2025 IEEE International Conference on Cyborg and Bionic Systems\n  (CBS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01968v2",
                "updated": "2025-08-28T15:23:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    23,
                    4,
                    3,
                    240,
                    0
                ],
                "published": "2025-05-04T02:36:57Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    2,
                    36,
                    57,
                    6,
                    124,
                    0
                ],
                "title": "HAS-GPU: Efficient Hybrid Auto-scaling with Fine-grained GPU Allocation\n  for SLO-aware Serverless Inferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAS-GPU: Efficient Hybrid Auto-scaling with Fine-grained GPU Allocation\n  for SLO-aware Serverless Inferences"
                },
                "summary": "Serverless Computing (FaaS) has become a popular paradigm for deep learning\ninference due to the ease of deployment and pay-per-use benefits. However,\ncurrent serverless inference platforms encounter the coarse-grained and static\nGPU resource allocation problems during scaling, which leads to high costs and\nService Level Objective (SLO) violations in fluctuating workloads. Meanwhile,\ncurrent platforms only support horizontal scaling for GPU inferences, thus the\ncold start problem further exacerbates the problems. In this paper, we propose\nHAS-GPU, an efficient Hybrid Auto-scaling Serverless architecture with\nfine-grained GPU allocation for deep learning inferences. HAS-GPU proposes an\nagile scheduler capable of allocating GPU Streaming Multiprocessor (SM)\npartitions and time quotas with arbitrary granularity and enables significant\nvertical quota scalability at runtime. To resolve performance uncertainty\nintroduced by massive fine-grained resource configuration spaces, we propose\nthe Resource-aware Performance Predictor (RaPP). Furthermore, we present an\nadaptive hybrid auto-scaling algorithm with both horizontal and vertical\nscaling to ensure inference SLOs and minimize GPU costs. The experiments\ndemonstrated that compared to the mainstream serverless inference platform,\nHAS-GPU reduces function costs by an average of 10.8x with better SLO\nguarantees. Compared to state-of-the-art spatio-temporal GPU sharing serverless\nframework, HAS-GPU reduces function SLO violation by 4.8x and cost by 1.72x on\naverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing (FaaS) has become a popular paradigm for deep learning\ninference due to the ease of deployment and pay-per-use benefits. However,\ncurrent serverless inference platforms encounter the coarse-grained and static\nGPU resource allocation problems during scaling, which leads to high costs and\nService Level Objective (SLO) violations in fluctuating workloads. Meanwhile,\ncurrent platforms only support horizontal scaling for GPU inferences, thus the\ncold start problem further exacerbates the problems. In this paper, we propose\nHAS-GPU, an efficient Hybrid Auto-scaling Serverless architecture with\nfine-grained GPU allocation for deep learning inferences. HAS-GPU proposes an\nagile scheduler capable of allocating GPU Streaming Multiprocessor (SM)\npartitions and time quotas with arbitrary granularity and enables significant\nvertical quota scalability at runtime. To resolve performance uncertainty\nintroduced by massive fine-grained resource configuration spaces, we propose\nthe Resource-aware Performance Predictor (RaPP). Furthermore, we present an\nadaptive hybrid auto-scaling algorithm with both horizontal and vertical\nscaling to ensure inference SLOs and minimize GPU costs. The experiments\ndemonstrated that compared to the mainstream serverless inference platform,\nHAS-GPU reduces function costs by an average of 10.8x with better SLO\nguarantees. Compared to state-of-the-art spatio-temporal GPU sharing serverless\nframework, HAS-GPU reduces function SLO violation by 4.8x and cost by 1.72x on\naverage."
                },
                "authors": [
                    {
                        "name": "Jianfeng Gu"
                    },
                    {
                        "name": "Puxuan Wang"
                    },
                    {
                        "name": "Isaac David Nunez Araya"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Michael Gerndt"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gerndt"
                },
                "author": "Michael Gerndt",
                "arxiv_doi": "10.1007/978-3-031-99854-6_11",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99854-6_11",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.01968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The paper has been accepted by Euro-Par 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20893v1",
                "updated": "2025-08-28T15:22:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    22,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:22:31Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    22,
                    31,
                    3,
                    240,
                    0
                ],
                "title": "The Uneven Impact of Post-Training Quantization in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Uneven Impact of Post-Training Quantization in Machine Translation"
                },
                "summary": "Quantization is essential for deploying large language models (LLMs) on\nresource-constrained hardware, but its implications for multilingual tasks\nremain underexplored. We conduct the first large-scale evaluation of\npost-training quantization (PTQ) on machine translation across 55 languages\nusing five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that\nwhile 4-bit quantization often preserves translation quality for high-resource\nlanguages and large models, significant degradation occurs for low-resource and\ntypologically diverse languages, particularly in 2-bit settings. We compare\nfour quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing\nthat algorithm choice and model size jointly determine robustness. GGUF\nvariants provide the most consistent performance, even at 2-bit precision.\nAdditionally, we quantify the interactions between quantization, decoding\nhyperparameters, and calibration languages, finding that language-matched\ncalibration offers benefits primarily in low-bit scenarios. Our findings offer\nactionable insights for deploying multilingual LLMs for machine translation\nunder quantization constraints, especially in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying large language models (LLMs) on\nresource-constrained hardware, but its implications for multilingual tasks\nremain underexplored. We conduct the first large-scale evaluation of\npost-training quantization (PTQ) on machine translation across 55 languages\nusing five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that\nwhile 4-bit quantization often preserves translation quality for high-resource\nlanguages and large models, significant degradation occurs for low-resource and\ntypologically diverse languages, particularly in 2-bit settings. We compare\nfour quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing\nthat algorithm choice and model size jointly determine robustness. GGUF\nvariants provide the most consistent performance, even at 2-bit precision.\nAdditionally, we quantify the interactions between quantization, decoding\nhyperparameters, and calibration languages, finding that language-matched\ncalibration offers benefits primarily in low-bit scenarios. Our findings offer\nactionable insights for deploying multilingual LLMs for machine translation\nunder quantization constraints, especially in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Benjamin Marie"
                    },
                    {
                        "name": "Atsushi Fujita"
                    }
                ],
                "author_detail": {
                    "name": "Atsushi Fujita"
                },
                "author": "Atsushi Fujita",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20890v1",
                "updated": "2025-08-28T15:19:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    19,
                    7,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:19:07Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    19,
                    7,
                    3,
                    240,
                    0
                ],
                "title": "PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications, from virtual assistants to autonomous agents. However, their\nflexibility also introduces new attack vectors-particularly Prompt Injection\n(PI), where adversaries manipulate model behavior through crafted inputs. As\nattackers continuously evolve with paraphrased, obfuscated, and even multi-task\ninjection strategies, existing benchmarks are no longer sufficient to capture\nthe full spectrum of emerging threats.\n  To address this gap, we construct a new benchmark that systematically extends\nprior efforts. Our benchmark subsumes the two widely-used existing ones while\nintroducing new manipulation techniques and multi-task scenarios, thereby\nproviding a more comprehensive evaluation setting. We find that existing\ndefenses, though effective on their original benchmarks, show clear weaknesses\nunder our benchmark, underscoring the need for more robust solutions. Our key\ninsight is that while attack forms may vary, the adversary's intent-injecting\nan unauthorized task-remains invariant. Building on this observation, we\npropose PromptSleuth, a semantic-oriented defense framework that detects prompt\ninjection by reasoning over task-level intent rather than surface features.\nEvaluated across state-of-the-art benchmarks, PromptSleuth consistently\noutperforms existing defense while maintaining comparable runtime and cost\nefficiency. These results demonstrate that intent-based semantic reasoning\noffers a robust, efficient, and generalizable strategy for defending LLMs\nagainst evolving prompt injection threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications, from virtual assistants to autonomous agents. However, their\nflexibility also introduces new attack vectors-particularly Prompt Injection\n(PI), where adversaries manipulate model behavior through crafted inputs. As\nattackers continuously evolve with paraphrased, obfuscated, and even multi-task\ninjection strategies, existing benchmarks are no longer sufficient to capture\nthe full spectrum of emerging threats.\n  To address this gap, we construct a new benchmark that systematically extends\nprior efforts. Our benchmark subsumes the two widely-used existing ones while\nintroducing new manipulation techniques and multi-task scenarios, thereby\nproviding a more comprehensive evaluation setting. We find that existing\ndefenses, though effective on their original benchmarks, show clear weaknesses\nunder our benchmark, underscoring the need for more robust solutions. Our key\ninsight is that while attack forms may vary, the adversary's intent-injecting\nan unauthorized task-remains invariant. Building on this observation, we\npropose PromptSleuth, a semantic-oriented defense framework that detects prompt\ninjection by reasoning over task-level intent rather than surface features.\nEvaluated across state-of-the-art benchmarks, PromptSleuth consistently\noutperforms existing defense while maintaining comparable runtime and cost\nefficiency. These results demonstrate that intent-based semantic reasoning\noffers a robust, efficient, and generalizable strategy for defending LLMs\nagainst evolving prompt injection threats."
                },
                "authors": [
                    {
                        "name": "Mengxiao Wang"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Guofei Gu"
                    }
                ],
                "author_detail": {
                    "name": "Guofei Gu"
                },
                "author": "Guofei Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17232v2",
                "updated": "2025-08-28T15:15:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    15,
                    18,
                    3,
                    240,
                    0
                ],
                "published": "2025-07-23T05:56:20Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    56,
                    20,
                    2,
                    204,
                    0
                ],
                "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for\n  State Probing Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Clean Recipe Dataset with Ingredient States Annotation for\n  State Probing Task"
                },
                "summary": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs. The dataset are publicly available\nat: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs. The dataset are publicly available\nat: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1"
                },
                "authors": [
                    {
                        "name": "Mashiro Toyooka"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    },
                    {
                        "name": "Yoko Yamakata"
                    }
                ],
                "author_detail": {
                    "name": "Yoko Yamakata"
                },
                "author": "Yoko Yamakata",
                "arxiv_comment": "Accepted to ACM Multimedia 2025. The dataset are publicly available\n  at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20867v1",
                "updated": "2025-08-28T14:59:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    59,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:59:55Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    59,
                    55,
                    3,
                    240,
                    0
                ],
                "title": "MSRS: Evaluating Multi-Source Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSRS: Evaluating Multi-Source Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented systems are typically evaluated in settings where\ninformation required to answer the query can be found within a single source or\nthe answer is short-form or factoid-based. However, many real-world\napplications demand the ability to integrate and summarize information\nscattered across multiple sources, where no single source is sufficient to\nrespond to the user's question. In such settings, the retrieval component of a\nRAG pipeline must recognize a variety of relevance signals, and the generation\ncomponent must connect and synthesize information across multiple sources. We\npresent a scalable framework for constructing evaluation benchmarks that\nchallenge RAG systems to integrate information across distinct sources and\ngenerate long-form responses. Using our framework, we build two new benchmarks\non Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing\nnarrative synthesis and summarization tasks, respectively, that require\nretrieval from large collections. Our extensive experiments with various RAG\npipelines -- including sparse and dense retrievers combined with frontier LLMs\n-- reveal that generation quality is highly dependent on retrieval\neffectiveness, which varies greatly by task. While multi-source synthesis\nproves challenging even in an oracle retrieval setting, we find that reasoning\nmodels significantly outperform standard LLMs at this distinct step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented systems are typically evaluated in settings where\ninformation required to answer the query can be found within a single source or\nthe answer is short-form or factoid-based. However, many real-world\napplications demand the ability to integrate and summarize information\nscattered across multiple sources, where no single source is sufficient to\nrespond to the user's question. In such settings, the retrieval component of a\nRAG pipeline must recognize a variety of relevance signals, and the generation\ncomponent must connect and synthesize information across multiple sources. We\npresent a scalable framework for constructing evaluation benchmarks that\nchallenge RAG systems to integrate information across distinct sources and\ngenerate long-form responses. Using our framework, we build two new benchmarks\non Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing\nnarrative synthesis and summarization tasks, respectively, that require\nretrieval from large collections. Our extensive experiments with various RAG\npipelines -- including sparse and dense retrievers combined with frontier LLMs\n-- reveal that generation quality is highly dependent on retrieval\neffectiveness, which varies greatly by task. While multi-source synthesis\nproves challenging even in an oracle retrieval setting, we find that reasoning\nmodels significantly outperform standard LLMs at this distinct step."
                },
                "authors": [
                    {
                        "name": "Rohan Phanse"
                    },
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Kejian Shi"
                    },
                    {
                        "name": "Wencai Zhang"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "COLM 2025; this article supersedes the preprint: arXiv:2309.08960",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20863v2",
                "updated": "2025-08-29T09:37:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    37,
                    59,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T14:57:04Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    57,
                    4,
                    3,
                    240,
                    0
                ],
                "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review"
                },
                "summary": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks."
                },
                "authors": [
                    {
                        "name": "Matteo Gioele Collu"
                    },
                    {
                        "name": "Umberto Salviati"
                    },
                    {
                        "name": "Roberto Confalonieri"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Giovanni Apruzzese"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Apruzzese"
                },
                "author": "Giovanni Apruzzese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20855v1",
                "updated": "2025-08-28T14:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    52,
                    37,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    52,
                    37,
                    3,
                    240,
                    0
                ],
                "title": "Uniform Quasi ML based inference for the panel AR(1) model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform Quasi ML based inference for the panel AR(1) model"
                },
                "summary": "This paper proposes new inference methods for panel AR models with arbitrary\ninitial conditions and heteroskedasticity and possibly additional regressors\nthat are robust to the strength of identification. Specifically, we consider\nseveral Maximum Likelihood based methods of constructing tests and confidence\nsets (CSs) and show that (Quasi) LM tests and CSs that use the expected Hessian\nrather than the observed Hessian of the log-likelihood have correct asymptotic\nsize (in a uniform sense). We derive the power envelope of a Fixed Effects\nversion of such a LM test for hypotheses involving the autoregressive parameter\nwhen the average information matrix is estimated by a centered OPG estimator\nand the model is only second-order identified, and show that it coincides with\nthe maximal attainable power curve in the worst case setting. We also study the\nempirical size and power properties of these (Quasi) LM tests and CSs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes new inference methods for panel AR models with arbitrary\ninitial conditions and heteroskedasticity and possibly additional regressors\nthat are robust to the strength of identification. Specifically, we consider\nseveral Maximum Likelihood based methods of constructing tests and confidence\nsets (CSs) and show that (Quasi) LM tests and CSs that use the expected Hessian\nrather than the observed Hessian of the log-likelihood have correct asymptotic\nsize (in a uniform sense). We derive the power envelope of a Fixed Effects\nversion of such a LM test for hypotheses involving the autoregressive parameter\nwhen the average information matrix is estimated by a centered OPG estimator\nand the model is only second-order identified, and show that it coincides with\nthe maximal attainable power curve in the worst case setting. We also study the\nempirical size and power properties of these (Quasi) LM tests and CSs."
                },
                "authors": [
                    {
                        "name": "Hugo Kruiniger"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Kruiniger"
                },
                "author": "Hugo Kruiniger",
                "arxiv_comment": "45 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F03, 62F05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20848v1",
                "updated": "2025-08-28T14:40:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    40,
                    27,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:40:27Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    40,
                    27,
                    3,
                    240,
                    0
                ],
                "title": "JADES: A Universal Framework for Jailbreak Assessment via\n  Decompositional Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JADES: A Universal Framework for Jailbreak Assessment via\n  Decompositional Scoring"
                },
                "summary": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Junjie Chu"
                    },
                    {
                        "name": "Mingjie Li"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Ye Leng"
                    },
                    {
                        "name": "Chenhao Lin"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yun Shen"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "arxiv_comment": "17 pages, 5 figures. For the code and data supporting this work, see\n  https://trustairlab.github.io/jades.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12140v2",
                "updated": "2025-08-28T14:32:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    32,
                    15,
                    3,
                    240,
                    0
                ],
                "published": "2025-04-16T14:52:22Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    52,
                    22,
                    2,
                    106,
                    0
                ],
                "title": "Multilingual Contextualization of Large Language Models for\n  Document-Level Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Contextualization of Large Language Models for\n  Document-Level Machine Translation"
                },
                "summary": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods."
                },
                "authors": [
                    {
                        "name": "Miguel Moura Ramos"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Andr√© F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr√© F. T. Martins"
                },
                "author": "Andr√© F. T. Martins",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20840v1",
                "updated": "2025-08-28T14:31:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    31,
                    48,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:31:48Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    31,
                    48,
                    3,
                    240,
                    0
                ],
                "title": "Learning Primitive Embodied World Models: Towards Scalable Robotic\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Primitive Embodied World Models: Towards Scalable Robotic\n  Learning"
                },
                "summary": "While video-generation-based embodied world models have gained increasing\nattention, their reliance on large-scale embodied interaction data remains a\nkey bottleneck. The scarcity, difficulty of collection, and high dimensionality\nof embodied data fundamentally limit the alignment granularity between language\nand actions and exacerbate the challenge of long-horizon video\ngeneration--hindering generative models from achieving a \"GPT moment\" in the\nembodied domain. There is a naive observation: the diversity of embodied data\nfar exceeds the relatively small space of possible primitive motions. Based on\nthis insight, we propose a novel paradigm for world modeling--Primitive\nEmbodied World Models (PEWM). By restricting video generation to fixed short\nhorizons, our approach 1) enables fine-grained alignment between linguistic\nconcepts and visual representations of robotic actions, 2) reduces learning\ncomplexity, 3) improves data efficiency in embodied data collection, and 4)\ndecreases inference latency. By equipping with a modular Vision-Language Model\n(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further\nenables flexible closed-loop control and supports compositional generalization\nof primitive-level policies over extended, complex tasks. Our framework\nleverages the spatiotemporal vision priors in video models and the semantic\nawareness of VLMs to bridge the gap between fine-grained physical interaction\nand high-level reasoning, paving the way toward scalable, interpretable, and\ngeneral-purpose embodied intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While video-generation-based embodied world models have gained increasing\nattention, their reliance on large-scale embodied interaction data remains a\nkey bottleneck. The scarcity, difficulty of collection, and high dimensionality\nof embodied data fundamentally limit the alignment granularity between language\nand actions and exacerbate the challenge of long-horizon video\ngeneration--hindering generative models from achieving a \"GPT moment\" in the\nembodied domain. There is a naive observation: the diversity of embodied data\nfar exceeds the relatively small space of possible primitive motions. Based on\nthis insight, we propose a novel paradigm for world modeling--Primitive\nEmbodied World Models (PEWM). By restricting video generation to fixed short\nhorizons, our approach 1) enables fine-grained alignment between linguistic\nconcepts and visual representations of robotic actions, 2) reduces learning\ncomplexity, 3) improves data efficiency in embodied data collection, and 4)\ndecreases inference latency. By equipping with a modular Vision-Language Model\n(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further\nenables flexible closed-loop control and supports compositional generalization\nof primitive-level policies over extended, complex tasks. Our framework\nleverages the spatiotemporal vision priors in video models and the semantic\nawareness of VLMs to bridge the gap between fine-grained physical interaction\nand high-level reasoning, paving the way toward scalable, interpretable, and\ngeneral-purpose embodied intelligence."
                },
                "authors": [
                    {
                        "name": "Qiao Sun"
                    },
                    {
                        "name": "Liujia Yang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Kaixin Xu"
                    },
                    {
                        "name": "Yongchao Chen"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Jiange Yang"
                    },
                    {
                        "name": "Haoyi Zhu"
                    },
                    {
                        "name": "Yating Wang"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xili Dai"
                    },
                    {
                        "name": "Nanyang Ye"
                    },
                    {
                        "name": "Qinying Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qinying Gu"
                },
                "author": "Qinying Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20828v1",
                "updated": "2025-08-28T14:23:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    23,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:23:39Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    23,
                    39,
                    3,
                    240,
                    0
                ],
                "title": "GDLLM: A Global Distance-aware Modeling Approach Based on Large Language\n  Models for Event Temporal Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDLLM: A Global Distance-aware Modeling Approach Based on Large Language\n  Models for Event Temporal Relation Extraction"
                },
                "summary": "In Natural Language Processing(NLP), Event Temporal Relation Extraction\n(ETRE) is to recognize the temporal relations of two events. Prior studies have\nnoted the importance of language models for ETRE. However, the restricted\npre-trained knowledge of Small Language Models(SLMs) limits their capability to\nhandle minority class relations in imbalanced classification datasets. For\nLarge Language Models(LLMs), researchers adopt manually designed prompts or\ninstructions, which may introduce extra noise, leading to interference with the\nmodel's judgment of the long-distance dependencies between events. To address\nthese issues, we propose GDLLM, a Global Distance-aware modeling approach based\non LLMs. We first present a distance-aware graph structure utilizing Graph\nAttention Network(GAT) to assist the LLMs in capturing long-distance dependency\nfeatures. Additionally, we design a temporal feature learning paradigm based on\nsoft inference to augment the identification of relations with a short-distance\nproximity band, which supplements the probabilistic information generated by\nLLMs into the multi-head attention mechanism. Since the global feature can be\ncaptured effectively, our framework substantially enhances the performance of\nminority relation classes and improves the overall learning ability.\nExperiments on two publicly available datasets, TB-Dense and MATRES,\ndemonstrate that our approach achieves state-of-the-art (SOTA) performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Natural Language Processing(NLP), Event Temporal Relation Extraction\n(ETRE) is to recognize the temporal relations of two events. Prior studies have\nnoted the importance of language models for ETRE. However, the restricted\npre-trained knowledge of Small Language Models(SLMs) limits their capability to\nhandle minority class relations in imbalanced classification datasets. For\nLarge Language Models(LLMs), researchers adopt manually designed prompts or\ninstructions, which may introduce extra noise, leading to interference with the\nmodel's judgment of the long-distance dependencies between events. To address\nthese issues, we propose GDLLM, a Global Distance-aware modeling approach based\non LLMs. We first present a distance-aware graph structure utilizing Graph\nAttention Network(GAT) to assist the LLMs in capturing long-distance dependency\nfeatures. Additionally, we design a temporal feature learning paradigm based on\nsoft inference to augment the identification of relations with a short-distance\nproximity band, which supplements the probabilistic information generated by\nLLMs into the multi-head attention mechanism. Since the global feature can be\ncaptured effectively, our framework substantially enhances the performance of\nminority relation classes and improves the overall learning ability.\nExperiments on two publicly available datasets, TB-Dense and MATRES,\ndemonstrate that our approach achieves state-of-the-art (SOTA) performance."
                },
                "authors": [
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Wanting Ning"
                    },
                    {
                        "name": "Yuxiao Fei"
                    },
                    {
                        "name": "Yubo Feng"
                    },
                    {
                        "name": "Lishuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Lishuang Li"
                },
                "author": "Lishuang Li",
                "arxiv_comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11022v3",
                "updated": "2025-08-28T14:17:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    17,
                    59,
                    3,
                    240,
                    0
                ],
                "published": "2024-11-17T09:58:37Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    9,
                    58,
                    37,
                    6,
                    322,
                    0
                ],
                "title": "ASiM: Modeling and Analyzing Inference Accuracy of SRAM-Based Analog CiM\n  Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASiM: Modeling and Analyzing Inference Accuracy of SRAM-Based Analog CiM\n  Circuits"
                },
                "summary": "SRAM-based Analog Compute-in-Memory (ACiM) demonstrates promising energy\nefficiency for deep neural network (DNN) processing. Nevertheless, efforts to\noptimize efficiency frequently compromise accuracy, and this trade-off remains\ninsufficiently studied due to the difficulty of performing full-system\nvalidation. Specifically, existing simulation tools rarely target SRAM-based\nACiM and exhibit inconsistent accuracy predictions, highlighting the need for a\nstandardized, SRAM CiM circuit-aware evaluation methodology. This paper\npresents ASiM, a simulation framework for evaluating inference accuracy in\nSRAM-based ACiM systems. ASiM captures critical effects in SRAM based analog\ncompute in memory systems, such as ADC quantization, bit parallel encoding, and\nanalog noise, which must be modeled with high fidelity due to their distinct\nbehavior in charge domain architectures compared to other memory technologies.\nASiM supports a wide range of modern DNN workloads, including CNN and\nTransformer-based models such as ViT, and scales to large-scale tasks like\nImageNet classification. Our results indicate that bit-parallel encoding can\nimprove energy efficiency with only modest accuracy degradation; however, even\n1 LSB of analog noise can significantly impair inference performance,\nparticularly in complex tasks such as ImageNet. To address this, we explore\nhybrid analog-digital execution and majority voting schemes, both of which\nenhance robustness without negating energy savings. ASiM bridges the gap\nbetween hardware design and inference performance, offering actionable insights\nfor energy-efficient, high-accuracy ACiM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRAM-based Analog Compute-in-Memory (ACiM) demonstrates promising energy\nefficiency for deep neural network (DNN) processing. Nevertheless, efforts to\noptimize efficiency frequently compromise accuracy, and this trade-off remains\ninsufficiently studied due to the difficulty of performing full-system\nvalidation. Specifically, existing simulation tools rarely target SRAM-based\nACiM and exhibit inconsistent accuracy predictions, highlighting the need for a\nstandardized, SRAM CiM circuit-aware evaluation methodology. This paper\npresents ASiM, a simulation framework for evaluating inference accuracy in\nSRAM-based ACiM systems. ASiM captures critical effects in SRAM based analog\ncompute in memory systems, such as ADC quantization, bit parallel encoding, and\nanalog noise, which must be modeled with high fidelity due to their distinct\nbehavior in charge domain architectures compared to other memory technologies.\nASiM supports a wide range of modern DNN workloads, including CNN and\nTransformer-based models such as ViT, and scales to large-scale tasks like\nImageNet classification. Our results indicate that bit-parallel encoding can\nimprove energy efficiency with only modest accuracy degradation; however, even\n1 LSB of analog noise can significantly impair inference performance,\nparticularly in complex tasks such as ImageNet. To address this, we explore\nhybrid analog-digital execution and majority voting schemes, both of which\nenhance robustness without negating energy savings. ASiM bridges the gap\nbetween hardware design and inference performance, offering actionable insights\nfor energy-efficient, high-accuracy ACiM deployment."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Yung-Chin Chen"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20818v1",
                "updated": "2025-08-28T14:16:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    16,
                    17,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:16:17Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    16,
                    17,
                    3,
                    240,
                    0
                ],
                "title": "cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with\n  Diversity-Based Context Blending",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with\n  Diversity-Based Context Blending"
                },
                "summary": "Many multi-agent reinforcement learning (MARL) algorithms are trained in\nfixed simulation environments, making them brittle when deployed in real-world\nscenarios with more complex and uncertain conditions. Contextual MARL (cMARL)\naddresses this by parameterizing environments with context variables and\ntraining a context-agnostic policy that performs well across all environment\nconfigurations. Existing cMARL methods attempt to use curriculum learning to\nhelp train and evaluate context-agnostic policies, but they often rely on\nunreliable proxy signals, such as value estimates or generalized advantage\nestimates that are noisy and unstable in multi-agent settings due to\ninter-agent dynamics and partial observability. To address these issues, we\npropose Contextual Multi-Agent LLM-Guided Curriculum Learning with\nDiversity-Based Context Blending (cMALC-D), a framework that uses Large\nLanguage Models (LLMs) to generate semantically meaningful curricula and\nprovide a more robust evaluation signal. To prevent mode collapse and encourage\nexploration, we introduce a novel diversity-based context blending mechanism\nthat creates new training scenarios by combining features from prior contexts.\nExperiments in traffic signal control domains demonstrate that cMALC-D\nsignificantly improves both generalization and sample efficiency compared to\nexisting curriculum learning baselines. We provide code at\nhttps://github.com/DaRL-LibSignal/cMALC-D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many multi-agent reinforcement learning (MARL) algorithms are trained in\nfixed simulation environments, making them brittle when deployed in real-world\nscenarios with more complex and uncertain conditions. Contextual MARL (cMARL)\naddresses this by parameterizing environments with context variables and\ntraining a context-agnostic policy that performs well across all environment\nconfigurations. Existing cMARL methods attempt to use curriculum learning to\nhelp train and evaluate context-agnostic policies, but they often rely on\nunreliable proxy signals, such as value estimates or generalized advantage\nestimates that are noisy and unstable in multi-agent settings due to\ninter-agent dynamics and partial observability. To address these issues, we\npropose Contextual Multi-Agent LLM-Guided Curriculum Learning with\nDiversity-Based Context Blending (cMALC-D), a framework that uses Large\nLanguage Models (LLMs) to generate semantically meaningful curricula and\nprovide a more robust evaluation signal. To prevent mode collapse and encourage\nexploration, we introduce a novel diversity-based context blending mechanism\nthat creates new training scenarios by combining features from prior contexts.\nExperiments in traffic signal control domains demonstrate that cMALC-D\nsignificantly improves both generalization and sample efficiency compared to\nexisting curriculum learning baselines. We provide code at\nhttps://github.com/DaRL-LibSignal/cMALC-D."
                },
                "authors": [
                    {
                        "name": "Anirudh Satheesh"
                    },
                    {
                        "name": "Keenan Powell"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "A shorter version has been accepted to the 2025 Conference on\n  Information and Knowledge Management",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20810v1",
                "updated": "2025-08-28T14:10:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    10,
                    59,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:10:59Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    10,
                    59,
                    3,
                    240,
                    0
                ],
                "title": "A Graph-Based Test-Harness for LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-Based Test-Harness for LLM Evaluation"
                },
                "summary": "We present a first known prototype of a dynamic, systematic benchmark of\nmedical guidelines for 400+ questions, with 3.3+ trillion possible\ncombinations, covering 100\\% of guideline relationships. We transformed the WHO\nIMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,\ntreatments, follow-ups, severities) and 300+ edges, then used graph traversal\nto generate questions that incorporated age-specific scenarios and contextual\ndistractors to ensure clinical relevance. Our graph-based approach enables\nsystematic evaluation across clinical tasks (45-67\\% accuracy), and we find\nmodels excel at symptom recognition but struggle with triaging severity,\ntreatment protocols and follow-up care, demonstrating how customized benchmarks\ncan identify specific capability gaps that general-domain evaluations miss.\nBeyond evaluation, this dynamic MCQA methodology enhances LLM post-training\n(supervised finetuning, GRPO, DPO), where correct answers provide high-reward\nsamples without expensive human annotation. The graph-based approach\nsuccessfully addresses the coverage limitations of manually curated benchmarks.\nThis methodology is a step toward scalable, contamination-resistant solution\nfor creating comprehensive benchmarks that can be dynamically generated,\nincluding when the guidelines are updated. Code and datasets are available at\nhttps://github.com/jessicalundin/graph_testing_harness",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a first known prototype of a dynamic, systematic benchmark of\nmedical guidelines for 400+ questions, with 3.3+ trillion possible\ncombinations, covering 100\\% of guideline relationships. We transformed the WHO\nIMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,\ntreatments, follow-ups, severities) and 300+ edges, then used graph traversal\nto generate questions that incorporated age-specific scenarios and contextual\ndistractors to ensure clinical relevance. Our graph-based approach enables\nsystematic evaluation across clinical tasks (45-67\\% accuracy), and we find\nmodels excel at symptom recognition but struggle with triaging severity,\ntreatment protocols and follow-up care, demonstrating how customized benchmarks\ncan identify specific capability gaps that general-domain evaluations miss.\nBeyond evaluation, this dynamic MCQA methodology enhances LLM post-training\n(supervised finetuning, GRPO, DPO), where correct answers provide high-reward\nsamples without expensive human annotation. The graph-based approach\nsuccessfully addresses the coverage limitations of manually curated benchmarks.\nThis methodology is a step toward scalable, contamination-resistant solution\nfor creating comprehensive benchmarks that can be dynamically generated,\nincluding when the guidelines are updated. Code and datasets are available at\nhttps://github.com/jessicalundin/graph_testing_harness"
                },
                "authors": [
                    {
                        "name": "Jessica Lundin"
                    },
                    {
                        "name": "Guillaume Chabot-Couture"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Chabot-Couture"
                },
                "author": "Guillaume Chabot-Couture",
                "arxiv_comment": "4 pages, 2 figures, dataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20806v1",
                "updated": "2025-08-28T14:08:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    8,
                    13,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:08:13Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    8,
                    13,
                    3,
                    240,
                    0
                ],
                "title": "The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic\n  Framework for Ordinal State Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic\n  Framework for Ordinal State Estimation"
                },
                "summary": "Traditional state estimation methods rely on probabilistic assumptions that\noften collapse epistemic uncertainty into scalar beliefs, risking\noverconfidence in sparse or adversarial sensing environments. We introduce the\nEpistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework\nfully grounded in possibility theory and epistemic humility. ESPF redefines the\nevolution of belief over state space using compatibility-weighted support\nupdates, surprisalaware pruning, and adaptive dispersion via sparse grid\nquadrature. Unlike conventional filters, ESPF does not seek a posterior\ndistribution, but rather maintains a structured region of plausibility or\nnon-rejection, updated using ordinal logic rather than integration. For\nmulti-model inference, we employ the Choquet integral to fuse competing\nhypotheses based on a dynamic epistemic capacity function, generalizing\nclassical winner-take-all strategies. The result is an inference engine capable\nof dynamically contracting or expanding belief support in direct response to\ninformation structure, without requiring prior statistical calibration. This\nwork presents a foundational shift in how inference, evidence, and ignorance\nare reconciled, supporting robust estimation where priors are unavailable,\nmisleading, or epistemically unjustified.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional state estimation methods rely on probabilistic assumptions that\noften collapse epistemic uncertainty into scalar beliefs, risking\noverconfidence in sparse or adversarial sensing environments. We introduce the\nEpistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework\nfully grounded in possibility theory and epistemic humility. ESPF redefines the\nevolution of belief over state space using compatibility-weighted support\nupdates, surprisalaware pruning, and adaptive dispersion via sparse grid\nquadrature. Unlike conventional filters, ESPF does not seek a posterior\ndistribution, but rather maintains a structured region of plausibility or\nnon-rejection, updated using ordinal logic rather than integration. For\nmulti-model inference, we employ the Choquet integral to fuse competing\nhypotheses based on a dynamic epistemic capacity function, generalizing\nclassical winner-take-all strategies. The result is an inference engine capable\nof dynamically contracting or expanding belief support in direct response to\ninformation structure, without requiring prior statistical calibration. This\nwork presents a foundational shift in how inference, evidence, and ignorance\nare reconciled, supporting robust estimation where priors are unavailable,\nmisleading, or epistemically unjustified."
                },
                "authors": [
                    {
                        "name": "Moriba Jah"
                    },
                    {
                        "name": "Van Haslett"
                    }
                ],
                "author_detail": {
                    "name": "Van Haslett"
                },
                "author": "Van Haslett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08846v2",
                "updated": "2025-08-28T14:07:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    7,
                    41,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-12T11:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    9,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "Steering Towards Fairness: Mitigating Political Bias in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Towards Fairness: Mitigating Political Bias in LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases along political\nand economic dimensions. In this paper, we employ a framework for probing and\nmitigating such biases in decoder-based LLMs through analysis of internal model\nrepresentations. Grounded in the Political Compass Test (PCT), this method uses\ncontrastive pairs to extract and compare hidden layer activations from models\nlike Mistral and DeepSeek. We introduce a comprehensive activation extraction\npipeline capable of layer-wise analysis across multiple ideological axes,\nrevealing meaningful disparities linked to political framing. Our results show\nthat decoder LLMs systematically encode representational bias across layers,\nwhich can be leveraged for effective steering vector-based mitigation. This\nwork provides new insights into how political bias is encoded in LLMs and\noffers a principled approach to debiasing beyond surface-level output\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases along political\nand economic dimensions. In this paper, we employ a framework for probing and\nmitigating such biases in decoder-based LLMs through analysis of internal model\nrepresentations. Grounded in the Political Compass Test (PCT), this method uses\ncontrastive pairs to extract and compare hidden layer activations from models\nlike Mistral and DeepSeek. We introduce a comprehensive activation extraction\npipeline capable of layer-wise analysis across multiple ideological axes,\nrevealing meaningful disparities linked to political framing. Our results show\nthat decoder LLMs systematically encode representational bias across layers,\nwhich can be leveraged for effective steering vector-based mitigation. This\nwork provides new insights into how political bias is encoded in LLMs and\noffers a principled approach to debiasing beyond surface-level output\ninterventions."
                },
                "authors": [
                    {
                        "name": "Afrozah Nadeem"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Accepted at CASE@RANLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20805v1",
                "updated": "2025-08-28T14:07:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    7,
                    7,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:07:07Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    7,
                    7,
                    3,
                    240,
                    0
                ],
                "title": "Exploring Machine Learning and Language Models for Multimodal Depression\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Machine Learning and Language Models for Multimodal Depression\n  Detection"
                },
                "summary": "This paper presents our approach to the first Multimodal Personality-Aware\nDepression Detection Challenge, focusing on multimodal depression detection\nusing machine learning and deep learning models. We explore and compare the\nperformance of XGBoost, transformer-based architectures, and large language\nmodels (LLMs) on audio, video, and text features. Our results highlight the\nstrengths and limitations of each type of model in capturing depression-related\nsignals across modalities, offering insights into effective multimodal\nrepresentation strategies for mental health prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our approach to the first Multimodal Personality-Aware\nDepression Detection Challenge, focusing on multimodal depression detection\nusing machine learning and deep learning models. We explore and compare the\nperformance of XGBoost, transformer-based architectures, and large language\nmodels (LLMs) on audio, video, and text features. Our results highlight the\nstrengths and limitations of each type of model in capturing depression-related\nsignals across modalities, offering insights into effective multimodal\nrepresentation strategies for mental health prediction."
                },
                "authors": [
                    {
                        "name": "Javier Si Zhao Hong"
                    },
                    {
                        "name": "Timothy Zoe Delaya"
                    },
                    {
                        "name": "Sherwyn Chan Yin Kit"
                    },
                    {
                        "name": "Pai Chet Ng"
                    },
                    {
                        "name": "Xiaoxiao Miao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxiao Miao"
                },
                "author": "Xiaoxiao Miao",
                "arxiv_comment": "This paper has been accepted by APCIPA ASC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20799v1",
                "updated": "2025-08-28T14:01:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    1,
                    54,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:01:54Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    1,
                    54,
                    3,
                    240,
                    0
                ],
                "title": "Evolution favours positively biased reasoning in sequential interactions\n  with high future gains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution favours positively biased reasoning in sequential interactions\n  with high future gains"
                },
                "summary": "Empirical evidence shows that human behaviour often deviates from\ngame-theoretical rationality. For instance, humans may hold unrealistic\nexpectations about future outcomes. As the evolutionary roots of such biases\nremain unclear, we investigate here how reasoning abilities and cognitive\nbiases co-evolve using Evolutionary Game Theory. In our model, individuals in a\npopulation deploy a variety of unbiased and biased level-k reasoning strategies\nto anticipate others' behaviour in sequential interactions, represented by the\nIncremental Centipede Game. Positively biased reasoning strategies have a\nsystematic inference bias towards higher but uncertain rewards, while\nnegatively biased strategies reflect the opposite tendency. We find that\nselection consistently favours positively biased reasoning, with rational\nbehaviour even going extinct. This bias co-evolves with bounded rationality, as\nthe reasoning depth remains limited in the population. Interestingly,\npositively biased agents may co-exist with non-reasoning agents, thus pointing\nto a novel equilibrium. Longer games further promote positively biased\nreasoning, as they can lead to higher future rewards. The biased reasoning\nstrategies proposed in this model may reflect cognitive phenomena like wishful\nthinking and defensive pessimism. This work therefore supports the claim that\ncertain cognitive biases, despite deviating from rational judgment, constitute\nan adaptive feature to better cope with social dilemmas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical evidence shows that human behaviour often deviates from\ngame-theoretical rationality. For instance, humans may hold unrealistic\nexpectations about future outcomes. As the evolutionary roots of such biases\nremain unclear, we investigate here how reasoning abilities and cognitive\nbiases co-evolve using Evolutionary Game Theory. In our model, individuals in a\npopulation deploy a variety of unbiased and biased level-k reasoning strategies\nto anticipate others' behaviour in sequential interactions, represented by the\nIncremental Centipede Game. Positively biased reasoning strategies have a\nsystematic inference bias towards higher but uncertain rewards, while\nnegatively biased strategies reflect the opposite tendency. We find that\nselection consistently favours positively biased reasoning, with rational\nbehaviour even going extinct. This bias co-evolves with bounded rationality, as\nthe reasoning depth remains limited in the population. Interestingly,\npositively biased agents may co-exist with non-reasoning agents, thus pointing\nto a novel equilibrium. Longer games further promote positively biased\nreasoning, as they can lead to higher future rewards. The biased reasoning\nstrategies proposed in this model may reflect cognitive phenomena like wishful\nthinking and defensive pessimism. This work therefore supports the claim that\ncertain cognitive biases, despite deviating from rational judgment, constitute\nan adaptive feature to better cope with social dilemmas."
                },
                "authors": [
                    {
                        "name": "Marco Saponara"
                    },
                    {
                        "name": "Elias Fernandez Domingos"
                    },
                    {
                        "name": "Jorge M. Pacheco"
                    },
                    {
                        "name": "Tom Lenaerts"
                    }
                ],
                "author_detail": {
                    "name": "Tom Lenaerts"
                },
                "author": "Tom Lenaerts",
                "arxiv_doi": "10.1098/rsif.2025.0153",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1098/rsif.2025.0153",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "33 pages, 5 figures",
                "arxiv_journal_ref": "Saponara Marco, Fernandez Domingos Elias, Pacheco Jorge M. and\n  Lenaerts Tom 2025 Evolution favours positively biased reasoning in sequential\n  interactions with high future gains J. R. Soc. Interface.2220250153",
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14330v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14330v3",
                "updated": "2025-08-28T13:50:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    50,
                    56,
                    3,
                    240,
                    0
                ],
                "published": "2025-07-18T19:15:50Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    19,
                    15,
                    50,
                    4,
                    199,
                    0
                ],
                "title": "Leveraging LLMs for Formal Software Requirements -- Challenges and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Formal Software Requirements -- Challenges and\n  Prospects"
                },
                "summary": "Software correctness is ensured mathematically through formal verification,\nwhich involves the resources of generating formal requirement specifications\nand having an implementation that must be verified. Tools such as\nmodel-checkers and theorem provers ensure software correctness by verifying the\nimplementation against the specification. Formal methods deployment is\nregularly enforced in the development of safety-critical systems e.g.\naerospace, medical devices and autonomous systems. Generating these\nspecifications from informal and ambiguous natural language requirements\nremains the key challenge. Our project, VERIFAI^{1}, aims to investigate\nautomated and semi-automated approaches to bridge this gap, using techniques\nfrom Natural Language Processing (NLP), ontology-based domain modelling,\nartefact reuse, and large language models (LLMs). This position paper presents\na preliminary synthesis of relevant literature to identify recurring challenges\nand prospective research directions in the generation of verifiable\nspecifications from informal requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software correctness is ensured mathematically through formal verification,\nwhich involves the resources of generating formal requirement specifications\nand having an implementation that must be verified. Tools such as\nmodel-checkers and theorem provers ensure software correctness by verifying the\nimplementation against the specification. Formal methods deployment is\nregularly enforced in the development of safety-critical systems e.g.\naerospace, medical devices and autonomous systems. Generating these\nspecifications from informal and ambiguous natural language requirements\nremains the key challenge. Our project, VERIFAI^{1}, aims to investigate\nautomated and semi-automated approaches to bridge this gap, using techniques\nfrom Natural Language Processing (NLP), ontology-based domain modelling,\nartefact reuse, and large language models (LLMs). This position paper presents\na preliminary synthesis of relevant literature to identify recurring challenges\nand prospective research directions in the generation of verifiable\nspecifications from informal requirements."
                },
                "authors": [
                    {
                        "name": "Arshad Beg"
                    },
                    {
                        "name": "Diarmuid O'Donoghue"
                    },
                    {
                        "name": "Rosemary Monahan"
                    }
                ],
                "author_detail": {
                    "name": "Rosemary Monahan"
                },
                "author": "Rosemary Monahan",
                "arxiv_comment": "Overlay2025 - 7th International Workshop on Artificial Intelligence\n  and fOrmal VERification, Logic, Automata, and sYnthesis. [Accepted]. To be\n  held on 26th of October, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14330v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14330v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07514v2",
                "updated": "2025-08-28T13:36:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    36,
                    34,
                    3,
                    240,
                    0
                ],
                "published": "2023-10-11T14:11:42Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    14,
                    11,
                    42,
                    2,
                    284,
                    0
                ],
                "title": "Causal resilience curves: A data-driven framework for quantifying the\n  spatiotemporal impacts of metro service disruptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal resilience curves: A data-driven framework for quantifying the\n  spatiotemporal impacts of metro service disruptions"
                },
                "summary": "Urban metro systems move vast numbers of passengers with a high level of\nefficiency in resource use, but frequently experience disruptions that result\nin delays, crowding, and deterioration in passenger satisfaction and patronage.\nTo quantify these adverse consequences, this paper presents a novel,\ndata-driven causal inference framework to measure metro resilience by\nestimating both the direct and spillover effects of service disruptions on\npassenger demand, journey time, travel speed and on-board crowding. By\nintegrating high-frequency smart card data into a synthetic control design, we\nuse weighted non-disrupted days to construct unbiased counterfactuals, which\nresolves confounding factors and accurately captures disruption propagation\nacross the network. The impact estimates are further translated into\nstation-level causal resilience curves that reveal spatial heterogeneity in the\ntemporal patterns of degradation and recovery across locations, providing metro\noperators with actionable insights for targeted interventions and resource\nallocation. A case study of the Hong Kong MTR demonstrates the framework's\nsuperiority over naive typical-day comparisons and machine-learning benchmarks\nin delivering unbiased resilience curves. This paper is the first to derive\ncausal estimates of dynamic metro resilience. This practical tool can be\ngeneralised to evaluate resilience in a broad range of public transport\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban metro systems move vast numbers of passengers with a high level of\nefficiency in resource use, but frequently experience disruptions that result\nin delays, crowding, and deterioration in passenger satisfaction and patronage.\nTo quantify these adverse consequences, this paper presents a novel,\ndata-driven causal inference framework to measure metro resilience by\nestimating both the direct and spillover effects of service disruptions on\npassenger demand, journey time, travel speed and on-board crowding. By\nintegrating high-frequency smart card data into a synthetic control design, we\nuse weighted non-disrupted days to construct unbiased counterfactuals, which\nresolves confounding factors and accurately captures disruption propagation\nacross the network. The impact estimates are further translated into\nstation-level causal resilience curves that reveal spatial heterogeneity in the\ntemporal patterns of degradation and recovery across locations, providing metro\noperators with actionable insights for targeted interventions and resource\nallocation. A case study of the Hong Kong MTR demonstrates the framework's\nsuperiority over naive typical-day comparisons and machine-learning benchmarks\nin delivering unbiased resilience curves. This paper is the first to derive\ncausal estimates of dynamic metro resilience. This practical tool can be\ngeneralised to evaluate resilience in a broad range of public transport\nsystems."
                },
                "authors": [
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Daniel H√∂rcher"
                    },
                    {
                        "name": "Prateek Bansal"
                    },
                    {
                        "name": "Daniel J. Graham"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Graham"
                },
                "author": "Daniel J. Graham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11535v2",
                "updated": "2025-08-28T13:32:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    32,
                    21,
                    3,
                    240,
                    0
                ],
                "published": "2025-07-15T17:58:55Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    58,
                    55,
                    1,
                    196,
                    0
                ],
                "title": "Canonical Bayesian Linear System Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Canonical Bayesian Linear System Identification"
                },
                "summary": "Standard Bayesian approaches for linear time-invariant (LTI) system\nidentification are hindered by parameter non-identifiability; the resulting\ncomplex, multi-modal posteriors make inference inefficient and impractical. We\nsolve this problem by embedding canonical forms of LTI systems within the\nBayesian framework. We rigorously establish that inference in these minimal\nparameterizations fully captures all invariant system dynamics (e.g., transfer\nfunctions, eigenvalues, predictive distributions of system outputs) while\nresolving identifiability. This approach unlocks the use of meaningful,\nstructure-aware priors (e.g., enforcing stability via eigenvalues) and ensures\nconditions for a Bernstein--von Mises theorem -- a link between Bayesian and\nfrequentist large-sample asymptotics that is broken in standard forms.\nExtensive simulations with modern MCMC methods highlight advantages over\nstandard parameterizations: canonical forms achieve higher computational\nefficiency, generate interpretable and well-behaved posteriors, and provide\nrobust uncertainty estimates, particularly from limited data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Bayesian approaches for linear time-invariant (LTI) system\nidentification are hindered by parameter non-identifiability; the resulting\ncomplex, multi-modal posteriors make inference inefficient and impractical. We\nsolve this problem by embedding canonical forms of LTI systems within the\nBayesian framework. We rigorously establish that inference in these minimal\nparameterizations fully captures all invariant system dynamics (e.g., transfer\nfunctions, eigenvalues, predictive distributions of system outputs) while\nresolving identifiability. This approach unlocks the use of meaningful,\nstructure-aware priors (e.g., enforcing stability via eigenvalues) and ensures\nconditions for a Bernstein--von Mises theorem -- a link between Bayesian and\nfrequentist large-sample asymptotics that is broken in standard forms.\nExtensive simulations with modern MCMC methods highlight advantages over\nstandard parameterizations: canonical forms achieve higher computational\nefficiency, generate interpretable and well-behaved posteriors, and provide\nrobust uncertainty estimates, particularly from limited data."
                },
                "authors": [
                    {
                        "name": "Andrey Bryutkin"
                    },
                    {
                        "name": "Matthew E. Levine"
                    },
                    {
                        "name": "I√±igo Urteaga"
                    },
                    {
                        "name": "Youssef Marzouk"
                    }
                ],
                "author_detail": {
                    "name": "Youssef Marzouk"
                },
                "author": "Youssef Marzouk",
                "arxiv_comment": "46 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20769v1",
                "updated": "2025-08-28T13:26:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    26,
                    13,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:26:13Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    26,
                    13,
                    3,
                    240,
                    0
                ],
                "title": "Solar peculiar motion inferred from dipole anisotropy in redshift\n  distribution of quasars appears to lie along the Galactic Centre direction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar peculiar motion inferred from dipole anisotropy in redshift\n  distribution of quasars appears to lie along the Galactic Centre direction"
                },
                "summary": "According to the Cosmological Principle an observer stationary with respect\nto the comoving coordinates of the expanding universe should find the redshift\ndistribution of distant quasars to be isotropic. However, the observed redshift\ndistribution in a large sample of 1.3 million quasars shows a significant\ndipole anisotropy. A peculiar motion of the observer could introduce such a\ndipole anisotropy in the observed redshift distribution. However, the motion\ninferred therefrom turns out to be not only many times the peculiar motion\nestimated from the anisotropy in the Cosmic Microwave Background (CMB), but\nalso nearly in a direction at a right angle. The Solar peculiar motion, in\nfact, turns out to be, quite unexpectedly, in the direction of the Galactic\nCentre. Such a statistically significant discrepancy in peculiar motion,\nderived by different methodologies, could imply a violation of the cosmological\nprinciple, a cornerstone in the foundation of the standard model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "According to the Cosmological Principle an observer stationary with respect\nto the comoving coordinates of the expanding universe should find the redshift\ndistribution of distant quasars to be isotropic. However, the observed redshift\ndistribution in a large sample of 1.3 million quasars shows a significant\ndipole anisotropy. A peculiar motion of the observer could introduce such a\ndipole anisotropy in the observed redshift distribution. However, the motion\ninferred therefrom turns out to be not only many times the peculiar motion\nestimated from the anisotropy in the Cosmic Microwave Background (CMB), but\nalso nearly in a direction at a right angle. The Solar peculiar motion, in\nfact, turns out to be, quite unexpectedly, in the direction of the Galactic\nCentre. Such a statistically significant discrepancy in peculiar motion,\nderived by different methodologies, could imply a violation of the cosmological\nprinciple, a cornerstone in the foundation of the standard model."
                },
                "authors": [
                    {
                        "name": "Ashok K. Singal"
                    }
                ],
                "author_detail": {
                    "name": "Ashok K. Singal"
                },
                "author": "Ashok K. Singal",
                "arxiv_doi": "10.1038/s41598-025-13426-0",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-025-13426-0",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 6 figures, 1 Table, accepted in Scientific Reports - Nature\n  Portfolio journal",
                "arxiv_journal_ref": "Sci. Rep. 15, 31805 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20766v1",
                "updated": "2025-08-28T13:22:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    22,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:22:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    22,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "Turning the Spell Around: Lightweight Alignment Amplification via\n  Rank-One Safety Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning the Spell Around: Lightweight Alignment Amplification via\n  Rank-One Safety Injection"
                },
                "summary": "Safety alignment in Large Language Models (LLMs) often involves mediating\ninternal representations to refuse harmful requests. Recent research has\ndemonstrated that these safety mechanisms can be bypassed by ablating or\nremoving specific representational directions within the model. In this paper,\nwe propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box\nmethod that amplifies a model's safety alignment by permanently steering its\nactivations toward the refusal-mediating subspace. ROSI operates as a simple,\nfine-tuning-free rank-one weight modification applied to all residual stream\nwrite matrices. The required safety direction can be computed from a small set\nof harmful and harmless instruction pairs. We show that ROSI consistently\nincreases safety refusal rates - as evaluated by Llama Guard 3 - while\npreserving the utility of the model on standard benchmarks such as MMLU,\nHellaSwag, and Arc. Furthermore, we show that ROSI can also re-align\n'uncensored' models by amplifying their own latent safety directions,\ndemonstrating its utility as an effective last-mile safety procedure. Our\nresults suggest that targeted, interpretable weight steering is a cheap and\npotent mechanism to improve LLM safety, complementing more resource-intensive\nfine-tuning paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment in Large Language Models (LLMs) often involves mediating\ninternal representations to refuse harmful requests. Recent research has\ndemonstrated that these safety mechanisms can be bypassed by ablating or\nremoving specific representational directions within the model. In this paper,\nwe propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box\nmethod that amplifies a model's safety alignment by permanently steering its\nactivations toward the refusal-mediating subspace. ROSI operates as a simple,\nfine-tuning-free rank-one weight modification applied to all residual stream\nwrite matrices. The required safety direction can be computed from a small set\nof harmful and harmless instruction pairs. We show that ROSI consistently\nincreases safety refusal rates - as evaluated by Llama Guard 3 - while\npreserving the utility of the model on standard benchmarks such as MMLU,\nHellaSwag, and Arc. Furthermore, we show that ROSI can also re-align\n'uncensored' models by amplifying their own latent safety directions,\ndemonstrating its utility as an effective last-mile safety procedure. Our\nresults suggest that targeted, interpretable weight steering is a cheap and\npotent mechanism to improve LLM safety, complementing more resource-intensive\nfine-tuning paradigms."
                },
                "authors": [
                    {
                        "name": "Harethah Abu Shairah"
                    },
                    {
                        "name": "Hasan Abed Al Kader Hammoud"
                    },
                    {
                        "name": "George Turkiyyah"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20764v1",
                "updated": "2025-08-28T13:19:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    19,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:19:31Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    19,
                    31,
                    3,
                    240,
                    0
                ],
                "title": "Feel the Difference? A Comparative Analysis of Emotional Arcs in Real\n  and LLM-Generated CBT Sessions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feel the Difference? A Comparative Analysis of Emotional Arcs in Real\n  and LLM-Generated CBT Sessions"
                },
                "summary": "Synthetic therapy dialogues generated by large language models (LLMs) are\nincreasingly used in mental health NLP to simulate counseling scenarios, train\nmodels, and supplement limited real-world data. However, it remains unclear\nwhether these synthetic conversations capture the nuanced emotional dynamics of\nreal therapy. In this work, we conduct the first comparative analysis of\nemotional arcs between real and LLM-generated Cognitive Behavioral Therapy\ndialogues. We adapt the Utterance Emotion Dynamics framework to analyze\nfine-grained affective trajectories across valence, arousal, and dominance\ndimensions. Our analysis spans both full dialogues and individual speaker roles\n(counselor and client), using real sessions transcribed from public videos and\nsynthetic dialogues from the CACTUS dataset. We find that while synthetic\ndialogues are fluent and structurally coherent, they diverge from real\nconversations in key emotional properties: real sessions exhibit greater\nemotional variability,more emotion-laden language, and more authentic patterns\nof reactivity and regulation. Moreover, emotional arc similarity between real\nand synthetic speakers is low, especially for clients. These findings\nunderscore the limitations of current LLM-generated therapy data and highlight\nthe importance of emotional fidelity in mental health applications. We\nintroduce RealCBT, a curated dataset of real CBT sessions, to support future\nresearch in this space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic therapy dialogues generated by large language models (LLMs) are\nincreasingly used in mental health NLP to simulate counseling scenarios, train\nmodels, and supplement limited real-world data. However, it remains unclear\nwhether these synthetic conversations capture the nuanced emotional dynamics of\nreal therapy. In this work, we conduct the first comparative analysis of\nemotional arcs between real and LLM-generated Cognitive Behavioral Therapy\ndialogues. We adapt the Utterance Emotion Dynamics framework to analyze\nfine-grained affective trajectories across valence, arousal, and dominance\ndimensions. Our analysis spans both full dialogues and individual speaker roles\n(counselor and client), using real sessions transcribed from public videos and\nsynthetic dialogues from the CACTUS dataset. We find that while synthetic\ndialogues are fluent and structurally coherent, they diverge from real\nconversations in key emotional properties: real sessions exhibit greater\nemotional variability,more emotion-laden language, and more authentic patterns\nof reactivity and regulation. Moreover, emotional arc similarity between real\nand synthetic speakers is low, especially for clients. These findings\nunderscore the limitations of current LLM-generated therapy data and highlight\nthe importance of emotional fidelity in mental health applications. We\nintroduce RealCBT, a curated dataset of real CBT sessions, to support future\nresearch in this space."
                },
                "authors": [
                    {
                        "name": "Xiaoyi Wang"
                    },
                    {
                        "name": "Jiwei Zhang"
                    },
                    {
                        "name": "Guangtao Zhang"
                    },
                    {
                        "name": "Honglei Guo"
                    }
                ],
                "author_detail": {
                    "name": "Honglei Guo"
                },
                "author": "Honglei Guo",
                "arxiv_comment": "Accepted at EMNLP 2025,14 page,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20757v1",
                "updated": "2025-08-28T13:14:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    14,
                    20,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:14:20Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    14,
                    20,
                    3,
                    240,
                    0
                ],
                "title": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and\n  Efficient Open-Ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and\n  Efficient Open-Ended Text Generation"
                },
                "summary": "Open-ended text generation faces a critical challenge: balancing coherence\nwith diversity in LLM outputs. While contrastive search-based decoding\nstrategies have emerged to address this trade-off, their practical utility is\noften limited by hyperparameter dependence and high computational costs. We\nintroduce GUARD, a self-adaptive decoding method that effectively balances\nthese competing objectives through a novel \"Glocal\" uncertainty-driven\nframework. GUARD combines global entropy estimates with local entropy\ndeviations to integrate both long-term and short-term uncertainty signals. We\ndemonstrate that our proposed global entropy formulation effectively mitigates\nabrupt variations in uncertainty, such as sudden overconfidence or high entropy\nspikes, and provides theoretical guarantees of unbiasedness and consistency. To\nreduce computational overhead, we incorporate a simple yet effective\ntoken-count-based penalty into GUARD. Experimental results demonstrate that\nGUARD achieves a good balance between text diversity and coherence, while\nexhibiting substantial improvements in generation speed. In a more nuanced\ncomparison study across different dimensions of text quality, both human and\nLLM evaluators validated its remarkable performance. Our code is available at\nhttps://github.com/YecanLee/GUARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended text generation faces a critical challenge: balancing coherence\nwith diversity in LLM outputs. While contrastive search-based decoding\nstrategies have emerged to address this trade-off, their practical utility is\noften limited by hyperparameter dependence and high computational costs. We\nintroduce GUARD, a self-adaptive decoding method that effectively balances\nthese competing objectives through a novel \"Glocal\" uncertainty-driven\nframework. GUARD combines global entropy estimates with local entropy\ndeviations to integrate both long-term and short-term uncertainty signals. We\ndemonstrate that our proposed global entropy formulation effectively mitigates\nabrupt variations in uncertainty, such as sudden overconfidence or high entropy\nspikes, and provides theoretical guarantees of unbiasedness and consistency. To\nreduce computational overhead, we incorporate a simple yet effective\ntoken-count-based penalty into GUARD. Experimental results demonstrate that\nGUARD achieves a good balance between text diversity and coherence, while\nexhibiting substantial improvements in generation speed. In a more nuanced\ncomparison study across different dimensions of text quality, both human and\nLLM evaluators validated its remarkable performance. Our code is available at\nhttps://github.com/YecanLee/GUARD."
                },
                "authors": [
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Esteban Garces Arias"
                    },
                    {
                        "name": "Meimingwei Li"
                    },
                    {
                        "name": "Julian Rodemann"
                    },
                    {
                        "name": "Matthias A√üenmacher"
                    },
                    {
                        "name": "Danlu Chen"
                    },
                    {
                        "name": "Gaojuan Fan"
                    },
                    {
                        "name": "Christian Heumann"
                    },
                    {
                        "name": "Chongsheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chongsheng Zhang"
                },
                "author": "Chongsheng Zhang",
                "arxiv_comment": "Accepted at Findings of the Association for Computational\n  Linguistics: EMNLP (Findings) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20750v1",
                "updated": "2025-08-28T13:08:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    8,
                    57,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:08:57Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    8,
                    57,
                    3,
                    240,
                    0
                ],
                "title": "Specializing General-purpose LLM Embeddings for Implicit Hate Speech\n  Detection across Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specializing General-purpose LLM Embeddings for Implicit Hate Speech\n  Detection across Datasets"
                },
                "summary": "Implicit hate speech (IHS) is indirect language that conveys prejudice or\nhatred through subtle cues, sarcasm or coded terminology. IHS is challenging to\ndetect as it does not include explicit derogatory or inflammatory words. To\naddress this challenge, task-specific pipelines can be complemented with\nexternal knowledge or additional information such as context, emotions and\nsentiment data. In this paper, we show that, by solely fine-tuning recent\ngeneral-purpose embedding models based on large language models (LLMs), such as\nStella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.\nExperiments on multiple IHS datasets show up to 1.10 percentage points\nimprovements for in-dataset, and up to 20.35 percentage points improvements in\ncross-dataset evaluation, in terms of F1-macro score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit hate speech (IHS) is indirect language that conveys prejudice or\nhatred through subtle cues, sarcasm or coded terminology. IHS is challenging to\ndetect as it does not include explicit derogatory or inflammatory words. To\naddress this challenge, task-specific pipelines can be complemented with\nexternal knowledge or additional information such as context, emotions and\nsentiment data. In this paper, we show that, by solely fine-tuning recent\ngeneral-purpose embedding models based on large language models (LLMs), such as\nStella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.\nExperiments on multiple IHS datasets show up to 1.10 percentage points\nimprovements for in-dataset, and up to 20.35 percentage points improvements in\ncross-dataset evaluation, in terms of F1-macro score."
                },
                "authors": [
                    {
                        "name": "Vassiliy Cheremetiev"
                    },
                    {
                        "name": "Quang Long Ho Ngo"
                    },
                    {
                        "name": "Chau Ying Kot"
                    },
                    {
                        "name": "Alina Elena Baia"
                    },
                    {
                        "name": "Andrea Cavallaro"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Cavallaro"
                },
                "author": "Andrea Cavallaro",
                "arxiv_doi": "10.1145/3746275.3762209",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746275.3762209",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper accepted at the DHOW Workshop at ACM Multimedia 2025. Code\n  available at https://github.com/idiap/implicit-hsd",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20745v1",
                "updated": "2025-08-28T13:04:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    4,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:04:55Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    4,
                    55,
                    3,
                    240,
                    0
                ],
                "title": "Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis\n  Classification"
                },
                "summary": "Atypical mitotic figures (AMFs) are important histopathological markers yet\nremain challenging to identify consistently, particularly under domain shift\nstemming from scanner, stain, and acquisition differences. We present a simple\ntraining-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.\nThe approach (i) increases feature diversity via style perturbations inserted\nat early and mid backbone stages, (ii) aligns attention-refined features across\nsites using weak domain labels (Scanner, Origin, Species, Tumor) through an\nauxiliary alignment loss, and (iii) stabilizes predictions by distilling from\nan exponential moving average (EMA) teacher with temperature-scaled KL\ndivergence. On the organizer-run preliminary leaderboard for atypical mitosis\nclassification, our submission attains balanced accuracy of 0.8762, sensitivity\nof 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs\nnegligible inference-time overhead, relies only on coarse domain metadata, and\ndelivers strong, balanced performance, positioning it as a competitive\nsubmission for the MIDOG 2025 challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atypical mitotic figures (AMFs) are important histopathological markers yet\nremain challenging to identify consistently, particularly under domain shift\nstemming from scanner, stain, and acquisition differences. We present a simple\ntraining-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.\nThe approach (i) increases feature diversity via style perturbations inserted\nat early and mid backbone stages, (ii) aligns attention-refined features across\nsites using weak domain labels (Scanner, Origin, Species, Tumor) through an\nauxiliary alignment loss, and (iii) stabilizes predictions by distilling from\nan exponential moving average (EMA) teacher with temperature-scaled KL\ndivergence. On the organizer-run preliminary leaderboard for atypical mitosis\nclassification, our submission attains balanced accuracy of 0.8762, sensitivity\nof 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs\nnegligible inference-time overhead, relies only on coarse domain metadata, and\ndelivers strong, balanced performance, positioning it as a competitive\nsubmission for the MIDOG 2025 challenge."
                },
                "authors": [
                    {
                        "name": "Kaustubh Atey"
                    },
                    {
                        "name": "Sameer Anand Jha"
                    },
                    {
                        "name": "Gouranga Bala"
                    },
                    {
                        "name": "Amit Sethi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sethi"
                },
                "author": "Amit Sethi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20744v1",
                "updated": "2025-08-28T13:04:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    4,
                    34,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:04:34Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    4,
                    34,
                    3,
                    240,
                    0
                ],
                "title": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of\n  LLM-Generated Behavioural Specifications from Food-Safety Regulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of\n  LLM-Generated Behavioural Specifications from Food-Safety Regulations"
                },
                "summary": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation."
                },
                "authors": [
                    {
                        "name": "Shabnam Hassani"
                    },
                    {
                        "name": "Mehrdad Sabetzadeh"
                    },
                    {
                        "name": "Daniel Amyot"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Amyot"
                },
                "author": "Daniel Amyot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20737v1",
                "updated": "2025-08-28T13:00:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    0,
                    28,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:00:28Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    0,
                    28,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges,\n  and a Lightweight Interaction Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Testing for LLM Applications: Characteristics, Challenges,\n  and a Lightweight Interaction Protocol"
                },
                "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework."
                },
                "authors": [
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Yixiao Yang"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Shi Ying"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Junjie Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Linxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Linxiao Jiang"
                },
                "author": "Linxiao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20736v1",
                "updated": "2025-08-28T12:59:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    59,
                    1,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:59:01Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    59,
                    1,
                    3,
                    240,
                    0
                ],
                "title": "Leveraging Semantic Triples for Private Document Generation with Local\n  Differential Privacy Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Triples for Private Document Generation with Local\n  Differential Privacy Guarantees"
                },
                "summary": "Many works at the intersection of Differential Privacy (DP) in Natural\nLanguage Processing aim to protect privacy by transforming texts under DP\nguarantees. This can be performed in a variety of ways, from word perturbations\nto full document rewriting, and most often under local DP. Here, an input text\nmust be made indistinguishable from any other potential text, within some bound\ngoverned by the privacy parameter $\\varepsilon$. Such a guarantee is quite\ndemanding, and recent works show that privatizing texts under local DP can only\nbe done reasonably under very high $\\varepsilon$ values. Addressing this\nchallenge, we introduce DP-ST, which leverages semantic triples for\nneighborhood-aware private document generation under local DP guarantees.\nThrough the evaluation of our method, we demonstrate the effectiveness of the\ndivide-and-conquer paradigm, particularly when limiting the DP notion (and\nprivacy guarantees) to that of a privatization neighborhood. When combined with\nLLM post-processing, our method allows for coherent text generation even at\nlower $\\varepsilon$ values, while still balancing privacy and utility. These\nfindings highlight the importance of coherence in achieving balanced\nprivatization outputs at reasonable $\\varepsilon$ levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many works at the intersection of Differential Privacy (DP) in Natural\nLanguage Processing aim to protect privacy by transforming texts under DP\nguarantees. This can be performed in a variety of ways, from word perturbations\nto full document rewriting, and most often under local DP. Here, an input text\nmust be made indistinguishable from any other potential text, within some bound\ngoverned by the privacy parameter $\\varepsilon$. Such a guarantee is quite\ndemanding, and recent works show that privatizing texts under local DP can only\nbe done reasonably under very high $\\varepsilon$ values. Addressing this\nchallenge, we introduce DP-ST, which leverages semantic triples for\nneighborhood-aware private document generation under local DP guarantees.\nThrough the evaluation of our method, we demonstrate the effectiveness of the\ndivide-and-conquer paradigm, particularly when limiting the DP notion (and\nprivacy guarantees) to that of a privatization neighborhood. When combined with\nLLM post-processing, our method allows for coherent text generation even at\nlower $\\varepsilon$ values, while still balancing privacy and utility. These\nfindings highlight the importance of coherence in achieving balanced\nprivatization outputs at reasonable $\\varepsilon$ levels."
                },
                "authors": [
                    {
                        "name": "Stephen Meisenbacher"
                    },
                    {
                        "name": "Maulik Chevli"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "17 pages, 2 figures, 11 tables. Accepted to EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20729v1",
                "updated": "2025-08-28T12:50:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    50,
                    48,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:50:48Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    50,
                    48,
                    3,
                    240,
                    0
                ],
                "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and\n  Revision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and\n  Revision"
                },
                "summary": "Large language models (LLMs) serve as an active and promising field of\ngenerative artificial intelligence and have demonstrated abilities to perform\ncomplex tasks in multiple domains, including mathematical and scientific\nreasoning. In this work, we construct a novel agent framework for solving\nrepresentative problems in scientific computing. The proposed agent,\nincorporating a \"rewriting-resolution-review-revision\" logical chain via three\nreasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,\nrespectively), is integrated in a collaborative and interactive manner. The\nConsultant module endows the agent with knowledge transfer capabilities to link\nproblems to professional domain insights, thereby rewriting problem\ndescriptions through text augmentation. The Programmer module is responsible\nfor generating and executing well-structured code to deliver the problem\nresolution. The Reviewer module equips the agent with the capacity for\nself-debugging and self-refinement through interactive feedback with code\nruntime outputs. By leveraging the end-to-end review mechanism, the executable\ncode provided by the Programmer attains the iterative revision. A comprehensive\nevaluation is conducted on the performance of the proposed agent framework in\nsolving PDEs, ill-conditioned linear systems, and data-driven physical analysis\nproblems. Compared to single-model, this collaborative framework significantly\nimproves the bug-free code generation rate and reduces the occurrence of\nnon-physical solutions, thereby establishing a highly reliable framework for\nautonomous code generation based on natural language descriptions. The review\nmechanism improved the average execution success (bug-free code and non-NaN\nsolutions) rate of the latest reasoning models. In summary, our agent framework\nestablishes automatic code generation and review as a promising scientific\ncomputing paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) serve as an active and promising field of\ngenerative artificial intelligence and have demonstrated abilities to perform\ncomplex tasks in multiple domains, including mathematical and scientific\nreasoning. In this work, we construct a novel agent framework for solving\nrepresentative problems in scientific computing. The proposed agent,\nincorporating a \"rewriting-resolution-review-revision\" logical chain via three\nreasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,\nrespectively), is integrated in a collaborative and interactive manner. The\nConsultant module endows the agent with knowledge transfer capabilities to link\nproblems to professional domain insights, thereby rewriting problem\ndescriptions through text augmentation. The Programmer module is responsible\nfor generating and executing well-structured code to deliver the problem\nresolution. The Reviewer module equips the agent with the capacity for\nself-debugging and self-refinement through interactive feedback with code\nruntime outputs. By leveraging the end-to-end review mechanism, the executable\ncode provided by the Programmer attains the iterative revision. A comprehensive\nevaluation is conducted on the performance of the proposed agent framework in\nsolving PDEs, ill-conditioned linear systems, and data-driven physical analysis\nproblems. Compared to single-model, this collaborative framework significantly\nimproves the bug-free code generation rate and reduces the occurrence of\nnon-physical solutions, thereby establishing a highly reliable framework for\nautonomous code generation based on natural language descriptions. The review\nmechanism improved the average execution success (bug-free code and non-NaN\nsolutions) rate of the latest reasoning models. In summary, our agent framework\nestablishes automatic code generation and review as a promising scientific\ncomputing paradigm."
                },
                "authors": [
                    {
                        "name": "Ao Cheng"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Guowei He"
                    }
                ],
                "author_detail": {
                    "name": "Guowei He"
                },
                "author": "Guowei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20721v1",
                "updated": "2025-08-28T12:44:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    44,
                    13,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:44:13Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    44,
                    13,
                    3,
                    240,
                    0
                ],
                "title": "Upper Limits on the Isotropic Gravitational-Wave Background from the\n  first part of LIGO, Virgo, and KAGRA's fourth Observing Run",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upper Limits on the Isotropic Gravitational-Wave Background from the\n  first part of LIGO, Virgo, and KAGRA's fourth Observing Run"
                },
                "summary": "We present results from the search for an isotropic gravitational-wave\nbackground using Advanced LIGO and Advanced Virgo data from O1 through O4a, the\nfirst part of the fourth observing run. This background is the accumulated\nsignal from unresolved sources throughout cosmic history and encodes\ninformation about the merger history of compact binaries throughout the\nUniverse, as well as exotic physics and potentially primordial processes from\nthe early cosmos. Our cross-correlation analysis reveals no statistically\nsignificant background signal, enabling us to constrain several theoretical\nscenarios. For compact binary coalescences which approximately follow a 2/3\npower-law spectrum, we constrain the fractional energy density to $\\Omega_{\\rm\nGW}(25{\\rm Hz})\\leq 2.0\\times 10^{-9}$ (95% cred.), a factor of 1.7 improvement\nover previous results. Scale-invariant backgrounds are constrained to\n$\\Omega_{\\rm GW}(25{\\rm Hz})\\leq 2.8\\times 10^{-9}$, representing a 2.1x\nsensitivity gain. We also place new limits on gravity theories predicting\nnon-standard polarization modes and confirm that terrestrial magnetic noise\nsources remain below detection threshold. Combining these spectral limits with\npopulation models for GWTC-4, the latest gravitational-wave event catalog, we\nfind our constraints remain above predicted merger backgrounds but are\napproaching detectability. The joint analysis combining the background limits\nshown here with the GWTC-4 catalog enables improved inference of the binary\nblack hole merger rate evolution across cosmic time. Employing GWTC-4 inference\nresults and standard modeling choices, we estimate that the total background\narising from compact binary coalescences is $\\Omega_{\\rm CBC}(25{\\rm\nHz})={0.9^{+1.1}_{-0.5}\\times 10^{-9}}$ at 90% confidence, where the largest\ncontribution is due to binary black holes only, $\\Omega_{\\rm BBH}(25{\\rm\nHz})=0.8^{+1.1}_{-0.5}\\times 10^{-9}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present results from the search for an isotropic gravitational-wave\nbackground using Advanced LIGO and Advanced Virgo data from O1 through O4a, the\nfirst part of the fourth observing run. This background is the accumulated\nsignal from unresolved sources throughout cosmic history and encodes\ninformation about the merger history of compact binaries throughout the\nUniverse, as well as exotic physics and potentially primordial processes from\nthe early cosmos. Our cross-correlation analysis reveals no statistically\nsignificant background signal, enabling us to constrain several theoretical\nscenarios. For compact binary coalescences which approximately follow a 2/3\npower-law spectrum, we constrain the fractional energy density to $\\Omega_{\\rm\nGW}(25{\\rm Hz})\\leq 2.0\\times 10^{-9}$ (95% cred.), a factor of 1.7 improvement\nover previous results. Scale-invariant backgrounds are constrained to\n$\\Omega_{\\rm GW}(25{\\rm Hz})\\leq 2.8\\times 10^{-9}$, representing a 2.1x\nsensitivity gain. We also place new limits on gravity theories predicting\nnon-standard polarization modes and confirm that terrestrial magnetic noise\nsources remain below detection threshold. Combining these spectral limits with\npopulation models for GWTC-4, the latest gravitational-wave event catalog, we\nfind our constraints remain above predicted merger backgrounds but are\napproaching detectability. The joint analysis combining the background limits\nshown here with the GWTC-4 catalog enables improved inference of the binary\nblack hole merger rate evolution across cosmic time. Employing GWTC-4 inference\nresults and standard modeling choices, we estimate that the total background\narising from compact binary coalescences is $\\Omega_{\\rm CBC}(25{\\rm\nHz})={0.9^{+1.1}_{-0.5}\\times 10^{-9}}$ at 90% confidence, where the largest\ncontribution is due to binary black holes only, $\\Omega_{\\rm BBH}(25{\\rm\nHz})=0.8^{+1.1}_{-0.5}\\times 10^{-9}$."
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    },
                    {
                        "name": "A. G. Abac"
                    },
                    {
                        "name": "I. Abouelfettouh"
                    },
                    {
                        "name": "F. Acernese"
                    },
                    {
                        "name": "K. Ackley"
                    },
                    {
                        "name": "C. Adamcewicz"
                    },
                    {
                        "name": "S. Adhicary"
                    },
                    {
                        "name": "D. Adhikari"
                    },
                    {
                        "name": "N. Adhikari"
                    },
                    {
                        "name": "R. X. Adhikari"
                    },
                    {
                        "name": "V. K. Adkins"
                    },
                    {
                        "name": "S. Afroz"
                    },
                    {
                        "name": "A. Agapito"
                    },
                    {
                        "name": "D. Agarwal"
                    },
                    {
                        "name": "M. Agathos"
                    },
                    {
                        "name": "N. Aggarwal"
                    },
                    {
                        "name": "S. Aggarwal"
                    },
                    {
                        "name": "O. D. Aguiar"
                    },
                    {
                        "name": "I. -L. Ahrend"
                    },
                    {
                        "name": "L. Aiello"
                    },
                    {
                        "name": "A. Ain"
                    },
                    {
                        "name": "P. Ajith"
                    },
                    {
                        "name": "T. Akutsu"
                    },
                    {
                        "name": "S. Albanesi"
                    },
                    {
                        "name": "W. Ali"
                    },
                    {
                        "name": "S. Al-Kershi"
                    },
                    {
                        "name": "C. All√©n√©"
                    },
                    {
                        "name": "A. Allocca"
                    },
                    {
                        "name": "S. Al-Shammari"
                    },
                    {
                        "name": "P. A. Altin"
                    },
                    {
                        "name": "S. Alvarez-Lopez"
                    },
                    {
                        "name": "W. Amar"
                    },
                    {
                        "name": "O. Amarasinghe"
                    },
                    {
                        "name": "A. Amato"
                    },
                    {
                        "name": "F. Amicucci"
                    },
                    {
                        "name": "C. Amra"
                    },
                    {
                        "name": "A. Ananyeva"
                    },
                    {
                        "name": "S. B. Anderson"
                    },
                    {
                        "name": "W. G. Anderson"
                    },
                    {
                        "name": "M. Andia"
                    },
                    {
                        "name": "M. Ando"
                    },
                    {
                        "name": "M. Andr√©s-Carcasona"
                    },
                    {
                        "name": "T. Andriƒá"
                    },
                    {
                        "name": "J. Anglin"
                    },
                    {
                        "name": "S. Ansoldi"
                    },
                    {
                        "name": "J. M. Antelis"
                    },
                    {
                        "name": "S. Antier"
                    },
                    {
                        "name": "M. Aoumi"
                    },
                    {
                        "name": "E. Z. Appavuravther"
                    },
                    {
                        "name": "S. Appert"
                    },
                    {
                        "name": "S. K. Apple"
                    },
                    {
                        "name": "K. Arai"
                    },
                    {
                        "name": "A. Araya"
                    },
                    {
                        "name": "M. C. Araya"
                    },
                    {
                        "name": "M. Arca Sedda"
                    },
                    {
                        "name": "J. S. Areeda"
                    },
                    {
                        "name": "N. Aritomi"
                    },
                    {
                        "name": "F. Armato"
                    },
                    {
                        "name": "S. Armstrong"
                    },
                    {
                        "name": "N. Arnaud"
                    },
                    {
                        "name": "M. Arogeti"
                    },
                    {
                        "name": "S. M. Aronson"
                    },
                    {
                        "name": "G. Ashton"
                    },
                    {
                        "name": "Y. Aso"
                    },
                    {
                        "name": "L. Asprea"
                    },
                    {
                        "name": "M. Assiduo"
                    },
                    {
                        "name": "S. Assis de Souza Melo"
                    },
                    {
                        "name": "S. M. Aston"
                    },
                    {
                        "name": "P. Astone"
                    },
                    {
                        "name": "F. Attadio"
                    },
                    {
                        "name": "F. Aubin"
                    },
                    {
                        "name": "K. AultONeal"
                    },
                    {
                        "name": "G. Avallone"
                    },
                    {
                        "name": "E. A. Avila"
                    },
                    {
                        "name": "S. Babak"
                    },
                    {
                        "name": "C. Badger"
                    },
                    {
                        "name": "S. Bae"
                    },
                    {
                        "name": "S. Bagnasco"
                    },
                    {
                        "name": "L. Baiotti"
                    },
                    {
                        "name": "R. Bajpai"
                    },
                    {
                        "name": "T. Baka"
                    },
                    {
                        "name": "A. M. Baker"
                    },
                    {
                        "name": "K. A. Baker"
                    },
                    {
                        "name": "T. Baker"
                    },
                    {
                        "name": "G. Baldi"
                    },
                    {
                        "name": "N. Baldicchi"
                    },
                    {
                        "name": "M. Ball"
                    },
                    {
                        "name": "G. Ballardin"
                    },
                    {
                        "name": "S. W. Ballmer"
                    },
                    {
                        "name": "S. Banagiri"
                    },
                    {
                        "name": "B. Banerjee"
                    },
                    {
                        "name": "D. Bankar"
                    },
                    {
                        "name": "T. M. Baptiste"
                    },
                    {
                        "name": "P. Baral"
                    },
                    {
                        "name": "M. Baratti"
                    },
                    {
                        "name": "J. C. Barayoga"
                    },
                    {
                        "name": "B. C. Barish"
                    },
                    {
                        "name": "D. Barker"
                    },
                    {
                        "name": "N. Barman"
                    },
                    {
                        "name": "P. Barneo"
                    },
                    {
                        "name": "F. Barone"
                    },
                    {
                        "name": "B. Barr"
                    },
                    {
                        "name": "L. Barsotti"
                    },
                    {
                        "name": "M. Barsuglia"
                    },
                    {
                        "name": "D. Barta"
                    },
                    {
                        "name": "A. M. Bartoletti"
                    },
                    {
                        "name": "M. A. Barton"
                    },
                    {
                        "name": "I. Bartos"
                    },
                    {
                        "name": "A. Basalaev"
                    },
                    {
                        "name": "R. Bassiri"
                    },
                    {
                        "name": "A. Basti"
                    },
                    {
                        "name": "M. Bawaj"
                    },
                    {
                        "name": "P. Baxi"
                    },
                    {
                        "name": "J. C. Bayley"
                    },
                    {
                        "name": "A. C. Baylor"
                    },
                    {
                        "name": "P. A. Baynard II"
                    },
                    {
                        "name": "M. Bazzan"
                    },
                    {
                        "name": "V. M. Bedakihale"
                    },
                    {
                        "name": "F. Beirnaert"
                    },
                    {
                        "name": "M. Bejger"
                    },
                    {
                        "name": "D. Belardinelli"
                    },
                    {
                        "name": "A. S. Bell"
                    },
                    {
                        "name": "D. S. Bellie"
                    },
                    {
                        "name": "L. Bellizzi"
                    },
                    {
                        "name": "W. Benoit"
                    },
                    {
                        "name": "I. Bentara"
                    },
                    {
                        "name": "J. D. Bentley"
                    },
                    {
                        "name": "M. Ben Yaala"
                    },
                    {
                        "name": "S. Bera"
                    },
                    {
                        "name": "F. Bergamin"
                    },
                    {
                        "name": "B. K. Berger"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "M. Beroiz"
                    },
                    {
                        "name": "D. Bersanetti"
                    },
                    {
                        "name": "T. Bertheas"
                    },
                    {
                        "name": "A. Bertolini"
                    },
                    {
                        "name": "J. Betzwieser"
                    },
                    {
                        "name": "D. Beveridge"
                    },
                    {
                        "name": "G. Bevilacqua"
                    },
                    {
                        "name": "N. Bevins"
                    },
                    {
                        "name": "R. Bhandare"
                    },
                    {
                        "name": "R. Bhatt"
                    },
                    {
                        "name": "D. Bhattacharjee"
                    },
                    {
                        "name": "S. Bhattacharyya"
                    },
                    {
                        "name": "S. Bhaumik"
                    },
                    {
                        "name": "V. Biancalana"
                    },
                    {
                        "name": "A. Bianchi"
                    },
                    {
                        "name": "I. A. Bilenko"
                    },
                    {
                        "name": "G. Billingsley"
                    },
                    {
                        "name": "A. Binetti"
                    },
                    {
                        "name": "S. Bini"
                    },
                    {
                        "name": "C. Binu"
                    },
                    {
                        "name": "S. Biot"
                    },
                    {
                        "name": "O. Birnholtz"
                    },
                    {
                        "name": "S. Biscoveanu"
                    },
                    {
                        "name": "A. Bisht"
                    },
                    {
                        "name": "M. Bitossi"
                    },
                    {
                        "name": "M. -A. Bizouard"
                    },
                    {
                        "name": "S. Blaber"
                    },
                    {
                        "name": "J. K. Blackburn"
                    },
                    {
                        "name": "L. A. Blagg"
                    },
                    {
                        "name": "C. D. Blair"
                    },
                    {
                        "name": "D. G. Blair"
                    },
                    {
                        "name": "N. Bode"
                    },
                    {
                        "name": "N. Boettner"
                    },
                    {
                        "name": "G. Boileau"
                    },
                    {
                        "name": "M. Boldrini"
                    },
                    {
                        "name": "G. N. Bolingbroke"
                    },
                    {
                        "name": "A. Bolliand"
                    },
                    {
                        "name": "L. D. Bonavena"
                    },
                    {
                        "name": "R. Bondarescu"
                    },
                    {
                        "name": "F. Bondu"
                    },
                    {
                        "name": "E. Bonilla"
                    },
                    {
                        "name": "M. S. Bonilla"
                    },
                    {
                        "name": "A. Bonino"
                    },
                    {
                        "name": "R. Bonnand"
                    },
                    {
                        "name": "A. Borchers"
                    },
                    {
                        "name": "S. Borhanian"
                    },
                    {
                        "name": "V. Boschi"
                    },
                    {
                        "name": "S. Bose"
                    },
                    {
                        "name": "V. Bossilkov"
                    },
                    {
                        "name": "Y. Bothra"
                    },
                    {
                        "name": "A. Boudon"
                    },
                    {
                        "name": "L. Bourg"
                    },
                    {
                        "name": "G. Bouyer"
                    },
                    {
                        "name": "M. Boyle"
                    },
                    {
                        "name": "A. Bozzi"
                    },
                    {
                        "name": "C. Bradaschia"
                    },
                    {
                        "name": "P. R. Brady"
                    },
                    {
                        "name": "A. Branch"
                    },
                    {
                        "name": "M. Branchesi"
                    },
                    {
                        "name": "I. Braun"
                    },
                    {
                        "name": "T. Briant"
                    },
                    {
                        "name": "A. Brillet"
                    },
                    {
                        "name": "M. Brinkmann"
                    },
                    {
                        "name": "P. Brockill"
                    },
                    {
                        "name": "E. Brockmueller"
                    },
                    {
                        "name": "A. F. Brooks"
                    },
                    {
                        "name": "B. C. Brown"
                    },
                    {
                        "name": "D. D. Brown"
                    },
                    {
                        "name": "M. L. Brozzetti"
                    },
                    {
                        "name": "S. Brunett"
                    },
                    {
                        "name": "G. Bruno"
                    },
                    {
                        "name": "R. Bruntz"
                    },
                    {
                        "name": "J. Bryant"
                    },
                    {
                        "name": "Y. Bu"
                    },
                    {
                        "name": "F. Bucci"
                    },
                    {
                        "name": "J. Buchanan"
                    },
                    {
                        "name": "O. Bulashenko"
                    },
                    {
                        "name": "T. Bulik"
                    },
                    {
                        "name": "H. J. Bulten"
                    },
                    {
                        "name": "A. Buonanno"
                    },
                    {
                        "name": "K. Burtnyk"
                    },
                    {
                        "name": "R. Buscicchio"
                    },
                    {
                        "name": "D. Buskulic"
                    },
                    {
                        "name": "C. Buy"
                    },
                    {
                        "name": "R. L. Byer"
                    },
                    {
                        "name": "G. S. Cabourn Davies"
                    },
                    {
                        "name": "R. Cabrita"
                    },
                    {
                        "name": "V. C√°ceres-Barbosa"
                    },
                    {
                        "name": "L. Cadonati"
                    },
                    {
                        "name": "G. Cagnoli"
                    },
                    {
                        "name": "C. Cahillane"
                    },
                    {
                        "name": "A. Calafat"
                    },
                    {
                        "name": "T. A. Callister"
                    },
                    {
                        "name": "E. Calloni"
                    },
                    {
                        "name": "S. R. Callos"
                    },
                    {
                        "name": "G. Caneva Santoro"
                    },
                    {
                        "name": "K. C. Cannon"
                    },
                    {
                        "name": "H. Cao"
                    },
                    {
                        "name": "L. A. Capistran"
                    },
                    {
                        "name": "E. Capocasa"
                    },
                    {
                        "name": "E. Capote"
                    },
                    {
                        "name": "G. Capurri"
                    },
                    {
                        "name": "G. Carapella"
                    },
                    {
                        "name": "F. Carbognani"
                    },
                    {
                        "name": "M. Carlassara"
                    },
                    {
                        "name": "J. B. Carlin"
                    },
                    {
                        "name": "T. K. Carlson"
                    },
                    {
                        "name": "M. F. Carney"
                    },
                    {
                        "name": "M. Carpinelli"
                    },
                    {
                        "name": "G. Carrillo"
                    },
                    {
                        "name": "J. J. Carter"
                    },
                    {
                        "name": "G. Carullo"
                    },
                    {
                        "name": "A. Casallas-Lagos"
                    },
                    {
                        "name": "J. Casanueva Diaz"
                    },
                    {
                        "name": "C. Casentini"
                    },
                    {
                        "name": "S. Y. Castro-Lucas"
                    },
                    {
                        "name": "S. Caudill"
                    },
                    {
                        "name": "M. Cavagli√†"
                    },
                    {
                        "name": "R. Cavalieri"
                    },
                    {
                        "name": "A. Ceja"
                    },
                    {
                        "name": "G. Cella"
                    },
                    {
                        "name": "P. Cerd√°-Dur√°n"
                    },
                    {
                        "name": "E. Cesarini"
                    },
                    {
                        "name": "N. Chabbra"
                    },
                    {
                        "name": "W. Chaibi"
                    },
                    {
                        "name": "A. Chakraborty"
                    },
                    {
                        "name": "P. Chakraborty"
                    },
                    {
                        "name": "S. Chakraborty"
                    },
                    {
                        "name": "S. Chalathadka Subrahmanya"
                    },
                    {
                        "name": "J. C. L. Chan"
                    },
                    {
                        "name": "M. Chan"
                    },
                    {
                        "name": "K. Chang"
                    },
                    {
                        "name": "S. Chao"
                    },
                    {
                        "name": "P. Charlton"
                    },
                    {
                        "name": "E. Chassande-Mottin"
                    },
                    {
                        "name": "C. Chatterjee"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "M. Chaturvedi"
                    },
                    {
                        "name": "S. Chaty"
                    },
                    {
                        "name": "K. Chatziioannou"
                    },
                    {
                        "name": "A. Chen"
                    },
                    {
                        "name": "A. H. -Y. Chen"
                    },
                    {
                        "name": "D. Chen"
                    },
                    {
                        "name": "H. Chen"
                    },
                    {
                        "name": "H. Y. Chen"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "Yanbei Chen"
                    },
                    {
                        "name": "Yitian Chen"
                    },
                    {
                        "name": "H. P. Cheng"
                    },
                    {
                        "name": "P. Chessa"
                    },
                    {
                        "name": "H. T. Cheung"
                    },
                    {
                        "name": "S. Y. Cheung"
                    },
                    {
                        "name": "F. Chiadini"
                    },
                    {
                        "name": "G. Chiarini"
                    },
                    {
                        "name": "A. Chiba"
                    },
                    {
                        "name": "A. Chincarini"
                    },
                    {
                        "name": "M. L. Chiofalo"
                    },
                    {
                        "name": "A. Chiummo"
                    },
                    {
                        "name": "C. Chou"
                    },
                    {
                        "name": "S. Choudhary"
                    },
                    {
                        "name": "N. Christensen"
                    },
                    {
                        "name": "S. S. Y. Chua"
                    },
                    {
                        "name": "G. Ciani"
                    },
                    {
                        "name": "P. Ciecielag"
                    },
                    {
                        "name": "M. Cie≈õlar"
                    },
                    {
                        "name": "M. Cifaldi"
                    },
                    {
                        "name": "B. Cirok"
                    },
                    {
                        "name": "F. Clara"
                    },
                    {
                        "name": "J. A. Clark"
                    },
                    {
                        "name": "T. A. Clarke"
                    },
                    {
                        "name": "P. Clearwater"
                    },
                    {
                        "name": "S. Clesse"
                    },
                    {
                        "name": "F. Cleva"
                    },
                    {
                        "name": "E. Coccia"
                    },
                    {
                        "name": "E. Codazzo"
                    },
                    {
                        "name": "P. -F. Cohadon"
                    },
                    {
                        "name": "S. Colace"
                    },
                    {
                        "name": "E. Colangeli"
                    },
                    {
                        "name": "M. Colleoni"
                    },
                    {
                        "name": "C. G. Collette"
                    },
                    {
                        "name": "J. Collins"
                    },
                    {
                        "name": "S. Colloms"
                    },
                    {
                        "name": "A. Colombo"
                    },
                    {
                        "name": "C. M. Compton"
                    },
                    {
                        "name": "G. Connolly"
                    },
                    {
                        "name": "L. Conti"
                    },
                    {
                        "name": "T. R. Corbitt"
                    },
                    {
                        "name": "I. Cordero-Carri√≥n"
                    },
                    {
                        "name": "S. Corezzi"
                    },
                    {
                        "name": "N. J. Cornish"
                    },
                    {
                        "name": "I. Coronado"
                    },
                    {
                        "name": "A. Corsi"
                    },
                    {
                        "name": "R. Cottingham"
                    },
                    {
                        "name": "M. W. Coughlin"
                    },
                    {
                        "name": "A. Couineaux"
                    },
                    {
                        "name": "P. Couvares"
                    },
                    {
                        "name": "D. M. Coward"
                    },
                    {
                        "name": "R. Coyne"
                    },
                    {
                        "name": "A. Cozzumbo"
                    },
                    {
                        "name": "J. D. E. Creighton"
                    },
                    {
                        "name": "T. D. Creighton"
                    },
                    {
                        "name": "P. Cremonese"
                    },
                    {
                        "name": "S. Crook"
                    },
                    {
                        "name": "R. Crouch"
                    },
                    {
                        "name": "J. Csizmazia"
                    },
                    {
                        "name": "J. R. Cudell"
                    },
                    {
                        "name": "T. J. Cullen"
                    },
                    {
                        "name": "A. Cumming"
                    },
                    {
                        "name": "E. Cuoco"
                    },
                    {
                        "name": "M. Cusinato"
                    },
                    {
                        "name": "L. V. Da Concei√ß√£o"
                    },
                    {
                        "name": "T. Dal Canton"
                    },
                    {
                        "name": "S. Dal Pra"
                    },
                    {
                        "name": "G. D√°lya"
                    },
                    {
                        "name": "B. D'Angelo"
                    },
                    {
                        "name": "S. Danilishin"
                    },
                    {
                        "name": "S. D'Antonio"
                    },
                    {
                        "name": "K. Danzmann"
                    },
                    {
                        "name": "K. E. Darroch"
                    },
                    {
                        "name": "L. P. Dartez"
                    },
                    {
                        "name": "R. Das"
                    },
                    {
                        "name": "A. Dasgupta"
                    },
                    {
                        "name": "V. Dattilo"
                    },
                    {
                        "name": "A. Daumas"
                    },
                    {
                        "name": "N. Davari"
                    },
                    {
                        "name": "I. Dave"
                    },
                    {
                        "name": "A. Davenport"
                    },
                    {
                        "name": "M. Davier"
                    },
                    {
                        "name": "T. F. Davies"
                    },
                    {
                        "name": "D. Davis"
                    },
                    {
                        "name": "L. Davis"
                    },
                    {
                        "name": "M. C. Davis"
                    },
                    {
                        "name": "P. Davis"
                    },
                    {
                        "name": "E. J. Daw"
                    },
                    {
                        "name": "M. Dax"
                    },
                    {
                        "name": "J. De Bolle"
                    },
                    {
                        "name": "M. Deenadayalan"
                    },
                    {
                        "name": "J. Degallaix"
                    },
                    {
                        "name": "M. De Laurentis"
                    },
                    {
                        "name": "F. De Lillo"
                    },
                    {
                        "name": "S. Della Torre"
                    },
                    {
                        "name": "W. Del Pozzo"
                    },
                    {
                        "name": "A. Demagny"
                    },
                    {
                        "name": "F. De Marco"
                    },
                    {
                        "name": "G. Demasi"
                    },
                    {
                        "name": "F. De Matteis"
                    },
                    {
                        "name": "N. Demos"
                    },
                    {
                        "name": "T. Dent"
                    },
                    {
                        "name": "A. Depasse"
                    },
                    {
                        "name": "N. DePergola"
                    },
                    {
                        "name": "R. De Pietri"
                    },
                    {
                        "name": "R. De Rosa"
                    },
                    {
                        "name": "C. De Rossi"
                    },
                    {
                        "name": "M. Desai"
                    },
                    {
                        "name": "R. DeSalvo"
                    },
                    {
                        "name": "A. DeSimone"
                    },
                    {
                        "name": "R. De Simone"
                    },
                    {
                        "name": "A. Dhani"
                    },
                    {
                        "name": "R. Diab"
                    },
                    {
                        "name": "M. C. D√≠az"
                    },
                    {
                        "name": "M. Di Cesare"
                    },
                    {
                        "name": "G. Dideron"
                    },
                    {
                        "name": "T. Dietrich"
                    },
                    {
                        "name": "L. Di Fiore"
                    },
                    {
                        "name": "C. Di Fronzo"
                    },
                    {
                        "name": "M. Di Giovanni"
                    },
                    {
                        "name": "T. Di Girolamo"
                    },
                    {
                        "name": "D. Diksha"
                    },
                    {
                        "name": "J. Ding"
                    },
                    {
                        "name": "S. Di Pace"
                    },
                    {
                        "name": "I. Di Palma"
                    },
                    {
                        "name": "D. Di Piero"
                    },
                    {
                        "name": "F. Di Renzo"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "A. Dmitriev"
                    },
                    {
                        "name": "J. P. Docherty"
                    },
                    {
                        "name": "Z. Doctor"
                    },
                    {
                        "name": "N. Doerksen"
                    },
                    {
                        "name": "E. Dohmen"
                    },
                    {
                        "name": "A. Doke"
                    },
                    {
                        "name": "A. Domiciano De Souza"
                    },
                    {
                        "name": "L. D'Onofrio"
                    },
                    {
                        "name": "F. Donovan"
                    },
                    {
                        "name": "K. L. Dooley"
                    },
                    {
                        "name": "T. Dooney"
                    },
                    {
                        "name": "S. Doravari"
                    },
                    {
                        "name": "O. Dorosh"
                    },
                    {
                        "name": "W. J. D. Doyle"
                    },
                    {
                        "name": "M. Drago"
                    },
                    {
                        "name": "J. C. Driggers"
                    },
                    {
                        "name": "L. Dunn"
                    },
                    {
                        "name": "U. Dupletsa"
                    },
                    {
                        "name": "D. D'Urso"
                    },
                    {
                        "name": "P. Dutta Roy"
                    },
                    {
                        "name": "H. Duval"
                    },
                    {
                        "name": "S. E. Dwyer"
                    },
                    {
                        "name": "C. Eassa"
                    },
                    {
                        "name": "M. Ebersold"
                    },
                    {
                        "name": "T. Eckhardt"
                    },
                    {
                        "name": "G. Eddolls"
                    },
                    {
                        "name": "A. Effler"
                    },
                    {
                        "name": "J. Eichholz"
                    },
                    {
                        "name": "H. Einsle"
                    },
                    {
                        "name": "M. Eisenmann"
                    },
                    {
                        "name": "M. Emma"
                    },
                    {
                        "name": "K. Endo"
                    },
                    {
                        "name": "R. Enficiaud"
                    },
                    {
                        "name": "L. Errico"
                    },
                    {
                        "name": "R. Espinosa"
                    },
                    {
                        "name": "M. C. Espitia"
                    },
                    {
                        "name": "M. Esposito"
                    },
                    {
                        "name": "R. C. Essick"
                    },
                    {
                        "name": "H. Estell√©s"
                    },
                    {
                        "name": "T. Etzel"
                    },
                    {
                        "name": "M. Evans"
                    },
                    {
                        "name": "T. Evstafyeva"
                    },
                    {
                        "name": "B. E. Ewing"
                    },
                    {
                        "name": "J. M. Ezquiaga"
                    },
                    {
                        "name": "F. Fabrizi"
                    },
                    {
                        "name": "V. Fafone"
                    },
                    {
                        "name": "S. Fairhurst"
                    },
                    {
                        "name": "A. M. Farah"
                    },
                    {
                        "name": "B. Farr"
                    },
                    {
                        "name": "W. M. Farr"
                    },
                    {
                        "name": "G. Favaro"
                    },
                    {
                        "name": "M. Favata"
                    },
                    {
                        "name": "M. Fays"
                    },
                    {
                        "name": "M. Fazio"
                    },
                    {
                        "name": "J. Feicht"
                    },
                    {
                        "name": "M. M. Fejer"
                    },
                    {
                        "name": "R. Felicetti"
                    },
                    {
                        "name": "E. Fenyvesi"
                    },
                    {
                        "name": "J. Fernandes"
                    },
                    {
                        "name": "T. Fernandes"
                    },
                    {
                        "name": "D. Fernando"
                    },
                    {
                        "name": "S. Ferraiuolo"
                    },
                    {
                        "name": "T. A. Ferreira"
                    },
                    {
                        "name": "F. Fidecaro"
                    },
                    {
                        "name": "P. Figura"
                    },
                    {
                        "name": "A. Fiori"
                    },
                    {
                        "name": "I. Fiori"
                    },
                    {
                        "name": "M. Fishbach"
                    },
                    {
                        "name": "R. P. Fisher"
                    },
                    {
                        "name": "R. Fittipaldi"
                    },
                    {
                        "name": "V. Fiumara"
                    },
                    {
                        "name": "R. Flaminio"
                    },
                    {
                        "name": "S. M. Fleischer"
                    },
                    {
                        "name": "L. S. Fleming"
                    },
                    {
                        "name": "E. Floden"
                    },
                    {
                        "name": "H. Fong"
                    },
                    {
                        "name": "J. A. Font"
                    },
                    {
                        "name": "F. Fontinele-Nunes"
                    },
                    {
                        "name": "C. Foo"
                    },
                    {
                        "name": "B. Fornal"
                    },
                    {
                        "name": "K. Franceschetti"
                    },
                    {
                        "name": "F. Frappez"
                    },
                    {
                        "name": "S. Frasca"
                    },
                    {
                        "name": "F. Frasconi"
                    },
                    {
                        "name": "J. P. Freed"
                    },
                    {
                        "name": "Z. Frei"
                    },
                    {
                        "name": "A. Freise"
                    },
                    {
                        "name": "O. Freitas"
                    },
                    {
                        "name": "R. Frey"
                    },
                    {
                        "name": "W. Frischhertz"
                    },
                    {
                        "name": "P. Fritschel"
                    },
                    {
                        "name": "V. V. Frolov"
                    },
                    {
                        "name": "G. G. Fronz√©"
                    },
                    {
                        "name": "M. Fuentes-Garcia"
                    },
                    {
                        "name": "S. Fujii"
                    },
                    {
                        "name": "T. Fujimori"
                    },
                    {
                        "name": "P. Fulda"
                    },
                    {
                        "name": "M. Fyffe"
                    },
                    {
                        "name": "B. Gadre"
                    },
                    {
                        "name": "J. R. Gair"
                    },
                    {
                        "name": "S. Galaudage"
                    },
                    {
                        "name": "V. Galdi"
                    },
                    {
                        "name": "R. Gamba"
                    },
                    {
                        "name": "A. Gamboa"
                    },
                    {
                        "name": "S. Gamoji"
                    },
                    {
                        "name": "D. Ganapathy"
                    },
                    {
                        "name": "A. Ganguly"
                    },
                    {
                        "name": "B. Garaventa"
                    },
                    {
                        "name": "J. Garc√≠a-Bellido"
                    },
                    {
                        "name": "C. Garc√≠a-Quir√≥s"
                    },
                    {
                        "name": "J. W. Gardner"
                    },
                    {
                        "name": "K. A. Gardner"
                    },
                    {
                        "name": "S. Garg"
                    },
                    {
                        "name": "J. Gargiulo"
                    },
                    {
                        "name": "X. Garrido"
                    },
                    {
                        "name": "A. Garron"
                    },
                    {
                        "name": "F. Garufi"
                    },
                    {
                        "name": "P. A. Garver"
                    },
                    {
                        "name": "C. Gasbarra"
                    },
                    {
                        "name": "B. Gateley"
                    },
                    {
                        "name": "F. Gautier"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "T. Gayer"
                    },
                    {
                        "name": "G. Gemme"
                    },
                    {
                        "name": "A. Gennai"
                    },
                    {
                        "name": "V. Gennari"
                    },
                    {
                        "name": "J. George"
                    },
                    {
                        "name": "R. George"
                    },
                    {
                        "name": "O. Gerberding"
                    },
                    {
                        "name": "L. Gergely"
                    },
                    {
                        "name": "Archisman Ghosh"
                    },
                    {
                        "name": "Sayantan Ghosh"
                    },
                    {
                        "name": "Shaon Ghosh"
                    },
                    {
                        "name": "Shrobana Ghosh"
                    },
                    {
                        "name": "Suprovo Ghosh"
                    },
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "J. A. Giaime"
                    },
                    {
                        "name": "K. D. Giardina"
                    },
                    {
                        "name": "D. R. Gibson"
                    },
                    {
                        "name": "C. Gier"
                    },
                    {
                        "name": "S. Gkaitatzis"
                    },
                    {
                        "name": "J. Glanzer"
                    },
                    {
                        "name": "F. Glotin"
                    },
                    {
                        "name": "J. Godfrey"
                    },
                    {
                        "name": "R. V. Godley"
                    },
                    {
                        "name": "P. Godwin"
                    },
                    {
                        "name": "A. S. Goettel"
                    },
                    {
                        "name": "E. Goetz"
                    },
                    {
                        "name": "J. Golomb"
                    },
                    {
                        "name": "S. Gomez Lopez"
                    },
                    {
                        "name": "B. Goncharov"
                    },
                    {
                        "name": "G. Gonz√°lez"
                    },
                    {
                        "name": "P. Goodarzi"
                    },
                    {
                        "name": "S. Goode"
                    },
                    {
                        "name": "A. W. Goodwin-Jones"
                    },
                    {
                        "name": "M. Gosselin"
                    },
                    {
                        "name": "R. Gouaty"
                    },
                    {
                        "name": "D. W. Gould"
                    },
                    {
                        "name": "K. Govorkova"
                    },
                    {
                        "name": "A. Grado"
                    },
                    {
                        "name": "V. Graham"
                    },
                    {
                        "name": "A. E. Granados"
                    },
                    {
                        "name": "M. Granata"
                    },
                    {
                        "name": "V. Granata"
                    },
                    {
                        "name": "S. Gras"
                    },
                    {
                        "name": "P. Grassia"
                    },
                    {
                        "name": "J. Graves"
                    },
                    {
                        "name": "C. Gray"
                    },
                    {
                        "name": "R. Gray"
                    },
                    {
                        "name": "G. Greco"
                    },
                    {
                        "name": "A. C. Green"
                    },
                    {
                        "name": "L. Green"
                    },
                    {
                        "name": "S. M. Green"
                    },
                    {
                        "name": "S. R. Green"
                    },
                    {
                        "name": "C. Greenberg"
                    },
                    {
                        "name": "A. M. Gretarsson"
                    },
                    {
                        "name": "H. K. Griffin"
                    },
                    {
                        "name": "D. Griffith"
                    },
                    {
                        "name": "H. L. Griggs"
                    },
                    {
                        "name": "G. Grignani"
                    },
                    {
                        "name": "C. Grimaud"
                    },
                    {
                        "name": "H. Grote"
                    },
                    {
                        "name": "S. Grunewald"
                    },
                    {
                        "name": "D. Guerra"
                    },
                    {
                        "name": "D. Guetta"
                    },
                    {
                        "name": "G. M. Guidi"
                    },
                    {
                        "name": "A. R. Guimaraes"
                    },
                    {
                        "name": "H. K. Gulati"
                    },
                    {
                        "name": "F. Gulminelli"
                    },
                    {
                        "name": "H. Guo"
                    },
                    {
                        "name": "W. Guo"
                    },
                    {
                        "name": "Y. Guo"
                    },
                    {
                        "name": "Anuradha Gupta"
                    },
                    {
                        "name": "I. Gupta"
                    },
                    {
                        "name": "N. C. Gupta"
                    },
                    {
                        "name": "S. K. Gupta"
                    },
                    {
                        "name": "V. Gupta"
                    },
                    {
                        "name": "N. Gupte"
                    },
                    {
                        "name": "J. Gurs"
                    },
                    {
                        "name": "N. Gutierrez"
                    },
                    {
                        "name": "N. Guttman"
                    },
                    {
                        "name": "F. Guzman"
                    },
                    {
                        "name": "D. Haba"
                    },
                    {
                        "name": "M. Haberland"
                    },
                    {
                        "name": "S. Haino"
                    },
                    {
                        "name": "E. D. Hall"
                    },
                    {
                        "name": "E. Z. Hamilton"
                    },
                    {
                        "name": "G. Hammond"
                    },
                    {
                        "name": "M. Haney"
                    },
                    {
                        "name": "J. Hanks"
                    },
                    {
                        "name": "C. Hanna"
                    },
                    {
                        "name": "M. D. Hannam"
                    },
                    {
                        "name": "O. A. Hannuksela"
                    },
                    {
                        "name": "A. G. Hanselman"
                    },
                    {
                        "name": "H. Hansen"
                    },
                    {
                        "name": "J. Hanson"
                    },
                    {
                        "name": "S. Hanumasagar"
                    },
                    {
                        "name": "R. Harada"
                    },
                    {
                        "name": "A. R. Hardison"
                    },
                    {
                        "name": "S. Harikumar"
                    },
                    {
                        "name": "K. Haris"
                    },
                    {
                        "name": "I. Harley-Trochimczyk"
                    },
                    {
                        "name": "T. Harmark"
                    },
                    {
                        "name": "J. Harms"
                    },
                    {
                        "name": "G. M. Harry"
                    },
                    {
                        "name": "I. W. Harry"
                    },
                    {
                        "name": "J. Hart"
                    },
                    {
                        "name": "B. Haskell"
                    },
                    {
                        "name": "C. J. Haster"
                    },
                    {
                        "name": "K. Haughian"
                    },
                    {
                        "name": "H. Hayakawa"
                    },
                    {
                        "name": "K. Hayama"
                    },
                    {
                        "name": "M. C. Heintze"
                    },
                    {
                        "name": "J. Heinze"
                    },
                    {
                        "name": "J. Heinzel"
                    },
                    {
                        "name": "H. Heitmann"
                    },
                    {
                        "name": "F. Hellman"
                    },
                    {
                        "name": "A. F. Helmling-Cornell"
                    },
                    {
                        "name": "G. Hemming"
                    },
                    {
                        "name": "O. Henderson-Sapir"
                    },
                    {
                        "name": "M. Hendry"
                    },
                    {
                        "name": "I. S. Heng"
                    },
                    {
                        "name": "M. H. Hennig"
                    },
                    {
                        "name": "C. Henshaw"
                    },
                    {
                        "name": "M. Heurs"
                    },
                    {
                        "name": "A. L. Hewitt"
                    },
                    {
                        "name": "J. Heynen"
                    },
                    {
                        "name": "J. Heyns"
                    },
                    {
                        "name": "S. Higginbotham"
                    },
                    {
                        "name": "S. Hild"
                    },
                    {
                        "name": "S. Hill"
                    },
                    {
                        "name": "Y. Himemoto"
                    },
                    {
                        "name": "N. Hirata"
                    },
                    {
                        "name": "C. Hirose"
                    },
                    {
                        "name": "D. Hofman"
                    },
                    {
                        "name": "B. E. Hogan"
                    },
                    {
                        "name": "N. A. Holland"
                    },
                    {
                        "name": "I. J. Hollows"
                    },
                    {
                        "name": "D. E. Holz"
                    },
                    {
                        "name": "L. Honet"
                    },
                    {
                        "name": "D. J. Horton-Bailey"
                    },
                    {
                        "name": "J. Hough"
                    },
                    {
                        "name": "S. Hourihane"
                    },
                    {
                        "name": "N. T. Howard"
                    },
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "C. G. Hoy"
                    },
                    {
                        "name": "C. A. Hrishikesh"
                    },
                    {
                        "name": "P. Hsi"
                    },
                    {
                        "name": "H. -F. Hsieh"
                    },
                    {
                        "name": "H. -Y. Hsieh"
                    },
                    {
                        "name": "C. Hsiung"
                    },
                    {
                        "name": "S. -H. Hsu"
                    },
                    {
                        "name": "W. -F. Hsu"
                    },
                    {
                        "name": "Q. Hu"
                    },
                    {
                        "name": "H. Y. Huang"
                    },
                    {
                        "name": "Y. Huang"
                    },
                    {
                        "name": "Y. T. Huang"
                    },
                    {
                        "name": "A. D. Huddart"
                    },
                    {
                        "name": "B. Hughey"
                    },
                    {
                        "name": "V. Hui"
                    },
                    {
                        "name": "S. Husa"
                    },
                    {
                        "name": "R. Huxford"
                    },
                    {
                        "name": "L. Iampieri"
                    },
                    {
                        "name": "G. A. Iandolo"
                    },
                    {
                        "name": "M. Ianni"
                    },
                    {
                        "name": "G. Iannone"
                    },
                    {
                        "name": "J. Iascau"
                    },
                    {
                        "name": "K. Ide"
                    },
                    {
                        "name": "R. Iden"
                    },
                    {
                        "name": "A. Ierardi"
                    },
                    {
                        "name": "S. Ikeda"
                    },
                    {
                        "name": "H. Imafuku"
                    },
                    {
                        "name": "Y. Inoue"
                    },
                    {
                        "name": "G. Iorio"
                    },
                    {
                        "name": "P. Iosif"
                    },
                    {
                        "name": "M. H. Iqbal"
                    },
                    {
                        "name": "J. Irwin"
                    },
                    {
                        "name": "R. Ishikawa"
                    },
                    {
                        "name": "M. Isi"
                    },
                    {
                        "name": "K. S. Isleif"
                    },
                    {
                        "name": "Y. Itoh"
                    },
                    {
                        "name": "M. Iwaya"
                    },
                    {
                        "name": "B. R. Iyer"
                    },
                    {
                        "name": "C. Jacquet"
                    },
                    {
                        "name": "P. -E. Jacquet"
                    },
                    {
                        "name": "T. Jacquot"
                    },
                    {
                        "name": "S. J. Jadhav"
                    },
                    {
                        "name": "S. P. Jadhav"
                    },
                    {
                        "name": "M. Jain"
                    },
                    {
                        "name": "T. Jain"
                    },
                    {
                        "name": "A. L. James"
                    },
                    {
                        "name": "K. Jani"
                    },
                    {
                        "name": "J. Janquart"
                    },
                    {
                        "name": "K. Janssens"
                    },
                    {
                        "name": "N. N. Janthalur"
                    },
                    {
                        "name": "S. Jaraba"
                    },
                    {
                        "name": "P. Jaranowski"
                    },
                    {
                        "name": "R. Jaume"
                    },
                    {
                        "name": "W. Javed"
                    },
                    {
                        "name": "A. Jennings"
                    },
                    {
                        "name": "M. Jensen"
                    },
                    {
                        "name": "W. Jia"
                    },
                    {
                        "name": "J. Jiang"
                    },
                    {
                        "name": "H. -B. Jin"
                    },
                    {
                        "name": "G. R. Johns"
                    },
                    {
                        "name": "N. A. Johnson"
                    },
                    {
                        "name": "M. C. Johnston"
                    },
                    {
                        "name": "R. Johnston"
                    },
                    {
                        "name": "N. Johny"
                    },
                    {
                        "name": "D. H. Jones"
                    },
                    {
                        "name": "D. I. Jones"
                    },
                    {
                        "name": "R. Jones"
                    },
                    {
                        "name": "H. E. Jose"
                    },
                    {
                        "name": "P. Joshi"
                    },
                    {
                        "name": "S. K. Joshi"
                    },
                    {
                        "name": "G. Joubert"
                    },
                    {
                        "name": "J. Ju"
                    },
                    {
                        "name": "L. Ju"
                    },
                    {
                        "name": "K. Jung"
                    },
                    {
                        "name": "J. Junker"
                    },
                    {
                        "name": "V. Juste"
                    },
                    {
                        "name": "H. B. Kabagoz"
                    },
                    {
                        "name": "T. Kajita"
                    },
                    {
                        "name": "I. Kaku"
                    },
                    {
                        "name": "V. Kalogera"
                    },
                    {
                        "name": "M. Kalomenopoulos"
                    },
                    {
                        "name": "M. Kamiizumi"
                    },
                    {
                        "name": "N. Kanda"
                    },
                    {
                        "name": "S. Kandhasamy"
                    },
                    {
                        "name": "G. Kang"
                    },
                    {
                        "name": "N. C. Kannachel"
                    },
                    {
                        "name": "J. B. Kanner"
                    },
                    {
                        "name": "S. A. KantiMahanty"
                    },
                    {
                        "name": "S. J. Kapadia"
                    },
                    {
                        "name": "D. P. Kapasi"
                    },
                    {
                        "name": "M. Karthikeyan"
                    },
                    {
                        "name": "M. Kasprzack"
                    },
                    {
                        "name": "H. Kato"
                    },
                    {
                        "name": "T. Kato"
                    },
                    {
                        "name": "E. Katsavounidis"
                    },
                    {
                        "name": "W. Katzman"
                    },
                    {
                        "name": "R. Kaushik"
                    },
                    {
                        "name": "K. Kawabe"
                    },
                    {
                        "name": "R. Kawamoto"
                    },
                    {
                        "name": "D. Keitel"
                    },
                    {
                        "name": "L. J. Kemperman"
                    },
                    {
                        "name": "J. Kennington"
                    },
                    {
                        "name": "F. A. Kerkow"
                    },
                    {
                        "name": "R. Kesharwani"
                    },
                    {
                        "name": "J. S. Key"
                    },
                    {
                        "name": "R. Khadela"
                    },
                    {
                        "name": "S. Khadka"
                    },
                    {
                        "name": "S. S. Khadkikar"
                    },
                    {
                        "name": "F. Y. Khalili"
                    },
                    {
                        "name": "F. Khan"
                    },
                    {
                        "name": "T. Khanam"
                    },
                    {
                        "name": "M. Khursheed"
                    },
                    {
                        "name": "N. M. Khusid"
                    },
                    {
                        "name": "W. Kiendrebeogo"
                    },
                    {
                        "name": "N. Kijbunchoo"
                    },
                    {
                        "name": "C. Kim"
                    },
                    {
                        "name": "J. C. Kim"
                    },
                    {
                        "name": "K. Kim"
                    },
                    {
                        "name": "M. H. Kim"
                    },
                    {
                        "name": "S. Kim"
                    },
                    {
                        "name": "Y. -M. Kim"
                    },
                    {
                        "name": "C. Kimball"
                    },
                    {
                        "name": "K. Kimes"
                    },
                    {
                        "name": "M. Kinnear"
                    },
                    {
                        "name": "J. S. Kissel"
                    },
                    {
                        "name": "S. Klimenko"
                    },
                    {
                        "name": "A. M. Knee"
                    },
                    {
                        "name": "E. J. Knox"
                    },
                    {
                        "name": "N. Knust"
                    },
                    {
                        "name": "K. Kobayashi"
                    },
                    {
                        "name": "S. M. Koehlenbeck"
                    },
                    {
                        "name": "G. Koekoek"
                    },
                    {
                        "name": "K. Kohri"
                    },
                    {
                        "name": "K. Kokeyama"
                    },
                    {
                        "name": "S. Koley"
                    },
                    {
                        "name": "P. Kolitsidou"
                    },
                    {
                        "name": "A. E. Koloniari"
                    },
                    {
                        "name": "K. Komori"
                    },
                    {
                        "name": "A. K. H. Kong"
                    },
                    {
                        "name": "A. Kontos"
                    },
                    {
                        "name": "L. M. Koponen"
                    },
                    {
                        "name": "M. Korobko"
                    },
                    {
                        "name": "X. Kou"
                    },
                    {
                        "name": "A. Koushik"
                    },
                    {
                        "name": "N. Kouvatsos"
                    },
                    {
                        "name": "M. Kovalam"
                    },
                    {
                        "name": "T. Koyama"
                    },
                    {
                        "name": "D. B. Kozak"
                    },
                    {
                        "name": "S. L. Kranzhoff"
                    },
                    {
                        "name": "V. Kringel"
                    },
                    {
                        "name": "N. V. Krishnendu"
                    },
                    {
                        "name": "S. Kroker"
                    },
                    {
                        "name": "A. Kr√≥lak"
                    },
                    {
                        "name": "K. Kruska"
                    },
                    {
                        "name": "J. Kubisz"
                    },
                    {
                        "name": "G. Kuehn"
                    },
                    {
                        "name": "S. Kulkarni"
                    },
                    {
                        "name": "A. Kulur Ramamohan"
                    },
                    {
                        "name": "Achal Kumar"
                    },
                    {
                        "name": "Anil Kumar"
                    },
                    {
                        "name": "Praveen Kumar"
                    },
                    {
                        "name": "Prayush Kumar"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Rakesh Kumar"
                    },
                    {
                        "name": "J. Kume"
                    },
                    {
                        "name": "K. Kuns"
                    },
                    {
                        "name": "N. Kuntimaddi"
                    },
                    {
                        "name": "S. Kuroyanagi"
                    },
                    {
                        "name": "S. Kuwahara"
                    },
                    {
                        "name": "K. Kwak"
                    },
                    {
                        "name": "K. Kwan"
                    },
                    {
                        "name": "S. Kwon"
                    },
                    {
                        "name": "G. Lacaille"
                    },
                    {
                        "name": "D. Laghi"
                    },
                    {
                        "name": "A. H. Laity"
                    },
                    {
                        "name": "E. Lalande"
                    },
                    {
                        "name": "M. Lalleman"
                    },
                    {
                        "name": "P. C. Lalremruati"
                    },
                    {
                        "name": "M. Landry"
                    },
                    {
                        "name": "B. B. Lane"
                    },
                    {
                        "name": "R. N. Lang"
                    },
                    {
                        "name": "J. Lange"
                    },
                    {
                        "name": "R. Langgin"
                    },
                    {
                        "name": "B. Lantz"
                    },
                    {
                        "name": "I. La Rosa"
                    },
                    {
                        "name": "J. Larsen"
                    },
                    {
                        "name": "A. Lartaux-Vollard"
                    },
                    {
                        "name": "P. D. Lasky"
                    },
                    {
                        "name": "J. Lawrence"
                    },
                    {
                        "name": "M. Laxen"
                    },
                    {
                        "name": "C. Lazarte"
                    },
                    {
                        "name": "A. Lazzarini"
                    },
                    {
                        "name": "C. Lazzaro"
                    },
                    {
                        "name": "P. Leaci"
                    },
                    {
                        "name": "L. Leali"
                    },
                    {
                        "name": "Y. K. Lecoeuche"
                    },
                    {
                        "name": "H. M. Lee"
                    },
                    {
                        "name": "H. W. Lee"
                    },
                    {
                        "name": "J. Lee"
                    },
                    {
                        "name": "K. Lee"
                    },
                    {
                        "name": "R. -K. Lee"
                    },
                    {
                        "name": "R. Lee"
                    },
                    {
                        "name": "Sungho Lee"
                    },
                    {
                        "name": "Sunjae Lee"
                    },
                    {
                        "name": "Y. Lee"
                    },
                    {
                        "name": "I. N. Legred"
                    },
                    {
                        "name": "J. Lehmann"
                    },
                    {
                        "name": "L. Lehner"
                    },
                    {
                        "name": "M. Le Jean"
                    },
                    {
                        "name": "A. Lema√Ætre"
                    },
                    {
                        "name": "M. Lenti"
                    },
                    {
                        "name": "M. Leonardi"
                    },
                    {
                        "name": "M. Lequime"
                    },
                    {
                        "name": "N. Leroy"
                    },
                    {
                        "name": "M. Lesovsky"
                    },
                    {
                        "name": "N. Letendre"
                    },
                    {
                        "name": "M. Lethuillier"
                    },
                    {
                        "name": "Y. Levin"
                    },
                    {
                        "name": "K. Leyde"
                    },
                    {
                        "name": "A. K. Y. Li"
                    },
                    {
                        "name": "K. L. Li"
                    },
                    {
                        "name": "T. G. F. Li"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Z. Li"
                    },
                    {
                        "name": "A. Lihos"
                    },
                    {
                        "name": "E. T. Lin"
                    },
                    {
                        "name": "F. Lin"
                    },
                    {
                        "name": "L. C. -C. Lin"
                    },
                    {
                        "name": "Y. -C. Lin"
                    },
                    {
                        "name": "C. Lindsay"
                    },
                    {
                        "name": "S. D. Linker"
                    },
                    {
                        "name": "A. Liu"
                    },
                    {
                        "name": "G. C. Liu"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "F. Llamas Villarreal"
                    },
                    {
                        "name": "J. Llobera-Querol"
                    },
                    {
                        "name": "R. K. L. Lo"
                    },
                    {
                        "name": "J. -P. Locquet"
                    },
                    {
                        "name": "S. C. G. Loggins"
                    },
                    {
                        "name": "M. R. Loizou"
                    },
                    {
                        "name": "L. T. London"
                    },
                    {
                        "name": "A. Longo"
                    },
                    {
                        "name": "D. Lopez"
                    },
                    {
                        "name": "M. Lopez Portilla"
                    },
                    {
                        "name": "M. Lorenzini"
                    },
                    {
                        "name": "A. Lorenzo-Medina"
                    },
                    {
                        "name": "V. Loriette"
                    },
                    {
                        "name": "M. Lormand"
                    },
                    {
                        "name": "G. Losurdo"
                    },
                    {
                        "name": "E. Lotti"
                    },
                    {
                        "name": "T. P. Lott IV"
                    },
                    {
                        "name": "J. D. Lough"
                    },
                    {
                        "name": "H. A. Loughlin"
                    },
                    {
                        "name": "C. O. Lousto"
                    },
                    {
                        "name": "N. Low"
                    },
                    {
                        "name": "N. Lu"
                    },
                    {
                        "name": "L. Lucchesi"
                    },
                    {
                        "name": "H. L√ºck"
                    },
                    {
                        "name": "D. Lumaca"
                    },
                    {
                        "name": "A. P. Lundgren"
                    },
                    {
                        "name": "A. W. Lussier"
                    },
                    {
                        "name": "R. Macas"
                    },
                    {
                        "name": "M. MacInnis"
                    },
                    {
                        "name": "D. M. Macleod"
                    },
                    {
                        "name": "I. A. O. MacMillan"
                    },
                    {
                        "name": "A. Macquet"
                    },
                    {
                        "name": "K. Maeda"
                    },
                    {
                        "name": "S. Maenaut"
                    },
                    {
                        "name": "S. S. Magare"
                    },
                    {
                        "name": "R. M. Magee"
                    },
                    {
                        "name": "E. Maggio"
                    },
                    {
                        "name": "R. Maggiore"
                    },
                    {
                        "name": "M. Magnozzi"
                    },
                    {
                        "name": "M. Mahesh"
                    },
                    {
                        "name": "M. Maini"
                    },
                    {
                        "name": "S. Majhi"
                    },
                    {
                        "name": "E. Majorana"
                    },
                    {
                        "name": "C. N. Makarem"
                    },
                    {
                        "name": "D. Malakar"
                    },
                    {
                        "name": "J. A. Malaquias-Reis"
                    },
                    {
                        "name": "U. Mali"
                    },
                    {
                        "name": "S. Maliakal"
                    },
                    {
                        "name": "A. Malik"
                    },
                    {
                        "name": "L. Mallick"
                    },
                    {
                        "name": "A. -K. Malz"
                    },
                    {
                        "name": "N. Man"
                    },
                    {
                        "name": "M. Mancarella"
                    },
                    {
                        "name": "V. Mandic"
                    },
                    {
                        "name": "V. Mangano"
                    },
                    {
                        "name": "B. Mannix"
                    },
                    {
                        "name": "G. L. Mansell"
                    },
                    {
                        "name": "M. Manske"
                    },
                    {
                        "name": "M. Mantovani"
                    },
                    {
                        "name": "M. Mapelli"
                    },
                    {
                        "name": "C. Marinelli"
                    },
                    {
                        "name": "F. Marion"
                    },
                    {
                        "name": "A. S. Markosyan"
                    },
                    {
                        "name": "A. Markowitz"
                    },
                    {
                        "name": "E. Maros"
                    },
                    {
                        "name": "S. Marsat"
                    },
                    {
                        "name": "F. Martelli"
                    },
                    {
                        "name": "I. W. Martin"
                    },
                    {
                        "name": "R. M. Martin"
                    },
                    {
                        "name": "B. B. Martinez"
                    },
                    {
                        "name": "D. A. Martinez"
                    },
                    {
                        "name": "M. Martinez"
                    },
                    {
                        "name": "V. Martinez"
                    },
                    {
                        "name": "A. Martini"
                    },
                    {
                        "name": "J. C. Martins"
                    },
                    {
                        "name": "D. V. Martynov"
                    },
                    {
                        "name": "E. J. Marx"
                    },
                    {
                        "name": "L. Massaro"
                    },
                    {
                        "name": "A. Masserot"
                    },
                    {
                        "name": "M. Masso-Reid"
                    },
                    {
                        "name": "S. Mastrogiovanni"
                    },
                    {
                        "name": "T. Matcovich"
                    },
                    {
                        "name": "M. Matiushechkina"
                    },
                    {
                        "name": "N. Mavalvala"
                    },
                    {
                        "name": "N. Maxwell"
                    },
                    {
                        "name": "G. McCarrol"
                    },
                    {
                        "name": "R. McCarthy"
                    },
                    {
                        "name": "D. E. McClelland"
                    },
                    {
                        "name": "S. McCormick"
                    },
                    {
                        "name": "L. McCuller"
                    },
                    {
                        "name": "S. McEachin"
                    },
                    {
                        "name": "C. McElhenny"
                    },
                    {
                        "name": "G. I. McGhee"
                    },
                    {
                        "name": "J. McGinn"
                    },
                    {
                        "name": "K. B. M. McGowan"
                    },
                    {
                        "name": "J. McIver"
                    },
                    {
                        "name": "A. McLeod"
                    },
                    {
                        "name": "I. McMahon"
                    },
                    {
                        "name": "T. McRae"
                    },
                    {
                        "name": "R. McTeague"
                    },
                    {
                        "name": "D. Meacher"
                    },
                    {
                        "name": "B. N. Meagher"
                    },
                    {
                        "name": "R. Mechum"
                    },
                    {
                        "name": "Q. Meijer"
                    },
                    {
                        "name": "A. Melatos"
                    },
                    {
                        "name": "C. S. Menoni"
                    },
                    {
                        "name": "F. Mera"
                    },
                    {
                        "name": "R. A. Mercer"
                    },
                    {
                        "name": "L. Mereni"
                    },
                    {
                        "name": "K. Merfeld"
                    },
                    {
                        "name": "E. L. Merilh"
                    },
                    {
                        "name": "J. R. M√©rou"
                    },
                    {
                        "name": "J. D. Merritt"
                    },
                    {
                        "name": "M. Merzougui"
                    },
                    {
                        "name": "C. Messick"
                    },
                    {
                        "name": "B. Mestichelli"
                    },
                    {
                        "name": "M. Meyer-Conde"
                    },
                    {
                        "name": "P. M. Meyers"
                    },
                    {
                        "name": "F. Meylahn"
                    },
                    {
                        "name": "A. Mhaske"
                    },
                    {
                        "name": "A. Miani"
                    },
                    {
                        "name": "H. Miao"
                    },
                    {
                        "name": "C. Michel"
                    },
                    {
                        "name": "Y. Michimura"
                    },
                    {
                        "name": "H. Middleton"
                    },
                    {
                        "name": "D. P. Mihaylov"
                    },
                    {
                        "name": "S. J. Miller"
                    },
                    {
                        "name": "M. Millhouse"
                    },
                    {
                        "name": "E. Milotti"
                    },
                    {
                        "name": "V. Milotti"
                    },
                    {
                        "name": "Y. Minenkov"
                    },
                    {
                        "name": "E. M. Minihan"
                    },
                    {
                        "name": "Ll. M. Mir"
                    },
                    {
                        "name": "L. Mirasola"
                    },
                    {
                        "name": "M. Miravet-Ten√©s"
                    },
                    {
                        "name": "C. -A. Miritescu"
                    },
                    {
                        "name": "A. Mishra"
                    },
                    {
                        "name": "C. Mishra"
                    },
                    {
                        "name": "T. Mishra"
                    },
                    {
                        "name": "A. L. Mitchell"
                    },
                    {
                        "name": "J. G. Mitchell"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "V. P. Mitrofanov"
                    },
                    {
                        "name": "K. Mitsuhashi"
                    },
                    {
                        "name": "R. Mittleman"
                    },
                    {
                        "name": "O. Miyakawa"
                    },
                    {
                        "name": "S. Miyoki"
                    },
                    {
                        "name": "A. Miyoko"
                    },
                    {
                        "name": "G. Mo"
                    },
                    {
                        "name": "L. Mobilia"
                    },
                    {
                        "name": "S. R. P. Mohapatra"
                    },
                    {
                        "name": "S. R. Mohite"
                    },
                    {
                        "name": "M. Molina-Ruiz"
                    },
                    {
                        "name": "M. Mondin"
                    },
                    {
                        "name": "J. K. Monsalve"
                    },
                    {
                        "name": "M. Montani"
                    },
                    {
                        "name": "C. J. Moore"
                    },
                    {
                        "name": "D. Moraru"
                    },
                    {
                        "name": "A. More"
                    },
                    {
                        "name": "S. More"
                    },
                    {
                        "name": "C. Moreno"
                    },
                    {
                        "name": "E. A. Moreno"
                    },
                    {
                        "name": "G. Moreno"
                    },
                    {
                        "name": "A. Moreso Serra"
                    },
                    {
                        "name": "S. Morisaki"
                    },
                    {
                        "name": "Y. Moriwaki"
                    },
                    {
                        "name": "G. Morras"
                    },
                    {
                        "name": "A. Moscatello"
                    },
                    {
                        "name": "M. Mould"
                    },
                    {
                        "name": "B. Mours"
                    },
                    {
                        "name": "C. M. Mow-Lowry"
                    },
                    {
                        "name": "L. Muccillo"
                    },
                    {
                        "name": "F. Muciaccia"
                    },
                    {
                        "name": "D. Mukherjee"
                    },
                    {
                        "name": "Samanwaya Mukherjee"
                    },
                    {
                        "name": "Soma Mukherjee"
                    },
                    {
                        "name": "Subroto Mukherjee"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    },
                    {
                        "name": "N. Mukund"
                    },
                    {
                        "name": "A. Mullavey"
                    },
                    {
                        "name": "H. Mullock"
                    },
                    {
                        "name": "J. Mundi"
                    },
                    {
                        "name": "C. L. Mungioli"
                    },
                    {
                        "name": "M. Murakoshi"
                    },
                    {
                        "name": "P. G. Murray"
                    },
                    {
                        "name": "D. Nabari"
                    },
                    {
                        "name": "S. L. Nadji"
                    },
                    {
                        "name": "A. Nagar"
                    },
                    {
                        "name": "N. Nagarajan"
                    },
                    {
                        "name": "K. Nakagaki"
                    },
                    {
                        "name": "K. Nakamura"
                    },
                    {
                        "name": "H. Nakano"
                    },
                    {
                        "name": "M. Nakano"
                    },
                    {
                        "name": "D. Nanadoumgar-Lacroze"
                    },
                    {
                        "name": "D. Nandi"
                    },
                    {
                        "name": "V. Napolano"
                    },
                    {
                        "name": "P. Narayan"
                    },
                    {
                        "name": "I. Nardecchia"
                    },
                    {
                        "name": "T. Narikawa"
                    },
                    {
                        "name": "H. Narola"
                    },
                    {
                        "name": "L. Naticchioni"
                    },
                    {
                        "name": "R. K. Nayak"
                    },
                    {
                        "name": "L. Negri"
                    },
                    {
                        "name": "A. Nela"
                    },
                    {
                        "name": "C. Nelle"
                    },
                    {
                        "name": "A. Nelson"
                    },
                    {
                        "name": "T. J. N. Nelson"
                    },
                    {
                        "name": "M. Nery"
                    },
                    {
                        "name": "A. Neunzert"
                    },
                    {
                        "name": "S. Ng"
                    },
                    {
                        "name": "L. Nguyen Quynh"
                    },
                    {
                        "name": "S. A. Nichols"
                    },
                    {
                        "name": "A. B. Nielsen"
                    },
                    {
                        "name": "Y. Nishino"
                    },
                    {
                        "name": "A. Nishizawa"
                    },
                    {
                        "name": "S. Nissanke"
                    },
                    {
                        "name": "W. Niu"
                    },
                    {
                        "name": "F. Nocera"
                    },
                    {
                        "name": "J. Noller"
                    },
                    {
                        "name": "M. Norman"
                    },
                    {
                        "name": "C. North"
                    },
                    {
                        "name": "J. Novak"
                    },
                    {
                        "name": "R. Nowicki"
                    },
                    {
                        "name": "J. F. Nu√±o Siles"
                    },
                    {
                        "name": "L. K. Nuttall"
                    },
                    {
                        "name": "K. Obayashi"
                    },
                    {
                        "name": "J. Oberling"
                    },
                    {
                        "name": "J. O'Dell"
                    },
                    {
                        "name": "E. Oelker"
                    },
                    {
                        "name": "M. Oertel"
                    },
                    {
                        "name": "G. Oganesyan"
                    },
                    {
                        "name": "T. O'Hanlon"
                    },
                    {
                        "name": "M. Ohashi"
                    },
                    {
                        "name": "F. Ohme"
                    },
                    {
                        "name": "R. Oliveri"
                    },
                    {
                        "name": "R. Omer"
                    },
                    {
                        "name": "B. O'Neal"
                    },
                    {
                        "name": "M. Onishi"
                    },
                    {
                        "name": "K. Oohara"
                    },
                    {
                        "name": "B. O'Reilly"
                    },
                    {
                        "name": "M. Orselli"
                    },
                    {
                        "name": "R. O'Shaughnessy"
                    },
                    {
                        "name": "S. O'Shea"
                    },
                    {
                        "name": "S. Oshino"
                    },
                    {
                        "name": "C. Osthelder"
                    },
                    {
                        "name": "I. Ota"
                    },
                    {
                        "name": "D. J. Ottaway"
                    },
                    {
                        "name": "A. Ouzriat"
                    },
                    {
                        "name": "H. Overmier"
                    },
                    {
                        "name": "B. J. Owen"
                    },
                    {
                        "name": "R. Ozaki"
                    },
                    {
                        "name": "A. E. Pace"
                    },
                    {
                        "name": "R. Pagano"
                    },
                    {
                        "name": "M. A. Page"
                    },
                    {
                        "name": "A. Pai"
                    },
                    {
                        "name": "L. Paiella"
                    },
                    {
                        "name": "A. Pal"
                    },
                    {
                        "name": "S. Pal"
                    },
                    {
                        "name": "M. A. Palaia"
                    },
                    {
                        "name": "M. P√°lfi"
                    },
                    {
                        "name": "P. P. Palma"
                    },
                    {
                        "name": "C. Palomba"
                    },
                    {
                        "name": "P. Palud"
                    },
                    {
                        "name": "H. Pan"
                    },
                    {
                        "name": "J. Pan"
                    },
                    {
                        "name": "K. C. Pan"
                    },
                    {
                        "name": "P. K. Panda"
                    },
                    {
                        "name": "Shiksha Pandey"
                    },
                    {
                        "name": "Swadha Pandey"
                    },
                    {
                        "name": "P. T. H. Pang"
                    },
                    {
                        "name": "F. Pannarale"
                    },
                    {
                        "name": "K. A. Pannone"
                    },
                    {
                        "name": "B. C. Pant"
                    },
                    {
                        "name": "F. H. Panther"
                    },
                    {
                        "name": "M. Panzeri"
                    },
                    {
                        "name": "F. Paoletti"
                    },
                    {
                        "name": "A. Paolone"
                    },
                    {
                        "name": "A. Papadopoulos"
                    },
                    {
                        "name": "E. E. Papalexakis"
                    },
                    {
                        "name": "L. Papalini"
                    },
                    {
                        "name": "G. Papigkiotis"
                    },
                    {
                        "name": "A. Paquis"
                    },
                    {
                        "name": "A. Parisi"
                    },
                    {
                        "name": "B. -J. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "W. Parker"
                    },
                    {
                        "name": "G. Pascale"
                    },
                    {
                        "name": "D. Pascucci"
                    },
                    {
                        "name": "A. Pasqualetti"
                    },
                    {
                        "name": "R. Passaquieti"
                    },
                    {
                        "name": "L. Passenger"
                    },
                    {
                        "name": "D. Passuello"
                    },
                    {
                        "name": "O. Patane"
                    },
                    {
                        "name": "A. V. Patel"
                    },
                    {
                        "name": "D. Pathak"
                    },
                    {
                        "name": "A. Patra"
                    },
                    {
                        "name": "B. Patricelli"
                    },
                    {
                        "name": "B. G. Patterson"
                    },
                    {
                        "name": "K. Paul"
                    },
                    {
                        "name": "S. Paul"
                    },
                    {
                        "name": "E. Payne"
                    },
                    {
                        "name": "T. Pearce"
                    },
                    {
                        "name": "M. Pedraza"
                    },
                    {
                        "name": "A. Pele"
                    },
                    {
                        "name": "F. E. Pe√±a Arellano"
                    },
                    {
                        "name": "X. Peng"
                    },
                    {
                        "name": "Y. Peng"
                    },
                    {
                        "name": "S. Penn"
                    },
                    {
                        "name": "M. D. Penuliar"
                    },
                    {
                        "name": "A. Perego"
                    },
                    {
                        "name": "Z. Pereira"
                    },
                    {
                        "name": "C. P√©rigois"
                    },
                    {
                        "name": "G. Perna"
                    },
                    {
                        "name": "A. Perreca"
                    },
                    {
                        "name": "J. Perret"
                    },
                    {
                        "name": "S. Perri√®s"
                    },
                    {
                        "name": "J. W. Perry"
                    },
                    {
                        "name": "D. Pesios"
                    },
                    {
                        "name": "S. Peters"
                    },
                    {
                        "name": "S. Petracca"
                    },
                    {
                        "name": "C. Petrillo"
                    },
                    {
                        "name": "H. P. Pfeiffer"
                    },
                    {
                        "name": "H. Pham"
                    },
                    {
                        "name": "K. A. Pham"
                    },
                    {
                        "name": "K. S. Phukon"
                    },
                    {
                        "name": "H. Phurailatpam"
                    },
                    {
                        "name": "M. Piarulli"
                    },
                    {
                        "name": "L. Piccari"
                    },
                    {
                        "name": "O. J. Piccinni"
                    },
                    {
                        "name": "M. Pichot"
                    },
                    {
                        "name": "M. Piendibene"
                    },
                    {
                        "name": "F. Piergiovanni"
                    },
                    {
                        "name": "L. Pierini"
                    },
                    {
                        "name": "G. Pierra"
                    },
                    {
                        "name": "V. Pierro"
                    },
                    {
                        "name": "M. Pietrzak"
                    },
                    {
                        "name": "M. Pillas"
                    },
                    {
                        "name": "F. Pilo"
                    },
                    {
                        "name": "L. Pinard"
                    },
                    {
                        "name": "I. M. Pinto"
                    },
                    {
                        "name": "M. Pinto"
                    },
                    {
                        "name": "B. J. Piotrzkowski"
                    },
                    {
                        "name": "M. Pirello"
                    },
                    {
                        "name": "M. D. Pitkin"
                    },
                    {
                        "name": "A. Placidi"
                    },
                    {
                        "name": "E. Placidi"
                    },
                    {
                        "name": "M. L. Planas"
                    },
                    {
                        "name": "W. Plastino"
                    },
                    {
                        "name": "C. Plunkett"
                    },
                    {
                        "name": "R. Poggiani"
                    },
                    {
                        "name": "E. Polini"
                    },
                    {
                        "name": "J. Pomper"
                    },
                    {
                        "name": "L. Pompili"
                    },
                    {
                        "name": "J. Poon"
                    },
                    {
                        "name": "E. Porcelli"
                    },
                    {
                        "name": "E. K. Porter"
                    },
                    {
                        "name": "C. Posnansky"
                    },
                    {
                        "name": "R. Poulton"
                    },
                    {
                        "name": "J. Powell"
                    },
                    {
                        "name": "G. S. Prabhu"
                    },
                    {
                        "name": "M. Pracchia"
                    },
                    {
                        "name": "B. K. Pradhan"
                    },
                    {
                        "name": "T. Pradier"
                    },
                    {
                        "name": "A. K. Prajapati"
                    },
                    {
                        "name": "K. Prasai"
                    },
                    {
                        "name": "R. Prasanna"
                    },
                    {
                        "name": "P. Prasia"
                    },
                    {
                        "name": "G. Pratten"
                    },
                    {
                        "name": "G. Principe"
                    },
                    {
                        "name": "G. A. Prodi"
                    },
                    {
                        "name": "P. Prosperi"
                    },
                    {
                        "name": "P. Prosposito"
                    },
                    {
                        "name": "A. C. Providence"
                    },
                    {
                        "name": "A. Puecher"
                    },
                    {
                        "name": "J. Pullin"
                    },
                    {
                        "name": "P. Puppo"
                    },
                    {
                        "name": "M. P√ºrrer"
                    },
                    {
                        "name": "H. Qi"
                    },
                    {
                        "name": "J. Qin"
                    },
                    {
                        "name": "G. Qu√©m√©ner"
                    },
                    {
                        "name": "V. Quetschke"
                    },
                    {
                        "name": "L. H. Quiceno"
                    },
                    {
                        "name": "P. J. Quinonez"
                    },
                    {
                        "name": "N. Qutob"
                    },
                    {
                        "name": "R. Rading"
                    },
                    {
                        "name": "I. Rainho"
                    },
                    {
                        "name": "S. Raja"
                    },
                    {
                        "name": "C. Rajan"
                    },
                    {
                        "name": "B. Rajbhandari"
                    },
                    {
                        "name": "K. E. Ramirez"
                    },
                    {
                        "name": "F. A. Ramis Vidal"
                    },
                    {
                        "name": "M. Ramos Arevalo"
                    },
                    {
                        "name": "A. Ramos-Buades"
                    },
                    {
                        "name": "S. Ranjan"
                    },
                    {
                        "name": "K. Ransom"
                    },
                    {
                        "name": "P. Rapagnani"
                    },
                    {
                        "name": "B. Ratto"
                    },
                    {
                        "name": "A. Ravichandran"
                    },
                    {
                        "name": "A. Ray"
                    },
                    {
                        "name": "V. Raymond"
                    },
                    {
                        "name": "M. Razzano"
                    },
                    {
                        "name": "J. Read"
                    },
                    {
                        "name": "T. Regimbau"
                    },
                    {
                        "name": "S. Reid"
                    },
                    {
                        "name": "C. Reissel"
                    },
                    {
                        "name": "D. H. Reitze"
                    },
                    {
                        "name": "A. I. Renzini"
                    },
                    {
                        "name": "A. Renzini"
                    },
                    {
                        "name": "B. Revenu"
                    },
                    {
                        "name": "A. Revilla Pe√±a"
                    },
                    {
                        "name": "R. Reyes"
                    },
                    {
                        "name": "L. Ricca"
                    },
                    {
                        "name": "F. Ricci"
                    },
                    {
                        "name": "M. Ricci"
                    },
                    {
                        "name": "A. Ricciardone"
                    },
                    {
                        "name": "J. Rice"
                    },
                    {
                        "name": "J. W. Richardson"
                    },
                    {
                        "name": "M. L. Richardson"
                    },
                    {
                        "name": "A. Rijal"
                    },
                    {
                        "name": "K. Riles"
                    },
                    {
                        "name": "H. K. Riley"
                    },
                    {
                        "name": "S. Rinaldi"
                    },
                    {
                        "name": "J. Rittmeyer"
                    },
                    {
                        "name": "C. Robertson"
                    },
                    {
                        "name": "F. Robinet"
                    },
                    {
                        "name": "M. Robinson"
                    },
                    {
                        "name": "A. Rocchi"
                    },
                    {
                        "name": "L. Rolland"
                    },
                    {
                        "name": "J. G. Rollins"
                    },
                    {
                        "name": "A. E. Romano"
                    },
                    {
                        "name": "J. D. Romano"
                    },
                    {
                        "name": "R. Romano"
                    },
                    {
                        "name": "A. Romero"
                    },
                    {
                        "name": "I. M. Romero-Shaw"
                    },
                    {
                        "name": "J. H. Romie"
                    },
                    {
                        "name": "S. Ronchini"
                    },
                    {
                        "name": "T. J. Roocke"
                    },
                    {
                        "name": "L. Rosa"
                    },
                    {
                        "name": "T. J. Rosauer"
                    },
                    {
                        "name": "C. A. Rose"
                    },
                    {
                        "name": "D. Rosi≈Ñska"
                    },
                    {
                        "name": "M. P. Ross"
                    },
                    {
                        "name": "M. Rossello-Sastre"
                    },
                    {
                        "name": "S. Rowan"
                    },
                    {
                        "name": "S. K. Roy"
                    },
                    {
                        "name": "S. Roy"
                    },
                    {
                        "name": "D. Rozza"
                    },
                    {
                        "name": "P. Ruggi"
                    },
                    {
                        "name": "N. Ruhama"
                    },
                    {
                        "name": "E. Ruiz Morales"
                    },
                    {
                        "name": "K. Ruiz-Rocha"
                    },
                    {
                        "name": "S. Sachdev"
                    },
                    {
                        "name": "T. Sadecki"
                    },
                    {
                        "name": "P. Saffarieh"
                    },
                    {
                        "name": "S. Safi-Harb"
                    },
                    {
                        "name": "M. R. Sah"
                    },
                    {
                        "name": "S. Saha"
                    },
                    {
                        "name": "T. Sainrat"
                    },
                    {
                        "name": "S. Sajith Menon"
                    },
                    {
                        "name": "K. Sakai"
                    },
                    {
                        "name": "Y. Sakai"
                    },
                    {
                        "name": "M. Sakellariadou"
                    },
                    {
                        "name": "S. Sakon"
                    },
                    {
                        "name": "O. S. Salafia"
                    },
                    {
                        "name": "F. Salces-Carcoba"
                    },
                    {
                        "name": "L. Salconi"
                    },
                    {
                        "name": "M. Saleem"
                    },
                    {
                        "name": "F. Salemi"
                    },
                    {
                        "name": "M. Sall√©"
                    },
                    {
                        "name": "S. U. Salunkhe"
                    },
                    {
                        "name": "S. Salvador"
                    },
                    {
                        "name": "A. Salvarese"
                    },
                    {
                        "name": "A. Samajdar"
                    },
                    {
                        "name": "A. Sanchez"
                    },
                    {
                        "name": "E. J. Sanchez"
                    },
                    {
                        "name": "L. E. Sanchez"
                    },
                    {
                        "name": "N. Sanchis-Gual"
                    },
                    {
                        "name": "J. R. Sanders"
                    },
                    {
                        "name": "E. M. S√§nger"
                    },
                    {
                        "name": "F. Santoliquido"
                    },
                    {
                        "name": "F. Sarandrea"
                    },
                    {
                        "name": "T. R. Saravanan"
                    },
                    {
                        "name": "N. Sarin"
                    },
                    {
                        "name": "P. Sarkar"
                    },
                    {
                        "name": "A. Sasli"
                    },
                    {
                        "name": "P. Sassi"
                    },
                    {
                        "name": "B. Sassolas"
                    },
                    {
                        "name": "B. S. Sathyaprakash"
                    },
                    {
                        "name": "R. Sato"
                    },
                    {
                        "name": "S. Sato"
                    },
                    {
                        "name": "Yukino Sato"
                    },
                    {
                        "name": "Yu Sato"
                    },
                    {
                        "name": "O. Sauter"
                    },
                    {
                        "name": "R. L. Savage"
                    },
                    {
                        "name": "T. Sawada"
                    },
                    {
                        "name": "H. L. Sawant"
                    },
                    {
                        "name": "S. Sayah"
                    },
                    {
                        "name": "V. Scacco"
                    },
                    {
                        "name": "D. Schaetzl"
                    },
                    {
                        "name": "M. Scheel"
                    },
                    {
                        "name": "A. Schiebelbein"
                    },
                    {
                        "name": "M. G. Schiworski"
                    },
                    {
                        "name": "P. Schmidt"
                    },
                    {
                        "name": "S. Schmidt"
                    },
                    {
                        "name": "R. Schnabel"
                    },
                    {
                        "name": "M. Schneewind"
                    },
                    {
                        "name": "R. M. S. Schofield"
                    },
                    {
                        "name": "K. Schouteden"
                    },
                    {
                        "name": "B. W. Schulte"
                    },
                    {
                        "name": "B. F. Schutz"
                    },
                    {
                        "name": "E. Schwartz"
                    },
                    {
                        "name": "M. Scialpi"
                    },
                    {
                        "name": "J. Scott"
                    },
                    {
                        "name": "S. M. Scott"
                    },
                    {
                        "name": "R. M. Sedas"
                    },
                    {
                        "name": "T. C. Seetharamu"
                    },
                    {
                        "name": "M. Seglar-Arroyo"
                    },
                    {
                        "name": "Y. Sekiguchi"
                    },
                    {
                        "name": "D. Sellers"
                    },
                    {
                        "name": "N. Sembo"
                    },
                    {
                        "name": "A. S. Sengupta"
                    },
                    {
                        "name": "E. G. Seo"
                    },
                    {
                        "name": "J. W. Seo"
                    },
                    {
                        "name": "V. Sequino"
                    },
                    {
                        "name": "M. Serra"
                    },
                    {
                        "name": "A. Sevrin"
                    },
                    {
                        "name": "T. Shaffer"
                    },
                    {
                        "name": "U. S. Shah"
                    },
                    {
                        "name": "M. A. Shaikh"
                    },
                    {
                        "name": "L. Shao"
                    },
                    {
                        "name": "A. K. Sharma"
                    },
                    {
                        "name": "Preeti Sharma"
                    },
                    {
                        "name": "Prianka Sharma"
                    },
                    {
                        "name": "Ritwik Sharma"
                    },
                    {
                        "name": "S. Sharma Chaudhary"
                    },
                    {
                        "name": "P. Shawhan"
                    },
                    {
                        "name": "N. S. Shcheblanov"
                    },
                    {
                        "name": "E. Sheridan"
                    },
                    {
                        "name": "Z. -H. Shi"
                    },
                    {
                        "name": "M. Shikauchi"
                    },
                    {
                        "name": "R. Shimomura"
                    },
                    {
                        "name": "H. Shinkai"
                    },
                    {
                        "name": "S. Shirke"
                    },
                    {
                        "name": "D. H. Shoemaker"
                    },
                    {
                        "name": "D. M. Shoemaker"
                    },
                    {
                        "name": "R. W. Short"
                    },
                    {
                        "name": "S. ShyamSundar"
                    },
                    {
                        "name": "A. Sider"
                    },
                    {
                        "name": "H. Siegel"
                    },
                    {
                        "name": "D. Sigg"
                    },
                    {
                        "name": "L. Silenzi"
                    },
                    {
                        "name": "L. Silvestri"
                    },
                    {
                        "name": "M. Simmonds"
                    },
                    {
                        "name": "L. P. Singer"
                    },
                    {
                        "name": "Amitesh Singh"
                    },
                    {
                        "name": "Anika Singh"
                    },
                    {
                        "name": "D. Singh"
                    },
                    {
                        "name": "N. Singh"
                    },
                    {
                        "name": "S. Singh"
                    },
                    {
                        "name": "A. M. Sintes"
                    },
                    {
                        "name": "V. Sipala"
                    },
                    {
                        "name": "V. Skliris"
                    },
                    {
                        "name": "B. J. J. Slagmolen"
                    },
                    {
                        "name": "D. A. Slater"
                    },
                    {
                        "name": "T. J. Slaven-Blair"
                    },
                    {
                        "name": "J. Smetana"
                    },
                    {
                        "name": "J. R. Smith"
                    },
                    {
                        "name": "L. Smith"
                    },
                    {
                        "name": "R. J. E. Smith"
                    },
                    {
                        "name": "W. J. Smith"
                    },
                    {
                        "name": "S. Soares de Albuquerque Filho"
                    },
                    {
                        "name": "M. Soares-Santos"
                    },
                    {
                        "name": "K. Somiya"
                    },
                    {
                        "name": "I. Song"
                    },
                    {
                        "name": "S. Soni"
                    },
                    {
                        "name": "V. Sordini"
                    },
                    {
                        "name": "F. Sorrentino"
                    },
                    {
                        "name": "H. Sotani"
                    },
                    {
                        "name": "F. Spada"
                    },
                    {
                        "name": "V. Spagnuolo"
                    },
                    {
                        "name": "A. P. Spencer"
                    },
                    {
                        "name": "P. Spinicelli"
                    },
                    {
                        "name": "A. K. Srivastava"
                    },
                    {
                        "name": "F. Stachurski"
                    },
                    {
                        "name": "C. J. Stark"
                    },
                    {
                        "name": "D. A. Steer"
                    },
                    {
                        "name": "N. Steinle"
                    },
                    {
                        "name": "J. Steinlechner"
                    },
                    {
                        "name": "S. Steinlechner"
                    },
                    {
                        "name": "N. Stergioulas"
                    },
                    {
                        "name": "P. Stevens"
                    },
                    {
                        "name": "M. StPierre"
                    },
                    {
                        "name": "M. D. Strong"
                    },
                    {
                        "name": "A. Strunk"
                    },
                    {
                        "name": "A. L. Stuver"
                    },
                    {
                        "name": "M. Suchenek"
                    },
                    {
                        "name": "S. Sudhagar"
                    },
                    {
                        "name": "Y. Sudo"
                    },
                    {
                        "name": "N. Sueltmann"
                    },
                    {
                        "name": "L. Suleiman"
                    },
                    {
                        "name": "K. D. Sullivan"
                    },
                    {
                        "name": "J. Sun"
                    },
                    {
                        "name": "L. Sun"
                    },
                    {
                        "name": "S. Sunil"
                    },
                    {
                        "name": "J. Suresh"
                    },
                    {
                        "name": "B. J. Sutton"
                    },
                    {
                        "name": "P. J. Sutton"
                    },
                    {
                        "name": "K. Suzuki"
                    },
                    {
                        "name": "M. Suzuki"
                    },
                    {
                        "name": "B. L. Swinkels"
                    },
                    {
                        "name": "A. Syx"
                    },
                    {
                        "name": "M. J. Szczepa≈Ñczyk"
                    },
                    {
                        "name": "P. Szewczyk"
                    },
                    {
                        "name": "M. Tacca"
                    },
                    {
                        "name": "H. Tagoshi"
                    },
                    {
                        "name": "K. Takada"
                    },
                    {
                        "name": "H. Takahashi"
                    },
                    {
                        "name": "R. Takahashi"
                    },
                    {
                        "name": "A. Takamori"
                    },
                    {
                        "name": "S. Takano"
                    },
                    {
                        "name": "H. Takeda"
                    },
                    {
                        "name": "K. Takeshita"
                    },
                    {
                        "name": "I. Takimoto Schmiegelow"
                    },
                    {
                        "name": "M. Takou-Ayaoh"
                    },
                    {
                        "name": "C. Talbot"
                    },
                    {
                        "name": "M. Tamaki"
                    },
                    {
                        "name": "N. Tamanini"
                    },
                    {
                        "name": "D. Tanabe"
                    },
                    {
                        "name": "K. Tanaka"
                    },
                    {
                        "name": "S. J. Tanaka"
                    },
                    {
                        "name": "S. Tanioka"
                    },
                    {
                        "name": "D. B. Tanner"
                    },
                    {
                        "name": "W. Tanner"
                    },
                    {
                        "name": "L. Tao"
                    },
                    {
                        "name": "R. D. Tapia"
                    },
                    {
                        "name": "E. N. Tapia San Mart√≠n"
                    },
                    {
                        "name": "C. Taranto"
                    },
                    {
                        "name": "A. Taruya"
                    },
                    {
                        "name": "J. D. Tasson"
                    },
                    {
                        "name": "J. G. Tau"
                    },
                    {
                        "name": "D. Tellez"
                    },
                    {
                        "name": "R. Tenorio"
                    },
                    {
                        "name": "H. Themann"
                    },
                    {
                        "name": "A. Theodoropoulos"
                    },
                    {
                        "name": "M. P. Thirugnanasambandam"
                    },
                    {
                        "name": "L. M. Thomas"
                    },
                    {
                        "name": "M. Thomas"
                    },
                    {
                        "name": "P. Thomas"
                    },
                    {
                        "name": "J. E. Thompson"
                    },
                    {
                        "name": "S. R. Thondapu"
                    },
                    {
                        "name": "K. A. Thorne"
                    },
                    {
                        "name": "E. Thrane"
                    },
                    {
                        "name": "J. Tissino"
                    },
                    {
                        "name": "A. Tiwari"
                    },
                    {
                        "name": "Pawan Tiwari"
                    },
                    {
                        "name": "Praveer Tiwari"
                    },
                    {
                        "name": "S. Tiwari"
                    },
                    {
                        "name": "V. Tiwari"
                    },
                    {
                        "name": "M. R. Todd"
                    },
                    {
                        "name": "M. Toffano"
                    },
                    {
                        "name": "A. M. Toivonen"
                    },
                    {
                        "name": "K. Toland"
                    },
                    {
                        "name": "A. E. Tolley"
                    },
                    {
                        "name": "T. Tomaru"
                    },
                    {
                        "name": "V. Tommasini"
                    },
                    {
                        "name": "T. Tomura"
                    },
                    {
                        "name": "H. Tong"
                    },
                    {
                        "name": "C. Tong-Yu"
                    },
                    {
                        "name": "A. Torres-Forn√©"
                    },
                    {
                        "name": "C. I. Torrie"
                    },
                    {
                        "name": "I. Tosta e Melo"
                    },
                    {
                        "name": "E. Tournefier"
                    },
                    {
                        "name": "M. Trad Nery"
                    },
                    {
                        "name": "K. Tran"
                    },
                    {
                        "name": "A. Trapananti"
                    },
                    {
                        "name": "R. Travaglini"
                    },
                    {
                        "name": "F. Travasso"
                    },
                    {
                        "name": "G. Traylor"
                    },
                    {
                        "name": "M. Trevor"
                    },
                    {
                        "name": "M. C. Tringali"
                    },
                    {
                        "name": "A. Tripathee"
                    },
                    {
                        "name": "G. Troian"
                    },
                    {
                        "name": "A. Trovato"
                    },
                    {
                        "name": "L. Trozzo"
                    },
                    {
                        "name": "R. J. Trudeau"
                    },
                    {
                        "name": "T. Tsang"
                    },
                    {
                        "name": "S. Tsuchida"
                    },
                    {
                        "name": "L. Tsukada"
                    },
                    {
                        "name": "K. Turbang"
                    },
                    {
                        "name": "M. Turconi"
                    },
                    {
                        "name": "C. Turski"
                    },
                    {
                        "name": "H. Ubach"
                    },
                    {
                        "name": "N. Uchikata"
                    },
                    {
                        "name": "T. Uchiyama"
                    },
                    {
                        "name": "R. P. Udall"
                    },
                    {
                        "name": "T. Uehara"
                    },
                    {
                        "name": "K. Ueno"
                    },
                    {
                        "name": "V. Undheim"
                    },
                    {
                        "name": "L. E. Uronen"
                    },
                    {
                        "name": "T. Ushiba"
                    },
                    {
                        "name": "M. Vacatello"
                    },
                    {
                        "name": "H. Vahlbruch"
                    },
                    {
                        "name": "N. Vaidya"
                    },
                    {
                        "name": "G. Vajente"
                    },
                    {
                        "name": "A. Vajpeyi"
                    },
                    {
                        "name": "J. Valencia"
                    },
                    {
                        "name": "M. Valentini"
                    },
                    {
                        "name": "S. A. Vallejo-Pe√±a"
                    },
                    {
                        "name": "S. Vallero"
                    },
                    {
                        "name": "V. Valsan"
                    },
                    {
                        "name": "M. van Dael"
                    },
                    {
                        "name": "E. Van den Bossche"
                    },
                    {
                        "name": "J. F. J. van den Brand"
                    },
                    {
                        "name": "C. Van Den Broeck"
                    },
                    {
                        "name": "M. van der Sluys"
                    },
                    {
                        "name": "A. Van de Walle"
                    },
                    {
                        "name": "J. van Dongen"
                    },
                    {
                        "name": "K. Vandra"
                    },
                    {
                        "name": "M. VanDyke"
                    },
                    {
                        "name": "H. van Haevermaet"
                    },
                    {
                        "name": "J. V. van Heijningen"
                    },
                    {
                        "name": "P. Van Hove"
                    },
                    {
                        "name": "J. Vanier"
                    },
                    {
                        "name": "M. VanKeuren"
                    },
                    {
                        "name": "J. Vanosky"
                    },
                    {
                        "name": "N. van Remortel"
                    },
                    {
                        "name": "M. Vardaro"
                    },
                    {
                        "name": "A. F. Vargas"
                    },
                    {
                        "name": "V. Varma"
                    },
                    {
                        "name": "A. N. Vazquez"
                    },
                    {
                        "name": "A. Vecchio"
                    },
                    {
                        "name": "G. Vedovato"
                    },
                    {
                        "name": "J. Veitch"
                    },
                    {
                        "name": "P. J. Veitch"
                    },
                    {
                        "name": "S. Venikoudis"
                    },
                    {
                        "name": "R. C. Venterea"
                    },
                    {
                        "name": "P. Verdier"
                    },
                    {
                        "name": "M. Vereecken"
                    },
                    {
                        "name": "D. Verkindt"
                    },
                    {
                        "name": "B. Verma"
                    },
                    {
                        "name": "Y. Verma"
                    },
                    {
                        "name": "S. M. Vermeulen"
                    },
                    {
                        "name": "F. Vetrano"
                    },
                    {
                        "name": "A. Veutro"
                    },
                    {
                        "name": "A. Vicer√©"
                    },
                    {
                        "name": "S. Vidyant"
                    },
                    {
                        "name": "A. D. Viets"
                    },
                    {
                        "name": "A. Vijaykumar"
                    },
                    {
                        "name": "A. Vilkha"
                    },
                    {
                        "name": "N. Villanueva Espinosa"
                    },
                    {
                        "name": "V. Villa-Ortega"
                    },
                    {
                        "name": "E. T. Vincent"
                    },
                    {
                        "name": "J. -Y. Vinet"
                    },
                    {
                        "name": "S. Viret"
                    },
                    {
                        "name": "S. Vitale"
                    },
                    {
                        "name": "H. Vocca"
                    },
                    {
                        "name": "D. Voigt"
                    },
                    {
                        "name": "E. R. G. von Reis"
                    },
                    {
                        "name": "J. S. A. von Wrangel"
                    },
                    {
                        "name": "W. E. Vossius"
                    },
                    {
                        "name": "L. Vujeva"
                    },
                    {
                        "name": "S. P. Vyatchanin"
                    },
                    {
                        "name": "J. Wack"
                    },
                    {
                        "name": "L. E. Wade"
                    },
                    {
                        "name": "M. Wade"
                    },
                    {
                        "name": "K. J. Wagner"
                    },
                    {
                        "name": "L. Wallace"
                    },
                    {
                        "name": "E. J. Wang"
                    },
                    {
                        "name": "H. Wang"
                    },
                    {
                        "name": "J. Z. Wang"
                    },
                    {
                        "name": "W. H. Wang"
                    },
                    {
                        "name": "Y. F. Wang"
                    },
                    {
                        "name": "G. Waratkar"
                    },
                    {
                        "name": "J. Warner"
                    },
                    {
                        "name": "M. Was"
                    },
                    {
                        "name": "T. Washimi"
                    },
                    {
                        "name": "N. Y. Washington"
                    },
                    {
                        "name": "D. Watarai"
                    },
                    {
                        "name": "B. Weaver"
                    },
                    {
                        "name": "S. A. Webster"
                    },
                    {
                        "name": "N. L. Weickhardt"
                    },
                    {
                        "name": "M. Weinert"
                    },
                    {
                        "name": "A. J. Weinstein"
                    },
                    {
                        "name": "R. Weiss"
                    },
                    {
                        "name": "L. Wen"
                    },
                    {
                        "name": "K. Wette"
                    },
                    {
                        "name": "J. T. Whelan"
                    },
                    {
                        "name": "B. F. Whiting"
                    },
                    {
                        "name": "C. Whittle"
                    },
                    {
                        "name": "E. G. Wickens"
                    },
                    {
                        "name": "D. Wilken"
                    },
                    {
                        "name": "A. T. Wilkin"
                    },
                    {
                        "name": "B. M. Williams"
                    },
                    {
                        "name": "D. Williams"
                    },
                    {
                        "name": "M. J. Williams"
                    },
                    {
                        "name": "N. S. Williams"
                    },
                    {
                        "name": "J. L. Willis"
                    },
                    {
                        "name": "B. Willke"
                    },
                    {
                        "name": "M. Wils"
                    },
                    {
                        "name": "L. Wilson"
                    },
                    {
                        "name": "C. W. Winborn"
                    },
                    {
                        "name": "J. Winterflood"
                    },
                    {
                        "name": "C. C. Wipf"
                    },
                    {
                        "name": "G. Woan"
                    },
                    {
                        "name": "J. Woehler"
                    },
                    {
                        "name": "N. E. Wolfe"
                    },
                    {
                        "name": "H. T. Wong"
                    },
                    {
                        "name": "I. C. F. Wong"
                    },
                    {
                        "name": "K. Wong"
                    },
                    {
                        "name": "T. Wouters"
                    },
                    {
                        "name": "J. L. Wright"
                    },
                    {
                        "name": "M. Wright"
                    },
                    {
                        "name": "B. Wu"
                    },
                    {
                        "name": "C. Wu"
                    },
                    {
                        "name": "D. S. Wu"
                    },
                    {
                        "name": "H. Wu"
                    },
                    {
                        "name": "K. Wu"
                    },
                    {
                        "name": "Q. Wu"
                    },
                    {
                        "name": "T. Y. Wu"
                    },
                    {
                        "name": "Y. Wu"
                    },
                    {
                        "name": "Z. Wu"
                    },
                    {
                        "name": "E. Wuchner"
                    },
                    {
                        "name": "D. M. Wysocki"
                    },
                    {
                        "name": "V. A. Xu"
                    },
                    {
                        "name": "Y. Xu"
                    },
                    {
                        "name": "N. Yadav"
                    },
                    {
                        "name": "H. Yamamoto"
                    },
                    {
                        "name": "K. Yamamoto"
                    },
                    {
                        "name": "T. S. Yamamoto"
                    },
                    {
                        "name": "T. Yamamoto"
                    },
                    {
                        "name": "R. Yamazaki"
                    },
                    {
                        "name": "T. Yan"
                    },
                    {
                        "name": "K. Z. Yang"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Z. Yarbrough"
                    },
                    {
                        "name": "J. Yebana"
                    },
                    {
                        "name": "S. -W. Yeh"
                    },
                    {
                        "name": "A. B. Yelikar"
                    },
                    {
                        "name": "X. Yin"
                    },
                    {
                        "name": "J. Yokoyama"
                    },
                    {
                        "name": "T. Yokozawa"
                    },
                    {
                        "name": "S. Yuan"
                    },
                    {
                        "name": "H. Yuzurihara"
                    },
                    {
                        "name": "M. Zanolin"
                    },
                    {
                        "name": "M. Zeeshan"
                    },
                    {
                        "name": "T. Zelenova"
                    },
                    {
                        "name": "J. -P. Zendri"
                    },
                    {
                        "name": "M. Zeoli"
                    },
                    {
                        "name": "M. Zerrad"
                    },
                    {
                        "name": "M. Zevin"
                    },
                    {
                        "name": "L. Zhang"
                    },
                    {
                        "name": "N. Zhang"
                    },
                    {
                        "name": "R. Zhang"
                    },
                    {
                        "name": "T. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yuhang Zhao"
                    },
                    {
                        "name": "Z. -C. Zhao"
                    },
                    {
                        "name": "Y. Zheng"
                    },
                    {
                        "name": "H. Zhong"
                    },
                    {
                        "name": "H. Zhou"
                    },
                    {
                        "name": "H. O. Zhu"
                    },
                    {
                        "name": "Z. -H. Zhu"
                    },
                    {
                        "name": "A. B. Zimmerman"
                    },
                    {
                        "name": "L. Zimmermann"
                    },
                    {
                        "name": "M. E. Zucker"
                    },
                    {
                        "name": "J. Zweizig"
                    }
                ],
                "author_detail": {
                    "name": "J. Zweizig"
                },
                "author": "J. Zweizig",
                "arxiv_comment": "31 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20712v1",
                "updated": "2025-08-28T12:30:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    30,
                    32,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:30:32Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    30,
                    32,
                    3,
                    240,
                    0
                ],
                "title": "Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label\n  Hierarchical Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label\n  Hierarchical Learning"
                },
                "summary": "This paper introduces the first multi-lingual and multi-label classification\nmodel for implicit discourse relation recognition (IDRR). Our model, HArch, is\nevaluated on the recently released DiscoGeM 2.0 corpus and leverages\nhierarchical dependencies between discourse senses to predict probability\ndistributions across all three sense levels in the PDTB 3.0 framework. We\ncompare several pre-trained encoder backbones and find that RoBERTa-HArch\nachieves the best performance in English, while XLM-RoBERTa-HArch performs best\nin the multi-lingual setting. In addition, we compare our fine-tuned models\nagainst GPT-4o and Llama-4-Maverick using few-shot prompting across all\nlanguage configurations. Our results show that our fine-tuned models\nconsistently outperform these LLMs, highlighting the advantages of\ntask-specific fine-tuning over prompting in IDRR. Finally, we report SOTA\nresults on the DiscoGeM 1.0 corpus, further validating the effectiveness of our\nhierarchical approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the first multi-lingual and multi-label classification\nmodel for implicit discourse relation recognition (IDRR). Our model, HArch, is\nevaluated on the recently released DiscoGeM 2.0 corpus and leverages\nhierarchical dependencies between discourse senses to predict probability\ndistributions across all three sense levels in the PDTB 3.0 framework. We\ncompare several pre-trained encoder backbones and find that RoBERTa-HArch\nachieves the best performance in English, while XLM-RoBERTa-HArch performs best\nin the multi-lingual setting. In addition, we compare our fine-tuned models\nagainst GPT-4o and Llama-4-Maverick using few-shot prompting across all\nlanguage configurations. Our results show that our fine-tuned models\nconsistently outperform these LLMs, highlighting the advantages of\ntask-specific fine-tuning over prompting in IDRR. Finally, we report SOTA\nresults on the DiscoGeM 1.0 corpus, further validating the effectiveness of our\nhierarchical approach."
                },
                "authors": [
                    {
                        "name": "Nelson Filipe Costa"
                    },
                    {
                        "name": "Leila Kosseim"
                    }
                ],
                "author_detail": {
                    "name": "Leila Kosseim"
                },
                "author": "Leila Kosseim",
                "arxiv_comment": "Published at SIGDIAL 2025. Best paper award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20711v1",
                "updated": "2025-08-28T12:29:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    29,
                    13,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:29:13Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    29,
                    13,
                    3,
                    240,
                    0
                ],
                "title": "HSTPROMO Internal Proper Motion Kinematics of Dwarf Spheroidal Galaxies:\n  II. Velocity Anisotropy and Dark Matter Cusp Slope of Sculptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSTPROMO Internal Proper Motion Kinematics of Dwarf Spheroidal Galaxies:\n  II. Velocity Anisotropy and Dark Matter Cusp Slope of Sculptor"
                },
                "summary": "We analyze three epochs of HST imaging over 20 years for the Sculptor dwarf\nspheroidal galaxy, measuring precise proper motions for 119 stars and combining\nthem with 1760 existing line-of-sight velocities. This catalog yields the first\nradially-resolved 3D velocity dispersion profiles for Sculptor. We confirm mild\noblate rotation, with major-axis velocities reaching $\\sim 2$ km s$^{-1}$\nbeyond 20.0 arcmin. Using a methodology similar to that in the first paper in\nthis series, we solve the Jeans equations in oblate axisymmetric geometry to\ninfer the galaxy's mass profile. Our modeling reveals a significant degeneracy\ndue to the unknown galaxy inclination, which is overlooked under spherical\nsymmetry assumptions. This degeneracy allows acceptable fits across a range of\ndark matter profiles, from cuspy to cored. While we do not directly constrain\nthe inclination with our Jeans models, higher-order line-of-sight velocity\nmoments provide useful additional constraints: comparisons with scalefree\nmodels from de Bruijne et al. (1996) favor highly flattened (more face-on)\nconfigurations. Adopting an inclination well consistent with these comparisons\n($i = 57.1$ degrees), we find, alongside radial velocity anisotropy, a dark\nmatter density slope of $\\Gamma_{\\rm dark} = 0.29^{+0.31}_{-0.41}$ within the\nradial extent of the 3D velocity data, ruling out a cusp with $\\Gamma_{\\rm\ndark} \\leq -1$ at 99.8% confidence. This confidence increases for lower\ninclinations and decreases drastically for nearly edge-on configurations. The\nresults qualitatively agree with $\\Lambda$CDM, SIDM, and Fuzzy DM scenarios\nthat predict core formation, while our specific measurements provide\nquantitative constraints on the prescriptions of feedback, cross sections, or\nparticle masses required by these models, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze three epochs of HST imaging over 20 years for the Sculptor dwarf\nspheroidal galaxy, measuring precise proper motions for 119 stars and combining\nthem with 1760 existing line-of-sight velocities. This catalog yields the first\nradially-resolved 3D velocity dispersion profiles for Sculptor. We confirm mild\noblate rotation, with major-axis velocities reaching $\\sim 2$ km s$^{-1}$\nbeyond 20.0 arcmin. Using a methodology similar to that in the first paper in\nthis series, we solve the Jeans equations in oblate axisymmetric geometry to\ninfer the galaxy's mass profile. Our modeling reveals a significant degeneracy\ndue to the unknown galaxy inclination, which is overlooked under spherical\nsymmetry assumptions. This degeneracy allows acceptable fits across a range of\ndark matter profiles, from cuspy to cored. While we do not directly constrain\nthe inclination with our Jeans models, higher-order line-of-sight velocity\nmoments provide useful additional constraints: comparisons with scalefree\nmodels from de Bruijne et al. (1996) favor highly flattened (more face-on)\nconfigurations. Adopting an inclination well consistent with these comparisons\n($i = 57.1$ degrees), we find, alongside radial velocity anisotropy, a dark\nmatter density slope of $\\Gamma_{\\rm dark} = 0.29^{+0.31}_{-0.41}$ within the\nradial extent of the 3D velocity data, ruling out a cusp with $\\Gamma_{\\rm\ndark} \\leq -1$ at 99.8% confidence. This confidence increases for lower\ninclinations and decreases drastically for nearly edge-on configurations. The\nresults qualitatively agree with $\\Lambda$CDM, SIDM, and Fuzzy DM scenarios\nthat predict core formation, while our specific measurements provide\nquantitative constraints on the prescriptions of feedback, cross sections, or\nparticle masses required by these models, respectively."
                },
                "authors": [
                    {
                        "name": "Eduardo Vitral"
                    },
                    {
                        "name": "Roeland P. van der Marel"
                    },
                    {
                        "name": "Sangmo Tony Sohn"
                    },
                    {
                        "name": "Jorge Pe√±arrubia"
                    },
                    {
                        "name": "Ekta Patel"
                    },
                    {
                        "name": "Laura L. Watkins"
                    },
                    {
                        "name": "Mattia Libralato"
                    },
                    {
                        "name": "Kevin McKinnon"
                    },
                    {
                        "name": "Andrea Bellini"
                    },
                    {
                        "name": "Paul Bennet"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bennet"
                },
                "author": "Paul Bennet",
                "arxiv_comment": "25 pages, 14 figures, 4 tables. Submitted, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08926v2",
                "updated": "2025-08-28T12:27:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    27,
                    6,
                    3,
                    240,
                    0
                ],
                "published": "2025-04-11T19:14:40Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    19,
                    14,
                    40,
                    4,
                    101,
                    0
                ],
                "title": "Constraints on QCD-based equation of state of quark stars from neutron\n  star maximum mass, radius, and tidal deformability observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on QCD-based equation of state of quark stars from neutron\n  star maximum mass, radius, and tidal deformability observations"
                },
                "summary": "(Abridged) Neutron stars (NSs), the densest known objects composed of matter,\nprovide a unique laboratory to probe whether strange quark matter is the true\nground state of matter. We investigate the parameter space of the equation of\nstate of strange stars using a quantum chromodynamics (QCD)-informed model. The\nparameters - related to the energy density difference between quark matter and\nthe QCD vacuum, the strength of strong interactions, and the gap parameter for\ncolor superconductivity - are sampled via quasi-random Latin hypercube sampling\nto ensure uniform coverage. To constrain them, we incorporate observational\ndata on the maximum mass of NSs (from binary and merger systems), the radii of\n$1.4$ M$_{\\odot}$ NSs (from gravitational wave and electromagnetic\nobservations), and tidal deformabilities (from GW170817). Our results show that\nquark strong interactions play a key role, requiring at least a $20\\%$\ndeviation from the free-quark limit. We also find that color superconductivity\nis relevant, with the gap parameter reaching up to $\\sim 230$ MeV for a strange\nquark mass of $100$ MeV. The surface-to-vacuum energy density jump lies in the\nrange $(1.1-2.2)$ $\\rho_{\\rm{sat}}$, where $\\rho_{\\rm{sat}} \\simeq 2.7 \\times\n10^{14}$ g cm$^{-3}$. Observational constraints also imply that a $1.4$\nM$_{\\odot}$ quark star has a radius of $(10.0-12.3)$ km and tidal deformability\nbetween $270$ and $970$. These are consistent with the low mass and radius\ninferred for the compact object XMMU J173203.3-344518. Our results provide\nuseful inputs for future studies on quark and hybrid stars, including their\ntidal properties, thermal evolution, quasi-normal modes, and ellipticities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(Abridged) Neutron stars (NSs), the densest known objects composed of matter,\nprovide a unique laboratory to probe whether strange quark matter is the true\nground state of matter. We investigate the parameter space of the equation of\nstate of strange stars using a quantum chromodynamics (QCD)-informed model. The\nparameters - related to the energy density difference between quark matter and\nthe QCD vacuum, the strength of strong interactions, and the gap parameter for\ncolor superconductivity - are sampled via quasi-random Latin hypercube sampling\nto ensure uniform coverage. To constrain them, we incorporate observational\ndata on the maximum mass of NSs (from binary and merger systems), the radii of\n$1.4$ M$_{\\odot}$ NSs (from gravitational wave and electromagnetic\nobservations), and tidal deformabilities (from GW170817). Our results show that\nquark strong interactions play a key role, requiring at least a $20\\%$\ndeviation from the free-quark limit. We also find that color superconductivity\nis relevant, with the gap parameter reaching up to $\\sim 230$ MeV for a strange\nquark mass of $100$ MeV. The surface-to-vacuum energy density jump lies in the\nrange $(1.1-2.2)$ $\\rho_{\\rm{sat}}$, where $\\rho_{\\rm{sat}} \\simeq 2.7 \\times\n10^{14}$ g cm$^{-3}$. Observational constraints also imply that a $1.4$\nM$_{\\odot}$ quark star has a radius of $(10.0-12.3)$ km and tidal deformability\nbetween $270$ and $970$. These are consistent with the low mass and radius\ninferred for the compact object XMMU J173203.3-344518. Our results provide\nuseful inputs for future studies on quark and hybrid stars, including their\ntidal properties, thermal evolution, quasi-normal modes, and ellipticities."
                },
                "authors": [
                    {
                        "name": "Jo√£o V. Zastrow"
                    },
                    {
                        "name": "Jonas P. Pereira"
                    },
                    {
                        "name": "Rafael C. R. de Lima"
                    },
                    {
                        "name": "Jorge E. Horvath"
                    }
                ],
                "author_detail": {
                    "name": "Jorge E. Horvath"
                },
                "author": "Jorge E. Horvath",
                "arxiv_comment": "14 pages, 11 figures. Accepted for publication in PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18321v2",
                "updated": "2025-08-28T12:18:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    18,
                    4,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-24T09:58:10Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    9,
                    58,
                    10,
                    6,
                    236,
                    0
                ],
                "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social\n  Interactions"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS."
                },
                "authors": [
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Tej Deep Pala"
                    },
                    {
                        "name": "Weisheng Jin"
                    },
                    {
                        "name": "Amir Zadeh"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Dorien Herremans"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17734v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17734v2",
                "updated": "2025-08-28T12:15:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    15,
                    57,
                    3,
                    240,
                    0
                ],
                "published": "2024-12-23T17:35:19Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    35,
                    19,
                    0,
                    358,
                    0
                ],
                "title": "LASE: Learned Adjacency Spectral Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASE: Learned Adjacency Spectral Embeddings"
                },
                "summary": "We put forth a principled design of a neural architecture to learn nodal\nAdjacency Spectral Embeddings (ASE) from graph inputs. By bringing to bear the\ngradient descent (GD) method and leveraging the principle of algorithm\nunrolling, we truncate and re-interpret each GD iteration as a layer in a graph\nneural network (GNN) that is trained to approximate the ASE. Accordingly, we\ncall the resulting embeddings and our parametric model Learned ASE (LASE),\nwhich is interpretable, parameter efficient, robust to inputs with unobserved\nedges, and offers controllable complexity during inference. LASE layers combine\nGraph Convolutional Network (GCN) and fully-connected Graph Attention Network\n(GAT) modules, which is intuitively pleasing since GCN-based local aggregations\nalone are insufficient to express the sought graph eigenvectors. We propose\nseveral refinements to the unrolled LASE architecture (such as sparse attention\nin the GAT module and decoupled layerwise parameters) that offer favorable\napproximation error versus computation tradeoffs; even outperforming\nheavily-optimized eigendecomposition routines from scientific computing\nlibraries. Because LASE is a differentiable function with respect to its\nparameters as well as its graph input, we can seamlessly integrate it as a\ntrainable module within a larger (semi-)supervised graph representation\nlearning pipeline. The resulting end-to-end system effectively learns\n``discriminative ASEs'' that exhibit competitive performance in supervised link\nprediction and node classification tasks, outperforming a GNN even when the\nlatter is endowed with open loop, meaning task-agnostic, precomputed spectral\npositional encodings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We put forth a principled design of a neural architecture to learn nodal\nAdjacency Spectral Embeddings (ASE) from graph inputs. By bringing to bear the\ngradient descent (GD) method and leveraging the principle of algorithm\nunrolling, we truncate and re-interpret each GD iteration as a layer in a graph\nneural network (GNN) that is trained to approximate the ASE. Accordingly, we\ncall the resulting embeddings and our parametric model Learned ASE (LASE),\nwhich is interpretable, parameter efficient, robust to inputs with unobserved\nedges, and offers controllable complexity during inference. LASE layers combine\nGraph Convolutional Network (GCN) and fully-connected Graph Attention Network\n(GAT) modules, which is intuitively pleasing since GCN-based local aggregations\nalone are insufficient to express the sought graph eigenvectors. We propose\nseveral refinements to the unrolled LASE architecture (such as sparse attention\nin the GAT module and decoupled layerwise parameters) that offer favorable\napproximation error versus computation tradeoffs; even outperforming\nheavily-optimized eigendecomposition routines from scientific computing\nlibraries. Because LASE is a differentiable function with respect to its\nparameters as well as its graph input, we can seamlessly integrate it as a\ntrainable module within a larger (semi-)supervised graph representation\nlearning pipeline. The resulting end-to-end system effectively learns\n``discriminative ASEs'' that exhibit competitive performance in supervised link\nprediction and node classification tasks, outperforming a GNN even when the\nlatter is endowed with open loop, meaning task-agnostic, precomputed spectral\npositional encodings."
                },
                "authors": [
                    {
                        "name": "Sof√≠a P√©rez Casulo"
                    },
                    {
                        "name": "Marcelo Fiori"
                    },
                    {
                        "name": "Federico Larroca"
                    },
                    {
                        "name": "Gonzalo Mateos"
                    }
                ],
                "author_detail": {
                    "name": "Gonzalo Mateos"
                },
                "author": "Gonzalo Mateos",
                "arxiv_journal_ref": "Transactions on Machine Learning Research, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17734v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20697v1",
                "updated": "2025-08-28T12:07:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    7,
                    11,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:07:11Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    7,
                    11,
                    3,
                    240,
                    0
                ],
                "title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning\n  Fine-Tuning"
                },
                "summary": "As large language models (LLMs) continue to grow in capability, so do the\nrisks of harmful misuse through fine-tuning. While most prior studies assume\nthat attackers rely on supervised fine-tuning (SFT) for such misuse, we\nsystematically demonstrate that reinforcement learning (RL) enables adversaries\nto more effectively break safety alignment and facilitate advanced harmful task\nassistance, under matched computational budgets. To counter this emerging\nthreat, we propose TokenBuncher, the first effective defense specifically\ntargeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation\non which RL relies: model response uncertainty. By constraining uncertainty,\nRL-based fine-tuning can no longer exploit distinct reward signals to drive the\nmodel toward harmful behaviors. We realize this defense through\nentropy-as-reward RL and a Token Noiser mechanism designed to prevent the\nescalation of expert-domain harmful capabilities. Extensive experiments across\nmultiple models and RL algorithms show that TokenBuncher robustly mitigates\nharmful RL fine-tuning while preserving benign task utility and finetunability.\nOur results highlight that RL-based harmful fine-tuning poses a greater\nsystemic risk than SFT, and that TokenBuncher provides an effective and general\ndefense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to grow in capability, so do the\nrisks of harmful misuse through fine-tuning. While most prior studies assume\nthat attackers rely on supervised fine-tuning (SFT) for such misuse, we\nsystematically demonstrate that reinforcement learning (RL) enables adversaries\nto more effectively break safety alignment and facilitate advanced harmful task\nassistance, under matched computational budgets. To counter this emerging\nthreat, we propose TokenBuncher, the first effective defense specifically\ntargeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation\non which RL relies: model response uncertainty. By constraining uncertainty,\nRL-based fine-tuning can no longer exploit distinct reward signals to drive the\nmodel toward harmful behaviors. We realize this defense through\nentropy-as-reward RL and a Token Noiser mechanism designed to prevent the\nescalation of expert-domain harmful capabilities. Extensive experiments across\nmultiple models and RL algorithms show that TokenBuncher robustly mitigates\nharmful RL fine-tuning while preserving benign task utility and finetunability.\nOur results highlight that RL-based harmful fine-tuning poses a greater\nsystemic risk than SFT, and that TokenBuncher provides an effective and general\ndefense."
                },
                "authors": [
                    {
                        "name": "Weitao Feng"
                    },
                    {
                        "name": "Lixu Wang"
                    },
                    {
                        "name": "Tianyi Wei"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Chongyang Gao"
                    },
                    {
                        "name": "Sinong Zhan"
                    },
                    {
                        "name": "Peizhuo Lv"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "arxiv_comment": "Project Hompage: https://tokenbuncher.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19724v2",
                "updated": "2025-08-28T12:05:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    5,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-27T09:34:28Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    9,
                    34,
                    28,
                    2,
                    239,
                    0
                ],
                "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for\n  Improving Small VLMs in Commonsense VQA Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLKI: A lightweight Natural Language Knowledge Integration Framework for\n  Improving Small VLMs in Commonsense VQA Tasks"
                },
                "summary": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models."
                },
                "authors": [
                    {
                        "name": "Aritra Dutta"
                    },
                    {
                        "name": "Swapnanil Mukherjee"
                    },
                    {
                        "name": "Deepanway Ghosal"
                    },
                    {
                        "name": "Somak Aditya"
                    }
                ],
                "author_detail": {
                    "name": "Somak Aditya"
                },
                "author": "Somak Aditya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20693v1",
                "updated": "2025-08-28T11:53:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    53,
                    45,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T11:53:45Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    53,
                    45,
                    3,
                    240,
                    0
                ],
                "title": "Leveraging Large Language Models for Generating Research Topic\n  Ontologies: A Multi-Disciplinary Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Generating Research Topic\n  Ontologies: A Multi-Disciplinary Study"
                },
                "summary": "Ontologies and taxonomies of research fields are critical for managing and\norganising scientific knowledge, as they facilitate efficient classification,\ndissemination and retrieval of information. However, the creation and\nmaintenance of such ontologies are expensive and time-consuming tasks, usually\nrequiring the coordinated effort of multiple domain experts. Consequently,\nontologies in this space often exhibit uneven coverage across different\ndisciplines, limited inter-domain connectivity, and infrequent updating cycles.\nIn this study, we investigate the capability of several large language models\nto identify semantic relationships among research topics within three academic\ndomains: biomedicine, physics, and engineering. The models were evaluated under\nthree distinct conditions: zero-shot prompting, chain-of-thought prompting, and\nfine-tuning on existing ontologies. Additionally, we assessed the cross-domain\ntransferability of fine-tuned models by measuring their performance when\ntrained in one domain and subsequently applied to a different one. To support\nthis analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over\n8,000 relationships extracted from the most widely adopted taxonomies in the\nthree disciplines considered in this study: MeSH, PhySH, and IEEE. Our\nexperiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent\nperformance across all disciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontologies and taxonomies of research fields are critical for managing and\norganising scientific knowledge, as they facilitate efficient classification,\ndissemination and retrieval of information. However, the creation and\nmaintenance of such ontologies are expensive and time-consuming tasks, usually\nrequiring the coordinated effort of multiple domain experts. Consequently,\nontologies in this space often exhibit uneven coverage across different\ndisciplines, limited inter-domain connectivity, and infrequent updating cycles.\nIn this study, we investigate the capability of several large language models\nto identify semantic relationships among research topics within three academic\ndomains: biomedicine, physics, and engineering. The models were evaluated under\nthree distinct conditions: zero-shot prompting, chain-of-thought prompting, and\nfine-tuning on existing ontologies. Additionally, we assessed the cross-domain\ntransferability of fine-tuned models by measuring their performance when\ntrained in one domain and subsequently applied to a different one. To support\nthis analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over\n8,000 relationships extracted from the most widely adopted taxonomies in the\nthree disciplines considered in this study: MeSH, PhySH, and IEEE. Our\nexperiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent\nperformance across all disciplines."
                },
                "authors": [
                    {
                        "name": "Tanay Aggarwal"
                    },
                    {
                        "name": "Angelo Salatino"
                    },
                    {
                        "name": "Francesco Osborne"
                    },
                    {
                        "name": "Enrico Motta"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Motta"
                },
                "author": "Enrico Motta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.13475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.13475v2",
                "updated": "2025-08-28T11:50:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    50,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2023-07-25T13:08:32Z",
                "published_parsed": [
                    2023,
                    7,
                    25,
                    13,
                    8,
                    32,
                    1,
                    206,
                    0
                ],
                "title": "Large sample properties of GMM estimators under second-order\n  identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large sample properties of GMM estimators under second-order\n  identification"
                },
                "summary": "Dovonon and Hall (Journal of Econometrics, 2018) proposed a limiting\ndistribution theory for GMM estimators for a p - dimensional globally\nidentified parameter vector {\\phi} when local identification conditions fail at\nfirst-order but hold at second-order. They assumed that the first-order\nunderidentification is due to the expected Jacobian having rank p-1 at the true\nvalue {\\phi}_{0}, i.e., having a rank deficiency of one. After reparametrizing\nthe model such that the last column of the Jacobian vanishes, they showed that\nthe GMM estimator of the first p-1 parameters converges at rate T^{-1/2} and\nthe GMM estimator of the remaining parameter, {\\phi}_{p}, converges at rate\nT^{-1/4}. They also provided a limiting distribution of\nT^{1/4}({\\phi}_{p}-{\\phi}_{0,p}) subject to a (non-transparent) condition which\nthey claimed to be not restrictive in general. However, as we show in this\npaper, their condition is in fact only satisfied when {\\phi} is overidentified\nand the limiting distribution of T^{1/4}({\\phi}_{p}-{\\phi}_{0,p}), which is\nnon-standard, depends on whether {\\phi} is exactly identified or\noveridentified. In particular, the limiting distributions of the sign of\nT^{1/4}({\\phi}_{p}-{\\phi}_{0,p}) for the cases of exact and overidentification,\nrespectively, are different and are obtained by using expansions of the GMM\nobjective function of different orders. Unsurprisingly, we find that the\nlimiting distribution theories of Dovonon and Hall (2018) for Indirect\nInference (II) estimation under two different scenarios with second-order\nidentification where the target function is a GMM estimator of the auxiliary\nparameter vector, are incomplete for similar reasons. We discuss how our\nresults for GMM estimation can be used to complete both theories and how they\ncan be used to obtain the limiting distributions of the II estimators in the\ncase of exact identification under either scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dovonon and Hall (Journal of Econometrics, 2018) proposed a limiting\ndistribution theory for GMM estimators for a p - dimensional globally\nidentified parameter vector {\\phi} when local identification conditions fail at\nfirst-order but hold at second-order. They assumed that the first-order\nunderidentification is due to the expected Jacobian having rank p-1 at the true\nvalue {\\phi}_{0}, i.e., having a rank deficiency of one. After reparametrizing\nthe model such that the last column of the Jacobian vanishes, they showed that\nthe GMM estimator of the first p-1 parameters converges at rate T^{-1/2} and\nthe GMM estimator of the remaining parameter, {\\phi}_{p}, converges at rate\nT^{-1/4}. They also provided a limiting distribution of\nT^{1/4}({\\phi}_{p}-{\\phi}_{0,p}) subject to a (non-transparent) condition which\nthey claimed to be not restrictive in general. However, as we show in this\npaper, their condition is in fact only satisfied when {\\phi} is overidentified\nand the limiting distribution of T^{1/4}({\\phi}_{p}-{\\phi}_{0,p}), which is\nnon-standard, depends on whether {\\phi} is exactly identified or\noveridentified. In particular, the limiting distributions of the sign of\nT^{1/4}({\\phi}_{p}-{\\phi}_{0,p}) for the cases of exact and overidentification,\nrespectively, are different and are obtained by using expansions of the GMM\nobjective function of different orders. Unsurprisingly, we find that the\nlimiting distribution theories of Dovonon and Hall (2018) for Indirect\nInference (II) estimation under two different scenarios with second-order\nidentification where the target function is a GMM estimator of the auxiliary\nparameter vector, are incomplete for similar reasons. We discuss how our\nresults for GMM estimation can be used to complete both theories and how they\ncan be used to obtain the limiting distributions of the II estimators in the\ncase of exact identification under either scenario."
                },
                "authors": [
                    {
                        "name": "Hugo Kruiniger"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Kruiniger"
                },
                "author": "Hugo Kruiniger",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.13475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.13475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62E20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16464v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16464v6",
                "updated": "2025-08-28T11:35:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    35,
                    48,
                    3,
                    240,
                    0
                ],
                "published": "2024-06-24T09:13:42Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    9,
                    13,
                    42,
                    0,
                    176,
                    0
                ],
                "title": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection"
                },
                "summary": "Sarcasm in social media, often expressed through text-image combinations,\nposes challenges for sentiment analysis and intention mining. Current\nmulti-modal sarcasm detection methods have been demonstrated to overly rely on\nspurious cues within the textual modality, revealing a limited ability to\ngenuinely identify sarcasm through nuanced text-image interactions. To solve\nthis problem, we propose InterCLIP-MEP, which introduces Interactive CLIP\n(InterCLIP) with an efficient training strategy to extract enriched text-image\nrepresentations by embedding cross-modal information directly into each\nencoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a\ndynamic dual-channel memory that stores valuable test sample knowledge during\ninference, acting as a non-parametric classifier for robust sarcasm\nrecognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP\nachieves state-of-the-art performance, with significant accuracy and F1 score\nimprovements on MMSD and MMSD2.0. Our code is available at\nhttps://github.com/CoderChen01/InterCLIP-MEP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sarcasm in social media, often expressed through text-image combinations,\nposes challenges for sentiment analysis and intention mining. Current\nmulti-modal sarcasm detection methods have been demonstrated to overly rely on\nspurious cues within the textual modality, revealing a limited ability to\ngenuinely identify sarcasm through nuanced text-image interactions. To solve\nthis problem, we propose InterCLIP-MEP, which introduces Interactive CLIP\n(InterCLIP) with an efficient training strategy to extract enriched text-image\nrepresentations by embedding cross-modal information directly into each\nencoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a\ndynamic dual-channel memory that stores valuable test sample knowledge during\ninference, acting as a non-parametric classifier for robust sarcasm\nrecognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP\nachieves state-of-the-art performance, with significant accuracy and F1 score\nimprovements on MMSD and MMSD2.0. Our code is available at\nhttps://github.com/CoderChen01/InterCLIP-MEP."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Subin Huang"
                    },
                    {
                        "name": "Sanmin Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ACM TOMM (Under Review); Code and data are available at\n  https://github.com/CoderChen01/InterCLIP-MEP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16464v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16464v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20670v1",
                "updated": "2025-08-28T11:22:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    22,
                    15,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T11:22:15Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    22,
                    15,
                    3,
                    240,
                    0
                ],
                "title": "\"Humor, Art, or Misinformation?\": A Multimodal Dataset for Intent-Aware\n  Synthetic Image Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Humor, Art, or Misinformation?\": A Multimodal Dataset for Intent-Aware\n  Synthetic Image Detection"
                },
                "summary": "Recent advances in multimodal AI have enabled progress in detecting synthetic\nand out-of-context content. However, existing efforts largely overlook the\nintent behind AI-generated images. To fill this gap, we introduce S-HArM, a\nmultimodal dataset for intent-aware classification, comprising 9,576 \"in the\nwild\" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,\nor Misinformation. Additionally, we explore three prompting strategies\n(image-guided, description-guided, and multimodally-guided) to construct a\nlarge-scale synthetic training dataset with Stable Diffusion. We conduct an\nextensive comparative study including modality fusion, contrastive learning,\nreconstruction networks, attention mechanisms, and large vision-language\nmodels. Our results show that models trained on image- and multimodally-guided\ndata generalize better to \"in the wild\" content, due to preserved visual\ncontext. However, overall performance remains limited, highlighting the\ncomplexity of inferring intent and the need for specialized architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal AI have enabled progress in detecting synthetic\nand out-of-context content. However, existing efforts largely overlook the\nintent behind AI-generated images. To fill this gap, we introduce S-HArM, a\nmultimodal dataset for intent-aware classification, comprising 9,576 \"in the\nwild\" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,\nor Misinformation. Additionally, we explore three prompting strategies\n(image-guided, description-guided, and multimodally-guided) to construct a\nlarge-scale synthetic training dataset with Stable Diffusion. We conduct an\nextensive comparative study including modality fusion, contrastive learning,\nreconstruction networks, attention mechanisms, and large vision-language\nmodels. Our results show that models trained on image- and multimodally-guided\ndata generalize better to \"in the wild\" content, due to preserved visual\ncontext. However, overall performance remains limited, highlighting the\ncomplexity of inferring intent and the need for specialized architectures."
                },
                "authors": [
                    {
                        "name": "Anastasios Skoularikis"
                    },
                    {
                        "name": "Stefanos-Iordanis Papadopoulos"
                    },
                    {
                        "name": "Symeon Papadopoulos"
                    },
                    {
                        "name": "Panagiotis C. Petrantonakis"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis C. Petrantonakis"
                },
                "author": "Panagiotis C. Petrantonakis",
                "arxiv_doi": "10.1145/3746275.3762215",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746275.3762215",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20660v1",
                "updated": "2025-08-28T11:07:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    7,
                    36,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T11:07:36Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    7,
                    36,
                    3,
                    240,
                    0
                ],
                "title": "CodecBench: A Comprehensive Benchmark for Acoustic and Semantic\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodecBench: A Comprehensive Benchmark for Acoustic and Semantic\n  Evaluation"
                },
                "summary": "With the rise of multimodal large language models (LLMs), audio codec plays\nan increasingly vital role in encoding audio into discrete tokens, enabling\nintegration of audio into text-based LLMs. Current audio codec captures two\ntypes of information: acoustic and semantic. As audio codec is applied to\ndiverse scenarios in speech language model , it needs to model increasingly\ncomplex information and adapt to varied contexts, such as scenarios with\nmultiple speakers, background noise, or richer paralinguistic information.\nHowever, existing codec's own evaluation has been limited by simplistic metrics\nand scenarios, and existing benchmarks for audio codec are not designed for\ncomplex application scenarios, which limits the assessment performance on\ncomplex datasets for acoustic and semantic capabilities. We introduce\nCodecBench, a comprehensive evaluation dataset to assess audio codec\nperformance from both acoustic and semantic perspectives across four data\ndomains. Through this benchmark, we aim to identify current limitations,\nhighlight future research directions, and foster advances in the development of\naudio codec. The codes are available at https://github.com/RayYuki/CodecBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of multimodal large language models (LLMs), audio codec plays\nan increasingly vital role in encoding audio into discrete tokens, enabling\nintegration of audio into text-based LLMs. Current audio codec captures two\ntypes of information: acoustic and semantic. As audio codec is applied to\ndiverse scenarios in speech language model , it needs to model increasingly\ncomplex information and adapt to varied contexts, such as scenarios with\nmultiple speakers, background noise, or richer paralinguistic information.\nHowever, existing codec's own evaluation has been limited by simplistic metrics\nand scenarios, and existing benchmarks for audio codec are not designed for\ncomplex application scenarios, which limits the assessment performance on\ncomplex datasets for acoustic and semantic capabilities. We introduce\nCodecBench, a comprehensive evaluation dataset to assess audio codec\nperformance from both acoustic and semantic perspectives across four data\ndomains. Through this benchmark, we aim to identify current limitations,\nhighlight future research directions, and foster advances in the development of\naudio codec. The codes are available at https://github.com/RayYuki/CodecBench."
                },
                "authors": [
                    {
                        "name": "Ruifan Deng"
                    },
                    {
                        "name": "Yitian Gong"
                    },
                    {
                        "name": "Qinghui Gao"
                    },
                    {
                        "name": "Luozhijie Jin"
                    },
                    {
                        "name": "Qinyuan Cheng"
                    },
                    {
                        "name": "Zhaoye Fei"
                    },
                    {
                        "name": "Shimin Li"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2105.02597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2105.02597v2",
                "updated": "2025-08-28T11:03:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    3,
                    40,
                    3,
                    240,
                    0
                ],
                "published": "2021-05-06T11:56:27Z",
                "published_parsed": [
                    2021,
                    5,
                    6,
                    11,
                    56,
                    27,
                    3,
                    126,
                    0
                ],
                "title": "Extreme Learning Machine for the Characterization of Anomalous Diffusion\n  from Single Trajectories (AnDi-ELM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Learning Machine for the Characterization of Anomalous Diffusion\n  from Single Trajectories (AnDi-ELM)"
                },
                "summary": "The study of the dynamics of natural and artificial systems has provided\nseveral examples of deviations from Brownian behavior, generally defined as\nanomalous diffusion. The investigation of these dynamics can provide a better\nunderstanding of diffusing objects and their surrounding media, but a\nquantitative characterization from individual trajectories is often\nchallenging. Efforts devoted to improving anomalous diffusion detection using\nclassical statistics and machine learning have produced several new methods.\nRecently, the anomalous diffusion challenge (AnDi, www.andi-challenge.org) was\nlaunched to objectively assess these approaches on a common dataset, focusing\non three aspects of anomalous diffusion: the inference of the anomalous\ndiffusion exponent; the classification of the diffusion model; and the\nsegmentation of trajectories. In this article, I describe a simple approach to\ntackle the tasks of the AnDi challenge by combining extreme learning machine\nand feature engineering (AnDi-ELM). The method reaches satisfactory performance\nwhile offering a straightforward implementation and fast training time with\nlimited computing resources, making it a suitable tool for fast preliminary\nscreening of anomalous diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of the dynamics of natural and artificial systems has provided\nseveral examples of deviations from Brownian behavior, generally defined as\nanomalous diffusion. The investigation of these dynamics can provide a better\nunderstanding of diffusing objects and their surrounding media, but a\nquantitative characterization from individual trajectories is often\nchallenging. Efforts devoted to improving anomalous diffusion detection using\nclassical statistics and machine learning have produced several new methods.\nRecently, the anomalous diffusion challenge (AnDi, www.andi-challenge.org) was\nlaunched to objectively assess these approaches on a common dataset, focusing\non three aspects of anomalous diffusion: the inference of the anomalous\ndiffusion exponent; the classification of the diffusion model; and the\nsegmentation of trajectories. In this article, I describe a simple approach to\ntackle the tasks of the AnDi challenge by combining extreme learning machine\nand feature engineering (AnDi-ELM). The method reaches satisfactory performance\nwhile offering a straightforward implementation and fast training time with\nlimited computing resources, making it a suitable tool for fast preliminary\nscreening of anomalous diffusion."
                },
                "authors": [
                    {
                        "name": "Carlo Manzo"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Manzo"
                },
                "author": "Carlo Manzo",
                "arxiv_doi": "10.1088/1751-8121/ac13dd",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1751-8121/ac13dd",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2105.02597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2105.02597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 7 figures. Author's Accepted Manuscript of a published\n  article in J. Phys. A: Math. Theor. 54: 334002 (2021).\n  https://doi.org/10.1088/1751-8121/ac13dd",
                "arxiv_journal_ref": "J. Phys. A: Math. Theor. 54, 334002 (2021)",
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20657v1",
                "updated": "2025-08-28T11:02:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    2,
                    32,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T11:02:32Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    2,
                    32,
                    3,
                    240,
                    0
                ],
                "title": "Prediction of EDS Maps from 4DSTEM Diffraction Patterns Using\n  Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction of EDS Maps from 4DSTEM Diffraction Patterns Using\n  Convolutional Neural Networks"
                },
                "summary": "Understanding the relationship between atomic structure (order) and chemical\ncomposition (chemistry) is critical for advancing materials science, yet\ntraditional spectroscopic techniques can be slow and damaging to sensitive\nsamples. Four-dimensional scanning transmission electron microscopy (4D-STEM)\ncaptures detailed diffraction patterns across scanned regions, providing rich\nstructural information, while energy dispersive X-ray spectroscopy (EDS) offers\ncomplementary chemical data. In this work, we develop a machine learning\nframework that predicts EDS spectra directly from 4D-STEM diffraction patterns,\nreducing beam exposure and acquisition time. A convolutional neural network\n(CNN) accurately infers elemental compositions, particularly for elements with\nstrong diffraction contrast or higher concentrations, such as Oxygen and\nTellurium. Both extrapolation and interpolation strategies demonstrate\nconsistent performance, with improved predictions when additional structural\ncontext is available. Visual and cross-correlation analyses confirm the model's\nability to capture global and local compositional trends. This approach\nestablishes a data-driven pathway to non-destructive, high-throughput materials\ncharacterization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the relationship between atomic structure (order) and chemical\ncomposition (chemistry) is critical for advancing materials science, yet\ntraditional spectroscopic techniques can be slow and damaging to sensitive\nsamples. Four-dimensional scanning transmission electron microscopy (4D-STEM)\ncaptures detailed diffraction patterns across scanned regions, providing rich\nstructural information, while energy dispersive X-ray spectroscopy (EDS) offers\ncomplementary chemical data. In this work, we develop a machine learning\nframework that predicts EDS spectra directly from 4D-STEM diffraction patterns,\nreducing beam exposure and acquisition time. A convolutional neural network\n(CNN) accurately infers elemental compositions, particularly for elements with\nstrong diffraction contrast or higher concentrations, such as Oxygen and\nTellurium. Both extrapolation and interpolation strategies demonstrate\nconsistent performance, with improved predictions when additional structural\ncontext is available. Visual and cross-correlation analyses confirm the model's\nability to capture global and local compositional trends. This approach\nestablishes a data-driven pathway to non-destructive, high-throughput materials\ncharacterization."
                },
                "authors": [
                    {
                        "name": "Mridul Kumar"
                    },
                    {
                        "name": "Yevgeny Rakita"
                    }
                ],
                "author_detail": {
                    "name": "Yevgeny Rakita"
                },
                "author": "Yevgeny Rakita",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20656v1",
                "updated": "2025-08-28T11:02:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    2,
                    21,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T11:02:21Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    2,
                    21,
                    3,
                    240,
                    0
                ],
                "title": "Compositionality in Time Series: A Proof of Concept using Symbolic\n  Dynamics and Compositional Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositionality in Time Series: A Proof of Concept using Symbolic\n  Dynamics and Compositional Data Augmentation"
                },
                "summary": "This work investigates whether time series of natural phenomena can be\nunderstood as being generated by sequences of latent states which are ordered\nin systematic and regular ways. We focus on clinical time series and ask\nwhether clinical measurements can be interpreted as being generated by\nmeaningful physiological states whose succession follows systematic principles.\nUncovering the underlying compositional structure will allow us to create\nsynthetic data to alleviate the notorious problem of sparse and low-resource\ndata settings in clinical time series forecasting, and deepen our understanding\nof clinical data. We start by conceptualizing compositionality for time series\nas a property of the data generation process, and then study data-driven\nprocedures that can reconstruct the elementary states and composition rules of\nthis process. We evaluate the success of this methods using two empirical tests\noriginating from a domain adaptation perspective. Both tests infer the\nsimilarity of the original time series distribution and the synthetic time\nseries distribution from the similarity of expected risk of time series\nforecasting models trained and tested on original and synthesized data in\nspecific ways. Our experimental results show that the test set performance\nachieved by training on compositionally synthesized data is comparable to\ntraining on original clinical time series data, and that evaluation of models\non compositionally synthesized test data shows similar results to evaluating on\noriginal test data, outperforming randomization-based data augmentation. An\nadditional downstream evaluation of the prediction task of sequential organ\nfailure assessment (SOFA) scores shows significant performance gains when model\ntraining is entirely based on compositionally synthesized data compared to\ntraining on original data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates whether time series of natural phenomena can be\nunderstood as being generated by sequences of latent states which are ordered\nin systematic and regular ways. We focus on clinical time series and ask\nwhether clinical measurements can be interpreted as being generated by\nmeaningful physiological states whose succession follows systematic principles.\nUncovering the underlying compositional structure will allow us to create\nsynthetic data to alleviate the notorious problem of sparse and low-resource\ndata settings in clinical time series forecasting, and deepen our understanding\nof clinical data. We start by conceptualizing compositionality for time series\nas a property of the data generation process, and then study data-driven\nprocedures that can reconstruct the elementary states and composition rules of\nthis process. We evaluate the success of this methods using two empirical tests\noriginating from a domain adaptation perspective. Both tests infer the\nsimilarity of the original time series distribution and the synthetic time\nseries distribution from the similarity of expected risk of time series\nforecasting models trained and tested on original and synthesized data in\nspecific ways. Our experimental results show that the test set performance\nachieved by training on compositionally synthesized data is comparable to\ntraining on original clinical time series data, and that evaluation of models\non compositionally synthesized test data shows similar results to evaluating on\noriginal test data, outperforming randomization-based data augmentation. An\nadditional downstream evaluation of the prediction task of sequential organ\nfailure assessment (SOFA) scores shows significant performance gains when model\ntraining is entirely based on compositionally synthesized data compared to\ntraining on original data."
                },
                "authors": [
                    {
                        "name": "Michael Hagmann"
                    },
                    {
                        "name": "Michael Staniek"
                    },
                    {
                        "name": "Stefan Riezler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Riezler"
                },
                "author": "Stefan Riezler",
                "arxiv_journal_ref": "Transactions on Machine Learning Research (TMLR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20655v1",
                "updated": "2025-08-28T11:01:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    1,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T11:01:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    1,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Alignment in LVLMs with Debiased Self-Judgment"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Weilong Yan"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19918v3",
                "updated": "2025-08-29T02:12:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    12,
                    5,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-27T14:24:13Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    14,
                    24,
                    13,
                    2,
                    239,
                    0
                ],
                "title": "Refining Text Generation for Realistic Conversational Recommendation via\n  Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Text Generation for Realistic Conversational Recommendation via\n  Direct Preference Optimization"
                },
                "summary": "Conversational Recommender Systems (CRSs) aim to elicit user preferences via\nnatural dialogue to provide suitable item recommendations. However, current\nCRSs often deviate from realistic human interactions by rapidly recommending\nitems in brief sessions. This work addresses this gap by leveraging Large\nLanguage Models (LLMs) to generate dialogue summaries from dialogue history and\nitem recommendation information from item description. This approach enables\nthe extraction of both explicit user statements and implicit preferences\ninferred from the dialogue context. We introduce a method using Direct\nPreference Optimization (DPO) to ensure dialogue summary and item\nrecommendation information are rich in information crucial for effective\nrecommendations. Experiments on two public datasets validate our method's\neffectiveness in fostering more natural and realistic conversational\nrecommendation processes. Our implementation is publicly available at:\nhttps://github.com/UEC-InabaLab/Refining-LLM-Text",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs) aim to elicit user preferences via\nnatural dialogue to provide suitable item recommendations. However, current\nCRSs often deviate from realistic human interactions by rapidly recommending\nitems in brief sessions. This work addresses this gap by leveraging Large\nLanguage Models (LLMs) to generate dialogue summaries from dialogue history and\nitem recommendation information from item description. This approach enables\nthe extraction of both explicit user statements and implicit preferences\ninferred from the dialogue context. We introduce a method using Direct\nPreference Optimization (DPO) to ensure dialogue summary and item\nrecommendation information are rich in information crucial for effective\nrecommendations. Experiments on two public datasets validate our method's\neffectiveness in fostering more natural and realistic conversational\nrecommendation processes. Our implementation is publicly available at:\nhttps://github.com/UEC-InabaLab/Refining-LLM-Text"
                },
                "authors": [
                    {
                        "name": "Manato Tajiri"
                    },
                    {
                        "name": "Michimasa Inaba"
                    }
                ],
                "author_detail": {
                    "name": "Michimasa Inaba"
                },
                "author": "Michimasa Inaba",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20643v1",
                "updated": "2025-08-28T10:45:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    45,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T10:45:31Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    45,
                    31,
                    3,
                    240,
                    0
                ],
                "title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics"
                },
                "summary": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents."
                },
                "authors": [
                    {
                        "name": "Stefano Fumero"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Matteo Boffa"
                    },
                    {
                        "name": "Danilo Giordano"
                    },
                    {
                        "name": "Marco Mellia"
                    },
                    {
                        "name": "Zied Ben Houidi"
                    },
                    {
                        "name": "Dario Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Dario Rossi"
                },
                "author": "Dario Rossi",
                "arxiv_comment": "Code:\n  https://github.com/SmartData-Polito/LLM_Agent_Cybersecurity_Forensic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20642v1",
                "updated": "2025-08-28T10:41:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    41,
                    41,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T10:41:41Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    41,
                    41,
                    3,
                    240,
                    0
                ],
                "title": "Emergent dynamics of active elastic microbeams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent dynamics of active elastic microbeams"
                },
                "summary": "In equilibrium, the physical properties of matter are set by the interactions\nbetween the constituents. In contrast, the energy input of the individual\ncomponents controls the behavior of synthetic or living active matter. Great\nprogress has been made in understanding the emergent phenomena in active\nfluids, though their inability to resist shear forces hinders their practical\nuse. This motivates the exploration of active solids as shape-shifting\nmaterials, yet, we lack controlled synthetic systems to devise active solids\nwith unconventional properties. %and bridge the gap between active solids made\nof macroscopic robots and the complexity of biological materials. Here we build\nactive elastic beams from dozens of active colloids and unveil complex emergent\nbehaviors such as self-oscillations or persistent rotations. Developing tensile\ntests at the microscale, we show that the active beams are ultra-soft\nmaterials, with large (non-equilibrium) fluctuations. Combining experiments,\ntheory, and stochastic inference, we show that the dynamics of the active beams\ncan be mapped on different phase transitions which are tuned by boundary\nconditions. More quantitatively, we assess all relevant parameters by\nindependent measurements or first-principles calculations, and find that our\ntheoretical description agrees with the experimental observations. Our results\ndemonstrate that the simple addition of activity to an elastic beam unveils\nnovel physics and can inspire design strategies for active solids and\nfunctional microscopic machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In equilibrium, the physical properties of matter are set by the interactions\nbetween the constituents. In contrast, the energy input of the individual\ncomponents controls the behavior of synthetic or living active matter. Great\nprogress has been made in understanding the emergent phenomena in active\nfluids, though their inability to resist shear forces hinders their practical\nuse. This motivates the exploration of active solids as shape-shifting\nmaterials, yet, we lack controlled synthetic systems to devise active solids\nwith unconventional properties. %and bridge the gap between active solids made\nof macroscopic robots and the complexity of biological materials. Here we build\nactive elastic beams from dozens of active colloids and unveil complex emergent\nbehaviors such as self-oscillations or persistent rotations. Developing tensile\ntests at the microscale, we show that the active beams are ultra-soft\nmaterials, with large (non-equilibrium) fluctuations. Combining experiments,\ntheory, and stochastic inference, we show that the dynamics of the active beams\ncan be mapped on different phase transitions which are tuned by boundary\nconditions. More quantitatively, we assess all relevant parameters by\nindependent measurements or first-principles calculations, and find that our\ntheoretical description agrees with the experimental observations. Our results\ndemonstrate that the simple addition of activity to an elastic beam unveils\nnovel physics and can inspire design strategies for active solids and\nfunctional microscopic machines."
                },
                "authors": [
                    {
                        "name": "Q. Martinet"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "A. Aubret"
                    },
                    {
                        "name": "E. Hannezo"
                    },
                    {
                        "name": "J. Palacci"
                    }
                ],
                "author_detail": {
                    "name": "J. Palacci"
                },
                "author": "J. Palacci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20637v1",
                "updated": "2025-08-28T10:35:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    35,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T10:35:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    35,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "GDS Agent: A Graph Algorithmic Reasoning Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDS Agent: A Graph Algorithmic Reasoning Agent"
                },
                "summary": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We also\nintroduce a new benchmark that evaluates intermediate tool calls as well as\nfinal responses. The results indicate that GDS agent is able to solve a wide\nspectrum of graph tasks. We also provide detailed case studies for more\nopen-ended tasks and study scenarios where the agent struggles. Finally, we\ndiscuss the remaining challenges and the future roadmap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We also\nintroduce a new benchmark that evaluates intermediate tool calls as well as\nfinal responses. The results indicate that GDS agent is able to solve a wide\nspectrum of graph tasks. We also provide detailed case studies for more\nopen-ended tasks and study scenarios where the agent struggles. Finally, we\ndiscuss the remaining challenges and the future roadmap."
                },
                "authors": [
                    {
                        "name": "Borun Shi"
                    },
                    {
                        "name": "Ioannis Panagiotas"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Panagiotas"
                },
                "author": "Ioannis Panagiotas",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20635v1",
                "updated": "2025-08-28T10:34:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    34,
                    50,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T10:34:50Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    34,
                    50,
                    3,
                    240,
                    0
                ],
                "title": "Schema-Guided Response Generation using Multi-Frame Dialogue State for\n  Motivational Interviewing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema-Guided Response Generation using Multi-Frame Dialogue State for\n  Motivational Interviewing Systems"
                },
                "summary": "The primary goal of Motivational Interviewing (MI) is to help clients build\ntheir own motivation for behavioral change. To support this in dialogue\nsystems, it is essential to guide large language models (LLMs) to generate\ncounselor responses aligned with MI principles. By employing a schema-guided\napproach, this study proposes a method for updating multi-frame dialogue states\nand a strategy decision mechanism that dynamically determines the response\nfocus in a manner grounded in MI principles. The proposed method was\nimplemented in a dialogue system and evaluated through a user study. Results\nshowed that the proposed system successfully generated MI-favorable responses\nand effectively encouraged the user's (client's) deliberation by asking\neliciting questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary goal of Motivational Interviewing (MI) is to help clients build\ntheir own motivation for behavioral change. To support this in dialogue\nsystems, it is essential to guide large language models (LLMs) to generate\ncounselor responses aligned with MI principles. By employing a schema-guided\napproach, this study proposes a method for updating multi-frame dialogue states\nand a strategy decision mechanism that dynamically determines the response\nfocus in a manner grounded in MI principles. The proposed method was\nimplemented in a dialogue system and evaluated through a user study. Results\nshowed that the proposed system successfully generated MI-favorable responses\nand effectively encouraged the user's (client's) deliberation by asking\neliciting questions."
                },
                "authors": [
                    {
                        "name": "Jie Zeng"
                    },
                    {
                        "name": "Yukiko I. Nakano"
                    }
                ],
                "author_detail": {
                    "name": "Yukiko I. Nakano"
                },
                "author": "Yukiko I. Nakano",
                "arxiv_comment": "28pages, 15 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19720v2",
                "updated": "2025-08-28T10:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    0,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-27T09:30:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    9,
                    30,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with\n  Proxy Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with\n  Proxy Models"
                },
                "summary": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS."
                },
                "authors": [
                    {
                        "name": "Yilin Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Yuyang Bai"
                    },
                    {
                        "name": "Minnan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Minnan Luo"
                },
                "author": "Minnan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20613v1",
                "updated": "2025-08-28T10:00:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    0,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T10:00:39Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    0,
                    39,
                    3,
                    240,
                    0
                ],
                "title": "Revisiting the Privacy Risks of Split Inference: A GAN-Based Data\n  Reconstruction Attack via Progressive Feature Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Privacy Risks of Split Inference: A GAN-Based Data\n  Reconstruction Attack via Progressive Feature Optimization"
                },
                "summary": "The growing complexity of Deep Neural Networks (DNNs) has led to the adoption\nof Split Inference (SI), a collaborative paradigm that partitions computation\nbetween edge devices and the cloud to reduce latency and protect user privacy.\nHowever, recent advances in Data Reconstruction Attacks (DRAs) reveal that\nintermediate features exchanged in SI can be exploited to recover sensitive\ninput data, posing significant privacy risks. Existing DRAs are typically\neffective only on shallow models and fail to fully leverage semantic priors,\nlimiting their reconstruction quality and generalizability across datasets and\nmodel architectures. In this paper, we propose a novel GAN-based DRA framework\nwith Progressive Feature Optimization (PFO), which decomposes the generator\ninto hierarchical blocks and incrementally refines intermediate representations\nto enhance the semantic fidelity of reconstructed images. To stabilize the\noptimization and improve image realism, we introduce an L1-ball constraint\nduring reconstruction. Extensive experiments show that our method outperforms\nprior attacks by a large margin, especially in high-resolution scenarios,\nout-of-distribution settings, and against deeper and more complex DNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of Deep Neural Networks (DNNs) has led to the adoption\nof Split Inference (SI), a collaborative paradigm that partitions computation\nbetween edge devices and the cloud to reduce latency and protect user privacy.\nHowever, recent advances in Data Reconstruction Attacks (DRAs) reveal that\nintermediate features exchanged in SI can be exploited to recover sensitive\ninput data, posing significant privacy risks. Existing DRAs are typically\neffective only on shallow models and fail to fully leverage semantic priors,\nlimiting their reconstruction quality and generalizability across datasets and\nmodel architectures. In this paper, we propose a novel GAN-based DRA framework\nwith Progressive Feature Optimization (PFO), which decomposes the generator\ninto hierarchical blocks and incrementally refines intermediate representations\nto enhance the semantic fidelity of reconstructed images. To stabilize the\noptimization and improve image realism, we introduce an L1-ball constraint\nduring reconstruction. Extensive experiments show that our method outperforms\nprior attacks by a large margin, especially in high-resolution scenarios,\nout-of-distribution settings, and against deeper and more complex DNNs."
                },
                "authors": [
                    {
                        "name": "Yixiang Qiu"
                    },
                    {
                        "name": "Yanhan Liu"
                    },
                    {
                        "name": "Hongyao Yu"
                    },
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Ke Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ke Xu"
                },
                "author": "Ke Xu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11752v2",
                "updated": "2025-08-28T09:45:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    45,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-06-13T13:05:41Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    5,
                    41,
                    4,
                    164,
                    0
                ],
                "title": "DART: Distilling Autoregressive Reasoning to Silent Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: Distilling Autoregressive Reasoning to Silent Thought"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART offers significant\nperformance gains compared with existing non-autoregressive baselines without\nextra inference latency, serving as a feasible alternative for efficient\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART offers significant\nperformance gains compared with existing non-autoregressive baselines without\nextra inference latency, serving as a feasible alternative for efficient\nreasoning."
                },
                "authors": [
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Ziming Wu"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Fuming Lai"
                    },
                    {
                        "name": "Shaobing Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shaobing Lian"
                },
                "author": "Shaobing Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.21065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21065v1",
                "updated": "2025-08-28T17:59:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    59,
                    34,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:59:34Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    59,
                    34,
                    3,
                    240,
                    0
                ],
                "title": "Learning on the Fly: Rapid Policy Adaptation via Differentiable\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning on the Fly: Rapid Policy Adaptation via Differentiable\n  Simulation"
                },
                "summary": "Learning control policies in simulation enables rapid, safe, and\ncost-effective development of advanced robotic capabilities. However,\ntransferring these policies to the real world remains difficult due to the\nsim-to-real gap, where unmodeled dynamics and environmental disturbances can\ndegrade policy performance. Existing approaches, such as domain randomization\nand Real2Sim2Real pipelines, can improve policy robustness, but either struggle\nunder out-of-distribution conditions or require costly offline retraining. In\nthis work, we approach these problems from a different perspective. Instead of\nrelying on diverse training conditions before deployment, we focus on rapidly\nadapting the learned policy in the real world in an online fashion. To achieve\nthis, we propose a novel online adaptive learning framework that unifies\nresidual dynamics learning with real-time policy adaptation inside a\ndifferentiable simulation. Starting from a simple dynamics model, our framework\nrefines the model continuously with real-world data to capture unmodeled\neffects and disturbances such as payload changes and wind. The refined dynamics\nmodel is embedded in a differentiable simulation framework, enabling gradient\nbackpropagation through the dynamics and thus rapid, sample-efficient policy\nupdates beyond the reach of classical RL methods like PPO. All components of\nour system are designed for rapid adaptation, enabling the policy to adjust to\nunseen disturbances within 5 seconds of training. We validate the approach on\nagile quadrotor control under various disturbances in both simulation and the\nreal world. Our framework reduces hovering error by up to 81% compared to\nL1-MPC and 55% compared to DATT, while also demonstrating robustness in\nvision-based control without explicit state estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning control policies in simulation enables rapid, safe, and\ncost-effective development of advanced robotic capabilities. However,\ntransferring these policies to the real world remains difficult due to the\nsim-to-real gap, where unmodeled dynamics and environmental disturbances can\ndegrade policy performance. Existing approaches, such as domain randomization\nand Real2Sim2Real pipelines, can improve policy robustness, but either struggle\nunder out-of-distribution conditions or require costly offline retraining. In\nthis work, we approach these problems from a different perspective. Instead of\nrelying on diverse training conditions before deployment, we focus on rapidly\nadapting the learned policy in the real world in an online fashion. To achieve\nthis, we propose a novel online adaptive learning framework that unifies\nresidual dynamics learning with real-time policy adaptation inside a\ndifferentiable simulation. Starting from a simple dynamics model, our framework\nrefines the model continuously with real-world data to capture unmodeled\neffects and disturbances such as payload changes and wind. The refined dynamics\nmodel is embedded in a differentiable simulation framework, enabling gradient\nbackpropagation through the dynamics and thus rapid, sample-efficient policy\nupdates beyond the reach of classical RL methods like PPO. All components of\nour system are designed for rapid adaptation, enabling the policy to adjust to\nunseen disturbances within 5 seconds of training. We validate the approach on\nagile quadrotor control under various disturbances in both simulation and the\nreal world. Our framework reduces hovering error by up to 81% compared to\nL1-MPC and 55% compared to DATT, while also demonstrating robustness in\nvision-based control without explicit state estimation."
                },
                "authors": [
                    {
                        "name": "Jiahe Pan"
                    },
                    {
                        "name": "Jiaxu Xing"
                    },
                    {
                        "name": "Rudolf Reiter"
                    },
                    {
                        "name": "Yifan Zhai"
                    },
                    {
                        "name": "Elie Aljalbout"
                    },
                    {
                        "name": "Davide Scaramuzza"
                    }
                ],
                "author_detail": {
                    "name": "Davide Scaramuzza"
                },
                "author": "Davide Scaramuzza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14862v2",
                "updated": "2025-08-28T17:59:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    59,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2024-05-23T17:59:22Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    59,
                    22,
                    3,
                    144,
                    0
                ],
                "title": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs"
                },
                "summary": "Decoder-only large language models typically rely solely on masked causal\nattention, which limits their expressiveness by restricting information flow to\none direction. We propose Bitune, a method that enhances pretrained\ndecoder-only LLMs by incorporating bidirectional attention into prompt\nprocessing. We evaluate Bitune in instruction-tuning and question-answering\nsettings, showing significant improvements in performance on commonsense\nreasoning, arithmetic, and language understanding tasks. Furthermore, extensive\nablation studies validate the role of each component of the method, and\ndemonstrate that Bitune is compatible with various parameter-efficient\nfinetuning techniques and full model finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only large language models typically rely solely on masked causal\nattention, which limits their expressiveness by restricting information flow to\none direction. We propose Bitune, a method that enhances pretrained\ndecoder-only LLMs by incorporating bidirectional attention into prompt\nprocessing. We evaluate Bitune in instruction-tuning and question-answering\nsettings, showing significant improvements in performance on commonsense\nreasoning, arithmetic, and language understanding tasks. Furthermore, extensive\nablation studies validate the role of each component of the method, and\ndemonstrate that Bitune is compatible with various parameter-efficient\nfinetuning techniques and full model finetuning."
                },
                "authors": [
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21061v1",
                "updated": "2025-08-28T17:58:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    58,
                    29,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:58:29Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    58,
                    29,
                    3,
                    240,
                    0
                ],
                "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn\n  Dialogue with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn\n  Dialogue with Large Language Models"
                },
                "summary": "As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance."
                },
                "authors": [
                    {
                        "name": "Adam Coscia"
                    },
                    {
                        "name": "Shunan Guo"
                    },
                    {
                        "name": "Eunyee Koh"
                    },
                    {
                        "name": "Alex Endert"
                    }
                ],
                "author_detail": {
                    "name": "Alex Endert"
                },
                "author": "Alex Endert",
                "arxiv_doi": "10.1145/3746059.3747746",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746059.3747746",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.21061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to UIST 2025. 18 pages, 9 figures, 2 tables. For a demo\n  video, see https://youtu.be/uobhmxo6EIE",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21051v1",
                "updated": "2025-08-28T17:55:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    55,
                    7,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:55:07Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    55,
                    7,
                    3,
                    240,
                    0
                ],
                "title": "Enabling Equitable Access to Trustworthy Financial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Equitable Access to Trustworthy Financial Reasoning"
                },
                "summary": "According to the United States Internal Revenue Service, ''the average\nAmerican spends $\\$270$ and 13 hours filing their taxes''. Even beyond the\nU.S., tax filing requires complex reasoning, combining application of\noverlapping rules with numerical calculations. Because errors can incur costly\npenalties, any automated system must deliver high accuracy and auditability,\nmaking modern large language models (LLMs) poorly suited for this task. We\npropose an approach that integrates LLMs with a symbolic solver to calculate\ntax obligations. We evaluate variants of this system on the challenging\nStAtutory Reasoning Assessment (SARA) dataset, and include a novel method for\nestimating the cost of deploying such a system based on real-world penalties\nfor tax errors. We further show how combining up-front translation of\nplain-text rules into formal logic programs, combined with intelligently\nretrieved exemplars for formal case representations, can dramatically improve\nperformance on this task and reduce costs to well below real-world averages.\nOur results demonstrate the promise and economic feasibility of neuro-symbolic\narchitectures for increasing equitable access to reliable tax assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "According to the United States Internal Revenue Service, ''the average\nAmerican spends $\\$270$ and 13 hours filing their taxes''. Even beyond the\nU.S., tax filing requires complex reasoning, combining application of\noverlapping rules with numerical calculations. Because errors can incur costly\npenalties, any automated system must deliver high accuracy and auditability,\nmaking modern large language models (LLMs) poorly suited for this task. We\npropose an approach that integrates LLMs with a symbolic solver to calculate\ntax obligations. We evaluate variants of this system on the challenging\nStAtutory Reasoning Assessment (SARA) dataset, and include a novel method for\nestimating the cost of deploying such a system based on real-world penalties\nfor tax errors. We further show how combining up-front translation of\nplain-text rules into formal logic programs, combined with intelligently\nretrieved exemplars for formal case representations, can dramatically improve\nperformance on this task and reduce costs to well below real-world averages.\nOur results demonstrate the promise and economic feasibility of neuro-symbolic\narchitectures for increasing equitable access to reliable tax assistance."
                },
                "authors": [
                    {
                        "name": "William Jurayj"
                    },
                    {
                        "name": "Nils Holzenberger"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10175v2",
                "updated": "2025-08-28T17:54:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    54,
                    36,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-13T20:22:58Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    20,
                    22,
                    58,
                    2,
                    225,
                    0
                ],
                "title": "Estimating Machine Translation Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Machine Translation Difficulty"
                },
                "summary": "Machine translation quality has steadily improved over the years, achieving\nnear-perfect translations in recent benchmarks. These high-quality outputs make\nit difficult to distinguish between state-of-the-art models and to identify\nareas for future improvement. In this context, automatically identifying texts\nwhere machine translation systems struggle holds promise for developing more\ndiscriminative evaluations and guiding future research.\n  In this work, we address this gap by formalizing the task of translation\ndifficulty estimation, defining a text's difficulty based on the expected\nquality of its translations. We introduce a new metric to evaluate difficulty\nestimators and use it to assess both baselines and novel approaches. Finally,\nwe demonstrate the practical utility of difficulty estimators by using them to\nconstruct more challenging benchmarks for machine translation. Our results show\nthat dedicated models outperform both heuristic-based methods and\nLLM-as-a-judge approaches, with Sentinel-src achieving the best performance.\nThus, we release two improved models for difficulty estimation, Sentinel-src-24\nand Sentinel-src-25, which can be used to scan large collections of texts and\nselect those most likely to challenge contemporary machine translation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation quality has steadily improved over the years, achieving\nnear-perfect translations in recent benchmarks. These high-quality outputs make\nit difficult to distinguish between state-of-the-art models and to identify\nareas for future improvement. In this context, automatically identifying texts\nwhere machine translation systems struggle holds promise for developing more\ndiscriminative evaluations and guiding future research.\n  In this work, we address this gap by formalizing the task of translation\ndifficulty estimation, defining a text's difficulty based on the expected\nquality of its translations. We introduce a new metric to evaluate difficulty\nestimators and use it to assess both baselines and novel approaches. Finally,\nwe demonstrate the practical utility of difficulty estimators by using them to\nconstruct more challenging benchmarks for machine translation. Our results show\nthat dedicated models outperform both heuristic-based methods and\nLLM-as-a-judge approaches, with Sentinel-src achieving the best performance.\nThus, we release two improved models for difficulty estimation, Sentinel-src-24\nand Sentinel-src-25, which can be used to scan large collections of texts and\nselect those most likely to challenge contemporary machine translation systems."
                },
                "authors": [
                    {
                        "name": "Lorenzo Proietti"
                    },
                    {
                        "name": "Stefano Perrella"
                    },
                    {
                        "name": "Vil√©m Zouhar"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Tom Kocmi"
                    }
                ],
                "author_detail": {
                    "name": "Tom Kocmi"
                },
                "author": "Tom Kocmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21048v1",
                "updated": "2025-08-28T17:53:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    53,
                    5,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:53:05Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    53,
                    5,
                    3,
                    240,
                    0
                ],
                "title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning"
                },
                "summary": "Deepfake detection remains a formidable challenge due to the complex and\nevolving nature of fake content in real-world scenarios. However, existing\nacademic benchmarks suffer from severe discrepancies from industrial practice,\ntypically featuring homogeneous training sources and low-quality testing\nimages, which hinder the practical deployments of current detectors. To\nmitigate this gap, we introduce HydraFake, a dataset that simulates real-world\nchallenges with hierarchical generalization testing. Specifically, HydraFake\ninvolves diversified deepfake techniques and in-the-wild forgeries, along with\nrigorous training and evaluation protocol, covering unseen model architectures,\nemerging forgery techniques and novel data domains. Building on this resource,\nwe propose Veritas, a multi-modal large language model (MLLM) based deepfake\ndetector. Different from vanilla chain-of-thought (CoT), we introduce\npattern-aware reasoning that involves critical reasoning patterns such as\n\"planning\" and \"self-reflection\" to emulate human forensic process. We further\npropose a two-stage training pipeline to seamlessly internalize such deepfake\nreasoning capacities into current MLLMs. Experiments on HydraFake dataset\nreveal that although previous detectors show great generalization on\ncross-model scenarios, they fall short on unseen forgeries and data domains.\nOur Veritas achieves significant gains across different OOD scenarios, and is\ncapable of delivering transparent and faithful detection outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deepfake detection remains a formidable challenge due to the complex and\nevolving nature of fake content in real-world scenarios. However, existing\nacademic benchmarks suffer from severe discrepancies from industrial practice,\ntypically featuring homogeneous training sources and low-quality testing\nimages, which hinder the practical deployments of current detectors. To\nmitigate this gap, we introduce HydraFake, a dataset that simulates real-world\nchallenges with hierarchical generalization testing. Specifically, HydraFake\ninvolves diversified deepfake techniques and in-the-wild forgeries, along with\nrigorous training and evaluation protocol, covering unseen model architectures,\nemerging forgery techniques and novel data domains. Building on this resource,\nwe propose Veritas, a multi-modal large language model (MLLM) based deepfake\ndetector. Different from vanilla chain-of-thought (CoT), we introduce\npattern-aware reasoning that involves critical reasoning patterns such as\n\"planning\" and \"self-reflection\" to emulate human forensic process. We further\npropose a two-stage training pipeline to seamlessly internalize such deepfake\nreasoning capacities into current MLLMs. Experiments on HydraFake dataset\nreveal that although previous detectors show great generalization on\ncross-model scenarios, they fall short on unseen forgeries and data domains.\nOur Veritas achieves significant gains across different OOD scenarios, and is\ncapable of delivering transparent and faithful detection outputs."
                },
                "authors": [
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jun Lan"
                    },
                    {
                        "name": "Zichang Tan"
                    },
                    {
                        "name": "Ajian Liu"
                    },
                    {
                        "name": "Chuanbiao Song"
                    },
                    {
                        "name": "Senyuan Shi"
                    },
                    {
                        "name": "Huijia Zhu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Jun Wan"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "Project: https://github.com/EricTan7/Veritas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21046v1",
                "updated": "2025-08-28T17:50:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    58,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:50:58Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    58,
                    3,
                    240,
                    0
                ],
                "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification"
                },
                "summary": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Renshan Zhang"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "23 pages, 8 figures, Project Page:\n  https://jiutian-vl.github.io/CogVLA-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21044v1",
                "updated": "2025-08-28T17:50:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    3,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:50:03Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    50,
                    3,
                    3,
                    240,
                    0
                ],
                "title": "MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for\n  Efficient Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for\n  Efficient Video LLMs"
                },
                "summary": "Video Large Language Models (VLLMs) excel in video understanding, but their\nexcessive visual tokens pose a significant computational challenge for\nreal-world applications. Current methods aim to enhance inference efficiency by\nvisual token pruning. However, they do not consider the dynamic characteristics\nand temporal dependencies of video frames, as they perceive video understanding\nas a multi-frame task. To address these challenges, we propose MMG-Vid, a novel\ntraining-free visual token pruning framework that removes redundancy by\nMaximizing Marginal Gains at both segment-level and token-level. Specifically,\nwe first divide the video into segments based on frame similarity, and then\ndynamically allocate the token budget for each segment to maximize the marginal\ngain of each segment. Subsequently, we propose a temporal-guided DPC algorithm\nthat jointly models inter-frame uniqueness and intra-frame diversity, thereby\nmaximizing the marginal gain of each token. By combining both stages, MMG-Vid\ncan maximize the utilization of the limited token budget, significantly\nimproving efficiency while maintaining strong performance. Extensive\nexperiments demonstrate that MMG-Vid can maintain over 99.5% of the original\nperformance, while effectively reducing 75% visual tokens and accelerating the\nprefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VLLMs) excel in video understanding, but their\nexcessive visual tokens pose a significant computational challenge for\nreal-world applications. Current methods aim to enhance inference efficiency by\nvisual token pruning. However, they do not consider the dynamic characteristics\nand temporal dependencies of video frames, as they perceive video understanding\nas a multi-frame task. To address these challenges, we propose MMG-Vid, a novel\ntraining-free visual token pruning framework that removes redundancy by\nMaximizing Marginal Gains at both segment-level and token-level. Specifically,\nwe first divide the video into segments based on frame similarity, and then\ndynamically allocate the token budget for each segment to maximize the marginal\ngain of each segment. Subsequently, we propose a temporal-guided DPC algorithm\nthat jointly models inter-frame uniqueness and intra-frame diversity, thereby\nmaximizing the marginal gain of each token. By combining both stages, MMG-Vid\ncan maximize the utilization of the limited token budget, significantly\nimproving efficiency while maintaining strong performance. Extensive\nexperiments demonstrate that MMG-Vid can maintain over 99.5% of the original\nperformance, while effectively reducing 75% visual tokens and accelerating the\nprefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon."
                },
                "authors": [
                    {
                        "name": "Junpeng Ma"
                    },
                    {
                        "name": "Qizhe Zhang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Jun Song"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "arxiv_comment": "10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21037v1",
                "updated": "2025-08-28T17:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    41,
                    28,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    41,
                    28,
                    3,
                    240,
                    0
                ],
                "title": "Predicting Trends in $V_{OC}$ Through Rapid, Multimodal Characterization\n  of State-of-the-Art p-i-n Perovskite Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Trends in $V_{OC}$ Through Rapid, Multimodal Characterization\n  of State-of-the-Art p-i-n Perovskite Devices"
                },
                "summary": "Perovskite photovoltaic technologies are approaching commercial deployment,\nyet single junction and tandem architectures both still have significant room\nto improve power conversion efficiency and stability. The ability to perform\nrapid screening of material quality after altering processing conditions is\ncritical to accelerating the optimization and commercialization of\nperovskite-based technologies. Currently, researchers utilize a wide range of\nstand-alone metrology tools to isolate sources of power loss throughout a\ndevice stack, which can be slow and labor intensive. Here, we demonstrate the\nuse of a multimodal metrology approach to rapidly determine the maximum\nachievable and predicted open circuit voltages of > 100 perovskite devices\nduring fabrication. Acquisition of these different data are facilitated by\ncombining them into a single integrated measurement platform. We show that\nthese data and automated analysis can be used to rapidly understand and\nultimately predict quantitative trends in open circuit voltages of\nstate-of-the-art devices architectures. The data and automated analysis\nworkflow presented provides a reliable approach to quickly identify absorber\nand charge transport layer combinations that can lead to improved open circuit\nvoltages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perovskite photovoltaic technologies are approaching commercial deployment,\nyet single junction and tandem architectures both still have significant room\nto improve power conversion efficiency and stability. The ability to perform\nrapid screening of material quality after altering processing conditions is\ncritical to accelerating the optimization and commercialization of\nperovskite-based technologies. Currently, researchers utilize a wide range of\nstand-alone metrology tools to isolate sources of power loss throughout a\ndevice stack, which can be slow and labor intensive. Here, we demonstrate the\nuse of a multimodal metrology approach to rapidly determine the maximum\nachievable and predicted open circuit voltages of > 100 perovskite devices\nduring fabrication. Acquisition of these different data are facilitated by\ncombining them into a single integrated measurement platform. We show that\nthese data and automated analysis can be used to rapidly understand and\nultimately predict quantitative trends in open circuit voltages of\nstate-of-the-art devices architectures. The data and automated analysis\nworkflow presented provides a reliable approach to quickly identify absorber\nand charge transport layer combinations that can lead to improved open circuit\nvoltages."
                },
                "authors": [
                    {
                        "name": "Amy E. Louks"
                    },
                    {
                        "name": "Brandon T. Motes"
                    },
                    {
                        "name": "Anthony T. Troupe"
                    },
                    {
                        "name": "Axel F. Palmstrom"
                    },
                    {
                        "name": "Joseph J. Berry"
                    },
                    {
                        "name": "Dane W. deQuilettes"
                    }
                ],
                "author_detail": {
                    "name": "Dane W. deQuilettes"
                },
                "author": "Dane W. deQuilettes",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19200v2",
                "updated": "2025-08-28T17:29:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    29,
                    36,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-26T17:03:43Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    43,
                    1,
                    238,
                    0
                ],
                "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ramon Llull's Thinking Machine for Automated Ideation"
                },
                "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI."
                },
                "authors": [
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Runlong Zhou"
                    },
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21024v1",
                "updated": "2025-08-28T17:27:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    27,
                    9,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:27:09Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    27,
                    9,
                    3,
                    240,
                    0
                ],
                "title": "An Agile Method for Implementing Retrieval Augmented Generation Tools in\n  Industrial SMEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agile Method for Implementing Retrieval Augmented Generation Tools in\n  Industrial SMEs"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to\nmitigate the limitations of Large Language Models (LLMs), such as\nhallucinations and outdated knowledge. However, deploying RAG-based tools in\nSmall and Medium Enterprises (SMEs) remains a challenge due to their limited\nresources and lack of expertise in natural language processing (NLP). This\npaper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a\nstructured, agile method designed to facilitate the deployment of RAG systems\nin industrial SME contexts. EASI-RAG is based on method engineering principles\nand comprises well-defined roles, activities, and techniques. The method was\nvalidated through a real-world case study in an environmental testing\nlaboratory, where a RAG tool was implemented to answer operators queries using\ndata extracted from operational procedures. The system was deployed in under a\nmonth by a team with no prior RAG experience and was later iteratively improved\nbased on user feedback. Results demonstrate that EASI-RAG supports fast\nimplementation, high user adoption, delivers accurate answers, and enhances the\nreliability of underlying data. This work highlights the potential of RAG\ndeployment in industrial SMEs. Future works include the need for generalization\nacross diverse use cases and further integration with fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to\nmitigate the limitations of Large Language Models (LLMs), such as\nhallucinations and outdated knowledge. However, deploying RAG-based tools in\nSmall and Medium Enterprises (SMEs) remains a challenge due to their limited\nresources and lack of expertise in natural language processing (NLP). This\npaper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a\nstructured, agile method designed to facilitate the deployment of RAG systems\nin industrial SME contexts. EASI-RAG is based on method engineering principles\nand comprises well-defined roles, activities, and techniques. The method was\nvalidated through a real-world case study in an environmental testing\nlaboratory, where a RAG tool was implemented to answer operators queries using\ndata extracted from operational procedures. The system was deployed in under a\nmonth by a team with no prior RAG experience and was later iteratively improved\nbased on user feedback. Results demonstrate that EASI-RAG supports fast\nimplementation, high user adoption, delivers accurate answers, and enhances the\nreliability of underlying data. This work highlights the potential of RAG\ndeployment in industrial SMEs. Future works include the need for generalization\nacross diverse use cases and further integration with fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Mathieu Bourdin"
                    },
                    {
                        "name": "Anas Neumann"
                    },
                    {
                        "name": "Thomas Paviot"
                    },
                    {
                        "name": "Robert Pellerin"
                    },
                    {
                        "name": "Samir Lamouri"
                    }
                ],
                "author_detail": {
                    "name": "Samir Lamouri"
                },
                "author": "Samir Lamouri",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21004v1",
                "updated": "2025-08-28T17:05:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    5,
                    18,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    5,
                    18,
                    3,
                    240,
                    0
                ],
                "title": "Lethe: Purifying Backdoored Large Language Models with Knowledge\n  Dilution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lethe: Purifying Backdoored Large Language Models with Knowledge\n  Dilution"
                },
                "summary": "Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks.\nHowever, they remain vulnerable to backdoor attacks, where models behave\nnormally for standard queries but generate harmful responses or unintended\noutput when specific triggers are activated. Existing backdoor defenses either\nlack comprehensiveness, focusing on narrow trigger settings, detection-only\nmechanisms, and limited domains, or fail to withstand advanced scenarios like\nmodel-editing-based, multi-trigger, and triggerless attacks. In this paper, we\npresent LETHE, a novel method to eliminate backdoor behaviors from LLMs through\nknowledge dilution using both internal and external mechanisms. Internally,\nLETHE leverages a lightweight dataset to train a clean model, which is then\nmerged with the backdoored model to neutralize malicious behaviors by diluting\nthe backdoor impact within the model's parametric memory. Externally, LETHE\nincorporates benign and semantically relevant evidence into the prompt to\ndistract LLM's attention from backdoor features. Experimental results on\nclassification and generation domains across 5 widely used LLMs demonstrate\nthat LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor\nattacks. LETHE reduces the attack success rate of advanced backdoor attacks by\nup to 98% while maintaining model utility. Furthermore, LETHE has proven to be\ncost-efficient and robust against adaptive backdoor attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks.\nHowever, they remain vulnerable to backdoor attacks, where models behave\nnormally for standard queries but generate harmful responses or unintended\noutput when specific triggers are activated. Existing backdoor defenses either\nlack comprehensiveness, focusing on narrow trigger settings, detection-only\nmechanisms, and limited domains, or fail to withstand advanced scenarios like\nmodel-editing-based, multi-trigger, and triggerless attacks. In this paper, we\npresent LETHE, a novel method to eliminate backdoor behaviors from LLMs through\nknowledge dilution using both internal and external mechanisms. Internally,\nLETHE leverages a lightweight dataset to train a clean model, which is then\nmerged with the backdoored model to neutralize malicious behaviors by diluting\nthe backdoor impact within the model's parametric memory. Externally, LETHE\nincorporates benign and semantically relevant evidence into the prompt to\ndistract LLM's attention from backdoor features. Experimental results on\nclassification and generation domains across 5 widely used LLMs demonstrate\nthat LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor\nattacks. LETHE reduces the attack success rate of advanced backdoor attacks by\nup to 98% while maintaining model utility. Furthermore, LETHE has proven to be\ncost-efficient and robust against adaptive backdoor attacks."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuchen Sun"
                    },
                    {
                        "name": "Jiaxin Gao"
                    },
                    {
                        "name": "Xueluan Gong"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yongsen Zheng"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok-Yan Lam"
                },
                "author": "Kwok-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21001v1",
                "updated": "2025-08-28T17:04:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    4,
                    0,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T17:04:00Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    4,
                    0,
                    3,
                    240,
                    0
                ],
                "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees"
                },
                "summary": "Kinodynamic motion planning is concerned with computing collision-free\ntrajectories while abiding by the robot's dynamic constraints. This critical\nproblem is often tackled using sampling-based planners (SBPs) that explore the\nrobot's high-dimensional state space by constructing a search tree via action\npropagations. Although SBPs can offer global guarantees on completeness and\nsolution quality, their performance is often hindered by slow exploration due\nto uninformed action sampling. Learning-based approaches can yield\nsignificantly faster runtimes, yet they fail to generalize to\nout-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,\nthus limiting their deployment on physical robots. We present Diffusion Tree\n(DiTree): a \\emph{provably-generalizable} framework leveraging diffusion\npolicies (DPs) as informed samplers to efficiently guide state-space search\nwithin SBPs. DiTree combines DP's ability to model complex distributions of\nexpert trajectories, conditioned on local observations, with the completeness\nof SBPs to yield \\emph{provably-safe} solutions within a few action propagation\niterations for complex dynamical systems. We demonstrate DiTree's power with an\nimplementation combining the popular RRT planner with a DP action sampler\ntrained on a \\emph{single environment}. In comprehensive evaluations on OOD\nscenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than\nclassical SBPs), while improving the average success rate over DP and SBPs.\nDiTree is on average 3x faster than classical SBPs, and outperforms all other\napproaches by achieving roughly 30\\% higher success rate. Project webpage:\nhttps://sites.google.com/view/ditree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kinodynamic motion planning is concerned with computing collision-free\ntrajectories while abiding by the robot's dynamic constraints. This critical\nproblem is often tackled using sampling-based planners (SBPs) that explore the\nrobot's high-dimensional state space by constructing a search tree via action\npropagations. Although SBPs can offer global guarantees on completeness and\nsolution quality, their performance is often hindered by slow exploration due\nto uninformed action sampling. Learning-based approaches can yield\nsignificantly faster runtimes, yet they fail to generalize to\nout-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,\nthus limiting their deployment on physical robots. We present Diffusion Tree\n(DiTree): a \\emph{provably-generalizable} framework leveraging diffusion\npolicies (DPs) as informed samplers to efficiently guide state-space search\nwithin SBPs. DiTree combines DP's ability to model complex distributions of\nexpert trajectories, conditioned on local observations, with the completeness\nof SBPs to yield \\emph{provably-safe} solutions within a few action propagation\niterations for complex dynamical systems. We demonstrate DiTree's power with an\nimplementation combining the popular RRT planner with a DP action sampler\ntrained on a \\emph{single environment}. In comprehensive evaluations on OOD\nscenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than\nclassical SBPs), while improving the average success rate over DP and SBPs.\nDiTree is on average 3x faster than classical SBPs, and outperforms all other\napproaches by achieving roughly 30\\% higher success rate. Project webpage:\nhttps://sites.google.com/view/ditree."
                },
                "authors": [
                    {
                        "name": "Yaniv Hassidof"
                    },
                    {
                        "name": "Tom Jurgenson"
                    },
                    {
                        "name": "Kiril Solovey"
                    }
                ],
                "author_detail": {
                    "name": "Kiril Solovey"
                },
                "author": "Kiril Solovey",
                "arxiv_comment": "Accepted to CoRL 2025. Project page:\n  https://sites.google.com/view/ditree",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20996v1",
                "updated": "2025-08-28T16:57:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:57:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic\n  Support in Addiction Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic\n  Support in Addiction Recovery"
                },
                "summary": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation."
                },
                "authors": [
                    {
                        "name": "Junda Wang"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Lingxi Li"
                    },
                    {
                        "name": "Junhui Qian"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14743v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14743v3",
                "updated": "2025-08-28T16:45:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    45,
                    48,
                    3,
                    240,
                    0
                ],
                "published": "2025-07-19T20:30:43Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    20,
                    30,
                    43,
                    5,
                    200,
                    0
                ],
                "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic"
                },
                "summary": "Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA"
                },
                "authors": [
                    {
                        "name": "Joseph Raj Vishal"
                    },
                    {
                        "name": "Divesh Basina"
                    },
                    {
                        "name": "Rutuja Patil"
                    },
                    {
                        "name": "Manas Srinivas Gowda"
                    },
                    {
                        "name": "Katha Naik"
                    },
                    {
                        "name": "Yezhou Yang"
                    },
                    {
                        "name": "Bharatesh Chakravarthi"
                    }
                ],
                "author_detail": {
                    "name": "Bharatesh Chakravarthi"
                },
                "author": "Bharatesh Chakravarthi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14743v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14743v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22931v2",
                "updated": "2025-08-28T16:42:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    42,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-07-24T13:46:51Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    46,
                    51,
                    3,
                    205,
                    0
                ],
                "title": "Dynamic Context Compression for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Compression for Efficient RAG"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy."
                },
                "authors": [
                    {
                        "name": "Shuyu Guo"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03818v2",
                "updated": "2025-08-28T16:38:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    38,
                    13,
                    3,
                    240,
                    0
                ],
                "published": "2025-05-02T20:03:35Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    20,
                    3,
                    35,
                    4,
                    122,
                    0
                ],
                "title": "Program Semantic Inequivalence Game with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Program Semantic Inequivalence Game with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can achieve strong performance on everyday\ncoding tasks, but they can fail on complex tasks that require non-trivial\nreasoning about program semantics. Finding training examples to teach LLMs to\nsolve these tasks can be challenging.\n  In this work, we explore a method to synthetically generate code reasoning\ntraining data based on a semantic inequivalence game SInQ: a generator agent\ncreates program variants that are semantically distinct, derived from a dataset\nof real-world programming tasks, while an evaluator agent has to identify input\nexamples that cause the original programs and the generated variants to diverge\nin their behaviour, with the agents training each other semi-adversarially. We\nprove that this setup enables theoretically unlimited improvement through\nself-play in the limit of infinite computational resources.\n  We evaluated our approach on multiple code generation and understanding\nbenchmarks, including cross-language vulnerability detection (Lu et al., 2021),\nwhere our method improves vulnerability detection in C/C++ code despite being\ntrained exclusively on Python code, and the challenging Python builtin\nidentifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas\nmodern LLMs still struggle with this benchmark, our approach yields substantial\nimprovements.\n  We release the code needed to replicate the experiments, as well as the\ngenerated synthetic data, which can be used to fine-tune LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can achieve strong performance on everyday\ncoding tasks, but they can fail on complex tasks that require non-trivial\nreasoning about program semantics. Finding training examples to teach LLMs to\nsolve these tasks can be challenging.\n  In this work, we explore a method to synthetically generate code reasoning\ntraining data based on a semantic inequivalence game SInQ: a generator agent\ncreates program variants that are semantically distinct, derived from a dataset\nof real-world programming tasks, while an evaluator agent has to identify input\nexamples that cause the original programs and the generated variants to diverge\nin their behaviour, with the agents training each other semi-adversarially. We\nprove that this setup enables theoretically unlimited improvement through\nself-play in the limit of infinite computational resources.\n  We evaluated our approach on multiple code generation and understanding\nbenchmarks, including cross-language vulnerability detection (Lu et al., 2021),\nwhere our method improves vulnerability detection in C/C++ code despite being\ntrained exclusively on Python code, and the challenging Python builtin\nidentifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas\nmodern LLMs still struggle with this benchmark, our approach yields substantial\nimprovements.\n  We release the code needed to replicate the experiments, as well as the\ngenerated synthetic data, which can be used to fine-tune LLMs."
                },
                "authors": [
                    {
                        "name": "Antonio Valerio Miceli-Barone"
                    },
                    {
                        "name": "Vaishak Belle"
                    },
                    {
                        "name": "Ali Payani"
                    }
                ],
                "author_detail": {
                    "name": "Ali Payani"
                },
                "author": "Ali Payani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20977v2",
                "updated": "2025-08-29T01:33:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    1,
                    33,
                    26,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T16:31:08Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    31,
                    8,
                    3,
                    240,
                    0
                ],
                "title": "ConfLogger: Enhance Systems' Configuration Diagnosability through\n  Configuration Logging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfLogger: Enhance Systems' Configuration Diagnosability through\n  Configuration Logging"
                },
                "summary": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%."
                },
                "authors": [
                    {
                        "name": "Shiwen Shan"
                    },
                    {
                        "name": "Yintong Huo"
                    },
                    {
                        "name": "Yuxin Su"
                    },
                    {
                        "name": "Zhining Wang"
                    },
                    {
                        "name": "Dan Li"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764570",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764570",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 6 figures, accepted by ICSE '26 (The 48th IEEE/ACM\n  International Conference on Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20973v1",
                "updated": "2025-08-28T16:26:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    26,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:26:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    26,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue\n  Agents"
                },
                "summary": "Proactive dialogue has emerged as a critical and challenging research problem\nin advancing large language models (LLMs). Existing works predominantly focus\non domain-specific or task-oriented scenarios, which leads to fragmented\nevaluations and limits the comprehensive exploration of models' proactive\nconversation abilities. In this work, we propose ProactiveEval, a unified\nframework designed for evaluating proactive dialogue capabilities of LLMs. This\nframework decomposes proactive dialogue into target planning and dialogue\nguidance, establishing evaluation metrics across various domains. Moreover, it\nalso enables the automatic generation of diverse and challenging evaluation\ndata. Based on the proposed framework, we develop 328 evaluation environments\nspanning 6 distinct domains. Through experiments with 22 different types of\nLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional\nperformance on target planning and dialogue guidance tasks, respectively.\nFinally, we investigate how reasoning capabilities influence proactive\nbehaviors and discuss their implications for future model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive dialogue has emerged as a critical and challenging research problem\nin advancing large language models (LLMs). Existing works predominantly focus\non domain-specific or task-oriented scenarios, which leads to fragmented\nevaluations and limits the comprehensive exploration of models' proactive\nconversation abilities. In this work, we propose ProactiveEval, a unified\nframework designed for evaluating proactive dialogue capabilities of LLMs. This\nframework decomposes proactive dialogue into target planning and dialogue\nguidance, establishing evaluation metrics across various domains. Moreover, it\nalso enables the automatic generation of diverse and challenging evaluation\ndata. Based on the proposed framework, we develop 328 evaluation environments\nspanning 6 distinct domains. Through experiments with 22 different types of\nLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional\nperformance on target planning and dialogue guidance tasks, respectively.\nFinally, we investigate how reasoning capabilities influence proactive\nbehaviors and discuss their implications for future model development."
                },
                "authors": [
                    {
                        "name": "Tianjian Liu"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Jiajian Guo"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "arxiv_comment": "21 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20965v1",
                "updated": "2025-08-28T16:22:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    22,
                    54,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:22:54Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    22,
                    54,
                    3,
                    240,
                    0
                ],
                "title": "DrivingGaussian++: Towards Realistic Reconstruction and Editable\n  Simulation for Surrounding Dynamic Driving Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrivingGaussian++: Towards Realistic Reconstruction and Editable\n  Simulation for Surrounding Dynamic Driving Scenes"
                },
                "summary": "We present DrivingGaussian++, an efficient and effective framework for\nrealistic reconstructing and controllable editing of surrounding dynamic\nautonomous driving scenes. DrivingGaussian++ models the static background using\nincremental 3D Gaussians and reconstructs moving objects with a composite\ndynamic Gaussian graph, ensuring accurate positions and occlusions. By\nintegrating a LiDAR prior, it achieves detailed and consistent scene\nreconstruction, outperforming existing methods in dynamic scene reconstruction\nand photorealistic surround-view synthesis. DrivingGaussian++ supports\ntraining-free controllable editing for dynamic driving scenes, including\ntexture modification, weather simulation, and object manipulation, leveraging\nmulti-view images and depth priors. By integrating large language models (LLMs)\nand controllable editing, our method can automatically generate dynamic object\nmotion trajectories and enhance their realism during the optimization process.\nDrivingGaussian++ demonstrates consistent and realistic editing results and\ngenerates dynamic multi-view driving scenarios, while significantly enhancing\nscene diversity. More results and code can be found at the project site:\nhttps://xiong-creator.github.io/DrivingGaussian_plus.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DrivingGaussian++, an efficient and effective framework for\nrealistic reconstructing and controllable editing of surrounding dynamic\nautonomous driving scenes. DrivingGaussian++ models the static background using\nincremental 3D Gaussians and reconstructs moving objects with a composite\ndynamic Gaussian graph, ensuring accurate positions and occlusions. By\nintegrating a LiDAR prior, it achieves detailed and consistent scene\nreconstruction, outperforming existing methods in dynamic scene reconstruction\nand photorealistic surround-view synthesis. DrivingGaussian++ supports\ntraining-free controllable editing for dynamic driving scenes, including\ntexture modification, weather simulation, and object manipulation, leveraging\nmulti-view images and depth priors. By integrating large language models (LLMs)\nand controllable editing, our method can automatically generate dynamic object\nmotion trajectories and enhance their realism during the optimization process.\nDrivingGaussian++ demonstrates consistent and realistic editing results and\ngenerates dynamic multi-view driving scenarios, while significantly enhancing\nscene diversity. More results and code can be found at the project site:\nhttps://xiong-creator.github.io/DrivingGaussian_plus.github.io"
                },
                "authors": [
                    {
                        "name": "Yajiao Xiong"
                    },
                    {
                        "name": "Xiaoyu Zhou"
                    },
                    {
                        "name": "Yongtao Wan"
                    },
                    {
                        "name": "Deqing Sun"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07612v2",
                "updated": "2025-08-28T16:22:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    22,
                    13,
                    3,
                    240,
                    0
                ],
                "published": "2025-04-10T10:03:29Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    3,
                    29,
                    3,
                    100,
                    0
                ],
                "title": "SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline\n  Dataset"
                },
                "summary": "The primary goal of a news headline is to summarize an event in as few words\nas possible. Depending on the media outlet, a headline can serve as a means to\nobjectively deliver a summary or improve its visibility. For the latter,\nspecific publications may employ stylistic approaches that incorporate the use\nof sarcasm, irony, and exaggeration, key elements of a satirical approach. As\nsuch, even the headline must reflect the tone of the satirical main content.\nCurrent approaches for the Romanian language tend to detect the\nnon-conventional tone (i.e., satire and clickbait) of the news content by\ncombining both the main article and the headline. Because we consider a\nheadline to be merely a brief summary of the main article, we investigate in\nthis paper the presence of satirical tone in headlines alone, testing multiple\nbaselines ranging from standard machine learning algorithms to deep learning\nmodels. Our experiments show that Bidirectional Transformer models outperform\nboth standard machine-learning approaches and Large Language Models (LLMs),\nparticularly when the meta-learning Reptile approach is employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary goal of a news headline is to summarize an event in as few words\nas possible. Depending on the media outlet, a headline can serve as a means to\nobjectively deliver a summary or improve its visibility. For the latter,\nspecific publications may employ stylistic approaches that incorporate the use\nof sarcasm, irony, and exaggeration, key elements of a satirical approach. As\nsuch, even the headline must reflect the tone of the satirical main content.\nCurrent approaches for the Romanian language tend to detect the\nnon-conventional tone (i.e., satire and clickbait) of the news content by\ncombining both the main article and the headline. Because we consider a\nheadline to be merely a brief summary of the main article, we investigate in\nthis paper the presence of satirical tone in headlines alone, testing multiple\nbaselines ranging from standard machine learning algorithms to deep learning\nmodels. Our experiments show that Bidirectional Transformer models outperform\nboth standard machine-learning approaches and Large Language Models (LLMs),\nparticularly when the meta-learning Reptile approach is employed."
                },
                "authors": [
                    {
                        "name": "Mihnea-Alexandru V√Ærlan"
                    },
                    {
                        "name": "RƒÉzvan-Alexandru SmƒÉdu"
                    },
                    {
                        "name": "Dumitru-Clementin Cercel"
                    },
                    {
                        "name": "Florin Pop"
                    },
                    {
                        "name": "Mihaela-Claudia Cercel"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela-Claudia Cercel"
                },
                "author": "Mihaela-Claudia Cercel",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20962v1",
                "updated": "2025-08-28T16:20:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    20,
                    8,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:20:08Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    20,
                    8,
                    3,
                    240,
                    0
                ],
                "title": "Characterizing Trust Boundary Vulnerabilities in TEE Containers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Trust Boundary Vulnerabilities in TEE Containers"
                },
                "summary": "Trusted Execution Environments (TEEs) have emerged as a cornerstone of\nconfidential computing, garnering significant attention from both academia and\nindustry. To enable the secure development, execution, and deployment, of\napplications on TEE platforms, TEE containers have been introduced as\nmiddleware solutions. These containers aim to shield applications from\npotentially malicious operating systems and orchestration interfaces while\nmaintaining usability and reliability. In this paper, we analyze the isolation\nstrategies employed by existing TEE containers to protect secure applications.\nTo address the challenges in analyzing these interfaces, we designed an\nautomated analyzer to precisely identify and evaluate their isolation\nboundaries. We observed that some TEE containers fail to achieve their intended\ngoals due to critical design and implementation flaws, such as information\nleakage, rollback attacks, denial-of-service, and Iago attacks, which pose\nsignificant security risks. Drawing from our findings, we share key lessons to\nguide the development of more secure container solutions and discuss emerging\ntrends in TEE containerization design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted Execution Environments (TEEs) have emerged as a cornerstone of\nconfidential computing, garnering significant attention from both academia and\nindustry. To enable the secure development, execution, and deployment, of\napplications on TEE platforms, TEE containers have been introduced as\nmiddleware solutions. These containers aim to shield applications from\npotentially malicious operating systems and orchestration interfaces while\nmaintaining usability and reliability. In this paper, we analyze the isolation\nstrategies employed by existing TEE containers to protect secure applications.\nTo address the challenges in analyzing these interfaces, we designed an\nautomated analyzer to precisely identify and evaluate their isolation\nboundaries. We observed that some TEE containers fail to achieve their intended\ngoals due to critical design and implementation flaws, such as information\nleakage, rollback attacks, denial-of-service, and Iago attacks, which pose\nsignificant security risks. Drawing from our findings, we share key lessons to\nguide the development of more secure container solutions and discuss emerging\ntrends in TEE containerization design."
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Shuo Huai"
                    },
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20957v1",
                "updated": "2025-08-28T16:18:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    18,
                    22,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:18:22Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    18,
                    22,
                    3,
                    240,
                    0
                ],
                "title": "Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF\n  Migration in Edge-Core Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF\n  Migration in Edge-Core Networks"
                },
                "summary": "The growing demand for services and the rapid deployment of virtualized\nnetwork functions (VNFs) pose significant challenges for achieving low-latency\nand energy-efficient orchestration in modern edge-core network infrastructures.\nTo address these challenges, this study proposes a Digital Twin (DT)-empowered\nDeep Reinforcement Learning framework for intelligent VNF migration that\njointly minimizes average end-to-end (E2E) delay and energy consumption. By\nformulating the VNF migration problem as a Markov Decision Process and\nutilizing the Advantage Actor-Critic model, the proposed framework enables\nadaptive and real-time migration decisions. A key innovation of the proposed\nframework is the integration of a DT module composed of a multi-task\nVariational Autoencoder and a multi-task Long Short-Term Memory network. This\ncombination collectively simulates environment dynamics and generates\nhigh-quality synthetic experiences, significantly enhancing training efficiency\nand accelerating policy convergence. Simulation results demonstrate substantial\nperformance gains, such as significant reductions in both average E2E delay and\nenergy consumption, thereby establishing new benchmarks for intelligent VNF\nmigration in edge-core networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for services and the rapid deployment of virtualized\nnetwork functions (VNFs) pose significant challenges for achieving low-latency\nand energy-efficient orchestration in modern edge-core network infrastructures.\nTo address these challenges, this study proposes a Digital Twin (DT)-empowered\nDeep Reinforcement Learning framework for intelligent VNF migration that\njointly minimizes average end-to-end (E2E) delay and energy consumption. By\nformulating the VNF migration problem as a Markov Decision Process and\nutilizing the Advantage Actor-Critic model, the proposed framework enables\nadaptive and real-time migration decisions. A key innovation of the proposed\nframework is the integration of a DT module composed of a multi-task\nVariational Autoencoder and a multi-task Long Short-Term Memory network. This\ncombination collectively simulates environment dynamics and generates\nhigh-quality synthetic experiences, significantly enhancing training efficiency\nand accelerating policy convergence. Simulation results demonstrate substantial\nperformance gains, such as significant reductions in both average E2E delay and\nenergy consumption, thereby establishing new benchmarks for intelligent VNF\nmigration in edge-core networks."
                },
                "authors": [
                    {
                        "name": "Faisal Ahmed"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    },
                    {
                        "name": "Motoharu Matsuura"
                    },
                    {
                        "name": "Hiroshi Hasegawa"
                    },
                    {
                        "name": "Shih-Chun Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Chun Lin"
                },
                "author": "Shih-Chun Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20944v1",
                "updated": "2025-08-28T16:04:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    4,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:04:39Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    4,
                    39,
                    3,
                    240,
                    0
                ],
                "title": "STARE at the Structure: Steering ICL Exemplar Selection with Structural\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STARE at the Structure: Steering ICL Exemplar Selection with Structural\n  Alignment"
                },
                "summary": "In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to\nperform a wide range of tasks without task-specific fine-tuning. However, the\neffectiveness of ICL heavily depends on the quality of exemplar selection. In\nparticular, for structured prediction tasks such as semantic parsing, existing\nICL selection strategies often overlook structural alignment, leading to\nsuboptimal performance and poor generalization. To address this issue, we\npropose a novel two-stage exemplar selection strategy that achieves a strong\nbalance between efficiency, generalizability, and performance. First, we\nfine-tune a BERT-based retriever using structure-aware supervision, guiding it\nto select exemplars that are both semantically relevant and structurally\naligned. Then, we enhance the retriever with a plug-in module, which amplifies\nsyntactically meaningful information in the hidden representations. This\nplug-in is model-agnostic, requires minimal overhead, and can be seamlessly\nintegrated into existing pipelines. Experiments on four benchmarks spanning\nthree semantic parsing tasks demonstrate that our method consistently\noutperforms existing baselines with multiple recent LLMs as inference-time\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to\nperform a wide range of tasks without task-specific fine-tuning. However, the\neffectiveness of ICL heavily depends on the quality of exemplar selection. In\nparticular, for structured prediction tasks such as semantic parsing, existing\nICL selection strategies often overlook structural alignment, leading to\nsuboptimal performance and poor generalization. To address this issue, we\npropose a novel two-stage exemplar selection strategy that achieves a strong\nbalance between efficiency, generalizability, and performance. First, we\nfine-tune a BERT-based retriever using structure-aware supervision, guiding it\nto select exemplars that are both semantically relevant and structurally\naligned. Then, we enhance the retriever with a plug-in module, which amplifies\nsyntactically meaningful information in the hidden representations. This\nplug-in is model-agnostic, requires minimal overhead, and can be seamlessly\nintegrated into existing pipelines. Experiments on four benchmarks spanning\nthree semantic parsing tasks demonstrate that our method consistently\noutperforms existing baselines with multiple recent LLMs as inference-time\nmodels."
                },
                "authors": [
                    {
                        "name": "Jiaqian Li"
                    },
                    {
                        "name": "Qisheng Hu"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Wenya Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenya Wang"
                },
                "author": "Wenya Wang",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20941v1",
                "updated": "2025-08-28T16:02:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    2,
                    37,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T16:02:37Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    16,
                    2,
                    37,
                    3,
                    240,
                    0
                ],
                "title": "AI Reasoning Models for Problem Solving in Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Reasoning Models for Problem Solving in Physics"
                },
                "summary": "Reasoning models are the new generation of Large Language Models (LLMs)\ncapable of complex problem solving. Their reliability in solving introductory\nphysics problems was tested by evaluating a sample of n = 5 solutions generated\nby one such model -- OpenAI's o3-mini -- per each problem from 20 chapters of a\nstandard undergraduate textbook. In total, N = 408 problems were given to the\nmodel and N x n = 2,040 generated solutions examined. The model successfully\nsolved 94% of the problems posed, excelling at the beginning topics in\nmechanics but struggling with the later ones such as waves and thermodynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models are the new generation of Large Language Models (LLMs)\ncapable of complex problem solving. Their reliability in solving introductory\nphysics problems was tested by evaluating a sample of n = 5 solutions generated\nby one such model -- OpenAI's o3-mini -- per each problem from 20 chapters of a\nstandard undergraduate textbook. In total, N = 408 problems were given to the\nmodel and N x n = 2,040 generated solutions examined. The model successfully\nsolved 94% of the problems posed, excelling at the beginning topics in\nmechanics but struggling with the later ones such as waves and thermodynamics."
                },
                "authors": [
                    {
                        "name": "Amir Bralin"
                    },
                    {
                        "name": "N. Sanjay Rebello"
                    }
                ],
                "author_detail": {
                    "name": "N. Sanjay Rebello"
                },
                "author": "N. Sanjay Rebello",
                "arxiv_comment": "6 pages, 1 table; Physics Education Research Conference (PERC) 2025\n  Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20931v1",
                "updated": "2025-08-28T15:57:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:57:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on $œÑ$-bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on $œÑ$-bench"
                },
                "summary": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Venkatesh Mishra"
                    },
                    {
                        "name": "Amir Saeidi"
                    },
                    {
                        "name": "Satyam Raj"
                    },
                    {
                        "name": "Mutsumi Nakamura"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Chitta Baral"
                    }
                ],
                "author_detail": {
                    "name": "Chitta Baral"
                },
                "author": "Chitta Baral",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06312v2",
                "updated": "2025-08-28T15:52:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    52,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-08T13:39:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    39,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha\n  Mining in Quantitative Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha\n  Mining in Quantitative Trading"
                },
                "summary": "Alpha factor mining is a fundamental task in quantitative trading, aimed at\ndiscovering interpretable signals that can predict asset returns beyond\nsystematic market risk. While traditional methods rely on manual formula design\nor heuristic search with machine learning, recent advances have leveraged Large\nLanguage Models (LLMs) for automated factor discovery. However, existing\nLLM-based alpha mining approaches remain limited in terms of automation,\ngenerality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,\nsimple, yet effective and efficient LLM-based framework for fully automated\nformulaic alpha mining. Our method features a dual-chain architecture,\nconsisting of a Factor Generation Chain and a Factor Optimization Chain, which\niteratively generate, evaluate, and refine candidate alpha factors using only\nmarket data, while leveraging backtest feedback and prior optimization\nknowledge. The two chains work synergistically to enable high-quality alpha\ndiscovery without human intervention and offer strong scalability. Extensive\nexperiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha\noutperforms existing baselines across multiple metrics, presenting a promising\ndirection for LLM-driven quantitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha factor mining is a fundamental task in quantitative trading, aimed at\ndiscovering interpretable signals that can predict asset returns beyond\nsystematic market risk. While traditional methods rely on manual formula design\nor heuristic search with machine learning, recent advances have leveraged Large\nLanguage Models (LLMs) for automated factor discovery. However, existing\nLLM-based alpha mining approaches remain limited in terms of automation,\ngenerality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,\nsimple, yet effective and efficient LLM-based framework for fully automated\nformulaic alpha mining. Our method features a dual-chain architecture,\nconsisting of a Factor Generation Chain and a Factor Optimization Chain, which\niteratively generate, evaluate, and refine candidate alpha factors using only\nmarket data, while leveraging backtest feedback and prior optimization\nknowledge. The two chains work synergistically to enable high-quality alpha\ndiscovery without human intervention and offer strong scalability. Extensive\nexperiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha\noutperforms existing baselines across multiple metrics, presenting a promising\ndirection for LLM-driven quantitative research."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Long Liao"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11017v2",
                "updated": "2025-08-28T15:51:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    51,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-14T18:44:13Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    18,
                    44,
                    13,
                    3,
                    226,
                    0
                ],
                "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics"
                },
                "summary": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs."
                },
                "authors": [
                    {
                        "name": "Carter Blum"
                    },
                    {
                        "name": "Katja Filippova"
                    },
                    {
                        "name": "Ann Yuan"
                    },
                    {
                        "name": "Asma Ghandeharioun"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Jessica Hoffmann"
                    },
                    {
                        "name": "Tal Linzen"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Lucas Dixon"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20916v1",
                "updated": "2025-08-28T15:47:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    47,
                    37,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:47:37Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    47,
                    37,
                    3,
                    240,
                    0
                ],
                "title": "SageLM: A Multi-aspect and Explainable Large Language Model for Speech\n  Judgement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SageLM: A Multi-aspect and Explainable Large Language Model for Speech\n  Judgement"
                },
                "summary": "Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to\nnatural human-computer interaction, enabling end-to-end spoken dialogue\nsystems. However, evaluating these models remains a fundamental challenge. We\npropose \\texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech\nLLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches\nthat disregard acoustic features, SageLM jointly assesses both semantic and\nacoustic dimensions. Second, it leverages rationale-based supervision to\nenhance explainability and guide model learning, achieving superior alignment\nwith evaluation outcomes compared to rule-based reinforcement learning methods.\nThird, we introduce \\textit{SpeechFeedback}, a synthetic preference dataset,\nand employ a two-stage training paradigm to mitigate the scarcity of speech\npreference data. Trained on both semantic and acoustic dimensions, SageLM\nachieves an 82.79\\% agreement rate with human evaluators, outperforming\ncascaded and SLM-based baselines by at least 7.42\\% and 26.20\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to\nnatural human-computer interaction, enabling end-to-end spoken dialogue\nsystems. However, evaluating these models remains a fundamental challenge. We\npropose \\texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech\nLLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches\nthat disregard acoustic features, SageLM jointly assesses both semantic and\nacoustic dimensions. Second, it leverages rationale-based supervision to\nenhance explainability and guide model learning, achieving superior alignment\nwith evaluation outcomes compared to rule-based reinforcement learning methods.\nThird, we introduce \\textit{SpeechFeedback}, a synthetic preference dataset,\nand employ a two-stage training paradigm to mitigate the scarcity of speech\npreference data. Trained on both semantic and acoustic dimensions, SageLM\nachieves an 82.79\\% agreement rate with human evaluators, outperforming\ncascaded and SLM-based baselines by at least 7.42\\% and 26.20\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Yuan Ge"
                    },
                    {
                        "name": "Junxiang Zhang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Xiangnan Ma"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Kaiyang Ye"
                    },
                    {
                        "name": "Yangfan Du"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "JingBo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "JingBo Zhu"
                },
                "author": "JingBo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20912v1",
                "updated": "2025-08-28T15:41:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    41,
                    49,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:41:49Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    41,
                    49,
                    3,
                    240,
                    0
                ],
                "title": "Research Challenges in Relational Database Management Systems for LLM\n  Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research Challenges in Relational Database Management Systems for LLM\n  Queries"
                },
                "summary": "Large language models (LLMs) have become essential for applications such as\ntext summarization, sentiment analysis, and automated question-answering.\nRecently, LLMs have also been integrated into relational database management\nsystems to enhance querying and support advanced data processing. Companies\nsuch as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly\nwithin SQL, denoted as LLM queries, to boost data insights. However,\nopen-source solutions currently have limited functionality and poor\nperformance. In this work, we present an early exploration of two open-source\nsystems and one enterprise platform, using five representative queries to\nexpose functional, performance, and scalability limits in today's SQL-invoked\nLLM integrations. We identify three main issues: enforcing structured outputs,\noptimizing resource utilization, and improving query planning. We implemented\ninitial solutions and observed improvements in accommodating LLM powered SQL\nqueries. These early gains demonstrate that tighter integration of LLM+DBMS is\nthe key to scalable and efficient processing of LLM queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become essential for applications such as\ntext summarization, sentiment analysis, and automated question-answering.\nRecently, LLMs have also been integrated into relational database management\nsystems to enhance querying and support advanced data processing. Companies\nsuch as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly\nwithin SQL, denoted as LLM queries, to boost data insights. However,\nopen-source solutions currently have limited functionality and poor\nperformance. In this work, we present an early exploration of two open-source\nsystems and one enterprise platform, using five representative queries to\nexpose functional, performance, and scalability limits in today's SQL-invoked\nLLM integrations. We identify three main issues: enforcing structured outputs,\noptimizing resource utilization, and improving query planning. We implemented\ninitial solutions and observed improvements in accommodating LLM powered SQL\nqueries. These early gains demonstrate that tighter integration of LLM+DBMS is\nthe key to scalable and efficient processing of LLM queries."
                },
                "authors": [
                    {
                        "name": "Kerem Akillioglu"
                    },
                    {
                        "name": "Anurag Chakraborty"
                    },
                    {
                        "name": "Sairaj Voruganti"
                    },
                    {
                        "name": "M. Tamer √ñzsu"
                    }
                ],
                "author_detail": {
                    "name": "M. Tamer √ñzsu"
                },
                "author": "M. Tamer √ñzsu",
                "arxiv_comment": "This paper will appear in the 6th International Workshop on Applied\n  AI for Database Systems and Applications, AIDB Workshop at VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20907v1",
                "updated": "2025-08-28T15:37:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    37,
                    40,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:37:40Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    37,
                    40,
                    3,
                    240,
                    0
                ],
                "title": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant"
                },
                "summary": "Qiskit is an open-source quantum computing framework that allows users to\ndesign, simulate, and run quantum circuits on real quantum hardware. We explore\npost-training techniques for LLMs to assist in writing Qiskit code. We\nintroduce quantum verification as an effective method for ensuring code quality\nand executability on quantum hardware. To support this, we developed a\nsynthetic data pipeline that generates quantum problem-unit test pairs and used\nit to create preference data for aligning LLMs with DPO. Additionally, we\ntrained models using GRPO, leveraging quantum-verifiable rewards provided by\nthe quantum hardware. Our best-performing model, combining DPO and GRPO,\nsurpasses the strongest open-source baselines on the challenging\nQiskit-HumanEval-hard benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qiskit is an open-source quantum computing framework that allows users to\ndesign, simulate, and run quantum circuits on real quantum hardware. We explore\npost-training techniques for LLMs to assist in writing Qiskit code. We\nintroduce quantum verification as an effective method for ensuring code quality\nand executability on quantum hardware. To support this, we developed a\nsynthetic data pipeline that generates quantum problem-unit test pairs and used\nit to create preference data for aligning LLMs with DPO. Additionally, we\ntrained models using GRPO, leveraging quantum-verifiable rewards provided by\nthe quantum hardware. Our best-performing model, combining DPO and GRPO,\nsurpasses the strongest open-source baselines on the challenging\nQiskit-HumanEval-hard benchmark."
                },
                "authors": [
                    {
                        "name": "Nicolas Dupuis"
                    },
                    {
                        "name": "Adarsh Tiwari"
                    },
                    {
                        "name": "Youssef Mroueh"
                    },
                    {
                        "name": "David Kremer"
                    },
                    {
                        "name": "Ismael Faro"
                    },
                    {
                        "name": "Juan Cruz-Benito"
                    }
                ],
                "author_detail": {
                    "name": "Juan Cruz-Benito"
                },
                "author": "Juan Cruz-Benito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20905v1",
                "updated": "2025-08-28T15:36:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    36,
                    34,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:36:34Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    36,
                    34,
                    3,
                    240,
                    0
                ],
                "title": "Real-Time Tracking Antenna System for Moving Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Tracking Antenna System for Moving Targets"
                },
                "summary": "This paper presents the design and implementation of a compact,\ncost-effective phased array antenna system. It is capable of real-time\nbeam-steering for dynamic target-tracking applications. The system employs a\n4$\\times$4 rectangular microstrip patch array, utilizing advanced beamforming\ntechniques and a Direction of Arrival (DoA) estimation algorithm. It achieves\n$\\pm 42^{\\circ}$ wide-angle scanning in both azimuth and elevation planes. The\ndesign emphasizes a balance between high angular coverage and consistent gain\nperformance. This makes it suitable for wireless tracking, radar, and satellite\ncommunication terminals. Fabricated on Rogers 6010.2LM substrate, the system\ndemonstrates reproducibility and scalability. All components are sourced\nlocally to ensure practical deployment. The system is built using commercially\navailable components, highlighting its affordability for research and\nprototyping purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the design and implementation of a compact,\ncost-effective phased array antenna system. It is capable of real-time\nbeam-steering for dynamic target-tracking applications. The system employs a\n4$\\times$4 rectangular microstrip patch array, utilizing advanced beamforming\ntechniques and a Direction of Arrival (DoA) estimation algorithm. It achieves\n$\\pm 42^{\\circ}$ wide-angle scanning in both azimuth and elevation planes. The\ndesign emphasizes a balance between high angular coverage and consistent gain\nperformance. This makes it suitable for wireless tracking, radar, and satellite\ncommunication terminals. Fabricated on Rogers 6010.2LM substrate, the system\ndemonstrates reproducibility and scalability. All components are sourced\nlocally to ensure practical deployment. The system is built using commercially\navailable components, highlighting its affordability for research and\nprototyping purposes."
                },
                "authors": [
                    {
                        "name": "Adham Saad"
                    },
                    {
                        "name": "Aya Sherif Nassef"
                    },
                    {
                        "name": "Mahmoud Mohamed Elshahed"
                    },
                    {
                        "name": "Mohamed Ismail Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Ismail Ahmed"
                },
                "author": "Mohamed Ismail Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17513v2",
                "updated": "2025-08-28T15:33:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    33,
                    2,
                    3,
                    240,
                    0
                ],
                "published": "2025-03-21T19:56:59Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    56,
                    59,
                    4,
                    80,
                    0
                ],
                "title": "Improving Quantization with Post-Training Model Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Quantization with Post-Training Model Expansion"
                },
                "summary": "The size of a model has been a strong predictor of its quality, as well as\nits cost. As such, the trade-off between model cost and quality has been\nwell-studied. Post-training optimizations like quantization and pruning have\ntypically focused on reducing the overall volume of pre-trained models to\nreduce inference costs while maintaining model quality. However, recent\nadvancements have introduced optimization techniques that, interestingly,\nexpand models post-training, increasing model size to improve quality when\nreducing volume. For instance, to enable 4-bit weight and activation\nquantization, incoherence processing often necessitates inserting online\nHadamard rotations in the compute graph, and preserving highly sensitive\nweights often calls for additional higher precision computations. However, if\napplication requirements cannot be met, the prevailing solution is to relax\nquantization constraints. In contrast, we demonstrate post-training model\nexpansion is a viable strategy to improve model quality within a quantization\nco-design space, and provide theoretical justification. We show it is possible\nto progressively and selectively expand the size of a pre-trained large\nlanguage model (LLM) to improve model quality without end-to-end retraining. In\nparticular, when quantizing the weights and activations to 4 bits for Llama3\n1B, we reduce the gap to full-precision perplexity by an average of 9% relative\nto both QuaRot and SpinQuant with only 5% more parameters, which is still a\n3.8% reduction in volume relative to a BF16 reference model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of a model has been a strong predictor of its quality, as well as\nits cost. As such, the trade-off between model cost and quality has been\nwell-studied. Post-training optimizations like quantization and pruning have\ntypically focused on reducing the overall volume of pre-trained models to\nreduce inference costs while maintaining model quality. However, recent\nadvancements have introduced optimization techniques that, interestingly,\nexpand models post-training, increasing model size to improve quality when\nreducing volume. For instance, to enable 4-bit weight and activation\nquantization, incoherence processing often necessitates inserting online\nHadamard rotations in the compute graph, and preserving highly sensitive\nweights often calls for additional higher precision computations. However, if\napplication requirements cannot be met, the prevailing solution is to relax\nquantization constraints. In contrast, we demonstrate post-training model\nexpansion is a viable strategy to improve model quality within a quantization\nco-design space, and provide theoretical justification. We show it is possible\nto progressively and selectively expand the size of a pre-trained large\nlanguage model (LLM) to improve model quality without end-to-end retraining. In\nparticular, when quantizing the weights and activations to 4 bits for Llama3\n1B, we reduce the gap to full-precision perplexity by an average of 9% relative\nto both QuaRot and SpinQuant with only 5% more parameters, which is still a\n3.8% reduction in volume relative to a BF16 reference model."
                },
                "authors": [
                    {
                        "name": "Giuseppe Franco"
                    },
                    {
                        "name": "Pablo Monteagudo-Lago"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Nicholas Fraser"
                    },
                    {
                        "name": "Michaela Blott"
                    }
                ],
                "author_detail": {
                    "name": "Michaela Blott"
                },
                "author": "Michaela Blott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20900v1",
                "updated": "2025-08-28T15:29:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    29,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:29:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    29,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "OneRec-V2 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneRec-V2 Technical Report"
                },
                "summary": "Recent breakthroughs in generative AI have transformed recommender systems\nthrough end-to-end generation. OneRec reformulates recommendation as an\nautoregressive generation task, achieving high Model FLOPs Utilization. While\nOneRec-V1 has shown significant empirical success in real-world deployment, two\ncritical challenges hinder its scalability and performance: (1) inefficient\ncomputational allocation where 97.66% of resources are consumed by sequence\nencoding rather than generation, and (2) limitations in reinforcement learning\nrelying solely on reward models.\n  To address these challenges, we propose OneRec-V2, featuring: (1) Lazy\nDecoder-Only Architecture: Eliminates encoder bottlenecks, reducing total\ncomputation by 94% and training resources by 90%, enabling successful scaling\nto 8B parameters. (2) Preference Alignment with Real-World User Interactions:\nIncorporates Duration-Aware Reward Shaping and Adaptive Ratio Clipping to\nbetter align with user preferences using real-world feedback.\n  Extensive A/B tests on Kuaishou demonstrate OneRec-V2's effectiveness,\nimproving App Stay Time by 0.467%/0.741% while balancing multi-objective\nrecommendations. This work advances generative recommendation scalability and\nalignment with real-world feedback, representing a step forward in the\ndevelopment of end-to-end recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in generative AI have transformed recommender systems\nthrough end-to-end generation. OneRec reformulates recommendation as an\nautoregressive generation task, achieving high Model FLOPs Utilization. While\nOneRec-V1 has shown significant empirical success in real-world deployment, two\ncritical challenges hinder its scalability and performance: (1) inefficient\ncomputational allocation where 97.66% of resources are consumed by sequence\nencoding rather than generation, and (2) limitations in reinforcement learning\nrelying solely on reward models.\n  To address these challenges, we propose OneRec-V2, featuring: (1) Lazy\nDecoder-Only Architecture: Eliminates encoder bottlenecks, reducing total\ncomputation by 94% and training resources by 90%, enabling successful scaling\nto 8B parameters. (2) Preference Alignment with Real-World User Interactions:\nIncorporates Duration-Aware Reward Shaping and Adaptive Ratio Clipping to\nbetter align with user preferences using real-world feedback.\n  Extensive A/B tests on Kuaishou demonstrate OneRec-V2's effectiveness,\nimproving App Stay Time by 0.467%/0.741% while balancing multi-objective\nrecommendations. This work advances generative recommendation scalability and\nalignment with real-world feedback, representing a step forward in the\ndevelopment of end-to-end recommender systems."
                },
                "authors": [
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Hengrui Hu"
                    },
                    {
                        "name": "Hongtao Cheng"
                    },
                    {
                        "name": "Huanjie Wang"
                    },
                    {
                        "name": "Jiaxin Deng"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Kuo Cai"
                    },
                    {
                        "name": "Lejian Ren"
                    },
                    {
                        "name": "Lu Ren"
                    },
                    {
                        "name": "Liao Yu"
                    },
                    {
                        "name": "Pengfei Zheng"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Shujie Yang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Wuchao Li"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Xingmei Wang"
                    },
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Zexuan Cheng"
                    },
                    {
                        "name": "Zhanyu Liu"
                    },
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Chaoyi Ma"
                    },
                    {
                        "name": "Chengru Song"
                    },
                    {
                        "name": "Chenhui Wang"
                    },
                    {
                        "name": "Chenglong Chu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Dongxue Meng"
                    },
                    {
                        "name": "Dunju Zang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Fangyu Zhang"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Fuxing Zhang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Guowang Zhang"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Honghui Bao"
                    },
                    {
                        "name": "Hongyang Cao"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Jiaqiang Liu"
                    },
                    {
                        "name": "Jinghui Jia"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Lantao Hu"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Qidong Zhou"
                    },
                    {
                        "name": "Rongzhou Zhang"
                    },
                    {
                        "name": "Shengzhe Wang"
                    },
                    {
                        "name": "Shihui He"
                    },
                    {
                        "name": "Shuang Yang"
                    },
                    {
                        "name": "Siyang Mao"
                    },
                    {
                        "name": "Sui Huang"
                    },
                    {
                        "name": "Tiantian He"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Xiaoxiao Xu"
                    },
                    {
                        "name": "Xugang Liu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yiwu Liu"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yunfeng Zhao"
                    },
                    {
                        "name": "Zhixin Ling"
                    },
                    {
                        "name": "Ziming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ziming Li"
                },
                "author": "Ziming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20899v1",
                "updated": "2025-08-28T15:27:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    27,
                    35,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:27:35Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    27,
                    35,
                    3,
                    240,
                    0
                ],
                "title": "Language-Enhanced Mobile Manipulation for Efficient Object Search in\n  Indoor Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Enhanced Mobile Manipulation for Efficient Object Search in\n  Indoor Environments"
                },
                "summary": "Enabling robots to efficiently search for and identify objects in complex,\nunstructured environments is critical for diverse applications ranging from\nhousehold assistance to industrial automation. However, traditional scene\nrepresentations typically capture only static semantics and lack interpretable\ncontextual reasoning, limiting their ability to guide object search in\ncompletely unfamiliar settings. To address this challenge, we propose a\nlanguage-enhanced hierarchical navigation framework that tightly integrates\nsemantic perception and spatial reasoning. Our method, Goal-Oriented\nDynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large\nlanguage models (LLMs) to infer scene semantics and guide the search process\nthrough a multi-level decision hierarchy. Reliability in reasoning is achieved\nthrough the use of structured prompts and logical constraints applied at each\nstage of the hierarchy. For the specific challenges of mobile manipulation, we\nintroduce a heuristic-based motion planner that combines polar angle sorting\nwith distance prioritization to efficiently generate exploration paths.\nComprehensive evaluations in Isaac Sim demonstrate the feasibility of our\nframework, showing that GODHS can locate target objects with higher search\nefficiency compared to conventional, non-semantic search strategies. Website\nand Video are available at: https://drapandiger.github.io/GODHS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling robots to efficiently search for and identify objects in complex,\nunstructured environments is critical for diverse applications ranging from\nhousehold assistance to industrial automation. However, traditional scene\nrepresentations typically capture only static semantics and lack interpretable\ncontextual reasoning, limiting their ability to guide object search in\ncompletely unfamiliar settings. To address this challenge, we propose a\nlanguage-enhanced hierarchical navigation framework that tightly integrates\nsemantic perception and spatial reasoning. Our method, Goal-Oriented\nDynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large\nlanguage models (LLMs) to infer scene semantics and guide the search process\nthrough a multi-level decision hierarchy. Reliability in reasoning is achieved\nthrough the use of structured prompts and logical constraints applied at each\nstage of the hierarchy. For the specific challenges of mobile manipulation, we\nintroduce a heuristic-based motion planner that combines polar angle sorting\nwith distance prioritization to efficiently generate exploration paths.\nComprehensive evaluations in Isaac Sim demonstrate the feasibility of our\nframework, showing that GODHS can locate target objects with higher search\nefficiency compared to conventional, non-semantic search strategies. Website\nand Video are available at: https://drapandiger.github.io/GODHS"
                },
                "authors": [
                    {
                        "name": "Liding Zhang"
                    },
                    {
                        "name": "Zeqi Li"
                    },
                    {
                        "name": "Kuanqi Cai"
                    },
                    {
                        "name": "Qian Huang"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_journal_ref": "2025 IEEE International Conference on Cyborg and Bionic Systems\n  (CBS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01968v2",
                "updated": "2025-08-28T15:23:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    23,
                    4,
                    3,
                    240,
                    0
                ],
                "published": "2025-05-04T02:36:57Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    2,
                    36,
                    57,
                    6,
                    124,
                    0
                ],
                "title": "HAS-GPU: Efficient Hybrid Auto-scaling with Fine-grained GPU Allocation\n  for SLO-aware Serverless Inferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAS-GPU: Efficient Hybrid Auto-scaling with Fine-grained GPU Allocation\n  for SLO-aware Serverless Inferences"
                },
                "summary": "Serverless Computing (FaaS) has become a popular paradigm for deep learning\ninference due to the ease of deployment and pay-per-use benefits. However,\ncurrent serverless inference platforms encounter the coarse-grained and static\nGPU resource allocation problems during scaling, which leads to high costs and\nService Level Objective (SLO) violations in fluctuating workloads. Meanwhile,\ncurrent platforms only support horizontal scaling for GPU inferences, thus the\ncold start problem further exacerbates the problems. In this paper, we propose\nHAS-GPU, an efficient Hybrid Auto-scaling Serverless architecture with\nfine-grained GPU allocation for deep learning inferences. HAS-GPU proposes an\nagile scheduler capable of allocating GPU Streaming Multiprocessor (SM)\npartitions and time quotas with arbitrary granularity and enables significant\nvertical quota scalability at runtime. To resolve performance uncertainty\nintroduced by massive fine-grained resource configuration spaces, we propose\nthe Resource-aware Performance Predictor (RaPP). Furthermore, we present an\nadaptive hybrid auto-scaling algorithm with both horizontal and vertical\nscaling to ensure inference SLOs and minimize GPU costs. The experiments\ndemonstrated that compared to the mainstream serverless inference platform,\nHAS-GPU reduces function costs by an average of 10.8x with better SLO\nguarantees. Compared to state-of-the-art spatio-temporal GPU sharing serverless\nframework, HAS-GPU reduces function SLO violation by 4.8x and cost by 1.72x on\naverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing (FaaS) has become a popular paradigm for deep learning\ninference due to the ease of deployment and pay-per-use benefits. However,\ncurrent serverless inference platforms encounter the coarse-grained and static\nGPU resource allocation problems during scaling, which leads to high costs and\nService Level Objective (SLO) violations in fluctuating workloads. Meanwhile,\ncurrent platforms only support horizontal scaling for GPU inferences, thus the\ncold start problem further exacerbates the problems. In this paper, we propose\nHAS-GPU, an efficient Hybrid Auto-scaling Serverless architecture with\nfine-grained GPU allocation for deep learning inferences. HAS-GPU proposes an\nagile scheduler capable of allocating GPU Streaming Multiprocessor (SM)\npartitions and time quotas with arbitrary granularity and enables significant\nvertical quota scalability at runtime. To resolve performance uncertainty\nintroduced by massive fine-grained resource configuration spaces, we propose\nthe Resource-aware Performance Predictor (RaPP). Furthermore, we present an\nadaptive hybrid auto-scaling algorithm with both horizontal and vertical\nscaling to ensure inference SLOs and minimize GPU costs. The experiments\ndemonstrated that compared to the mainstream serverless inference platform,\nHAS-GPU reduces function costs by an average of 10.8x with better SLO\nguarantees. Compared to state-of-the-art spatio-temporal GPU sharing serverless\nframework, HAS-GPU reduces function SLO violation by 4.8x and cost by 1.72x on\naverage."
                },
                "authors": [
                    {
                        "name": "Jianfeng Gu"
                    },
                    {
                        "name": "Puxuan Wang"
                    },
                    {
                        "name": "Isaac David Nunez Araya"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Michael Gerndt"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gerndt"
                },
                "author": "Michael Gerndt",
                "arxiv_doi": "10.1007/978-3-031-99854-6_11",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99854-6_11",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.01968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The paper has been accepted by Euro-Par 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20893v1",
                "updated": "2025-08-28T15:22:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    22,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:22:31Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    22,
                    31,
                    3,
                    240,
                    0
                ],
                "title": "The Uneven Impact of Post-Training Quantization in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Uneven Impact of Post-Training Quantization in Machine Translation"
                },
                "summary": "Quantization is essential for deploying large language models (LLMs) on\nresource-constrained hardware, but its implications for multilingual tasks\nremain underexplored. We conduct the first large-scale evaluation of\npost-training quantization (PTQ) on machine translation across 55 languages\nusing five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that\nwhile 4-bit quantization often preserves translation quality for high-resource\nlanguages and large models, significant degradation occurs for low-resource and\ntypologically diverse languages, particularly in 2-bit settings. We compare\nfour quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing\nthat algorithm choice and model size jointly determine robustness. GGUF\nvariants provide the most consistent performance, even at 2-bit precision.\nAdditionally, we quantify the interactions between quantization, decoding\nhyperparameters, and calibration languages, finding that language-matched\ncalibration offers benefits primarily in low-bit scenarios. Our findings offer\nactionable insights for deploying multilingual LLMs for machine translation\nunder quantization constraints, especially in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying large language models (LLMs) on\nresource-constrained hardware, but its implications for multilingual tasks\nremain underexplored. We conduct the first large-scale evaluation of\npost-training quantization (PTQ) on machine translation across 55 languages\nusing five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that\nwhile 4-bit quantization often preserves translation quality for high-resource\nlanguages and large models, significant degradation occurs for low-resource and\ntypologically diverse languages, particularly in 2-bit settings. We compare\nfour quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing\nthat algorithm choice and model size jointly determine robustness. GGUF\nvariants provide the most consistent performance, even at 2-bit precision.\nAdditionally, we quantify the interactions between quantization, decoding\nhyperparameters, and calibration languages, finding that language-matched\ncalibration offers benefits primarily in low-bit scenarios. Our findings offer\nactionable insights for deploying multilingual LLMs for machine translation\nunder quantization constraints, especially in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Benjamin Marie"
                    },
                    {
                        "name": "Atsushi Fujita"
                    }
                ],
                "author_detail": {
                    "name": "Atsushi Fujita"
                },
                "author": "Atsushi Fujita",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20890v1",
                "updated": "2025-08-28T15:19:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    19,
                    7,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:19:07Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    19,
                    7,
                    3,
                    240,
                    0
                ],
                "title": "PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications, from virtual assistants to autonomous agents. However, their\nflexibility also introduces new attack vectors-particularly Prompt Injection\n(PI), where adversaries manipulate model behavior through crafted inputs. As\nattackers continuously evolve with paraphrased, obfuscated, and even multi-task\ninjection strategies, existing benchmarks are no longer sufficient to capture\nthe full spectrum of emerging threats.\n  To address this gap, we construct a new benchmark that systematically extends\nprior efforts. Our benchmark subsumes the two widely-used existing ones while\nintroducing new manipulation techniques and multi-task scenarios, thereby\nproviding a more comprehensive evaluation setting. We find that existing\ndefenses, though effective on their original benchmarks, show clear weaknesses\nunder our benchmark, underscoring the need for more robust solutions. Our key\ninsight is that while attack forms may vary, the adversary's intent-injecting\nan unauthorized task-remains invariant. Building on this observation, we\npropose PromptSleuth, a semantic-oriented defense framework that detects prompt\ninjection by reasoning over task-level intent rather than surface features.\nEvaluated across state-of-the-art benchmarks, PromptSleuth consistently\noutperforms existing defense while maintaining comparable runtime and cost\nefficiency. These results demonstrate that intent-based semantic reasoning\noffers a robust, efficient, and generalizable strategy for defending LLMs\nagainst evolving prompt injection threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications, from virtual assistants to autonomous agents. However, their\nflexibility also introduces new attack vectors-particularly Prompt Injection\n(PI), where adversaries manipulate model behavior through crafted inputs. As\nattackers continuously evolve with paraphrased, obfuscated, and even multi-task\ninjection strategies, existing benchmarks are no longer sufficient to capture\nthe full spectrum of emerging threats.\n  To address this gap, we construct a new benchmark that systematically extends\nprior efforts. Our benchmark subsumes the two widely-used existing ones while\nintroducing new manipulation techniques and multi-task scenarios, thereby\nproviding a more comprehensive evaluation setting. We find that existing\ndefenses, though effective on their original benchmarks, show clear weaknesses\nunder our benchmark, underscoring the need for more robust solutions. Our key\ninsight is that while attack forms may vary, the adversary's intent-injecting\nan unauthorized task-remains invariant. Building on this observation, we\npropose PromptSleuth, a semantic-oriented defense framework that detects prompt\ninjection by reasoning over task-level intent rather than surface features.\nEvaluated across state-of-the-art benchmarks, PromptSleuth consistently\noutperforms existing defense while maintaining comparable runtime and cost\nefficiency. These results demonstrate that intent-based semantic reasoning\noffers a robust, efficient, and generalizable strategy for defending LLMs\nagainst evolving prompt injection threats."
                },
                "authors": [
                    {
                        "name": "Mengxiao Wang"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Guofei Gu"
                    }
                ],
                "author_detail": {
                    "name": "Guofei Gu"
                },
                "author": "Guofei Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17232v2",
                "updated": "2025-08-28T15:15:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    15,
                    18,
                    3,
                    240,
                    0
                ],
                "published": "2025-07-23T05:56:20Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    56,
                    20,
                    2,
                    204,
                    0
                ],
                "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for\n  State Probing Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Clean Recipe Dataset with Ingredient States Annotation for\n  State Probing Task"
                },
                "summary": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs. The dataset are publicly available\nat: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs. The dataset are publicly available\nat: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1"
                },
                "authors": [
                    {
                        "name": "Mashiro Toyooka"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    },
                    {
                        "name": "Yoko Yamakata"
                    }
                ],
                "author_detail": {
                    "name": "Yoko Yamakata"
                },
                "author": "Yoko Yamakata",
                "arxiv_comment": "Accepted to ACM Multimedia 2025. The dataset are publicly available\n  at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20877v1",
                "updated": "2025-08-28T15:07:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    7,
                    4,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T15:07:04Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    7,
                    4,
                    3,
                    240,
                    0
                ],
                "title": "Deep Learning Framework for Early Detection of Pancreatic Cancer Using\n  Multi-Modal Medical Imaging Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Framework for Early Detection of Pancreatic Cancer Using\n  Multi-Modal Medical Imaging Analysis"
                },
                "summary": "Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms\nof cancer, with a five-year survival rate below 10% primarily due to late\ndetection. This research develops and validates a deep learning framework for\nearly PDAC detection through analysis of dual-modality imaging:\nautofluorescence and second harmonic generation (SHG). We analyzed 40 unique\npatient samples to create a specialized neural network capable of\ndistinguishing between normal, fibrotic, and cancerous tissue. Our methodology\nevaluated six distinct deep learning architectures, comparing traditional\nConvolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).\nThrough systematic experimentation, we identified and overcome significant\nchallenges in medical image analysis, including limited dataset size and class\nimbalance. The final optimized framework, based on a modified ResNet\narchitecture with frozen pre-trained layers and class-weighted training,\nachieved over 90% accuracy in cancer detection. This represents a significant\nimprovement over current manual analysis methods an demonstrates potential for\nclinical deployment. This work establishes a robust pipeline for automated PDAC\ndetection that can augment pathologists' capabilities while providing a\nfoundation for future expansion to other cancer types. The developed\nmethodology also offers valuable insights for applying deep learning to\nlimited-size medical imaging datasets, a common challenge in clinical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms\nof cancer, with a five-year survival rate below 10% primarily due to late\ndetection. This research develops and validates a deep learning framework for\nearly PDAC detection through analysis of dual-modality imaging:\nautofluorescence and second harmonic generation (SHG). We analyzed 40 unique\npatient samples to create a specialized neural network capable of\ndistinguishing between normal, fibrotic, and cancerous tissue. Our methodology\nevaluated six distinct deep learning architectures, comparing traditional\nConvolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).\nThrough systematic experimentation, we identified and overcome significant\nchallenges in medical image analysis, including limited dataset size and class\nimbalance. The final optimized framework, based on a modified ResNet\narchitecture with frozen pre-trained layers and class-weighted training,\nachieved over 90% accuracy in cancer detection. This represents a significant\nimprovement over current manual analysis methods an demonstrates potential for\nclinical deployment. This work establishes a robust pipeline for automated PDAC\ndetection that can augment pathologists' capabilities while providing a\nfoundation for future expansion to other cancer types. The developed\nmethodology also offers valuable insights for applying deep learning to\nlimited-size medical imaging datasets, a common challenge in clinical\napplications."
                },
                "authors": [
                    {
                        "name": "Dennis Slobodzian"
                    },
                    {
                        "name": "Karissa Tilbury"
                    },
                    {
                        "name": "Amir Kordijazi"
                    }
                ],
                "author_detail": {
                    "name": "Amir Kordijazi"
                },
                "author": "Amir Kordijazi",
                "arxiv_comment": "21 pages, 17 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20867v1",
                "updated": "2025-08-28T14:59:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    59,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:59:55Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    59,
                    55,
                    3,
                    240,
                    0
                ],
                "title": "MSRS: Evaluating Multi-Source Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSRS: Evaluating Multi-Source Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented systems are typically evaluated in settings where\ninformation required to answer the query can be found within a single source or\nthe answer is short-form or factoid-based. However, many real-world\napplications demand the ability to integrate and summarize information\nscattered across multiple sources, where no single source is sufficient to\nrespond to the user's question. In such settings, the retrieval component of a\nRAG pipeline must recognize a variety of relevance signals, and the generation\ncomponent must connect and synthesize information across multiple sources. We\npresent a scalable framework for constructing evaluation benchmarks that\nchallenge RAG systems to integrate information across distinct sources and\ngenerate long-form responses. Using our framework, we build two new benchmarks\non Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing\nnarrative synthesis and summarization tasks, respectively, that require\nretrieval from large collections. Our extensive experiments with various RAG\npipelines -- including sparse and dense retrievers combined with frontier LLMs\n-- reveal that generation quality is highly dependent on retrieval\neffectiveness, which varies greatly by task. While multi-source synthesis\nproves challenging even in an oracle retrieval setting, we find that reasoning\nmodels significantly outperform standard LLMs at this distinct step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented systems are typically evaluated in settings where\ninformation required to answer the query can be found within a single source or\nthe answer is short-form or factoid-based. However, many real-world\napplications demand the ability to integrate and summarize information\nscattered across multiple sources, where no single source is sufficient to\nrespond to the user's question. In such settings, the retrieval component of a\nRAG pipeline must recognize a variety of relevance signals, and the generation\ncomponent must connect and synthesize information across multiple sources. We\npresent a scalable framework for constructing evaluation benchmarks that\nchallenge RAG systems to integrate information across distinct sources and\ngenerate long-form responses. Using our framework, we build two new benchmarks\non Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing\nnarrative synthesis and summarization tasks, respectively, that require\nretrieval from large collections. Our extensive experiments with various RAG\npipelines -- including sparse and dense retrievers combined with frontier LLMs\n-- reveal that generation quality is highly dependent on retrieval\neffectiveness, which varies greatly by task. While multi-source synthesis\nproves challenging even in an oracle retrieval setting, we find that reasoning\nmodels significantly outperform standard LLMs at this distinct step."
                },
                "authors": [
                    {
                        "name": "Rohan Phanse"
                    },
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Kejian Shi"
                    },
                    {
                        "name": "Wencai Zhang"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "COLM 2025; this article supersedes the preprint: arXiv:2309.08960",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20864v1",
                "updated": "2025-08-28T14:58:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    9,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:09Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    9,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign\n  Detection Using Mm-Wave MIMO FMCW Radar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign\n  Detection Using Mm-Wave MIMO FMCW Radar"
                },
                "summary": "This paper explores the deployment of mm-wave Frequency Modulated Continuous\nWave (FMCW) radar for vital sign detection across multiple scenarios. We focus\non overcoming the limitations of traditional sensing methods by enhancing\nsignal processing techniques to capture subtle physiological changes\neffectively. Our study introduces novel adaptations of the Prony and MUSIC\nalgorithms tailored for real-time heart and respiration rate monitoring,\nsignificantly advancing the accuracy and reliability of non-contact vital sign\nmonitoring using radar technologies. Notably, these algorithms demonstrate a\nrobust ability to suppress noise and harmonic interference. For instance, the\nmean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8\nand 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8,\nrespectively. These results underscore the potential of FMCW radar as a\nreliable, non-invasive solution for continuous vital sign monitoring in\nhealthcare settings, particularly in clinical and emergency scenarios where\ntraditional contact-based monitoring is impractical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the deployment of mm-wave Frequency Modulated Continuous\nWave (FMCW) radar for vital sign detection across multiple scenarios. We focus\non overcoming the limitations of traditional sensing methods by enhancing\nsignal processing techniques to capture subtle physiological changes\neffectively. Our study introduces novel adaptations of the Prony and MUSIC\nalgorithms tailored for real-time heart and respiration rate monitoring,\nsignificantly advancing the accuracy and reliability of non-contact vital sign\nmonitoring using radar technologies. Notably, these algorithms demonstrate a\nrobust ability to suppress noise and harmonic interference. For instance, the\nmean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8\nand 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8,\nrespectively. These results underscore the potential of FMCW radar as a\nreliable, non-invasive solution for continuous vital sign monitoring in\nhealthcare settings, particularly in clinical and emergency scenarios where\ntraditional contact-based monitoring is impractical."
                },
                "authors": [
                    {
                        "name": "Ehsan Sadeghi"
                    },
                    {
                        "name": "Paul Havinga"
                    }
                ],
                "author_detail": {
                    "name": "Paul Havinga"
                },
                "author": "Paul Havinga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20863v2",
                "updated": "2025-08-29T09:37:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    37,
                    59,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T14:57:04Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    57,
                    4,
                    3,
                    240,
                    0
                ],
                "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review"
                },
                "summary": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks."
                },
                "authors": [
                    {
                        "name": "Matteo Gioele Collu"
                    },
                    {
                        "name": "Umberto Salviati"
                    },
                    {
                        "name": "Roberto Confalonieri"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Giovanni Apruzzese"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Apruzzese"
                },
                "author": "Giovanni Apruzzese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20848v1",
                "updated": "2025-08-28T14:40:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    40,
                    27,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:40:27Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    40,
                    27,
                    3,
                    240,
                    0
                ],
                "title": "JADES: A Universal Framework for Jailbreak Assessment via\n  Decompositional Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JADES: A Universal Framework for Jailbreak Assessment via\n  Decompositional Scoring"
                },
                "summary": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately determining whether a jailbreak attempt has succeeded is a\nfundamental yet unresolved challenge. Existing evaluation methods rely on\nmisaligned proxy indicators or naive holistic judgments. They frequently\nmisinterpret model responses, leading to inconsistent and subjective\nassessments that misalign with human perception. To address this gap, we\nintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal\njailbreak evaluation framework. Its key mechanism is to automatically decompose\nan input harmful question into a set of weighted sub-questions, score each\nsub-answer, and weight-aggregate the sub-scores into a final decision. JADES\nalso incorporates an optional fact-checking module to strengthen the detection\nof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a\nnewly introduced benchmark proposed in this work, consisting of 400 pairs of\njailbreak prompts and responses, each meticulously annotated by humans. In a\nbinary setting (success/failure), JADES achieves 98.5% agreement with human\nevaluators, outperforming strong baselines by over 9%. Re-evaluating five\npopular attacks on four LLMs reveals substantial overestimation (e.g., LAA's\nattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show\nthat JADES could deliver accurate, consistent, and interpretable evaluations,\nproviding a reliable basis for measuring future jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Junjie Chu"
                    },
                    {
                        "name": "Mingjie Li"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Ye Leng"
                    },
                    {
                        "name": "Chenhao Lin"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yun Shen"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "arxiv_comment": "17 pages, 5 figures. For the code and data supporting this work, see\n  https://trustairlab.github.io/jades.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12140v2",
                "updated": "2025-08-28T14:32:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    32,
                    15,
                    3,
                    240,
                    0
                ],
                "published": "2025-04-16T14:52:22Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    14,
                    52,
                    22,
                    2,
                    106,
                    0
                ],
                "title": "Multilingual Contextualization of Large Language Models for\n  Document-Level Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Contextualization of Large Language Models for\n  Document-Level Machine Translation"
                },
                "summary": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods."
                },
                "authors": [
                    {
                        "name": "Miguel Moura Ramos"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Andr√© F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr√© F. T. Martins"
                },
                "author": "Andr√© F. T. Martins",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20828v1",
                "updated": "2025-08-28T14:23:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    23,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:23:39Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    23,
                    39,
                    3,
                    240,
                    0
                ],
                "title": "GDLLM: A Global Distance-aware Modeling Approach Based on Large Language\n  Models for Event Temporal Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDLLM: A Global Distance-aware Modeling Approach Based on Large Language\n  Models for Event Temporal Relation Extraction"
                },
                "summary": "In Natural Language Processing(NLP), Event Temporal Relation Extraction\n(ETRE) is to recognize the temporal relations of two events. Prior studies have\nnoted the importance of language models for ETRE. However, the restricted\npre-trained knowledge of Small Language Models(SLMs) limits their capability to\nhandle minority class relations in imbalanced classification datasets. For\nLarge Language Models(LLMs), researchers adopt manually designed prompts or\ninstructions, which may introduce extra noise, leading to interference with the\nmodel's judgment of the long-distance dependencies between events. To address\nthese issues, we propose GDLLM, a Global Distance-aware modeling approach based\non LLMs. We first present a distance-aware graph structure utilizing Graph\nAttention Network(GAT) to assist the LLMs in capturing long-distance dependency\nfeatures. Additionally, we design a temporal feature learning paradigm based on\nsoft inference to augment the identification of relations with a short-distance\nproximity band, which supplements the probabilistic information generated by\nLLMs into the multi-head attention mechanism. Since the global feature can be\ncaptured effectively, our framework substantially enhances the performance of\nminority relation classes and improves the overall learning ability.\nExperiments on two publicly available datasets, TB-Dense and MATRES,\ndemonstrate that our approach achieves state-of-the-art (SOTA) performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Natural Language Processing(NLP), Event Temporal Relation Extraction\n(ETRE) is to recognize the temporal relations of two events. Prior studies have\nnoted the importance of language models for ETRE. However, the restricted\npre-trained knowledge of Small Language Models(SLMs) limits their capability to\nhandle minority class relations in imbalanced classification datasets. For\nLarge Language Models(LLMs), researchers adopt manually designed prompts or\ninstructions, which may introduce extra noise, leading to interference with the\nmodel's judgment of the long-distance dependencies between events. To address\nthese issues, we propose GDLLM, a Global Distance-aware modeling approach based\non LLMs. We first present a distance-aware graph structure utilizing Graph\nAttention Network(GAT) to assist the LLMs in capturing long-distance dependency\nfeatures. Additionally, we design a temporal feature learning paradigm based on\nsoft inference to augment the identification of relations with a short-distance\nproximity band, which supplements the probabilistic information generated by\nLLMs into the multi-head attention mechanism. Since the global feature can be\ncaptured effectively, our framework substantially enhances the performance of\nminority relation classes and improves the overall learning ability.\nExperiments on two publicly available datasets, TB-Dense and MATRES,\ndemonstrate that our approach achieves state-of-the-art (SOTA) performance."
                },
                "authors": [
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Wanting Ning"
                    },
                    {
                        "name": "Yuxiao Fei"
                    },
                    {
                        "name": "Yubo Feng"
                    },
                    {
                        "name": "Lishuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Lishuang Li"
                },
                "author": "Lishuang Li",
                "arxiv_comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11022v3",
                "updated": "2025-08-28T14:17:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    17,
                    59,
                    3,
                    240,
                    0
                ],
                "published": "2024-11-17T09:58:37Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    9,
                    58,
                    37,
                    6,
                    322,
                    0
                ],
                "title": "ASiM: Modeling and Analyzing Inference Accuracy of SRAM-Based Analog CiM\n  Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASiM: Modeling and Analyzing Inference Accuracy of SRAM-Based Analog CiM\n  Circuits"
                },
                "summary": "SRAM-based Analog Compute-in-Memory (ACiM) demonstrates promising energy\nefficiency for deep neural network (DNN) processing. Nevertheless, efforts to\noptimize efficiency frequently compromise accuracy, and this trade-off remains\ninsufficiently studied due to the difficulty of performing full-system\nvalidation. Specifically, existing simulation tools rarely target SRAM-based\nACiM and exhibit inconsistent accuracy predictions, highlighting the need for a\nstandardized, SRAM CiM circuit-aware evaluation methodology. This paper\npresents ASiM, a simulation framework for evaluating inference accuracy in\nSRAM-based ACiM systems. ASiM captures critical effects in SRAM based analog\ncompute in memory systems, such as ADC quantization, bit parallel encoding, and\nanalog noise, which must be modeled with high fidelity due to their distinct\nbehavior in charge domain architectures compared to other memory technologies.\nASiM supports a wide range of modern DNN workloads, including CNN and\nTransformer-based models such as ViT, and scales to large-scale tasks like\nImageNet classification. Our results indicate that bit-parallel encoding can\nimprove energy efficiency with only modest accuracy degradation; however, even\n1 LSB of analog noise can significantly impair inference performance,\nparticularly in complex tasks such as ImageNet. To address this, we explore\nhybrid analog-digital execution and majority voting schemes, both of which\nenhance robustness without negating energy savings. ASiM bridges the gap\nbetween hardware design and inference performance, offering actionable insights\nfor energy-efficient, high-accuracy ACiM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRAM-based Analog Compute-in-Memory (ACiM) demonstrates promising energy\nefficiency for deep neural network (DNN) processing. Nevertheless, efforts to\noptimize efficiency frequently compromise accuracy, and this trade-off remains\ninsufficiently studied due to the difficulty of performing full-system\nvalidation. Specifically, existing simulation tools rarely target SRAM-based\nACiM and exhibit inconsistent accuracy predictions, highlighting the need for a\nstandardized, SRAM CiM circuit-aware evaluation methodology. This paper\npresents ASiM, a simulation framework for evaluating inference accuracy in\nSRAM-based ACiM systems. ASiM captures critical effects in SRAM based analog\ncompute in memory systems, such as ADC quantization, bit parallel encoding, and\nanalog noise, which must be modeled with high fidelity due to their distinct\nbehavior in charge domain architectures compared to other memory technologies.\nASiM supports a wide range of modern DNN workloads, including CNN and\nTransformer-based models such as ViT, and scales to large-scale tasks like\nImageNet classification. Our results indicate that bit-parallel encoding can\nimprove energy efficiency with only modest accuracy degradation; however, even\n1 LSB of analog noise can significantly impair inference performance,\nparticularly in complex tasks such as ImageNet. To address this, we explore\nhybrid analog-digital execution and majority voting schemes, both of which\nenhance robustness without negating energy savings. ASiM bridges the gap\nbetween hardware design and inference performance, offering actionable insights\nfor energy-efficient, high-accuracy ACiM deployment."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Yung-Chin Chen"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20818v1",
                "updated": "2025-08-28T14:16:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    16,
                    17,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:16:17Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    16,
                    17,
                    3,
                    240,
                    0
                ],
                "title": "cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with\n  Diversity-Based Context Blending",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with\n  Diversity-Based Context Blending"
                },
                "summary": "Many multi-agent reinforcement learning (MARL) algorithms are trained in\nfixed simulation environments, making them brittle when deployed in real-world\nscenarios with more complex and uncertain conditions. Contextual MARL (cMARL)\naddresses this by parameterizing environments with context variables and\ntraining a context-agnostic policy that performs well across all environment\nconfigurations. Existing cMARL methods attempt to use curriculum learning to\nhelp train and evaluate context-agnostic policies, but they often rely on\nunreliable proxy signals, such as value estimates or generalized advantage\nestimates that are noisy and unstable in multi-agent settings due to\ninter-agent dynamics and partial observability. To address these issues, we\npropose Contextual Multi-Agent LLM-Guided Curriculum Learning with\nDiversity-Based Context Blending (cMALC-D), a framework that uses Large\nLanguage Models (LLMs) to generate semantically meaningful curricula and\nprovide a more robust evaluation signal. To prevent mode collapse and encourage\nexploration, we introduce a novel diversity-based context blending mechanism\nthat creates new training scenarios by combining features from prior contexts.\nExperiments in traffic signal control domains demonstrate that cMALC-D\nsignificantly improves both generalization and sample efficiency compared to\nexisting curriculum learning baselines. We provide code at\nhttps://github.com/DaRL-LibSignal/cMALC-D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many multi-agent reinforcement learning (MARL) algorithms are trained in\nfixed simulation environments, making them brittle when deployed in real-world\nscenarios with more complex and uncertain conditions. Contextual MARL (cMARL)\naddresses this by parameterizing environments with context variables and\ntraining a context-agnostic policy that performs well across all environment\nconfigurations. Existing cMARL methods attempt to use curriculum learning to\nhelp train and evaluate context-agnostic policies, but they often rely on\nunreliable proxy signals, such as value estimates or generalized advantage\nestimates that are noisy and unstable in multi-agent settings due to\ninter-agent dynamics and partial observability. To address these issues, we\npropose Contextual Multi-Agent LLM-Guided Curriculum Learning with\nDiversity-Based Context Blending (cMALC-D), a framework that uses Large\nLanguage Models (LLMs) to generate semantically meaningful curricula and\nprovide a more robust evaluation signal. To prevent mode collapse and encourage\nexploration, we introduce a novel diversity-based context blending mechanism\nthat creates new training scenarios by combining features from prior contexts.\nExperiments in traffic signal control domains demonstrate that cMALC-D\nsignificantly improves both generalization and sample efficiency compared to\nexisting curriculum learning baselines. We provide code at\nhttps://github.com/DaRL-LibSignal/cMALC-D."
                },
                "authors": [
                    {
                        "name": "Anirudh Satheesh"
                    },
                    {
                        "name": "Keenan Powell"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "A shorter version has been accepted to the 2025 Conference on\n  Information and Knowledge Management",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15544v2",
                "updated": "2025-08-28T14:14:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    14,
                    14,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-21T13:22:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    22,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "Lightweight Gradient Descent Optimization for Mitigating Hardware\n  Imperfections in RIS Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Gradient Descent Optimization for Mitigating Hardware\n  Imperfections in RIS Systems"
                },
                "summary": "Ongoing discussions about the future of wireless communications are reaching\na turning point as standardization activities for the sixth generation of\nmobile networks (6G) become more mature. New technologies must now face renewed\nscrutiny by the industry and academia in order to be ready for deployment in\nthe near future. Recently, reconfigurable intelligent surfaces (RISs) gained\nattention as a promising solution for improving the propagation conditions of\nsignal transmission in general. The RIS is a planar array of tunable resonant\nelements designed to dynamically and precisely manipulate the reflection of\nincident electromagnetic waves. However, the physical structure of the RIS and\nits components may be subject to practical limitations and imperfections. It is\nimperative that the hardware imperfections (HWIs) associated with the RIS be\nanalyzed, so that it remains a feasible technology from a practical standpoint.\nMoreover, solutions for mitigating the HWIs must be considered, as is discussed\nin this work. More specifically, we introduce a gradient descent optimization\nfor mitigating HWIs in RIS-aided wideband communication systems. Numerical\nresults show that the proposed optimization is able to compensate for HWIs such\nas the phase-shift noise (PSN) and RIS surface deformations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ongoing discussions about the future of wireless communications are reaching\na turning point as standardization activities for the sixth generation of\nmobile networks (6G) become more mature. New technologies must now face renewed\nscrutiny by the industry and academia in order to be ready for deployment in\nthe near future. Recently, reconfigurable intelligent surfaces (RISs) gained\nattention as a promising solution for improving the propagation conditions of\nsignal transmission in general. The RIS is a planar array of tunable resonant\nelements designed to dynamically and precisely manipulate the reflection of\nincident electromagnetic waves. However, the physical structure of the RIS and\nits components may be subject to practical limitations and imperfections. It is\nimperative that the hardware imperfections (HWIs) associated with the RIS be\nanalyzed, so that it remains a feasible technology from a practical standpoint.\nMoreover, solutions for mitigating the HWIs must be considered, as is discussed\nin this work. More specifically, we introduce a gradient descent optimization\nfor mitigating HWIs in RIS-aided wideband communication systems. Numerical\nresults show that the proposed optimization is able to compensate for HWIs such\nas the phase-shift noise (PSN) and RIS surface deformations."
                },
                "authors": [
                    {
                        "name": "Pedro H. C. de Souza"
                    },
                    {
                        "name": "Luiz A. M. Pereira"
                    },
                    {
                        "name": "Faustino R. G√≥mez"
                    },
                    {
                        "name": "Elsa M. Mater√≥n"
                    },
                    {
                        "name": "Jorge Ricardo Mej√≠a-Salazar"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Ricardo Mej√≠a-Salazar"
                },
                "arxiv_affiliation": "National Institute of Telecommunications",
                "author": "Jorge Ricardo Mej√≠a-Salazar",
                "arxiv_comment": "This work has been submitted to the IEEE Access for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20810v1",
                "updated": "2025-08-28T14:10:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    10,
                    59,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:10:59Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    10,
                    59,
                    3,
                    240,
                    0
                ],
                "title": "A Graph-Based Test-Harness for LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-Based Test-Harness for LLM Evaluation"
                },
                "summary": "We present a first known prototype of a dynamic, systematic benchmark of\nmedical guidelines for 400+ questions, with 3.3+ trillion possible\ncombinations, covering 100\\% of guideline relationships. We transformed the WHO\nIMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,\ntreatments, follow-ups, severities) and 300+ edges, then used graph traversal\nto generate questions that incorporated age-specific scenarios and contextual\ndistractors to ensure clinical relevance. Our graph-based approach enables\nsystematic evaluation across clinical tasks (45-67\\% accuracy), and we find\nmodels excel at symptom recognition but struggle with triaging severity,\ntreatment protocols and follow-up care, demonstrating how customized benchmarks\ncan identify specific capability gaps that general-domain evaluations miss.\nBeyond evaluation, this dynamic MCQA methodology enhances LLM post-training\n(supervised finetuning, GRPO, DPO), where correct answers provide high-reward\nsamples without expensive human annotation. The graph-based approach\nsuccessfully addresses the coverage limitations of manually curated benchmarks.\nThis methodology is a step toward scalable, contamination-resistant solution\nfor creating comprehensive benchmarks that can be dynamically generated,\nincluding when the guidelines are updated. Code and datasets are available at\nhttps://github.com/jessicalundin/graph_testing_harness",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a first known prototype of a dynamic, systematic benchmark of\nmedical guidelines for 400+ questions, with 3.3+ trillion possible\ncombinations, covering 100\\% of guideline relationships. We transformed the WHO\nIMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,\ntreatments, follow-ups, severities) and 300+ edges, then used graph traversal\nto generate questions that incorporated age-specific scenarios and contextual\ndistractors to ensure clinical relevance. Our graph-based approach enables\nsystematic evaluation across clinical tasks (45-67\\% accuracy), and we find\nmodels excel at symptom recognition but struggle with triaging severity,\ntreatment protocols and follow-up care, demonstrating how customized benchmarks\ncan identify specific capability gaps that general-domain evaluations miss.\nBeyond evaluation, this dynamic MCQA methodology enhances LLM post-training\n(supervised finetuning, GRPO, DPO), where correct answers provide high-reward\nsamples without expensive human annotation. The graph-based approach\nsuccessfully addresses the coverage limitations of manually curated benchmarks.\nThis methodology is a step toward scalable, contamination-resistant solution\nfor creating comprehensive benchmarks that can be dynamically generated,\nincluding when the guidelines are updated. Code and datasets are available at\nhttps://github.com/jessicalundin/graph_testing_harness"
                },
                "authors": [
                    {
                        "name": "Jessica Lundin"
                    },
                    {
                        "name": "Guillaume Chabot-Couture"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Chabot-Couture"
                },
                "author": "Guillaume Chabot-Couture",
                "arxiv_comment": "4 pages, 2 figures, dataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08846v2",
                "updated": "2025-08-28T14:07:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    7,
                    41,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-12T11:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    9,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "Steering Towards Fairness: Mitigating Political Bias in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Towards Fairness: Mitigating Political Bias in LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases along political\nand economic dimensions. In this paper, we employ a framework for probing and\nmitigating such biases in decoder-based LLMs through analysis of internal model\nrepresentations. Grounded in the Political Compass Test (PCT), this method uses\ncontrastive pairs to extract and compare hidden layer activations from models\nlike Mistral and DeepSeek. We introduce a comprehensive activation extraction\npipeline capable of layer-wise analysis across multiple ideological axes,\nrevealing meaningful disparities linked to political framing. Our results show\nthat decoder LLMs systematically encode representational bias across layers,\nwhich can be leveraged for effective steering vector-based mitigation. This\nwork provides new insights into how political bias is encoded in LLMs and\noffers a principled approach to debiasing beyond surface-level output\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases along political\nand economic dimensions. In this paper, we employ a framework for probing and\nmitigating such biases in decoder-based LLMs through analysis of internal model\nrepresentations. Grounded in the Political Compass Test (PCT), this method uses\ncontrastive pairs to extract and compare hidden layer activations from models\nlike Mistral and DeepSeek. We introduce a comprehensive activation extraction\npipeline capable of layer-wise analysis across multiple ideological axes,\nrevealing meaningful disparities linked to political framing. Our results show\nthat decoder LLMs systematically encode representational bias across layers,\nwhich can be leveraged for effective steering vector-based mitigation. This\nwork provides new insights into how political bias is encoded in LLMs and\noffers a principled approach to debiasing beyond surface-level output\ninterventions."
                },
                "authors": [
                    {
                        "name": "Afrozah Nadeem"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Accepted at CASE@RANLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20805v1",
                "updated": "2025-08-28T14:07:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    7,
                    7,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:07:07Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    7,
                    7,
                    3,
                    240,
                    0
                ],
                "title": "Exploring Machine Learning and Language Models for Multimodal Depression\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Machine Learning and Language Models for Multimodal Depression\n  Detection"
                },
                "summary": "This paper presents our approach to the first Multimodal Personality-Aware\nDepression Detection Challenge, focusing on multimodal depression detection\nusing machine learning and deep learning models. We explore and compare the\nperformance of XGBoost, transformer-based architectures, and large language\nmodels (LLMs) on audio, video, and text features. Our results highlight the\nstrengths and limitations of each type of model in capturing depression-related\nsignals across modalities, offering insights into effective multimodal\nrepresentation strategies for mental health prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our approach to the first Multimodal Personality-Aware\nDepression Detection Challenge, focusing on multimodal depression detection\nusing machine learning and deep learning models. We explore and compare the\nperformance of XGBoost, transformer-based architectures, and large language\nmodels (LLMs) on audio, video, and text features. Our results highlight the\nstrengths and limitations of each type of model in capturing depression-related\nsignals across modalities, offering insights into effective multimodal\nrepresentation strategies for mental health prediction."
                },
                "authors": [
                    {
                        "name": "Javier Si Zhao Hong"
                    },
                    {
                        "name": "Timothy Zoe Delaya"
                    },
                    {
                        "name": "Sherwyn Chan Yin Kit"
                    },
                    {
                        "name": "Pai Chet Ng"
                    },
                    {
                        "name": "Xiaoxiao Miao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxiao Miao"
                },
                "author": "Xiaoxiao Miao",
                "arxiv_comment": "This paper has been accepted by APCIPA ASC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20800v1",
                "updated": "2025-08-28T14:01:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    1,
                    54,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:01:54Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    1,
                    54,
                    3,
                    240,
                    0
                ],
                "title": "EPICS for Small-Scale Laboratories with Python Soft IOCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPICS for Small-Scale Laboratories with Python Soft IOCs"
                },
                "summary": "While the Experimental Physics and Industrial Control System (EPICS) is\nwidely used at large laboratories for slow controls and instrumentation, the\ndeployment of a full EPICS installation can be difficult, with a steep learning\ncurve to new users. Taking advantage of the pythonSoftIOC module, we developed\nan EPICS slow controls implementation for Jefferson Lab's Hall B cryotarget\nwritten entirely in Python and based on software IOCs that communicate with\ninstruments over Ethernet. This system ran successfully, interfacing with\nJefferson Lab's full EPICS network, and we offer it as an example of the\ncapabilities of pythonSoftIOC to build lightweight, yet robust and flexible\ninstrumentation platforms that would be easily adapted for use at a small-scale\nlaboratory. University groups can use these examples to build complete slow\ncontrols systems, from device communication to data archiving and display,\nusing open-source, mature EPICS tools and student-friendly Python as an\nalternative to expensive and proprietary systems such as LabView.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the Experimental Physics and Industrial Control System (EPICS) is\nwidely used at large laboratories for slow controls and instrumentation, the\ndeployment of a full EPICS installation can be difficult, with a steep learning\ncurve to new users. Taking advantage of the pythonSoftIOC module, we developed\nan EPICS slow controls implementation for Jefferson Lab's Hall B cryotarget\nwritten entirely in Python and based on software IOCs that communicate with\ninstruments over Ethernet. This system ran successfully, interfacing with\nJefferson Lab's full EPICS network, and we offer it as an example of the\ncapabilities of pythonSoftIOC to build lightweight, yet robust and flexible\ninstrumentation platforms that would be easily adapted for use at a small-scale\nlaboratory. University groups can use these examples to build complete slow\ncontrols systems, from device communication to data archiving and display,\nusing open-source, mature EPICS tools and student-friendly Python as an\nalternative to expensive and proprietary systems such as LabView."
                },
                "authors": [
                    {
                        "name": "James D. Maxwell"
                    }
                ],
                "author_detail": {
                    "name": "James D. Maxwell"
                },
                "author": "James D. Maxwell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14330v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14330v3",
                "updated": "2025-08-28T13:50:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    50,
                    56,
                    3,
                    240,
                    0
                ],
                "published": "2025-07-18T19:15:50Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    19,
                    15,
                    50,
                    4,
                    199,
                    0
                ],
                "title": "Leveraging LLMs for Formal Software Requirements -- Challenges and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Formal Software Requirements -- Challenges and\n  Prospects"
                },
                "summary": "Software correctness is ensured mathematically through formal verification,\nwhich involves the resources of generating formal requirement specifications\nand having an implementation that must be verified. Tools such as\nmodel-checkers and theorem provers ensure software correctness by verifying the\nimplementation against the specification. Formal methods deployment is\nregularly enforced in the development of safety-critical systems e.g.\naerospace, medical devices and autonomous systems. Generating these\nspecifications from informal and ambiguous natural language requirements\nremains the key challenge. Our project, VERIFAI^{1}, aims to investigate\nautomated and semi-automated approaches to bridge this gap, using techniques\nfrom Natural Language Processing (NLP), ontology-based domain modelling,\nartefact reuse, and large language models (LLMs). This position paper presents\na preliminary synthesis of relevant literature to identify recurring challenges\nand prospective research directions in the generation of verifiable\nspecifications from informal requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software correctness is ensured mathematically through formal verification,\nwhich involves the resources of generating formal requirement specifications\nand having an implementation that must be verified. Tools such as\nmodel-checkers and theorem provers ensure software correctness by verifying the\nimplementation against the specification. Formal methods deployment is\nregularly enforced in the development of safety-critical systems e.g.\naerospace, medical devices and autonomous systems. Generating these\nspecifications from informal and ambiguous natural language requirements\nremains the key challenge. Our project, VERIFAI^{1}, aims to investigate\nautomated and semi-automated approaches to bridge this gap, using techniques\nfrom Natural Language Processing (NLP), ontology-based domain modelling,\nartefact reuse, and large language models (LLMs). This position paper presents\na preliminary synthesis of relevant literature to identify recurring challenges\nand prospective research directions in the generation of verifiable\nspecifications from informal requirements."
                },
                "authors": [
                    {
                        "name": "Arshad Beg"
                    },
                    {
                        "name": "Diarmuid O'Donoghue"
                    },
                    {
                        "name": "Rosemary Monahan"
                    }
                ],
                "author_detail": {
                    "name": "Rosemary Monahan"
                },
                "author": "Rosemary Monahan",
                "arxiv_comment": "Overlay2025 - 7th International Workshop on Artificial Intelligence\n  and fOrmal VERification, Logic, Automata, and sYnthesis. [Accepted]. To be\n  held on 26th of October, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14330v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14330v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20766v1",
                "updated": "2025-08-28T13:22:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    22,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:22:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    22,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "Turning the Spell Around: Lightweight Alignment Amplification via\n  Rank-One Safety Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning the Spell Around: Lightweight Alignment Amplification via\n  Rank-One Safety Injection"
                },
                "summary": "Safety alignment in Large Language Models (LLMs) often involves mediating\ninternal representations to refuse harmful requests. Recent research has\ndemonstrated that these safety mechanisms can be bypassed by ablating or\nremoving specific representational directions within the model. In this paper,\nwe propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box\nmethod that amplifies a model's safety alignment by permanently steering its\nactivations toward the refusal-mediating subspace. ROSI operates as a simple,\nfine-tuning-free rank-one weight modification applied to all residual stream\nwrite matrices. The required safety direction can be computed from a small set\nof harmful and harmless instruction pairs. We show that ROSI consistently\nincreases safety refusal rates - as evaluated by Llama Guard 3 - while\npreserving the utility of the model on standard benchmarks such as MMLU,\nHellaSwag, and Arc. Furthermore, we show that ROSI can also re-align\n'uncensored' models by amplifying their own latent safety directions,\ndemonstrating its utility as an effective last-mile safety procedure. Our\nresults suggest that targeted, interpretable weight steering is a cheap and\npotent mechanism to improve LLM safety, complementing more resource-intensive\nfine-tuning paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment in Large Language Models (LLMs) often involves mediating\ninternal representations to refuse harmful requests. Recent research has\ndemonstrated that these safety mechanisms can be bypassed by ablating or\nremoving specific representational directions within the model. In this paper,\nwe propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box\nmethod that amplifies a model's safety alignment by permanently steering its\nactivations toward the refusal-mediating subspace. ROSI operates as a simple,\nfine-tuning-free rank-one weight modification applied to all residual stream\nwrite matrices. The required safety direction can be computed from a small set\nof harmful and harmless instruction pairs. We show that ROSI consistently\nincreases safety refusal rates - as evaluated by Llama Guard 3 - while\npreserving the utility of the model on standard benchmarks such as MMLU,\nHellaSwag, and Arc. Furthermore, we show that ROSI can also re-align\n'uncensored' models by amplifying their own latent safety directions,\ndemonstrating its utility as an effective last-mile safety procedure. Our\nresults suggest that targeted, interpretable weight steering is a cheap and\npotent mechanism to improve LLM safety, complementing more resource-intensive\nfine-tuning paradigms."
                },
                "authors": [
                    {
                        "name": "Harethah Abu Shairah"
                    },
                    {
                        "name": "Hasan Abed Al Kader Hammoud"
                    },
                    {
                        "name": "George Turkiyyah"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20764v1",
                "updated": "2025-08-28T13:19:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    19,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:19:31Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    19,
                    31,
                    3,
                    240,
                    0
                ],
                "title": "Feel the Difference? A Comparative Analysis of Emotional Arcs in Real\n  and LLM-Generated CBT Sessions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feel the Difference? A Comparative Analysis of Emotional Arcs in Real\n  and LLM-Generated CBT Sessions"
                },
                "summary": "Synthetic therapy dialogues generated by large language models (LLMs) are\nincreasingly used in mental health NLP to simulate counseling scenarios, train\nmodels, and supplement limited real-world data. However, it remains unclear\nwhether these synthetic conversations capture the nuanced emotional dynamics of\nreal therapy. In this work, we conduct the first comparative analysis of\nemotional arcs between real and LLM-generated Cognitive Behavioral Therapy\ndialogues. We adapt the Utterance Emotion Dynamics framework to analyze\nfine-grained affective trajectories across valence, arousal, and dominance\ndimensions. Our analysis spans both full dialogues and individual speaker roles\n(counselor and client), using real sessions transcribed from public videos and\nsynthetic dialogues from the CACTUS dataset. We find that while synthetic\ndialogues are fluent and structurally coherent, they diverge from real\nconversations in key emotional properties: real sessions exhibit greater\nemotional variability,more emotion-laden language, and more authentic patterns\nof reactivity and regulation. Moreover, emotional arc similarity between real\nand synthetic speakers is low, especially for clients. These findings\nunderscore the limitations of current LLM-generated therapy data and highlight\nthe importance of emotional fidelity in mental health applications. We\nintroduce RealCBT, a curated dataset of real CBT sessions, to support future\nresearch in this space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic therapy dialogues generated by large language models (LLMs) are\nincreasingly used in mental health NLP to simulate counseling scenarios, train\nmodels, and supplement limited real-world data. However, it remains unclear\nwhether these synthetic conversations capture the nuanced emotional dynamics of\nreal therapy. In this work, we conduct the first comparative analysis of\nemotional arcs between real and LLM-generated Cognitive Behavioral Therapy\ndialogues. We adapt the Utterance Emotion Dynamics framework to analyze\nfine-grained affective trajectories across valence, arousal, and dominance\ndimensions. Our analysis spans both full dialogues and individual speaker roles\n(counselor and client), using real sessions transcribed from public videos and\nsynthetic dialogues from the CACTUS dataset. We find that while synthetic\ndialogues are fluent and structurally coherent, they diverge from real\nconversations in key emotional properties: real sessions exhibit greater\nemotional variability,more emotion-laden language, and more authentic patterns\nof reactivity and regulation. Moreover, emotional arc similarity between real\nand synthetic speakers is low, especially for clients. These findings\nunderscore the limitations of current LLM-generated therapy data and highlight\nthe importance of emotional fidelity in mental health applications. We\nintroduce RealCBT, a curated dataset of real CBT sessions, to support future\nresearch in this space."
                },
                "authors": [
                    {
                        "name": "Xiaoyi Wang"
                    },
                    {
                        "name": "Jiwei Zhang"
                    },
                    {
                        "name": "Guangtao Zhang"
                    },
                    {
                        "name": "Honglei Guo"
                    }
                ],
                "author_detail": {
                    "name": "Honglei Guo"
                },
                "author": "Honglei Guo",
                "arxiv_comment": "Accepted at EMNLP 2025,14 page,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20760v1",
                "updated": "2025-08-28T13:16:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    16,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:16:55Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    16,
                    55,
                    3,
                    240,
                    0
                ],
                "title": "Occlusion Robustness of CLIP for Military Vehicle Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occlusion Robustness of CLIP for Military Vehicle Classification"
                },
                "summary": "Vision-language models (VLMs) like CLIP enable zero-shot classification by\naligning images and text in a shared embedding space, offering advantages for\ndefense applications with scarce labeled data. However, CLIP's robustness in\nchallenging military environments, with partial occlusion and degraded\nsignal-to-noise ratio (SNR), remains underexplored. We investigate CLIP\nvariants' robustness to occlusion using a custom dataset of 18 military vehicle\nclasses and evaluate using Normalized Area Under the Curve (NAUC) across\nocclusion percentages. Four key insights emerge: (1) Transformer-based CLIP\nmodels consistently outperform CNNs, (2) fine-grained, dispersed occlusions\ndegrade performance more than larger contiguous occlusions, (3) despite\nimproved accuracy, performance of linear-probed models sharply drops at around\n35% occlusion, (4) by finetuning the model's backbone, this performance drop\noccurs at more than 60% occlusion. These results underscore the importance of\nocclusion-specific augmentations during training and the need for further\nexploration into patch-level sensitivity and architectural resilience for\nreal-world deployment of CLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) like CLIP enable zero-shot classification by\naligning images and text in a shared embedding space, offering advantages for\ndefense applications with scarce labeled data. However, CLIP's robustness in\nchallenging military environments, with partial occlusion and degraded\nsignal-to-noise ratio (SNR), remains underexplored. We investigate CLIP\nvariants' robustness to occlusion using a custom dataset of 18 military vehicle\nclasses and evaluate using Normalized Area Under the Curve (NAUC) across\nocclusion percentages. Four key insights emerge: (1) Transformer-based CLIP\nmodels consistently outperform CNNs, (2) fine-grained, dispersed occlusions\ndegrade performance more than larger contiguous occlusions, (3) despite\nimproved accuracy, performance of linear-probed models sharply drops at around\n35% occlusion, (4) by finetuning the model's backbone, this performance drop\noccurs at more than 60% occlusion. These results underscore the importance of\nocclusion-specific augmentations during training and the need for further\nexploration into patch-level sensitivity and architectural resilience for\nreal-world deployment of CLIP."
                },
                "authors": [
                    {
                        "name": "Jan Erik van Woerden"
                    },
                    {
                        "name": "Gertjan Burghouts"
                    },
                    {
                        "name": "Lotte Nijskens"
                    },
                    {
                        "name": "Alma M. Liezenga"
                    },
                    {
                        "name": "Sabina van Rooij"
                    },
                    {
                        "name": "Frank Ruis"
                    },
                    {
                        "name": "Hugo J. Kuijf"
                    }
                ],
                "author_detail": {
                    "name": "Hugo J. Kuijf"
                },
                "author": "Hugo J. Kuijf",
                "arxiv_comment": "To be presented at SPIE: Sensors + Imaging, Artificial Intelligence\n  for Security and Defence Applications II",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20757v1",
                "updated": "2025-08-28T13:14:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    14,
                    20,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:14:20Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    14,
                    20,
                    3,
                    240,
                    0
                ],
                "title": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and\n  Efficient Open-Ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and\n  Efficient Open-Ended Text Generation"
                },
                "summary": "Open-ended text generation faces a critical challenge: balancing coherence\nwith diversity in LLM outputs. While contrastive search-based decoding\nstrategies have emerged to address this trade-off, their practical utility is\noften limited by hyperparameter dependence and high computational costs. We\nintroduce GUARD, a self-adaptive decoding method that effectively balances\nthese competing objectives through a novel \"Glocal\" uncertainty-driven\nframework. GUARD combines global entropy estimates with local entropy\ndeviations to integrate both long-term and short-term uncertainty signals. We\ndemonstrate that our proposed global entropy formulation effectively mitigates\nabrupt variations in uncertainty, such as sudden overconfidence or high entropy\nspikes, and provides theoretical guarantees of unbiasedness and consistency. To\nreduce computational overhead, we incorporate a simple yet effective\ntoken-count-based penalty into GUARD. Experimental results demonstrate that\nGUARD achieves a good balance between text diversity and coherence, while\nexhibiting substantial improvements in generation speed. In a more nuanced\ncomparison study across different dimensions of text quality, both human and\nLLM evaluators validated its remarkable performance. Our code is available at\nhttps://github.com/YecanLee/GUARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended text generation faces a critical challenge: balancing coherence\nwith diversity in LLM outputs. While contrastive search-based decoding\nstrategies have emerged to address this trade-off, their practical utility is\noften limited by hyperparameter dependence and high computational costs. We\nintroduce GUARD, a self-adaptive decoding method that effectively balances\nthese competing objectives through a novel \"Glocal\" uncertainty-driven\nframework. GUARD combines global entropy estimates with local entropy\ndeviations to integrate both long-term and short-term uncertainty signals. We\ndemonstrate that our proposed global entropy formulation effectively mitigates\nabrupt variations in uncertainty, such as sudden overconfidence or high entropy\nspikes, and provides theoretical guarantees of unbiasedness and consistency. To\nreduce computational overhead, we incorporate a simple yet effective\ntoken-count-based penalty into GUARD. Experimental results demonstrate that\nGUARD achieves a good balance between text diversity and coherence, while\nexhibiting substantial improvements in generation speed. In a more nuanced\ncomparison study across different dimensions of text quality, both human and\nLLM evaluators validated its remarkable performance. Our code is available at\nhttps://github.com/YecanLee/GUARD."
                },
                "authors": [
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Esteban Garces Arias"
                    },
                    {
                        "name": "Meimingwei Li"
                    },
                    {
                        "name": "Julian Rodemann"
                    },
                    {
                        "name": "Matthias A√üenmacher"
                    },
                    {
                        "name": "Danlu Chen"
                    },
                    {
                        "name": "Gaojuan Fan"
                    },
                    {
                        "name": "Christian Heumann"
                    },
                    {
                        "name": "Chongsheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chongsheng Zhang"
                },
                "author": "Chongsheng Zhang",
                "arxiv_comment": "Accepted at Findings of the Association for Computational\n  Linguistics: EMNLP (Findings) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20750v1",
                "updated": "2025-08-28T13:08:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    8,
                    57,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:08:57Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    8,
                    57,
                    3,
                    240,
                    0
                ],
                "title": "Specializing General-purpose LLM Embeddings for Implicit Hate Speech\n  Detection across Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specializing General-purpose LLM Embeddings for Implicit Hate Speech\n  Detection across Datasets"
                },
                "summary": "Implicit hate speech (IHS) is indirect language that conveys prejudice or\nhatred through subtle cues, sarcasm or coded terminology. IHS is challenging to\ndetect as it does not include explicit derogatory or inflammatory words. To\naddress this challenge, task-specific pipelines can be complemented with\nexternal knowledge or additional information such as context, emotions and\nsentiment data. In this paper, we show that, by solely fine-tuning recent\ngeneral-purpose embedding models based on large language models (LLMs), such as\nStella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.\nExperiments on multiple IHS datasets show up to 1.10 percentage points\nimprovements for in-dataset, and up to 20.35 percentage points improvements in\ncross-dataset evaluation, in terms of F1-macro score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit hate speech (IHS) is indirect language that conveys prejudice or\nhatred through subtle cues, sarcasm or coded terminology. IHS is challenging to\ndetect as it does not include explicit derogatory or inflammatory words. To\naddress this challenge, task-specific pipelines can be complemented with\nexternal knowledge or additional information such as context, emotions and\nsentiment data. In this paper, we show that, by solely fine-tuning recent\ngeneral-purpose embedding models based on large language models (LLMs), such as\nStella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.\nExperiments on multiple IHS datasets show up to 1.10 percentage points\nimprovements for in-dataset, and up to 20.35 percentage points improvements in\ncross-dataset evaluation, in terms of F1-macro score."
                },
                "authors": [
                    {
                        "name": "Vassiliy Cheremetiev"
                    },
                    {
                        "name": "Quang Long Ho Ngo"
                    },
                    {
                        "name": "Chau Ying Kot"
                    },
                    {
                        "name": "Alina Elena Baia"
                    },
                    {
                        "name": "Andrea Cavallaro"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Cavallaro"
                },
                "author": "Andrea Cavallaro",
                "arxiv_doi": "10.1145/3746275.3762209",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746275.3762209",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper accepted at the DHOW Workshop at ACM Multimedia 2025. Code\n  available at https://github.com/idiap/implicit-hsd",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04758v2",
                "updated": "2025-08-28T13:05:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    5,
                    48,
                    3,
                    240,
                    0
                ],
                "published": "2025-04-07T06:11:39Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    11,
                    39,
                    0,
                    97,
                    0
                ],
                "title": "Feature Importance-Aware Deep Joint Source-Channel Coding for\n  Computationally Efficient and Adjustable Image Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Importance-Aware Deep Joint Source-Channel Coding for\n  Computationally Efficient and Adjustable Image Transmission"
                },
                "summary": "Recent advances in deep learning-based joint source-channel coding (deepJSCC)\nhave substantially improved communication performance, but their high\ncomputational cost hinders practical deployment. Moreover, certain applications\nrequire the ability to dynamically adapt computational complexity. To address\nthese issues, we propose a Feature Importance-Aware deepJSCC (FAJSCC) model for\nimage transmission that is both computationally efficient and adjustable.\nFAJSCC employs axis-dimension specialized computation, which performs efficient\noperations individually for each spatial and channel axis, significantly\nreducing computational cost while representing features effectively. It further\nincorporates selective deformable self-attention, which applies self-attention\nonly to selected and adaptively adjusted regions, leveraging the importance and\nrelations of input features to efficiently capture complex feature\ncorrelations. Another key feature of FAJSCC is that the number of selected\nimportant areas can be controlled separately by the encoder and the decoder,\ndepending on the available computational budget. It makes FAJSCC the first\ndeepJSCC architecture to allow independent adjustment of encoder and decoder\ncomplexity within a single trained model. Experimental results show that FAJSCC\nachieves superior image transmission performance under various channel\nconditions while requiring less computational complexity than recent\nstate-of-the-art models. Furthermore, experiments independently varying the\nencoder and decoder's computational resources reveal, for the first time in the\ndeepJSCC literature, that understanding the meaning of noisy features in the\ndecoder demands the greatest computational cost. The code is publicly available\nat github.com/hansung-choi/FAJSCCv2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning-based joint source-channel coding (deepJSCC)\nhave substantially improved communication performance, but their high\ncomputational cost hinders practical deployment. Moreover, certain applications\nrequire the ability to dynamically adapt computational complexity. To address\nthese issues, we propose a Feature Importance-Aware deepJSCC (FAJSCC) model for\nimage transmission that is both computationally efficient and adjustable.\nFAJSCC employs axis-dimension specialized computation, which performs efficient\noperations individually for each spatial and channel axis, significantly\nreducing computational cost while representing features effectively. It further\nincorporates selective deformable self-attention, which applies self-attention\nonly to selected and adaptively adjusted regions, leveraging the importance and\nrelations of input features to efficiently capture complex feature\ncorrelations. Another key feature of FAJSCC is that the number of selected\nimportant areas can be controlled separately by the encoder and the decoder,\ndepending on the available computational budget. It makes FAJSCC the first\ndeepJSCC architecture to allow independent adjustment of encoder and decoder\ncomplexity within a single trained model. Experimental results show that FAJSCC\nachieves superior image transmission performance under various channel\nconditions while requiring less computational complexity than recent\nstate-of-the-art models. Furthermore, experiments independently varying the\nencoder and decoder's computational resources reveal, for the first time in the\ndeepJSCC literature, that understanding the meaning of noisy features in the\ndecoder demands the greatest computational cost. The code is publicly available\nat github.com/hansung-choi/FAJSCCv2."
                },
                "authors": [
                    {
                        "name": "Hansung Choi"
                    },
                    {
                        "name": "Daewon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Daewon Seo"
                },
                "author": "Daewon Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20744v1",
                "updated": "2025-08-28T13:04:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    4,
                    34,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:04:34Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    4,
                    34,
                    3,
                    240,
                    0
                ],
                "title": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of\n  LLM-Generated Behavioural Specifications from Food-Safety Regulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of\n  LLM-Generated Behavioural Specifications from Food-Safety Regulations"
                },
                "summary": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation."
                },
                "authors": [
                    {
                        "name": "Shabnam Hassani"
                    },
                    {
                        "name": "Mehrdad Sabetzadeh"
                    },
                    {
                        "name": "Daniel Amyot"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Amyot"
                },
                "author": "Daniel Amyot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20737v1",
                "updated": "2025-08-28T13:00:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    0,
                    28,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T13:00:28Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    13,
                    0,
                    28,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges,\n  and a Lightweight Interaction Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Testing for LLM Applications: Characteristics, Challenges,\n  and a Lightweight Interaction Protocol"
                },
                "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework."
                },
                "authors": [
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Yixiao Yang"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Shi Ying"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Junjie Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Linxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Linxiao Jiang"
                },
                "author": "Linxiao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20736v1",
                "updated": "2025-08-28T12:59:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    59,
                    1,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:59:01Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    59,
                    1,
                    3,
                    240,
                    0
                ],
                "title": "Leveraging Semantic Triples for Private Document Generation with Local\n  Differential Privacy Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Triples for Private Document Generation with Local\n  Differential Privacy Guarantees"
                },
                "summary": "Many works at the intersection of Differential Privacy (DP) in Natural\nLanguage Processing aim to protect privacy by transforming texts under DP\nguarantees. This can be performed in a variety of ways, from word perturbations\nto full document rewriting, and most often under local DP. Here, an input text\nmust be made indistinguishable from any other potential text, within some bound\ngoverned by the privacy parameter $\\varepsilon$. Such a guarantee is quite\ndemanding, and recent works show that privatizing texts under local DP can only\nbe done reasonably under very high $\\varepsilon$ values. Addressing this\nchallenge, we introduce DP-ST, which leverages semantic triples for\nneighborhood-aware private document generation under local DP guarantees.\nThrough the evaluation of our method, we demonstrate the effectiveness of the\ndivide-and-conquer paradigm, particularly when limiting the DP notion (and\nprivacy guarantees) to that of a privatization neighborhood. When combined with\nLLM post-processing, our method allows for coherent text generation even at\nlower $\\varepsilon$ values, while still balancing privacy and utility. These\nfindings highlight the importance of coherence in achieving balanced\nprivatization outputs at reasonable $\\varepsilon$ levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many works at the intersection of Differential Privacy (DP) in Natural\nLanguage Processing aim to protect privacy by transforming texts under DP\nguarantees. This can be performed in a variety of ways, from word perturbations\nto full document rewriting, and most often under local DP. Here, an input text\nmust be made indistinguishable from any other potential text, within some bound\ngoverned by the privacy parameter $\\varepsilon$. Such a guarantee is quite\ndemanding, and recent works show that privatizing texts under local DP can only\nbe done reasonably under very high $\\varepsilon$ values. Addressing this\nchallenge, we introduce DP-ST, which leverages semantic triples for\nneighborhood-aware private document generation under local DP guarantees.\nThrough the evaluation of our method, we demonstrate the effectiveness of the\ndivide-and-conquer paradigm, particularly when limiting the DP notion (and\nprivacy guarantees) to that of a privatization neighborhood. When combined with\nLLM post-processing, our method allows for coherent text generation even at\nlower $\\varepsilon$ values, while still balancing privacy and utility. These\nfindings highlight the importance of coherence in achieving balanced\nprivatization outputs at reasonable $\\varepsilon$ levels."
                },
                "authors": [
                    {
                        "name": "Stephen Meisenbacher"
                    },
                    {
                        "name": "Maulik Chevli"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "17 pages, 2 figures, 11 tables. Accepted to EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20729v1",
                "updated": "2025-08-28T12:50:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    50,
                    48,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:50:48Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    50,
                    48,
                    3,
                    240,
                    0
                ],
                "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and\n  Revision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and\n  Revision"
                },
                "summary": "Large language models (LLMs) serve as an active and promising field of\ngenerative artificial intelligence and have demonstrated abilities to perform\ncomplex tasks in multiple domains, including mathematical and scientific\nreasoning. In this work, we construct a novel agent framework for solving\nrepresentative problems in scientific computing. The proposed agent,\nincorporating a \"rewriting-resolution-review-revision\" logical chain via three\nreasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,\nrespectively), is integrated in a collaborative and interactive manner. The\nConsultant module endows the agent with knowledge transfer capabilities to link\nproblems to professional domain insights, thereby rewriting problem\ndescriptions through text augmentation. The Programmer module is responsible\nfor generating and executing well-structured code to deliver the problem\nresolution. The Reviewer module equips the agent with the capacity for\nself-debugging and self-refinement through interactive feedback with code\nruntime outputs. By leveraging the end-to-end review mechanism, the executable\ncode provided by the Programmer attains the iterative revision. A comprehensive\nevaluation is conducted on the performance of the proposed agent framework in\nsolving PDEs, ill-conditioned linear systems, and data-driven physical analysis\nproblems. Compared to single-model, this collaborative framework significantly\nimproves the bug-free code generation rate and reduces the occurrence of\nnon-physical solutions, thereby establishing a highly reliable framework for\nautonomous code generation based on natural language descriptions. The review\nmechanism improved the average execution success (bug-free code and non-NaN\nsolutions) rate of the latest reasoning models. In summary, our agent framework\nestablishes automatic code generation and review as a promising scientific\ncomputing paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) serve as an active and promising field of\ngenerative artificial intelligence and have demonstrated abilities to perform\ncomplex tasks in multiple domains, including mathematical and scientific\nreasoning. In this work, we construct a novel agent framework for solving\nrepresentative problems in scientific computing. The proposed agent,\nincorporating a \"rewriting-resolution-review-revision\" logical chain via three\nreasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,\nrespectively), is integrated in a collaborative and interactive manner. The\nConsultant module endows the agent with knowledge transfer capabilities to link\nproblems to professional domain insights, thereby rewriting problem\ndescriptions through text augmentation. The Programmer module is responsible\nfor generating and executing well-structured code to deliver the problem\nresolution. The Reviewer module equips the agent with the capacity for\nself-debugging and self-refinement through interactive feedback with code\nruntime outputs. By leveraging the end-to-end review mechanism, the executable\ncode provided by the Programmer attains the iterative revision. A comprehensive\nevaluation is conducted on the performance of the proposed agent framework in\nsolving PDEs, ill-conditioned linear systems, and data-driven physical analysis\nproblems. Compared to single-model, this collaborative framework significantly\nimproves the bug-free code generation rate and reduces the occurrence of\nnon-physical solutions, thereby establishing a highly reliable framework for\nautonomous code generation based on natural language descriptions. The review\nmechanism improved the average execution success (bug-free code and non-NaN\nsolutions) rate of the latest reasoning models. In summary, our agent framework\nestablishes automatic code generation and review as a promising scientific\ncomputing paradigm."
                },
                "authors": [
                    {
                        "name": "Ao Cheng"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Guowei He"
                    }
                ],
                "author_detail": {
                    "name": "Guowei He"
                },
                "author": "Guowei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20717v1",
                "updated": "2025-08-28T12:37:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    37,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:37:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    37,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "Unified Multi-task Learning for Voice-Based Detection of Diverse\n  Clinical Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Multi-task Learning for Voice-Based Detection of Diverse\n  Clinical Conditions"
                },
                "summary": "Voice-based health assessment offers unprecedented opportunities for\nscalable, non-invasive disease screening, yet existing approaches typically\nfocus on single conditions and fail to leverage the rich, multi-faceted\ninformation embedded in speech. We present MARVEL (Multi-task Acoustic\nRepresentations for Voice-based Health Analysis), a privacy-conscious multitask\nlearning framework that simultaneously detects nine distinct neurological,\nrespiratory, and voice disorders using only derived acoustic features,\neliminating the need for raw audio transmission. Our dual-branch architecture\nemploys specialized encoders with task-specific heads sharing a common acoustic\nbackbone, enabling effective cross-condition knowledge transfer. Evaluated on\nthe large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC\nof 0.78, with exceptional performance on neurological disorders (AUROC = 0.89),\nparticularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97).\nOur framework consistently outperforms single-modal baselines by 5-19% and\nsurpasses state-of-the-art self-supervised models on 7 of 9 tasks, while\ncorrelation analysis reveals that the learned representations exhibit\nmeaningful similarities with established acoustic features, indicating that the\nmodel's internal representations are consistent with clinically recognized\nacoustic patterns. By demonstrating that a single unified model can effectively\nscreen for diverse conditions, this work establishes a foundation for\ndeployable voice-based diagnostics in resource-constrained and remote\nhealthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice-based health assessment offers unprecedented opportunities for\nscalable, non-invasive disease screening, yet existing approaches typically\nfocus on single conditions and fail to leverage the rich, multi-faceted\ninformation embedded in speech. We present MARVEL (Multi-task Acoustic\nRepresentations for Voice-based Health Analysis), a privacy-conscious multitask\nlearning framework that simultaneously detects nine distinct neurological,\nrespiratory, and voice disorders using only derived acoustic features,\neliminating the need for raw audio transmission. Our dual-branch architecture\nemploys specialized encoders with task-specific heads sharing a common acoustic\nbackbone, enabling effective cross-condition knowledge transfer. Evaluated on\nthe large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC\nof 0.78, with exceptional performance on neurological disorders (AUROC = 0.89),\nparticularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97).\nOur framework consistently outperforms single-modal baselines by 5-19% and\nsurpasses state-of-the-art self-supervised models on 7 of 9 tasks, while\ncorrelation analysis reveals that the learned representations exhibit\nmeaningful similarities with established acoustic features, indicating that the\nmodel's internal representations are consistent with clinically recognized\nacoustic patterns. By demonstrating that a single unified model can effectively\nscreen for diverse conditions, this work establishes a foundation for\ndeployable voice-based diagnostics in resource-constrained and remote\nhealthcare settings."
                },
                "authors": [
                    {
                        "name": "Ran Piao"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Hareld Kemps"
                    },
                    {
                        "name": "Tong Xia"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20712v1",
                "updated": "2025-08-28T12:30:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    30,
                    32,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:30:32Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    30,
                    32,
                    3,
                    240,
                    0
                ],
                "title": "Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label\n  Hierarchical Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label\n  Hierarchical Learning"
                },
                "summary": "This paper introduces the first multi-lingual and multi-label classification\nmodel for implicit discourse relation recognition (IDRR). Our model, HArch, is\nevaluated on the recently released DiscoGeM 2.0 corpus and leverages\nhierarchical dependencies between discourse senses to predict probability\ndistributions across all three sense levels in the PDTB 3.0 framework. We\ncompare several pre-trained encoder backbones and find that RoBERTa-HArch\nachieves the best performance in English, while XLM-RoBERTa-HArch performs best\nin the multi-lingual setting. In addition, we compare our fine-tuned models\nagainst GPT-4o and Llama-4-Maverick using few-shot prompting across all\nlanguage configurations. Our results show that our fine-tuned models\nconsistently outperform these LLMs, highlighting the advantages of\ntask-specific fine-tuning over prompting in IDRR. Finally, we report SOTA\nresults on the DiscoGeM 1.0 corpus, further validating the effectiveness of our\nhierarchical approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the first multi-lingual and multi-label classification\nmodel for implicit discourse relation recognition (IDRR). Our model, HArch, is\nevaluated on the recently released DiscoGeM 2.0 corpus and leverages\nhierarchical dependencies between discourse senses to predict probability\ndistributions across all three sense levels in the PDTB 3.0 framework. We\ncompare several pre-trained encoder backbones and find that RoBERTa-HArch\nachieves the best performance in English, while XLM-RoBERTa-HArch performs best\nin the multi-lingual setting. In addition, we compare our fine-tuned models\nagainst GPT-4o and Llama-4-Maverick using few-shot prompting across all\nlanguage configurations. Our results show that our fine-tuned models\nconsistently outperform these LLMs, highlighting the advantages of\ntask-specific fine-tuning over prompting in IDRR. Finally, we report SOTA\nresults on the DiscoGeM 1.0 corpus, further validating the effectiveness of our\nhierarchical approach."
                },
                "authors": [
                    {
                        "name": "Nelson Filipe Costa"
                    },
                    {
                        "name": "Leila Kosseim"
                    }
                ],
                "author_detail": {
                    "name": "Leila Kosseim"
                },
                "author": "Leila Kosseim",
                "arxiv_comment": "Published at SIGDIAL 2025. Best paper award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20708v1",
                "updated": "2025-08-28T12:27:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    27,
                    1,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:27:01Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    27,
                    1,
                    3,
                    240,
                    0
                ],
                "title": "What is the Most Efficient Technique for Uplink Cell-Free Massive MIMO?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Most Efficient Technique for Uplink Cell-Free Massive MIMO?"
                },
                "summary": "This paper seeks to determine the most efficient uplink technique for\ncell-free massive MIMO systems. Despite offering great advances, existing works\nsuffer from fragmented methodologies and inconsistent assumptions (e.g.,\nsingle- vs. multi-antenna access points, ideal vs. spatially correlated\nchannels). To address these limitations, we: (1) establish a unified analytical\nframework compatible with centralized/distributed processing and diverse\ncombining schemes; (2) develop a universal optimization strategy for max-min\npower control; and (3) conduct a holistic study among four critical metrics:\nworst-case user spectral efficiency (fairness), system capacity, fronthaul\nsignaling, and computational complexity. Through analyses and evaluation, this\nwork ultimately identifies the optimal uplink technique for practical cell-free\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper seeks to determine the most efficient uplink technique for\ncell-free massive MIMO systems. Despite offering great advances, existing works\nsuffer from fragmented methodologies and inconsistent assumptions (e.g.,\nsingle- vs. multi-antenna access points, ideal vs. spatially correlated\nchannels). To address these limitations, we: (1) establish a unified analytical\nframework compatible with centralized/distributed processing and diverse\ncombining schemes; (2) develop a universal optimization strategy for max-min\npower control; and (3) conduct a holistic study among four critical metrics:\nworst-case user spectral efficiency (fairness), system capacity, fronthaul\nsignaling, and computational complexity. Through analyses and evaluation, this\nwork ultimately identifies the optimal uplink technique for practical cell-free\ndeployments."
                },
                "authors": [
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "IEEE VTC 2025-Fall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20704v1",
                "updated": "2025-08-28T12:22:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    22,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:22:52Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    22,
                    52,
                    3,
                    240,
                    0
                ],
                "title": "Achieving Optimal Performance-Cost Trade-Off in Hierarchical Cell-Free\n  Massive MIMO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving Optimal Performance-Cost Trade-Off in Hierarchical Cell-Free\n  Massive MIMO"
                },
                "summary": "Cell-free (CF) massive MIMO offers uniform service via distributed access\npoints (APs), which impose high deployment costs. A novel design called\nhierarchical cell-free (HCF) addresses this problem by replacing some APs with\na central base station, thereby lowering the costs of fronthaul network\n(wireless sites and fiber cables) while preserving performance. To identify the\noptimal uplink configuration in HCF massive MIMO, this paper provides the first\ncomprehensive analysis, benchmarking it against cellular and CF systems. We\ndevelop a unified analytical framework for spectral efficiency that supports\narbitrary combining schemes and introduce a novel hierarchical combining\napproach tailored to HCF two-tier architecture. Through analysis and evaluation\nof user fairness, system capacity, fronthaul requirements, and computational\ncomplexity, this paper identifies that HCF using centralized zero-forcing\ncombining achieves the optimal balance between performance and cost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free (CF) massive MIMO offers uniform service via distributed access\npoints (APs), which impose high deployment costs. A novel design called\nhierarchical cell-free (HCF) addresses this problem by replacing some APs with\na central base station, thereby lowering the costs of fronthaul network\n(wireless sites and fiber cables) while preserving performance. To identify the\noptimal uplink configuration in HCF massive MIMO, this paper provides the first\ncomprehensive analysis, benchmarking it against cellular and CF systems. We\ndevelop a unified analytical framework for spectral efficiency that supports\narbitrary combining schemes and introduce a novel hierarchical combining\napproach tailored to HCF two-tier architecture. Through analysis and evaluation\nof user fairness, system capacity, fronthaul requirements, and computational\ncomplexity, this paper identifies that HCF using centralized zero-forcing\ncombining achieves the optimal balance between performance and cost-efficiency."
                },
                "authors": [
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Hans D Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D Schotten"
                },
                "author": "Hans D Schotten",
                "arxiv_comment": "IEEE Globecom 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18321v2",
                "updated": "2025-08-28T12:18:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    18,
                    4,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-24T09:58:10Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    9,
                    58,
                    10,
                    6,
                    236,
                    0
                ],
                "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social\n  Interactions"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS."
                },
                "authors": [
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Tej Deep Pala"
                    },
                    {
                        "name": "Weisheng Jin"
                    },
                    {
                        "name": "Amir Zadeh"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Dorien Herremans"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20697v1",
                "updated": "2025-08-28T12:07:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    7,
                    11,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T12:07:11Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    7,
                    11,
                    3,
                    240,
                    0
                ],
                "title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning\n  Fine-Tuning"
                },
                "summary": "As large language models (LLMs) continue to grow in capability, so do the\nrisks of harmful misuse through fine-tuning. While most prior studies assume\nthat attackers rely on supervised fine-tuning (SFT) for such misuse, we\nsystematically demonstrate that reinforcement learning (RL) enables adversaries\nto more effectively break safety alignment and facilitate advanced harmful task\nassistance, under matched computational budgets. To counter this emerging\nthreat, we propose TokenBuncher, the first effective defense specifically\ntargeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation\non which RL relies: model response uncertainty. By constraining uncertainty,\nRL-based fine-tuning can no longer exploit distinct reward signals to drive the\nmodel toward harmful behaviors. We realize this defense through\nentropy-as-reward RL and a Token Noiser mechanism designed to prevent the\nescalation of expert-domain harmful capabilities. Extensive experiments across\nmultiple models and RL algorithms show that TokenBuncher robustly mitigates\nharmful RL fine-tuning while preserving benign task utility and finetunability.\nOur results highlight that RL-based harmful fine-tuning poses a greater\nsystemic risk than SFT, and that TokenBuncher provides an effective and general\ndefense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to grow in capability, so do the\nrisks of harmful misuse through fine-tuning. While most prior studies assume\nthat attackers rely on supervised fine-tuning (SFT) for such misuse, we\nsystematically demonstrate that reinforcement learning (RL) enables adversaries\nto more effectively break safety alignment and facilitate advanced harmful task\nassistance, under matched computational budgets. To counter this emerging\nthreat, we propose TokenBuncher, the first effective defense specifically\ntargeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation\non which RL relies: model response uncertainty. By constraining uncertainty,\nRL-based fine-tuning can no longer exploit distinct reward signals to drive the\nmodel toward harmful behaviors. We realize this defense through\nentropy-as-reward RL and a Token Noiser mechanism designed to prevent the\nescalation of expert-domain harmful capabilities. Extensive experiments across\nmultiple models and RL algorithms show that TokenBuncher robustly mitigates\nharmful RL fine-tuning while preserving benign task utility and finetunability.\nOur results highlight that RL-based harmful fine-tuning poses a greater\nsystemic risk than SFT, and that TokenBuncher provides an effective and general\ndefense."
                },
                "authors": [
                    {
                        "name": "Weitao Feng"
                    },
                    {
                        "name": "Lixu Wang"
                    },
                    {
                        "name": "Tianyi Wei"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Chongyang Gao"
                    },
                    {
                        "name": "Sinong Zhan"
                    },
                    {
                        "name": "Peizhuo Lv"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "arxiv_comment": "Project Hompage: https://tokenbuncher.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19724v2",
                "updated": "2025-08-28T12:05:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    5,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-27T09:34:28Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    9,
                    34,
                    28,
                    2,
                    239,
                    0
                ],
                "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for\n  Improving Small VLMs in Commonsense VQA Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLKI: A lightweight Natural Language Knowledge Integration Framework for\n  Improving Small VLMs in Commonsense VQA Tasks"
                },
                "summary": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models."
                },
                "authors": [
                    {
                        "name": "Aritra Dutta"
                    },
                    {
                        "name": "Swapnanil Mukherjee"
                    },
                    {
                        "name": "Deepanway Ghosal"
                    },
                    {
                        "name": "Somak Aditya"
                    }
                ],
                "author_detail": {
                    "name": "Somak Aditya"
                },
                "author": "Somak Aditya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02549v2",
                "updated": "2025-08-28T12:01:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    12,
                    1,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-03-04T12:20:06Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    20,
                    6,
                    1,
                    63,
                    0
                ],
                "title": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation"
                },
                "summary": "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the collected data is stored in the\nsame location where nnU-Net is trained. This centralized approach has various\nlimitations, such as potential leakage of sensitive patient information and\nviolation of patient privacy. Federated learning has emerged as a key approach\nfor training segmentation models in a decentralized manner, enabling\ncollaborative development while prioritising patient privacy. In this paper, we\npropose FednnU-Net, a plug-and-play, federated learning extension of the\nnnU-Net framework. To this end, we contribute two federated methodologies to\nunlock decentralized training of nnU-Net, namely, Federated Fingerprint\nExtraction (FFE) and Asymmetric Federated Averaging (AsymFedAvg). We conduct a\ncomprehensive set of experiments demonstrating high and consistent performance\nof our methods for breast, cardiac and fetal segmentation based on a\nmulti-modal collection of 6 datasets representing samples from 18 different\ninstitutions. To democratize research as well as real-world deployments of\ndecentralized training in clinical centres, we publicly share our framework at\nhttps://github.com/faildeny/FednnUNet .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the collected data is stored in the\nsame location where nnU-Net is trained. This centralized approach has various\nlimitations, such as potential leakage of sensitive patient information and\nviolation of patient privacy. Federated learning has emerged as a key approach\nfor training segmentation models in a decentralized manner, enabling\ncollaborative development while prioritising patient privacy. In this paper, we\npropose FednnU-Net, a plug-and-play, federated learning extension of the\nnnU-Net framework. To this end, we contribute two federated methodologies to\nunlock decentralized training of nnU-Net, namely, Federated Fingerprint\nExtraction (FFE) and Asymmetric Federated Averaging (AsymFedAvg). We conduct a\ncomprehensive set of experiments demonstrating high and consistent performance\nof our methods for breast, cardiac and fetal segmentation based on a\nmulti-modal collection of 6 datasets representing samples from 18 different\ninstitutions. To democratize research as well as real-world deployments of\ndecentralized training in clinical centres, we publicly share our framework at\nhttps://github.com/faildeny/FednnUNet ."
                },
                "authors": [
                    {
                        "name": "Grzegorz Skorupko"
                    },
                    {
                        "name": "Fotios Avgoustidis"
                    },
                    {
                        "name": "Carlos Mart√≠n-Isla"
                    },
                    {
                        "name": "Lidia Garrucho"
                    },
                    {
                        "name": "Dimitri A. Kessler"
                    },
                    {
                        "name": "Esmeralda Ruiz Pujadas"
                    },
                    {
                        "name": "Oliver D√≠az"
                    },
                    {
                        "name": "Maciej Bobowicz"
                    },
                    {
                        "name": "Katarzyna Gwo≈∫dziewicz"
                    },
                    {
                        "name": "Xavier Bargall√≥"
                    },
                    {
                        "name": "Paulius Jaru≈°eviƒçius"
                    },
                    {
                        "name": "Richard Osuala"
                    },
                    {
                        "name": "Kaisar Kushibar"
                    },
                    {
                        "name": "Karim Lekadir"
                    }
                ],
                "author_detail": {
                    "name": "Karim Lekadir"
                },
                "author": "Karim Lekadir",
                "arxiv_comment": "In review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20693v1",
                "updated": "2025-08-28T11:53:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    53,
                    45,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T11:53:45Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    53,
                    45,
                    3,
                    240,
                    0
                ],
                "title": "Leveraging Large Language Models for Generating Research Topic\n  Ontologies: A Multi-Disciplinary Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Generating Research Topic\n  Ontologies: A Multi-Disciplinary Study"
                },
                "summary": "Ontologies and taxonomies of research fields are critical for managing and\norganising scientific knowledge, as they facilitate efficient classification,\ndissemination and retrieval of information. However, the creation and\nmaintenance of such ontologies are expensive and time-consuming tasks, usually\nrequiring the coordinated effort of multiple domain experts. Consequently,\nontologies in this space often exhibit uneven coverage across different\ndisciplines, limited inter-domain connectivity, and infrequent updating cycles.\nIn this study, we investigate the capability of several large language models\nto identify semantic relationships among research topics within three academic\ndomains: biomedicine, physics, and engineering. The models were evaluated under\nthree distinct conditions: zero-shot prompting, chain-of-thought prompting, and\nfine-tuning on existing ontologies. Additionally, we assessed the cross-domain\ntransferability of fine-tuned models by measuring their performance when\ntrained in one domain and subsequently applied to a different one. To support\nthis analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over\n8,000 relationships extracted from the most widely adopted taxonomies in the\nthree disciplines considered in this study: MeSH, PhySH, and IEEE. Our\nexperiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent\nperformance across all disciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontologies and taxonomies of research fields are critical for managing and\norganising scientific knowledge, as they facilitate efficient classification,\ndissemination and retrieval of information. However, the creation and\nmaintenance of such ontologies are expensive and time-consuming tasks, usually\nrequiring the coordinated effort of multiple domain experts. Consequently,\nontologies in this space often exhibit uneven coverage across different\ndisciplines, limited inter-domain connectivity, and infrequent updating cycles.\nIn this study, we investigate the capability of several large language models\nto identify semantic relationships among research topics within three academic\ndomains: biomedicine, physics, and engineering. The models were evaluated under\nthree distinct conditions: zero-shot prompting, chain-of-thought prompting, and\nfine-tuning on existing ontologies. Additionally, we assessed the cross-domain\ntransferability of fine-tuned models by measuring their performance when\ntrained in one domain and subsequently applied to a different one. To support\nthis analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over\n8,000 relationships extracted from the most widely adopted taxonomies in the\nthree disciplines considered in this study: MeSH, PhySH, and IEEE. Our\nexperiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent\nperformance across all disciplines."
                },
                "authors": [
                    {
                        "name": "Tanay Aggarwal"
                    },
                    {
                        "name": "Angelo Salatino"
                    },
                    {
                        "name": "Francesco Osborne"
                    },
                    {
                        "name": "Enrico Motta"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Motta"
                },
                "author": "Enrico Motta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20661v2",
                "updated": "2025-08-29T11:00:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    0,
                    18,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T11:09:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    9,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework\n  for Humanoid Beam Walking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework\n  for Humanoid Beam Walking"
                },
                "summary": "Traversing narrow beams is challenging for humanoids due to sparse,\nsafety-critical contacts and the fragility of purely learned policies. We\npropose a physically grounded, two-stage framework that couples an XCoM/LIPM\nfootstep template with a lightweight residual planner and a simple low-level\ntracker. Stage-1 is trained on flat ground: the tracker learns to robustly\nfollow footstep targets by adding small random perturbations to heuristic\nfootsteps, without any hand-crafted centerline locking, so it acquires stable\ncontact scheduling and strong target-tracking robustness. Stage-2 is trained in\nsimulation on a beam: a high-level planner predicts a body-frame residual\n(Delta x, Delta y, Delta psi) for the swing foot only, refining the template\nstep to prioritize safe, precise placement under narrow support while\npreserving interpretability. To ease deployment, sensing is kept minimal and\nconsistent between simulation and hardware: the planner consumes compact,\nforward-facing elevation cues together with onboard IMU and joint signals. On a\nUnitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across\nsimulation and real-world studies, residual refinement consistently outperforms\ntemplate-only and monolithic baselines in success rate, centerline adherence,\nand safety margins, while the structured footstep interface enables transparent\nanalysis and low-friction sim-to-real transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traversing narrow beams is challenging for humanoids due to sparse,\nsafety-critical contacts and the fragility of purely learned policies. We\npropose a physically grounded, two-stage framework that couples an XCoM/LIPM\nfootstep template with a lightweight residual planner and a simple low-level\ntracker. Stage-1 is trained on flat ground: the tracker learns to robustly\nfollow footstep targets by adding small random perturbations to heuristic\nfootsteps, without any hand-crafted centerline locking, so it acquires stable\ncontact scheduling and strong target-tracking robustness. Stage-2 is trained in\nsimulation on a beam: a high-level planner predicts a body-frame residual\n(Delta x, Delta y, Delta psi) for the swing foot only, refining the template\nstep to prioritize safe, precise placement under narrow support while\npreserving interpretability. To ease deployment, sensing is kept minimal and\nconsistent between simulation and hardware: the planner consumes compact,\nforward-facing elevation cues together with onboard IMU and joint signals. On a\nUnitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across\nsimulation and real-world studies, residual refinement consistently outperforms\ntemplate-only and monolithic baselines in success rate, centerline adherence,\nand safety margins, while the structured footstep interface enables transparent\nanalysis and low-friction sim-to-real transfer."
                },
                "authors": [
                    {
                        "name": "TianChen Huang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Runchen Xu"
                    },
                    {
                        "name": "Shiwu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shiwu Zhang"
                },
                "author": "Shiwu Zhang",
                "arxiv_comment": "Project website:\n  https://huangtc233.github.io/Traversing-the-Narrow-Path/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20660v1",
                "updated": "2025-08-28T11:07:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    7,
                    36,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T11:07:36Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    7,
                    36,
                    3,
                    240,
                    0
                ],
                "title": "CodecBench: A Comprehensive Benchmark for Acoustic and Semantic\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodecBench: A Comprehensive Benchmark for Acoustic and Semantic\n  Evaluation"
                },
                "summary": "With the rise of multimodal large language models (LLMs), audio codec plays\nan increasingly vital role in encoding audio into discrete tokens, enabling\nintegration of audio into text-based LLMs. Current audio codec captures two\ntypes of information: acoustic and semantic. As audio codec is applied to\ndiverse scenarios in speech language model , it needs to model increasingly\ncomplex information and adapt to varied contexts, such as scenarios with\nmultiple speakers, background noise, or richer paralinguistic information.\nHowever, existing codec's own evaluation has been limited by simplistic metrics\nand scenarios, and existing benchmarks for audio codec are not designed for\ncomplex application scenarios, which limits the assessment performance on\ncomplex datasets for acoustic and semantic capabilities. We introduce\nCodecBench, a comprehensive evaluation dataset to assess audio codec\nperformance from both acoustic and semantic perspectives across four data\ndomains. Through this benchmark, we aim to identify current limitations,\nhighlight future research directions, and foster advances in the development of\naudio codec. The codes are available at https://github.com/RayYuki/CodecBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of multimodal large language models (LLMs), audio codec plays\nan increasingly vital role in encoding audio into discrete tokens, enabling\nintegration of audio into text-based LLMs. Current audio codec captures two\ntypes of information: acoustic and semantic. As audio codec is applied to\ndiverse scenarios in speech language model , it needs to model increasingly\ncomplex information and adapt to varied contexts, such as scenarios with\nmultiple speakers, background noise, or richer paralinguistic information.\nHowever, existing codec's own evaluation has been limited by simplistic metrics\nand scenarios, and existing benchmarks for audio codec are not designed for\ncomplex application scenarios, which limits the assessment performance on\ncomplex datasets for acoustic and semantic capabilities. We introduce\nCodecBench, a comprehensive evaluation dataset to assess audio codec\nperformance from both acoustic and semantic perspectives across four data\ndomains. Through this benchmark, we aim to identify current limitations,\nhighlight future research directions, and foster advances in the development of\naudio codec. The codes are available at https://github.com/RayYuki/CodecBench."
                },
                "authors": [
                    {
                        "name": "Ruifan Deng"
                    },
                    {
                        "name": "Yitian Gong"
                    },
                    {
                        "name": "Qinghui Gao"
                    },
                    {
                        "name": "Luozhijie Jin"
                    },
                    {
                        "name": "Qinyuan Cheng"
                    },
                    {
                        "name": "Zhaoye Fei"
                    },
                    {
                        "name": "Shimin Li"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20655v1",
                "updated": "2025-08-28T11:01:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    1,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T11:01:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    1,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Alignment in LVLMs with Debiased Self-Judgment"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Weilong Yan"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19918v3",
                "updated": "2025-08-29T02:12:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    12,
                    5,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-27T14:24:13Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    14,
                    24,
                    13,
                    2,
                    239,
                    0
                ],
                "title": "Refining Text Generation for Realistic Conversational Recommendation via\n  Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Text Generation for Realistic Conversational Recommendation via\n  Direct Preference Optimization"
                },
                "summary": "Conversational Recommender Systems (CRSs) aim to elicit user preferences via\nnatural dialogue to provide suitable item recommendations. However, current\nCRSs often deviate from realistic human interactions by rapidly recommending\nitems in brief sessions. This work addresses this gap by leveraging Large\nLanguage Models (LLMs) to generate dialogue summaries from dialogue history and\nitem recommendation information from item description. This approach enables\nthe extraction of both explicit user statements and implicit preferences\ninferred from the dialogue context. We introduce a method using Direct\nPreference Optimization (DPO) to ensure dialogue summary and item\nrecommendation information are rich in information crucial for effective\nrecommendations. Experiments on two public datasets validate our method's\neffectiveness in fostering more natural and realistic conversational\nrecommendation processes. Our implementation is publicly available at:\nhttps://github.com/UEC-InabaLab/Refining-LLM-Text",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs) aim to elicit user preferences via\nnatural dialogue to provide suitable item recommendations. However, current\nCRSs often deviate from realistic human interactions by rapidly recommending\nitems in brief sessions. This work addresses this gap by leveraging Large\nLanguage Models (LLMs) to generate dialogue summaries from dialogue history and\nitem recommendation information from item description. This approach enables\nthe extraction of both explicit user statements and implicit preferences\ninferred from the dialogue context. We introduce a method using Direct\nPreference Optimization (DPO) to ensure dialogue summary and item\nrecommendation information are rich in information crucial for effective\nrecommendations. Experiments on two public datasets validate our method's\neffectiveness in fostering more natural and realistic conversational\nrecommendation processes. Our implementation is publicly available at:\nhttps://github.com/UEC-InabaLab/Refining-LLM-Text"
                },
                "authors": [
                    {
                        "name": "Manato Tajiri"
                    },
                    {
                        "name": "Michimasa Inaba"
                    }
                ],
                "author_detail": {
                    "name": "Michimasa Inaba"
                },
                "author": "Michimasa Inaba",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20643v1",
                "updated": "2025-08-28T10:45:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    45,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T10:45:31Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    45,
                    31,
                    3,
                    240,
                    0
                ],
                "title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics"
                },
                "summary": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents."
                },
                "authors": [
                    {
                        "name": "Stefano Fumero"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Matteo Boffa"
                    },
                    {
                        "name": "Danilo Giordano"
                    },
                    {
                        "name": "Marco Mellia"
                    },
                    {
                        "name": "Zied Ben Houidi"
                    },
                    {
                        "name": "Dario Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Dario Rossi"
                },
                "author": "Dario Rossi",
                "arxiv_comment": "Code:\n  https://github.com/SmartData-Polito/LLM_Agent_Cybersecurity_Forensic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20640v1",
                "updated": "2025-08-28T10:38:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    38,
                    13,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T10:38:13Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    38,
                    13,
                    3,
                    240,
                    0
                ],
                "title": "CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via\n  Facial-Preserving Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via\n  Facial-Preserving Diffusion Models"
                },
                "summary": "Preserving facial identity under extreme stylistic transformation remains a\nmajor challenge in generative art. In graffiti, a high-contrast, abstract\nmedium, subtle distortions to the eyes, nose, or mouth can erase the subject's\nrecognizability, undermining both personal and cultural authenticity. We\npresent CraftGraffiti, an end-to-end text-guided graffiti generation framework\ndesigned with facial feature preservation as a primary objective. Given an\ninput image and a style and pose descriptive prompt, CraftGraffiti first\napplies graffiti style transfer via LoRA-fine-tuned pretrained diffusion\ntransformer, then enforces identity fidelity through a face-consistent\nself-attention mechanism that augments attention layers with explicit identity\nembeddings. Pose customization is achieved without keypoints, using CLIP-guided\nprompt extension to enable dynamic re-posing while retaining facial coherence.\nWe formally justify and empirically validate the \"style-first, identity-after\"\nparadigm, showing it reduces attribute drift compared to the reverse order.\nQuantitative results demonstrate competitive facial feature consistency and\nstate-of-the-art aesthetic and human preference scores, while qualitative\nanalyses and a live deployment at the Cruilla Festival highlight the system's\nreal-world creative impact. CraftGraffiti advances the goal of\nidentity-respectful AI-assisted artistry, offering a principled approach for\nblending stylistic freedom with recognizability in creative AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving facial identity under extreme stylistic transformation remains a\nmajor challenge in generative art. In graffiti, a high-contrast, abstract\nmedium, subtle distortions to the eyes, nose, or mouth can erase the subject's\nrecognizability, undermining both personal and cultural authenticity. We\npresent CraftGraffiti, an end-to-end text-guided graffiti generation framework\ndesigned with facial feature preservation as a primary objective. Given an\ninput image and a style and pose descriptive prompt, CraftGraffiti first\napplies graffiti style transfer via LoRA-fine-tuned pretrained diffusion\ntransformer, then enforces identity fidelity through a face-consistent\nself-attention mechanism that augments attention layers with explicit identity\nembeddings. Pose customization is achieved without keypoints, using CLIP-guided\nprompt extension to enable dynamic re-posing while retaining facial coherence.\nWe formally justify and empirically validate the \"style-first, identity-after\"\nparadigm, showing it reduces attribute drift compared to the reverse order.\nQuantitative results demonstrate competitive facial feature consistency and\nstate-of-the-art aesthetic and human preference scores, while qualitative\nanalyses and a live deployment at the Cruilla Festival highlight the system's\nreal-world creative impact. CraftGraffiti advances the goal of\nidentity-respectful AI-assisted artistry, offering a principled approach for\nblending stylistic freedom with recognizability in creative AI applications."
                },
                "authors": [
                    {
                        "name": "Ayan Banerjee"
                    },
                    {
                        "name": "Fernando Vilari√±o"
                    },
                    {
                        "name": "Josep Llad√≥s"
                    }
                ],
                "author_detail": {
                    "name": "Josep Llad√≥s"
                },
                "author": "Josep Llad√≥s",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20637v1",
                "updated": "2025-08-28T10:35:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    35,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T10:35:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    35,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "GDS Agent: A Graph Algorithmic Reasoning Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDS Agent: A Graph Algorithmic Reasoning Agent"
                },
                "summary": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We also\nintroduce a new benchmark that evaluates intermediate tool calls as well as\nfinal responses. The results indicate that GDS agent is able to solve a wide\nspectrum of graph tasks. We also provide detailed case studies for more\nopen-ended tasks and study scenarios where the agent struggles. Finally, we\ndiscuss the remaining challenges and the future roadmap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable multimodal information\nprocessing and reasoning ability. When equipped with tools through function\ncalling and enhanced with retrieval-augmented techniques, compound LLM-based\nsystems can access closed data sources and answer questions about them.\nHowever, they still struggle to process and reason over large-scale\ngraph-structure data. We introduce the GDS (Graph Data Science) agent in this\ntechnical report. The GDS agent introduces a comprehensive set of graph\nalgorithms as tools, together with preprocessing (retrieval) and postprocessing\nof algorithm results, in a model context protocol (MCP) server. The server can\nbe used with any modern LLM out-of-the-box. GDS agent allows users to ask any\nquestion that implicitly and intrinsically requires graph algorithmic reasoning\nabout their data, and quickly obtain accurate and grounded answers. We also\nintroduce a new benchmark that evaluates intermediate tool calls as well as\nfinal responses. The results indicate that GDS agent is able to solve a wide\nspectrum of graph tasks. We also provide detailed case studies for more\nopen-ended tasks and study scenarios where the agent struggles. Finally, we\ndiscuss the remaining challenges and the future roadmap."
                },
                "authors": [
                    {
                        "name": "Borun Shi"
                    },
                    {
                        "name": "Ioannis Panagiotas"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Panagiotas"
                },
                "author": "Ioannis Panagiotas",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20635v1",
                "updated": "2025-08-28T10:34:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    34,
                    50,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T10:34:50Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    34,
                    50,
                    3,
                    240,
                    0
                ],
                "title": "Schema-Guided Response Generation using Multi-Frame Dialogue State for\n  Motivational Interviewing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema-Guided Response Generation using Multi-Frame Dialogue State for\n  Motivational Interviewing Systems"
                },
                "summary": "The primary goal of Motivational Interviewing (MI) is to help clients build\ntheir own motivation for behavioral change. To support this in dialogue\nsystems, it is essential to guide large language models (LLMs) to generate\ncounselor responses aligned with MI principles. By employing a schema-guided\napproach, this study proposes a method for updating multi-frame dialogue states\nand a strategy decision mechanism that dynamically determines the response\nfocus in a manner grounded in MI principles. The proposed method was\nimplemented in a dialogue system and evaluated through a user study. Results\nshowed that the proposed system successfully generated MI-favorable responses\nand effectively encouraged the user's (client's) deliberation by asking\neliciting questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary goal of Motivational Interviewing (MI) is to help clients build\ntheir own motivation for behavioral change. To support this in dialogue\nsystems, it is essential to guide large language models (LLMs) to generate\ncounselor responses aligned with MI principles. By employing a schema-guided\napproach, this study proposes a method for updating multi-frame dialogue states\nand a strategy decision mechanism that dynamically determines the response\nfocus in a manner grounded in MI principles. The proposed method was\nimplemented in a dialogue system and evaluated through a user study. Results\nshowed that the proposed system successfully generated MI-favorable responses\nand effectively encouraged the user's (client's) deliberation by asking\neliciting questions."
                },
                "authors": [
                    {
                        "name": "Jie Zeng"
                    },
                    {
                        "name": "Yukiko I. Nakano"
                    }
                ],
                "author_detail": {
                    "name": "Yukiko I. Nakano"
                },
                "author": "Yukiko I. Nakano",
                "arxiv_comment": "28pages, 15 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19720v2",
                "updated": "2025-08-28T10:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    0,
                    55,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-27T09:30:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    9,
                    30,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with\n  Proxy Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with\n  Proxy Models"
                },
                "summary": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS."
                },
                "authors": [
                    {
                        "name": "Yilin Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Yuyang Bai"
                    },
                    {
                        "name": "Minnan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Minnan Luo"
                },
                "author": "Minnan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11752v2",
                "updated": "2025-08-28T09:45:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    45,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-06-13T13:05:41Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    5,
                    41,
                    4,
                    164,
                    0
                ],
                "title": "DART: Distilling Autoregressive Reasoning to Silent Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: Distilling Autoregressive Reasoning to Silent Thought"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART offers significant\nperformance gains compared with existing non-autoregressive baselines without\nextra inference latency, serving as a feasible alternative for efficient\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART offers significant\nperformance gains compared with existing non-autoregressive baselines without\nextra inference latency, serving as a feasible alternative for efficient\nreasoning."
                },
                "authors": [
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Ziming Wu"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Fuming Lai"
                    },
                    {
                        "name": "Shaobing Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shaobing Lian"
                },
                "author": "Shaobing Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20600v1",
                "updated": "2025-08-28T09:43:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    43,
                    59,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T09:43:59Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    43,
                    59,
                    3,
                    240,
                    0
                ],
                "title": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac\n  MRI Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac\n  MRI Reconstruction"
                },
                "summary": "Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols."
                },
                "authors": [
                    {
                        "name": "Kian Anvari Hamedani"
                    },
                    {
                        "name": "Narges Razizadeh"
                    },
                    {
                        "name": "Shahabedin Nabavi"
                    },
                    {
                        "name": "Mohsen Ebrahimi Moghaddam"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Ebrahimi Moghaddam"
                },
                "author": "Mohsen Ebrahimi Moghaddam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03661v2",
                "updated": "2025-08-28T09:35:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    35,
                    57,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-05T17:18:20Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    17,
                    18,
                    20,
                    1,
                    217,
                    0
                ],
                "title": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided\n  by LLM-Informed Evolutionary Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided\n  by LLM-Informed Evolutionary Monte Carlo Tree Search"
                },
                "summary": "Gravitational-wave signal detection with unknown source parameters buried in\ndynamic detector noise remains a formidable computational challenge. Existing\napproaches face core limitations from restrictive assumptions: traditional\nmethods rely on predefined theoretical priors, while neural networks introduce\nhidden biases and lack interpretability. We propose Evolutionary Monte Carlo\nTree Search (Evo-MCTS), the first integration of large language model (LLM)\nguidance with domain-aware physical constraints for automated gravitational\nwave detection. This framework systematically explores algorithmic solution\nspaces through tree-structured search enhanced by evolutionary optimization,\ncombining MCTS for strategic exploration with evolutionary algorithms for\nsolution refinement. The LLM component provides domain-aware heuristics while\nmaintaining interpretability through explicit algorithmic pathway generation.\nExperimental validation demonstrates substantial performance improvements,\nachieving a 20.2% improvement over state-of-the-art gravitational wave\ndetection algorithms on the MLGWSC-1 benchmark dataset and a remarkable 59.1%\nimprovement over other LLM-based algorithm optimization frameworks. Beyond\nperformance improvements, our framework establishes a transferable methodology\nfor automated algorithmic discovery across computational science domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational-wave signal detection with unknown source parameters buried in\ndynamic detector noise remains a formidable computational challenge. Existing\napproaches face core limitations from restrictive assumptions: traditional\nmethods rely on predefined theoretical priors, while neural networks introduce\nhidden biases and lack interpretability. We propose Evolutionary Monte Carlo\nTree Search (Evo-MCTS), the first integration of large language model (LLM)\nguidance with domain-aware physical constraints for automated gravitational\nwave detection. This framework systematically explores algorithmic solution\nspaces through tree-structured search enhanced by evolutionary optimization,\ncombining MCTS for strategic exploration with evolutionary algorithms for\nsolution refinement. The LLM component provides domain-aware heuristics while\nmaintaining interpretability through explicit algorithmic pathway generation.\nExperimental validation demonstrates substantial performance improvements,\nachieving a 20.2% improvement over state-of-the-art gravitational wave\ndetection algorithms on the MLGWSC-1 benchmark dataset and a remarkable 59.1%\nimprovement over other LLM-based algorithm optimization frameworks. Beyond\nperformance improvements, our framework establishes a transferable methodology\nfor automated algorithmic discovery across computational science domains."
                },
                "authors": [
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Liang Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zeng"
                },
                "author": "Liang Zeng",
                "arxiv_comment": "79 pages (29 main), with 6+6 figures and 2 tables, presenting a more\n  concise and updated manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07999v2",
                "updated": "2025-08-28T09:31:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    31,
                    57,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-11T14:03:09Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    3,
                    9,
                    0,
                    223,
                    0
                ],
                "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WideSearch: Benchmarking Agentic Broad Info-Seeking"
                },
                "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/"
                },
                "authors": [
                    {
                        "name": "Ryan Wong"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Junjie Zhao"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Xuan Zhou"
                    },
                    {
                        "name": "Zuo Wang"
                    },
                    {
                        "name": "Kai Xiang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Ke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Wang"
                },
                "author": "Ke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20591v1",
                "updated": "2025-08-28T09:28:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    28,
                    11,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T09:28:11Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    28,
                    11,
                    3,
                    240,
                    0
                ],
                "title": "Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit\n  Timestamping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bitcoin as an Interplanetary Monetary Standard with Proof-of-Transit\n  Timestamping"
                },
                "summary": "We explore the feasibility of deploying Bitcoin as the shared monetary\nstandard between Earth and Mars, accounting for physical constraints of\ninterplanetary communication. We introduce a novel primitive, Proof-of-Transit\nTimestamping (PoTT), to provide cryptographic, tamper-evident audit trails for\nBitcoin data across high-latency, intermittently-connected links. Leveraging\nDelay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)\nmesh constellations, we propose an architecture for header-first replication,\nlong-horizon Lightning channels with planetary watchtowers, and secure\nsettlement through federated sidechains or blind-merge-mined (BMM) commit\nchains. We formalize PoTT, analyze its security model, and show how it\nmeasurably improves reliability and accountability without altering Bitcoin\nconsensus or its monetary base. Near-term deployments favor strong federations\nfor local settlement; longer-term, blind-merge-mined commit chains (if adopted)\nprovide an alternative. The Earth L1 monetary base remains unchanged, while\nMars can operate a pegged commit chain or strong federation with 1:1 pegged\nassets for local block production. For transparency, if both time-beacon\nregimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to\nadministrative assertions rather than cryptographic time-anchoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the feasibility of deploying Bitcoin as the shared monetary\nstandard between Earth and Mars, accounting for physical constraints of\ninterplanetary communication. We introduce a novel primitive, Proof-of-Transit\nTimestamping (PoTT), to provide cryptographic, tamper-evident audit trails for\nBitcoin data across high-latency, intermittently-connected links. Leveraging\nDelay/Disruption-Tolerant Networking (DTN) and optical low-Earth-orbit (LEO)\nmesh constellations, we propose an architecture for header-first replication,\nlong-horizon Lightning channels with planetary watchtowers, and secure\nsettlement through federated sidechains or blind-merge-mined (BMM) commit\nchains. We formalize PoTT, analyze its security model, and show how it\nmeasurably improves reliability and accountability without altering Bitcoin\nconsensus or its monetary base. Near-term deployments favor strong federations\nfor local settlement; longer-term, blind-merge-mined commit chains (if adopted)\nprovide an alternative. The Earth L1 monetary base remains unchanged, while\nMars can operate a pegged commit chain or strong federation with 1:1 pegged\nassets for local block production. For transparency, if both time-beacon\nregimes are simultaneously compromised, PoTT-M2 (and PoTT generally) reduces to\nadministrative assertions rather than cryptographic time-anchoring."
                },
                "authors": [
                    {
                        "name": "Jose E. Puente"
                    },
                    {
                        "name": "Carlos Puente"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Puente"
                },
                "author": "Carlos Puente",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20587v1",
                "updated": "2025-08-28T09:25:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    25,
                    54,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T09:25:54Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    25,
                    54,
                    3,
                    240,
                    0
                ],
                "title": "SemSR: Semantics aware robust Session-based Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemSR: Semantics aware robust Session-based Recommendations"
                },
                "summary": "Session-based recommendation (SR) models aim to recommend items to anonymous\nusers based on their behavior during the current session. While various SR\nmodels in the literature utilize item sequences to predict the next item, they\noften fail to leverage semantic information from item titles or descriptions\nimpeding session intent identification and interpretability. Recent research\nhas explored Large Language Models (LLMs) as promising approaches to enhance\nsession-based recommendations, with both prompt-based and fine-tuning based\nmethods being widely investigated. However, prompt-based methods struggle to\nidentify optimal prompts that elicit correct reasoning and lack task-specific\nfeedback at test time, resulting in sub-optimal recommendations. Fine-tuning\nmethods incorporate domain-specific knowledge but incur significant\ncomputational costs for implementation and maintenance. In this paper, we\npresent multiple approaches to utilize LLMs for session-based recommendation:\n(i) in-context LLMs as recommendation agents, (ii) LLM-generated\nrepresentations for semantic initialization of deep learning SR models, and\n(iii) integration of LLMs with data-driven SR models. Through comprehensive\nexperiments on two real-world publicly available datasets, we demonstrate that\nLLM-based methods excel at coarse-level retrieval (high recall values), while\ntraditional data-driven techniques perform well at fine-grained ranking (high\nMean Reciprocal Rank values). Furthermore, the integration of LLMs with\ndata-driven SR models significantly out performs both standalone LLM approaches\nand data-driven deep learning models, as well as baseline SR models, in terms\nof both Recall and MRR metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Session-based recommendation (SR) models aim to recommend items to anonymous\nusers based on their behavior during the current session. While various SR\nmodels in the literature utilize item sequences to predict the next item, they\noften fail to leverage semantic information from item titles or descriptions\nimpeding session intent identification and interpretability. Recent research\nhas explored Large Language Models (LLMs) as promising approaches to enhance\nsession-based recommendations, with both prompt-based and fine-tuning based\nmethods being widely investigated. However, prompt-based methods struggle to\nidentify optimal prompts that elicit correct reasoning and lack task-specific\nfeedback at test time, resulting in sub-optimal recommendations. Fine-tuning\nmethods incorporate domain-specific knowledge but incur significant\ncomputational costs for implementation and maintenance. In this paper, we\npresent multiple approaches to utilize LLMs for session-based recommendation:\n(i) in-context LLMs as recommendation agents, (ii) LLM-generated\nrepresentations for semantic initialization of deep learning SR models, and\n(iii) integration of LLMs with data-driven SR models. Through comprehensive\nexperiments on two real-world publicly available datasets, we demonstrate that\nLLM-based methods excel at coarse-level retrieval (high recall values), while\ntraditional data-driven techniques perform well at fine-grained ranking (high\nMean Reciprocal Rank values). Furthermore, the integration of LLMs with\ndata-driven SR models significantly out performs both standalone LLM approaches\nand data-driven deep learning models, as well as baseline SR models, in terms\nof both Recall and MRR metrics."
                },
                "authors": [
                    {
                        "name": "Jyoti Narwariya"
                    },
                    {
                        "name": "Priyanka Gupta"
                    },
                    {
                        "name": "Muskan Gupta"
                    },
                    {
                        "name": "Jyotsana Khatri"
                    },
                    {
                        "name": "Lovekesh Vig"
                    }
                ],
                "author_detail": {
                    "name": "Lovekesh Vig"
                },
                "author": "Lovekesh Vig",
                "arxiv_comment": "Accepted at EARL workshop @RecSys'25, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20583v1",
                "updated": "2025-08-28T09:20:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    20,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T09:20:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    20,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "A Graph Talks, But Who's Listening? Rethinking Evaluations for\n  Graph-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph Talks, But Who's Listening? Rethinking Evaluations for\n  Graph-Language Models"
                },
                "summary": "Developments in Graph-Language Models (GLMs) aim to integrate the structural\nreasoning capabilities of Graph Neural Networks (GNNs) with the semantic\nunderstanding of Large Language Models (LLMs). However, we demonstrate that\ncurrent evaluation benchmarks for GLMs, which are primarily repurposed\nnode-level classification datasets, are insufficient to assess multimodal\nreasoning. Our analysis reveals that strong performance on these benchmarks is\nachievable using unimodal information alone, suggesting that they do not\nnecessitate graph-language integration. To address this evaluation gap, we\nintroduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed\nto evaluate multimodal reasoning at various complexity levels. Our benchmark\nemploys a synthetic graph generation pipeline paired with questions that\nrequire joint reasoning over structure and textual semantics. We perform a\nthorough evaluation of representative GLM architectures and find that\nsoft-prompted LLM baselines perform on par with GLMs that incorporate a full\nGNN backbone. This result calls into question the architectural necessity of\nincorporating graph structure into LLMs. We further show that GLMs exhibit\nsignificant performance degradation in tasks that require structural reasoning.\nThese findings highlight limitations in the graph reasoning capabilities of\ncurrent GLMs and provide a foundation for advancing the community toward\nexplicit multimodal reasoning involving graph structure and language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developments in Graph-Language Models (GLMs) aim to integrate the structural\nreasoning capabilities of Graph Neural Networks (GNNs) with the semantic\nunderstanding of Large Language Models (LLMs). However, we demonstrate that\ncurrent evaluation benchmarks for GLMs, which are primarily repurposed\nnode-level classification datasets, are insufficient to assess multimodal\nreasoning. Our analysis reveals that strong performance on these benchmarks is\nachievable using unimodal information alone, suggesting that they do not\nnecessitate graph-language integration. To address this evaluation gap, we\nintroduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed\nto evaluate multimodal reasoning at various complexity levels. Our benchmark\nemploys a synthetic graph generation pipeline paired with questions that\nrequire joint reasoning over structure and textual semantics. We perform a\nthorough evaluation of representative GLM architectures and find that\nsoft-prompted LLM baselines perform on par with GLMs that incorporate a full\nGNN backbone. This result calls into question the architectural necessity of\nincorporating graph structure into LLMs. We further show that GLMs exhibit\nsignificant performance degradation in tasks that require structural reasoning.\nThese findings highlight limitations in the graph reasoning capabilities of\ncurrent GLMs and provide a foundation for advancing the community toward\nexplicit multimodal reasoning involving graph structure and language."
                },
                "authors": [
                    {
                        "name": "Soham Petkar"
                    },
                    {
                        "name": "Hari Aakash K"
                    },
                    {
                        "name": "Anirudh Vempati"
                    },
                    {
                        "name": "Akshit Sinha"
                    },
                    {
                        "name": "Ponnurangam Kumarauguru"
                    },
                    {
                        "name": "Chirag Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Agarwal"
                },
                "author": "Chirag Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11302v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11302v4",
                "updated": "2025-08-28T09:20:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    20,
                    39,
                    3,
                    240,
                    0
                ],
                "published": "2025-03-14T11:11:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Are formal and functional linguistic mechanisms dissociated in language\n  models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are formal and functional linguistic mechanisms dissociated in language\n  models?"
                },
                "summary": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist."
                },
                "authors": [
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pezzelle"
                },
                "author": "Sandro Pezzelle",
                "arxiv_comment": "To appear in Computational Linguistics. Pre-MIT Press publication\n  version. 40 pages, 14 figures, 3 tables. Code available at\n  https://github.com/hannamw/formal-functional-dissociation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11302v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11302v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20578v1",
                "updated": "2025-08-28T09:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    17,
                    35,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T09:17:35Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    17,
                    35,
                    3,
                    240,
                    0
                ],
                "title": "Human-AI Collaborative Bot Detection in MMORPGs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-AI Collaborative Bot Detection in MMORPGs"
                },
                "summary": "In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling\nbots exploit automated programs to level up characters at scale, undermining\ngameplay balance and fairness. Detecting such bots is challenging, not only\nbecause they mimic human behavior, but also because punitive actions require\nexplainable justification to avoid legal and user experience issues. In this\npaper, we present a novel framework for detecting auto-leveling bots by\nleveraging contrastive representation learning and clustering techniques in a\nfully unsupervised manner to identify groups of characters with similar\nlevel-up patterns. To ensure reliable decisions, we incorporate a Large\nLanguage Model (LLM) as an auxiliary reviewer to validate the clustered groups,\neffectively mimicking a secondary human judgment. We also introduce a growth\ncurve-based visualization to assist both the LLM and human moderators in\nassessing leveling behavior. This collaborative approach improves the\nefficiency of bot detection workflows while maintaining explainability, thereby\nsupporting scalable and accountable bot regulation in MMORPGs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling\nbots exploit automated programs to level up characters at scale, undermining\ngameplay balance and fairness. Detecting such bots is challenging, not only\nbecause they mimic human behavior, but also because punitive actions require\nexplainable justification to avoid legal and user experience issues. In this\npaper, we present a novel framework for detecting auto-leveling bots by\nleveraging contrastive representation learning and clustering techniques in a\nfully unsupervised manner to identify groups of characters with similar\nlevel-up patterns. To ensure reliable decisions, we incorporate a Large\nLanguage Model (LLM) as an auxiliary reviewer to validate the clustered groups,\neffectively mimicking a secondary human judgment. We also introduce a growth\ncurve-based visualization to assist both the LLM and human moderators in\nassessing leveling behavior. This collaborative approach improves the\nefficiency of bot detection workflows while maintaining explainability, thereby\nsupporting scalable and accountable bot regulation in MMORPGs."
                },
                "authors": [
                    {
                        "name": "Jaeman Son"
                    },
                    {
                        "name": "Hyunsoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsoo Kim"
                },
                "author": "Hyunsoo Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19320v2",
                "updated": "2025-08-28T09:15:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    15,
                    43,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-26T14:00:16Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    0,
                    16,
                    1,
                    238,
                    0
                ],
                "title": "MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time\n  Autoregressive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time\n  Autoregressive Video Generation"
                },
                "summary": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with heavy computational\ncost and limited controllability. In this work, we introduce an autoregressive\nvideo generation framework that enables interactive multimodal control and\nlow-latency extrapolation in a streaming manner. With minimal modifications to\na standard large language model (LLM), our framework accepts multimodal\ncondition encodings including audio, pose, and text, and outputs spatially and\nsemantically coherent representations to guide the denoising process of a\ndiffusion head. To support this, we construct a large-scale dialogue dataset of\napproximately 20,000 hours from multiple sources, providing rich conversational\nscenarios for training. We further introduce a deep compression autoencoder\nwith up to 64$\\times$ reduction ratio, which effectively alleviates the\nlong-horizon inference burden of the autoregressive model. Extensive\nexperiments on duplex conversation, multilingual human synthesis, and\ninteractive world model highlight the advantages of our approach in low\nlatency, high efficiency, and fine-grained multimodal controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with heavy computational\ncost and limited controllability. In this work, we introduce an autoregressive\nvideo generation framework that enables interactive multimodal control and\nlow-latency extrapolation in a streaming manner. With minimal modifications to\na standard large language model (LLM), our framework accepts multimodal\ncondition encodings including audio, pose, and text, and outputs spatially and\nsemantically coherent representations to guide the denoising process of a\ndiffusion head. To support this, we construct a large-scale dialogue dataset of\napproximately 20,000 hours from multiple sources, providing rich conversational\nscenarios for training. We further introduce a deep compression autoencoder\nwith up to 64$\\times$ reduction ratio, which effectively alleviates the\nlong-horizon inference burden of the autoregressive model. Extensive\nexperiments on duplex conversation, multilingual human synthesis, and\ninteractive world model highlight the advantages of our approach in low\nlatency, high efficiency, and fine-grained multimodal controllability."
                },
                "authors": [
                    {
                        "name": "Ming Chen"
                    },
                    {
                        "name": "Liyuan Cui"
                    },
                    {
                        "name": "Wenyuan Zhang"
                    },
                    {
                        "name": "Haoxian Zhang"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Songlin Tang"
                    },
                    {
                        "name": "Jiwen Liu"
                    },
                    {
                        "name": "Borui Liao"
                    },
                    {
                        "name": "Hejia Chen"
                    },
                    {
                        "name": "Xiaoqiang Liu"
                    },
                    {
                        "name": "Pengfei Wan"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Wan"
                },
                "author": "Pengfei Wan",
                "arxiv_comment": "Technical Report. Project Page: https://chenmingthu.github.io/milm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08952v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08952v6",
                "updated": "2025-08-28T09:09:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    9,
                    9,
                    46,
                    3,
                    240,
                    0
                ],
                "published": "2024-07-12T03:15:01Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    3,
                    15,
                    1,
                    4,
                    194,
                    0
                ],
                "title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection"
                },
                "summary": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Jiajun Zhu"
                    },
                    {
                        "name": "Xukai Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Yanghai Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08952v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08952v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20559v1",
                "updated": "2025-08-28T08:51:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    51,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:51:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    51,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "Leveraging Generative Models for Real-Time Query-Driven Text\n  Summarization in Large-Scale Web Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Generative Models for Real-Time Query-Driven Text\n  Summarization in Large-Scale Web Search"
                },
                "summary": "In the dynamic landscape of large-scale web search, Query-Driven Text\nSummarization (QDTS) aims to generate concise and informative summaries from\ntextual documents based on a given query, which is essential for improving user\nengagement and facilitating rapid decision-making. Traditional extractive\nsummarization models, based primarily on ranking candidate summary segments,\nhave been the dominant approach in industrial applications. However, these\napproaches suffer from two key limitations: 1) The multi-stage pipeline often\nintroduces cumulative information loss and architectural bottlenecks due to its\nweakest component; 2) Traditional models lack sufficient semantic understanding\nof both user queries and documents, particularly when dealing with complex\nsearch intents. In this study, we propose a novel framework to pioneer the\napplication of generative models to address real-time QDTS in industrial web\nsearch. Our approach integrates large model distillation, supervised\nfine-tuning, direct preference optimization, and lookahead decoding to\ntransform a lightweight model with only 0.1B parameters into a\ndomain-specialized QDTS expert. Evaluated on multiple industry-relevant\nmetrics, our model outperforms the production baseline and achieves a new state\nof the art. Furthermore, it demonstrates excellent deployment efficiency,\nrequiring only 334 NVIDIA L20 GPUs to handle \\textasciitilde50,000 queries per\nsecond under 55~ms average latency per query.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the dynamic landscape of large-scale web search, Query-Driven Text\nSummarization (QDTS) aims to generate concise and informative summaries from\ntextual documents based on a given query, which is essential for improving user\nengagement and facilitating rapid decision-making. Traditional extractive\nsummarization models, based primarily on ranking candidate summary segments,\nhave been the dominant approach in industrial applications. However, these\napproaches suffer from two key limitations: 1) The multi-stage pipeline often\nintroduces cumulative information loss and architectural bottlenecks due to its\nweakest component; 2) Traditional models lack sufficient semantic understanding\nof both user queries and documents, particularly when dealing with complex\nsearch intents. In this study, we propose a novel framework to pioneer the\napplication of generative models to address real-time QDTS in industrial web\nsearch. Our approach integrates large model distillation, supervised\nfine-tuning, direct preference optimization, and lookahead decoding to\ntransform a lightweight model with only 0.1B parameters into a\ndomain-specialized QDTS expert. Evaluated on multiple industry-relevant\nmetrics, our model outperforms the production baseline and achieves a new state\nof the art. Furthermore, it demonstrates excellent deployment efficiency,\nrequiring only 334 NVIDIA L20 GPUs to handle \\textasciitilde50,000 queries per\nsecond under 55~ms average latency per query."
                },
                "authors": [
                    {
                        "name": "Zeyu Xiong"
                    },
                    {
                        "name": "Yixuan Nan"
                    },
                    {
                        "name": "Li Gao"
                    },
                    {
                        "name": "Hengzhu Tang"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "arxiv_comment": "CIKM'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15165v2",
                "updated": "2025-08-28T08:44:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    44,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2024-05-24T02:44:14Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    2,
                    44,
                    14,
                    4,
                    145,
                    0
                ],
                "title": "SoAy: A Solution-based LLM API-using Methodology for Academic\n  Information Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoAy: A Solution-based LLM API-using Methodology for Academic\n  Information Seeking"
                },
                "summary": "Applying large language models (LLMs) for academic API usage shows promise in\nreducing researchers' academic information seeking efforts. However, current\nLLM API-using methods struggle with complex API coupling commonly encountered\nin academic queries. To address this, we introduce SoAy, a solution-based LLM\nAPI-using methodology for academic information seeking. It uses code with a\nsolution as the reasoning method, where a solution is a pre-constructed API\ncalling sequence. The addition of the solution reduces the difficulty for the\nmodel to understand the complex relationships between APIs. Code improves the\nefficiency of reasoning.\n  To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied\nby SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental\nresults demonstrate a 34.58-75.99\\% performance improvement compared to\nstate-of-the-art LLM API-based baselines. All datasets, codes, tuned models,\nand deployed online services are publicly accessible at\nhttps://github.com/RUCKBReasoning/SoAy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying large language models (LLMs) for academic API usage shows promise in\nreducing researchers' academic information seeking efforts. However, current\nLLM API-using methods struggle with complex API coupling commonly encountered\nin academic queries. To address this, we introduce SoAy, a solution-based LLM\nAPI-using methodology for academic information seeking. It uses code with a\nsolution as the reasoning method, where a solution is a pre-constructed API\ncalling sequence. The addition of the solution reduces the difficulty for the\nmodel to understand the complex relationships between APIs. Code improves the\nefficiency of reasoning.\n  To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied\nby SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental\nresults demonstrate a 34.58-75.99\\% performance improvement compared to\nstate-of-the-art LLM API-based baselines. All datasets, codes, tuned models,\nand deployed online services are publicly accessible at\nhttps://github.com/RUCKBReasoning/SoAy."
                },
                "authors": [
                    {
                        "name": "Yuanchun Wang"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yuyang Xie"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Yiyang Fu"
                    },
                    {
                        "name": "Youhe Feng"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Jingyao Zhang"
                    },
                    {
                        "name": "Bowen Huang"
                    },
                    {
                        "name": "Yuanyao Li"
                    },
                    {
                        "name": "Huihui Yuan"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_doi": "10.1145/3690624.3709412",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3690624.3709412",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.15165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "KDD 2025; 22 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06989v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06989v4",
                "updated": "2025-08-28T08:43:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    43,
                    31,
                    3,
                    240,
                    0
                ],
                "published": "2025-03-10T07:10:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    10,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From\n  Quantification to Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From\n  Quantification to Application"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal content. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on input image to maximize jailbreak probability, and further\nenhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To\ncounteract attacks, we also propose Jailbreak-Probability-based Finetuning\n(JPF), which minimizes jailbreak probability through MLLM parameter updates.\nExtensive experiments show that (1) (M)JPA yields significant improvements when\nattacking a wide range of models under both white and black box settings. (2)\nJPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal content. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on input image to maximize jailbreak probability, and further\nenhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To\ncounteract attacks, we also propose Jailbreak-Probability-based Finetuning\n(JPF), which minimizes jailbreak probability through MLLM parameter updates.\nExtensive experiments show that (1) (M)JPA yields significant improvements when\nattacking a wide range of models under both white and black box settings. (2)\nJPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities."
                },
                "authors": [
                    {
                        "name": "Wenzhuo Xu"
                    },
                    {
                        "name": "Zhipeng Wei"
                    },
                    {
                        "name": "Xiongtao Sun"
                    },
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Deyue Zhang"
                    },
                    {
                        "name": "Dongdong Yang"
                    },
                    {
                        "name": "Xiangzheng Zhang"
                    },
                    {
                        "name": "Quanchen Zou"
                    }
                ],
                "author_detail": {
                    "name": "Quanchen Zou"
                },
                "author": "Quanchen Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06989v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06989v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20534v1",
                "updated": "2025-08-28T08:21:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    21,
                    10,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:21:10Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    21,
                    10,
                    3,
                    240,
                    0
                ],
                "title": "Digital Scale: Open-Source On-Device BMI Estimation from Smartphone\n  Camera Images Trained on a Large-Scale Real-World Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Scale: Open-Source On-Device BMI Estimation from Smartphone\n  Camera Images Trained on a Large-Scale Real-World Dataset"
                },
                "summary": "Estimating Body Mass Index (BMI) from camera images with machine learning\nmodels enables rapid weight assessment when traditional methods are unavailable\nor impractical, such as in telehealth or emergency scenarios. Existing computer\nvision approaches have been limited to datasets of up to 14,500 images. In this\nstudy, we present a deep learning-based BMI estimation method trained on our\nWayBED dataset, a large proprietary collection of 84,963 smartphone images from\n25,353 individuals. We introduce an automatic filtering method that uses\nposture clustering and person detection to curate the dataset by removing\nlow-quality images, such as those with atypical postures or incomplete views.\nThis process retained 71,322 high-quality images suitable for training. We\nachieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test\nset (WayBED data) using full-body images, the lowest value in the published\nliterature to the best of our knowledge. Further, we achieve a MAPE of 13% on\nthe completely unseen~(during training) VisualBodyToBMI dataset, comparable\nwith state-of-the-art approaches trained on it, demonstrating robust\ngeneralization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a\nMAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the\nfull pipeline, including image filtering and BMI estimation, on Android devices\nusing the CLAID framework. We release our complete code for model training,\nfiltering, and the CLAID package for mobile deployment as open-source\ncontributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Body Mass Index (BMI) from camera images with machine learning\nmodels enables rapid weight assessment when traditional methods are unavailable\nor impractical, such as in telehealth or emergency scenarios. Existing computer\nvision approaches have been limited to datasets of up to 14,500 images. In this\nstudy, we present a deep learning-based BMI estimation method trained on our\nWayBED dataset, a large proprietary collection of 84,963 smartphone images from\n25,353 individuals. We introduce an automatic filtering method that uses\nposture clustering and person detection to curate the dataset by removing\nlow-quality images, such as those with atypical postures or incomplete views.\nThis process retained 71,322 high-quality images suitable for training. We\nachieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test\nset (WayBED data) using full-body images, the lowest value in the published\nliterature to the best of our knowledge. Further, we achieve a MAPE of 13% on\nthe completely unseen~(during training) VisualBodyToBMI dataset, comparable\nwith state-of-the-art approaches trained on it, demonstrating robust\ngeneralization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a\nMAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the\nfull pipeline, including image filtering and BMI estimation, on Android devices\nusing the CLAID framework. We release our complete code for model training,\nfiltering, and the CLAID package for mobile deployment as open-source\ncontributions."
                },
                "authors": [
                    {
                        "name": "Frederik Rajiv Manichand"
                    },
                    {
                        "name": "Robin Deuber"
                    },
                    {
                        "name": "Robert Jakob"
                    },
                    {
                        "name": "Steve Swerling"
                    },
                    {
                        "name": "Jamie Rosen"
                    },
                    {
                        "name": "Elgar Fleisch"
                    },
                    {
                        "name": "Patrick Langer"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Langer"
                },
                "author": "Patrick Langer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17387v2",
                "updated": "2025-08-28T08:20:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    20,
                    32,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-24T14:49:02Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    14,
                    49,
                    2,
                    6,
                    236,
                    0
                ],
                "title": "Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs\n  via Explicit Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs\n  via Explicit Reasoning"
                },
                "summary": "Generalizing to unseen graph tasks without task-pecific supervision remains\nchallenging. Graph Neural Networks (GNNs) are limited by fixed label spaces,\nwhile Large Language Models (LLMs) lack structural inductive biases. Recent\nadvances in Large Reasoning Models (LRMs) provide a zero-shot alternative via\nexplicit, long chain-of-thought reasoning. Inspired by this, we propose a\nGNN-free approach that reformulates graph tasks--node classification, link\nprediction, and graph classification--as textual reasoning problems solved by\nLRMs. We introduce the first datasets with detailed reasoning traces for these\ntasks and develop Graph-R1, a reinforcement learning framework that leverages\ntask-specific rethink templates to guide reasoning over linearized graphs.\nExperiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in\nzero-shot settings, producing interpretable and effective predictions. Our work\nhighlights the promise of explicit reasoning for graph learning and provides\nnew resources for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing to unseen graph tasks without task-pecific supervision remains\nchallenging. Graph Neural Networks (GNNs) are limited by fixed label spaces,\nwhile Large Language Models (LLMs) lack structural inductive biases. Recent\nadvances in Large Reasoning Models (LRMs) provide a zero-shot alternative via\nexplicit, long chain-of-thought reasoning. Inspired by this, we propose a\nGNN-free approach that reformulates graph tasks--node classification, link\nprediction, and graph classification--as textual reasoning problems solved by\nLRMs. We introduce the first datasets with detailed reasoning traces for these\ntasks and develop Graph-R1, a reinforcement learning framework that leverages\ntask-specific rethink templates to guide reasoning over linearized graphs.\nExperiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in\nzero-shot settings, producing interpretable and effective predictions. Our work\nhighlights the promise of explicit reasoning for graph learning and provides\nnew resources for future research."
                },
                "authors": [
                    {
                        "name": "Yicong Wu"
                    },
                    {
                        "name": "Guangyue Lu"
                    },
                    {
                        "name": "Yuan Zuo"
                    },
                    {
                        "name": "Huarong Zhang"
                    },
                    {
                        "name": "Junjie Wu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Wu"
                },
                "author": "Junjie Wu",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20525v1",
                "updated": "2025-08-28T08:06:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    6,
                    33,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:06:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    6,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "Enhancing Health Fact-Checking with LLM-Generated Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Health Fact-Checking with LLM-Generated Synthetic Data"
                },
                "summary": "Fact-checking for health-related content is challenging due to the limited\navailability of annotated training data. In this study, we propose a synthetic\ndata generation pipeline that leverages large language models (LLMs) to augment\ntraining data for health-related fact checking. In this pipeline, we summarize\nsource documents, decompose the summaries into atomic facts, and use an LLM to\nconstruct sentence-fact entailment tables. From the entailment relations in the\ntable, we further generate synthetic text-claim pairs with binary veracity\nlabels. These synthetic data are then combined with the original data to\nfine-tune a BERT-based fact-checking model. Evaluation on two public datasets,\nPubHealth and SciFact, shows that our pipeline improved F1 scores by up to\n0.019 and 0.049, respectively, compared to models trained only on the original\ndata. These results highlight the effectiveness of LLM-driven synthetic data\naugmentation in enhancing the performance of health-related fact-checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking for health-related content is challenging due to the limited\navailability of annotated training data. In this study, we propose a synthetic\ndata generation pipeline that leverages large language models (LLMs) to augment\ntraining data for health-related fact checking. In this pipeline, we summarize\nsource documents, decompose the summaries into atomic facts, and use an LLM to\nconstruct sentence-fact entailment tables. From the entailment relations in the\ntable, we further generate synthetic text-claim pairs with binary veracity\nlabels. These synthetic data are then combined with the original data to\nfine-tune a BERT-based fact-checking model. Evaluation on two public datasets,\nPubHealth and SciFact, shows that our pipeline improved F1 scores by up to\n0.019 and 0.049, respectively, compared to models trained only on the original\ndata. These results highlight the effectiveness of LLM-driven synthetic data\naugmentation in enhancing the performance of health-related fact-checkers."
                },
                "authors": [
                    {
                        "name": "Jingze Zhang"
                    },
                    {
                        "name": "Jiahe Qian"
                    },
                    {
                        "name": "Yiliang Zhou"
                    },
                    {
                        "name": "Yifan Peng"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Peng"
                },
                "author": "Yifan Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18948v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18948v5",
                "updated": "2025-08-29T06:03:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    6,
                    3,
                    42,
                    4,
                    241,
                    0
                ],
                "published": "2024-11-28T06:29:46Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    6,
                    29,
                    46,
                    3,
                    333,
                    0
                ],
                "title": "RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation\n  through LLM Activation Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation\n  through LLM Activation Analysis"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving\ninformation from the relevant knowledge database, enabling them to produce\nresponses that are more accurate and contextually appropriate. It is worth\nnoting that the knowledge database, being sourced from publicly available\nchannels such as Wikipedia, inevitably introduces a new attack surface. RAG\npoisoning involves injecting malicious texts into the knowledge database,\nultimately leading to the generation of the attacker's target response (also\ncalled poisoned response). However, there are currently limited methods\navailable for detecting such poisoning attacks. We aim to bridge the gap in\nthis work. Particularly, we introduce RevPRAG, a flexible and automated\ndetection pipeline that leverages the activations of LLMs for poisoned response\ndetection. Our investigation uncovers distinct patterns in LLMs' activations\nwhen generating correct responses versus poisoned responses. Our results on\nmultiple benchmark datasets and RAG architectures show our approach could\nachieve 98% true positive rate, while maintaining false positive rates close to\n1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving\ninformation from the relevant knowledge database, enabling them to produce\nresponses that are more accurate and contextually appropriate. It is worth\nnoting that the knowledge database, being sourced from publicly available\nchannels such as Wikipedia, inevitably introduces a new attack surface. RAG\npoisoning involves injecting malicious texts into the knowledge database,\nultimately leading to the generation of the attacker's target response (also\ncalled poisoned response). However, there are currently limited methods\navailable for detecting such poisoning attacks. We aim to bridge the gap in\nthis work. Particularly, we introduce RevPRAG, a flexible and automated\ndetection pipeline that leverages the activations of LLMs for poisoned response\ndetection. Our investigation uncovers distinct patterns in LLMs' activations\nwhen generating correct responses versus poisoned responses. Our results on\nmultiple benchmark datasets and RAG architectures show our approach could\nachieve 98% true positive rate, while maintaining false positive rates close to\n1%."
                },
                "authors": [
                    {
                        "name": "Xue Tan"
                    },
                    {
                        "name": "Hao Luan"
                    },
                    {
                        "name": "Mingyu Luo"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Jun Dai"
                    }
                ],
                "author_detail": {
                    "name": "Jun Dai"
                },
                "author": "Jun Dai",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18948v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18948v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]