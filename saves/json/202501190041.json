[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v3",
                "updated": "2025-01-14T20:04:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    4,
                    15,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v1",
                "updated": "2025-01-13T17:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_doi": "10.3847/1538-4365/ad9b8d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4365/ad9b8d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "arxiv_journal_ref": "Astrophys. j., suppl. ser. 276 (2025) 40",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v1",
                "updated": "2024-12-25T10:11:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently serving large multimedia models using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large multimedia models using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Ivan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.09757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09757v1",
                "updated": "2025-01-16T18:59:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    53,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:59:53Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    53,
                    3,
                    16,
                    0
                ],
                "title": "Distilling Multi-modal Large Language Models for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Multi-modal Large Language Models for Autonomous Driving"
                },
                "summary": "Autonomous driving demands safe motion planning, especially in critical\n\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\nlarge language models (LLMs) as planners to improve generalizability to rare\nevents. However, using LLMs at test time introduces high computational costs.\nTo address this, we propose DiMA, an end-to-end autonomous driving system that\nmaintains the efficiency of an LLM-free (or vision-based) planner while\nleveraging the world knowledge of an LLM. DiMA distills the information from a\nmulti-modal LLM to a vision-based end-to-end planner through a set of specially\ndesigned surrogate tasks. Under a joint training strategy, a scene encoder\ncommon to both networks produces structured representations that are\nsemantically grounded as well as aligned to the final planning objective.\nNotably, the LLM is optional at inference, enabling robust planning without\ncompromising on efficiency. Training with DiMA results in a 37% reduction in\nthe L2 trajectory error and an 80% reduction in the collision rate of the\nvision-based planner, as well as a 44% trajectory error reduction in longtail\nscenarios. DiMA also achieves state-of-the-art performance on the nuScenes\nplanning benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving demands safe motion planning, especially in critical\n\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\nlarge language models (LLMs) as planners to improve generalizability to rare\nevents. However, using LLMs at test time introduces high computational costs.\nTo address this, we propose DiMA, an end-to-end autonomous driving system that\nmaintains the efficiency of an LLM-free (or vision-based) planner while\nleveraging the world knowledge of an LLM. DiMA distills the information from a\nmulti-modal LLM to a vision-based end-to-end planner through a set of specially\ndesigned surrogate tasks. Under a joint training strategy, a scene encoder\ncommon to both networks produces structured representations that are\nsemantically grounded as well as aligned to the final planning objective.\nNotably, the LLM is optional at inference, enabling robust planning without\ncompromising on efficiency. Training with DiMA results in a 37% reduction in\nthe L2 trajectory error and an 80% reduction in the collision rate of the\nvision-based planner, as well as a 44% trajectory error reduction in longtail\nscenarios. DiMA also achieves state-of-the-art performance on the nuScenes\nplanning benchmark."
                },
                "authors": [
                    {
                        "name": "Deepti Hegde"
                    },
                    {
                        "name": "Rajeev Yasarla"
                    },
                    {
                        "name": "Hong Cai"
                    },
                    {
                        "name": "Shizhong Han"
                    },
                    {
                        "name": "Apratim Bhattacharyya"
                    },
                    {
                        "name": "Shweta Mahajan"
                    },
                    {
                        "name": "Litian Liu"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Vishal M. Patel"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09756v1",
                "updated": "2025-01-16T18:59:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    48,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:59:48Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    48,
                    3,
                    16,
                    0
                ],
                "title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to\n  Re-render Synthetic Faces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynthLight: Portrait Relighting with Diffusion Model by Learning to\n  Re-render Synthetic Faces"
                },
                "summary": "We introduce SynthLight, a diffusion model for portrait relighting. Our\napproach frames image relighting as a re-rendering problem, where pixels are\ntransformed in response to changes in environmental lighting conditions. Using\na physically-based rendering engine, we synthesize a dataset to simulate this\nlighting-conditioned transformation with 3D head assets under varying lighting.\nWe propose two training and inference strategies to bridge the gap between the\nsynthetic and real image domains: (1) multi-task training that takes advantage\nof real human portraits without lighting labels; (2) an inference time\ndiffusion sampling procedure based on classifier-free guidance that leverages\nthe input portrait to better preserve details. Our method generalizes to\ndiverse real photographs and produces realistic illumination effects, including\nspecular highlights and cast shadows, while preserving the subject's identity.\nOur quantitative experiments on Light Stage data demonstrate results comparable\nto state-of-the-art relighting methods. Our qualitative results on in-the-wild\nimages showcase rich and unprecedented illumination effects. Project Page:\n\\url{https://vrroom.github.io/synthlight/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SynthLight, a diffusion model for portrait relighting. Our\napproach frames image relighting as a re-rendering problem, where pixels are\ntransformed in response to changes in environmental lighting conditions. Using\na physically-based rendering engine, we synthesize a dataset to simulate this\nlighting-conditioned transformation with 3D head assets under varying lighting.\nWe propose two training and inference strategies to bridge the gap between the\nsynthetic and real image domains: (1) multi-task training that takes advantage\nof real human portraits without lighting labels; (2) an inference time\ndiffusion sampling procedure based on classifier-free guidance that leverages\nthe input portrait to better preserve details. Our method generalizes to\ndiverse real photographs and produces realistic illumination effects, including\nspecular highlights and cast shadows, while preserving the subject's identity.\nOur quantitative experiments on Light Stage data demonstrate results comparable\nto state-of-the-art relighting methods. Our qualitative results on in-the-wild\nimages showcase rich and unprecedented illumination effects. Project Page:\n\\url{https://vrroom.github.io/synthlight/}"
                },
                "authors": [
                    {
                        "name": "Sumit Chaturvedi"
                    },
                    {
                        "name": "Mengwei Ren"
                    },
                    {
                        "name": "Yannick Hold-Geoffroy"
                    },
                    {
                        "name": "Jingyuan Liu"
                    },
                    {
                        "name": "Julie Dorsey"
                    },
                    {
                        "name": "Zhixin Shu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixin Shu"
                },
                "author": "Zhixin Shu",
                "arxiv_comment": "27 pages, 25 figures, Project Page\n  https://vrroom.github.io/synthlight/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09754v1",
                "updated": "2025-01-16T18:59:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    3,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:59:03Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    3,
                    3,
                    16,
                    0
                ],
                "title": "Lost in Translation, Found in Context: Sign Language Translation with\n  Contextual Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Translation, Found in Context: Sign Language Translation with\n  Contextual Cues"
                },
                "summary": "Our objective is to translate continuous sign language into spoken language\ntext. Inspired by the way human interpreters rely on context for accurate\ntranslation, we incorporate additional contextual cues together with the\nsigning video, into a new translation framework. Specifically, besides visual\nsign recognition features that encode the input video, we integrate\ncomplementary textual information from (i) captions describing the background\nshow, (ii) translation of previous sentences, as well as (iii) pseudo-glosses\ntranscribing the signing. These are automatically extracted and inputted along\nwith the visual features to a pre-trained large language model (LLM), which we\nfine-tune to generate spoken language translations in text form. Through\nextensive ablation studies, we show the positive contribution of each input cue\nto the translation performance. We train and evaluate our approach on BOBSL --\nthe largest British Sign Language dataset currently available. We show that our\ncontextual approach significantly enhances the quality of the translations\ncompared to previously reported results on BOBSL, and also to state-of-the-art\nmethods that we implement as baselines. Furthermore, we demonstrate the\ngenerality of our approach by applying it also to How2Sign, an American Sign\nLanguage dataset, and achieve competitive results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our objective is to translate continuous sign language into spoken language\ntext. Inspired by the way human interpreters rely on context for accurate\ntranslation, we incorporate additional contextual cues together with the\nsigning video, into a new translation framework. Specifically, besides visual\nsign recognition features that encode the input video, we integrate\ncomplementary textual information from (i) captions describing the background\nshow, (ii) translation of previous sentences, as well as (iii) pseudo-glosses\ntranscribing the signing. These are automatically extracted and inputted along\nwith the visual features to a pre-trained large language model (LLM), which we\nfine-tune to generate spoken language translations in text form. Through\nextensive ablation studies, we show the positive contribution of each input cue\nto the translation performance. We train and evaluate our approach on BOBSL --\nthe largest British Sign Language dataset currently available. We show that our\ncontextual approach significantly enhances the quality of the translations\ncompared to previously reported results on BOBSL, and also to state-of-the-art\nmethods that we implement as baselines. Furthermore, we demonstrate the\ngenerality of our approach by applying it also to How2Sign, an American Sign\nLanguage dataset, and achieve competitive results."
                },
                "authors": [
                    {
                        "name": "Youngjoon Jang"
                    },
                    {
                        "name": "Haran Raajesh"
                    },
                    {
                        "name": "Liliane Momeni"
                    },
                    {
                        "name": "Gül Varol"
                    },
                    {
                        "name": "Andrew Zisserman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zisserman"
                },
                "author": "Andrew Zisserman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12953v2",
                "updated": "2025-01-16T18:58:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    58,
                    31,
                    3,
                    16,
                    0
                ],
                "published": "2024-03-19T17:55:22Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    55,
                    22,
                    1,
                    79,
                    0
                ],
                "title": "FutureDepth: Learning to Predict the Future Improves Video Depth\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureDepth: Learning to Predict the Future Improves Video Depth\n  Estimation"
                },
                "summary": "In this paper, we propose a novel video depth estimation approach,\nFutureDepth, which enables the model to implicitly leverage multi-frame and\nmotion cues to improve depth estimation by making it learn to predict the\nfuture at training. More specifically, we propose a future prediction network,\nF-Net, which takes the features of multiple consecutive frames and is trained\nto predict multi-frame features one time step ahead iteratively. In this way,\nF-Net learns the underlying motion and correspondence information, and we\nincorporate its features into the depth decoding process. Additionally, to\nenrich the learning of multiframe correspondence cues, we further leverage a\nreconstruction network, R-Net, which is trained via adaptively masked\nauto-encoding of multiframe feature volumes. At inference time, both F-Net and\nR-Net are used to produce queries to work with the depth decoder, as well as a\nfinal refinement network. Through extensive experiments on several benchmarks,\ni.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and\nopen-domain scenarios, we show that FutureDepth significantly improves upon\nbaseline models, outperforms existing video depth estimation methods, and sets\nnew state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more\nefficient than existing SOTA video depth estimation models and has similar\nlatencies when comparing to monocular models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel video depth estimation approach,\nFutureDepth, which enables the model to implicitly leverage multi-frame and\nmotion cues to improve depth estimation by making it learn to predict the\nfuture at training. More specifically, we propose a future prediction network,\nF-Net, which takes the features of multiple consecutive frames and is trained\nto predict multi-frame features one time step ahead iteratively. In this way,\nF-Net learns the underlying motion and correspondence information, and we\nincorporate its features into the depth decoding process. Additionally, to\nenrich the learning of multiframe correspondence cues, we further leverage a\nreconstruction network, R-Net, which is trained via adaptively masked\nauto-encoding of multiframe feature volumes. At inference time, both F-Net and\nR-Net are used to produce queries to work with the depth decoder, as well as a\nfinal refinement network. Through extensive experiments on several benchmarks,\ni.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and\nopen-domain scenarios, we show that FutureDepth significantly improves upon\nbaseline models, outperforms existing video depth estimation methods, and sets\nnew state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more\nefficient than existing SOTA video depth estimation models and has similar\nlatencies when comparing to monocular models"
                },
                "authors": [
                    {
                        "name": "Rajeev Yasarla"
                    },
                    {
                        "name": "Manish Kumar Singh"
                    },
                    {
                        "name": "Hong Cai"
                    },
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Jisoo Jeong"
                    },
                    {
                        "name": "Yinhao Zhu"
                    },
                    {
                        "name": "Shizhong Han"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "arxiv_comment": "ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09749v1",
                "updated": "2025-01-16T18:57:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    57,
                    20,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:57:20Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    57,
                    20,
                    3,
                    16,
                    0
                ],
                "title": "Enhancing Lexicon-Based Text Embeddings with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lexicon-Based Text Embeddings with Large Language Models"
                },
                "summary": "Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR)."
                },
                "authors": [
                    {
                        "name": "Yibin Lei"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Andrew Yates"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Yates"
                },
                "author": "Andrew Yates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08965v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08965v3",
                "updated": "2025-01-16T18:56:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    56,
                    27,
                    3,
                    16,
                    0
                ],
                "published": "2024-05-14T21:12:01Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    21,
                    12,
                    1,
                    1,
                    135,
                    0
                ],
                "title": "Meaning-Typed Programming: Language-level Abstractions and Runtime for\n  GenAI Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meaning-Typed Programming: Language-level Abstractions and Runtime for\n  GenAI Applications"
                },
                "summary": "Software is rapidly evolving from being programmed with traditional logical\ncode, to neuro-integrated applications that leverage generative AI and large\nlanguage models (LLMs) for application functionality. This shift increases the\ncomplexity of building applications, as developers now must reasoning about,\nprogram, and prompt LLMs. Despite efforts to create tools to assist with prompt\nengineering, these solutions often introduce additional layers of complexity to\nthe development of neuro-integrated applications. This paper proposes\nmeaning-typed programming (MTP), a novel approach to simplify the creation of\nneuro-integrated applications by introducing new language-level abstractions\nthat hide the complexities of LLM integration. Our key insight is that typical\nconventional code already possesses a high level of semantic richness that can\nbe automatically reasoned about, as it is designed to be readable and\nmaintainable by humans. Leveraging this insight, we conceptualize LLMs as\nmeaning-typed code constructs and introduce a by abstraction at the language\nlevel, MT-IR, a new meaning-based intermediate representation at the compiler\nlevel, and MT Runtime, an automated run-time engine for LLM integration and\noperations. We implement MTP in a production-grade Python super-set language\ncalled Jac and perform an extensive evaluation. Our results demonstrate that\nMTP not only simplifies the development process but also meets or exceeds the\nefficacy of state-of-the-art manual and tool-assisted prompt engineering\ntechniques in terms of accuracy and usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software is rapidly evolving from being programmed with traditional logical\ncode, to neuro-integrated applications that leverage generative AI and large\nlanguage models (LLMs) for application functionality. This shift increases the\ncomplexity of building applications, as developers now must reasoning about,\nprogram, and prompt LLMs. Despite efforts to create tools to assist with prompt\nengineering, these solutions often introduce additional layers of complexity to\nthe development of neuro-integrated applications. This paper proposes\nmeaning-typed programming (MTP), a novel approach to simplify the creation of\nneuro-integrated applications by introducing new language-level abstractions\nthat hide the complexities of LLM integration. Our key insight is that typical\nconventional code already possesses a high level of semantic richness that can\nbe automatically reasoned about, as it is designed to be readable and\nmaintainable by humans. Leveraging this insight, we conceptualize LLMs as\nmeaning-typed code constructs and introduce a by abstraction at the language\nlevel, MT-IR, a new meaning-based intermediate representation at the compiler\nlevel, and MT Runtime, an automated run-time engine for LLM integration and\noperations. We implement MTP in a production-grade Python super-set language\ncalled Jac and perform an extensive evaluation. Our results demonstrate that\nMTP not only simplifies the development process but also meets or exceeds the\nefficacy of state-of-the-art manual and tool-assisted prompt engineering\ntechniques in terms of accuracy and usability."
                },
                "authors": [
                    {
                        "name": "Jason Mars"
                    },
                    {
                        "name": "Yiping Kang"
                    },
                    {
                        "name": "Jayanaka L. Dantanarayana"
                    },
                    {
                        "name": "Kugesan Sivasothynathan"
                    },
                    {
                        "name": "Christopher Clarke"
                    },
                    {
                        "name": "Baichuan Li"
                    },
                    {
                        "name": "Krisztian Flautner"
                    },
                    {
                        "name": "Lingjia Tang"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Tang"
                },
                "author": "Lingjia Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08965v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08965v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09745v1",
                "updated": "2025-01-16T18:55:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    55,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:55:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    55,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Suggesting Code Edits in Interactive Machine Learning Notebooks Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suggesting Code Edits in Interactive Machine Learning Notebooks Using\n  Large Language Models"
                },
                "summary": "Machine learning developers frequently use interactive computational\nnotebooks, such as Jupyter notebooks, to host code for data processing and\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\nlearning pipelines and interactively observing outputs, however, maintaining\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\ndue to the length and complexity of the notebooks. Moreover, there is no\nexisting benchmark related to developer edits on Jupyter notebooks. To address\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\nperform the first study of the using LLMs to predict code edits in Jupyter\nnotebooks. Our dataset captures granular details of cell-level and line-level\nmodifications, offering a foundation for understanding real-world maintenance\npatterns in machine learning workflows. We observed that the edits on Jupyter\nnotebooks are highly localized, with changes averaging only 166 lines of code\nin repositories. While larger models outperform smaller counterparts in code\nediting, all models have low accuracy on our dataset even after finetuning,\ndemonstrating the complexity of real-world machine learning maintenance tasks.\nOur findings emphasize the critical role of contextual information in improving\nmodel performance and point toward promising avenues for advancing large\nlanguage models' capabilities in engineering machine learning code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning developers frequently use interactive computational\nnotebooks, such as Jupyter notebooks, to host code for data processing and\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\nlearning pipelines and interactively observing outputs, however, maintaining\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\ndue to the length and complexity of the notebooks. Moreover, there is no\nexisting benchmark related to developer edits on Jupyter notebooks. To address\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\nperform the first study of the using LLMs to predict code edits in Jupyter\nnotebooks. Our dataset captures granular details of cell-level and line-level\nmodifications, offering a foundation for understanding real-world maintenance\npatterns in machine learning workflows. We observed that the edits on Jupyter\nnotebooks are highly localized, with changes averaging only 166 lines of code\nin repositories. While larger models outperform smaller counterparts in code\nediting, all models have low accuracy on our dataset even after finetuning,\ndemonstrating the complexity of real-world machine learning maintenance tasks.\nOur findings emphasize the critical role of contextual information in improving\nmodel performance and point toward promising avenues for advancing large\nlanguage models' capabilities in engineering machine learning code."
                },
                "authors": [
                    {
                        "name": "Bihui Jin"
                    },
                    {
                        "name": "Jiayue Wang"
                    },
                    {
                        "name": "Pengyu Nie"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Nie"
                },
                "author": "Pengyu Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09732v1",
                "updated": "2025-01-16T18:30:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    30,
                    37,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:30:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    30,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\n  Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\n  Steps"
                },
                "summary": "Generative models have made significant impacts across various domains,\nlargely due to their ability to scale during training by increasing data,\ncomputational resources, and model size, a phenomenon characterized by the\nscaling laws. Recent research has begun to explore inference-time scaling\nbehavior in Large Language Models (LLMs), revealing how performance can further\nimprove with additional computation during inference. Unlike LLMs, diffusion\nmodels inherently possess the flexibility to adjust inference-time computation\nvia the number of denoising steps, although the performance gains typically\nflatten after a few dozen. In this work, we explore the inference-time scaling\nbehavior of diffusion models beyond increasing denoising steps and investigate\nhow the generation performance can further improve with increased computation.\nSpecifically, we consider a search problem aimed at identifying better noises\nfor the diffusion sampling process. We structure the design space along two\naxes: the verifiers used to provide feedback, and the algorithms used to find\nbetter noise candidates. Through extensive experiments on class-conditioned and\ntext-conditioned image generation benchmarks, our findings reveal that\nincreasing inference-time compute leads to substantial improvements in the\nquality of samples generated by diffusion models, and with the complicated\nnature of images, combinations of the components in the framework can be\nspecifically chosen to conform with different application scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have made significant impacts across various domains,\nlargely due to their ability to scale during training by increasing data,\ncomputational resources, and model size, a phenomenon characterized by the\nscaling laws. Recent research has begun to explore inference-time scaling\nbehavior in Large Language Models (LLMs), revealing how performance can further\nimprove with additional computation during inference. Unlike LLMs, diffusion\nmodels inherently possess the flexibility to adjust inference-time computation\nvia the number of denoising steps, although the performance gains typically\nflatten after a few dozen. In this work, we explore the inference-time scaling\nbehavior of diffusion models beyond increasing denoising steps and investigate\nhow the generation performance can further improve with increased computation.\nSpecifically, we consider a search problem aimed at identifying better noises\nfor the diffusion sampling process. We structure the design space along two\naxes: the verifiers used to provide feedback, and the algorithms used to find\nbetter noise candidates. Through extensive experiments on class-conditioned and\ntext-conditioned image generation benchmarks, our findings reveal that\nincreasing inference-time compute leads to substantial improvements in the\nquality of samples generated by diffusion models, and with the complicated\nnature of images, combinations of the components in the framework can be\nspecifically chosen to conform with different application scenario."
                },
                "authors": [
                    {
                        "name": "Nanye Ma"
                    },
                    {
                        "name": "Shangyuan Tong"
                    },
                    {
                        "name": "Haolin Jia"
                    },
                    {
                        "name": "Hexiang Hu"
                    },
                    {
                        "name": "Yu-Chuan Su"
                    },
                    {
                        "name": "Mingda Zhang"
                    },
                    {
                        "name": "Xuan Yang"
                    },
                    {
                        "name": "Yandong Li"
                    },
                    {
                        "name": "Tommi Jaakkola"
                    },
                    {
                        "name": "Xuhui Jia"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09731v1",
                "updated": "2025-01-16T18:30:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    30,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:30:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    30,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "Predictions as Surrogates: Revisiting Surrogate Outcomes in the Age of\n  AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictions as Surrogates: Revisiting Surrogate Outcomes in the Age of\n  AI"
                },
                "summary": "We establish a formal connection between the decades-old surrogate outcome\nmodel in biostatistics and economics and the emerging field of\nprediction-powered inference (PPI). The connection treats predictions from\npre-trained models, prevalent in the age of AI, as cost-effective surrogates\nfor expensive outcomes. Building on the surrogate outcomes literature, we\ndevelop recalibrated prediction-powered inference, a more efficient approach to\nstatistical inference than existing PPI proposals. Our method departs from the\nexisting proposals by using flexible machine learning techniques to learn the\noptimal ``imputed loss'' through a step we call recalibration. Importantly, the\nmethod always improves upon the estimator that relies solely on the data with\navailable true outcomes, even when the optimal imputed loss is estimated\nimperfectly, and it achieves the smallest asymptotic variance among PPI\nestimators if the estimate is consistent. Computationally, our optimization\nobjective is convex whenever the loss function that defines the target\nparameter is convex. We further analyze the benefits of recalibration, both\ntheoretically and numerically, in several common scenarios where machine\nlearning predictions systematically deviate from the outcome of interest. We\ndemonstrate significant gains in effective sample size over existing PPI\nproposals via three applications leveraging state-of-the-art machine\nlearning/AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We establish a formal connection between the decades-old surrogate outcome\nmodel in biostatistics and economics and the emerging field of\nprediction-powered inference (PPI). The connection treats predictions from\npre-trained models, prevalent in the age of AI, as cost-effective surrogates\nfor expensive outcomes. Building on the surrogate outcomes literature, we\ndevelop recalibrated prediction-powered inference, a more efficient approach to\nstatistical inference than existing PPI proposals. Our method departs from the\nexisting proposals by using flexible machine learning techniques to learn the\noptimal ``imputed loss'' through a step we call recalibration. Importantly, the\nmethod always improves upon the estimator that relies solely on the data with\navailable true outcomes, even when the optimal imputed loss is estimated\nimperfectly, and it achieves the smallest asymptotic variance among PPI\nestimators if the estimate is consistent. Computationally, our optimization\nobjective is convex whenever the loss function that defines the target\nparameter is convex. We further analyze the benefits of recalibration, both\ntheoretically and numerically, in several common scenarios where machine\nlearning predictions systematically deviate from the outcome of interest. We\ndemonstrate significant gains in effective sample size over existing PPI\nproposals via three applications leveraging state-of-the-art machine\nlearning/AI models."
                },
                "authors": [
                    {
                        "name": "Wenlong Ji"
                    },
                    {
                        "name": "Lihua Lei"
                    },
                    {
                        "name": "Tijana Zrnic"
                    }
                ],
                "author_detail": {
                    "name": "Tijana Zrnic"
                },
                "author": "Tijana Zrnic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02933v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02933v3",
                "updated": "2025-01-16T18:08:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    8,
                    22,
                    3,
                    16,
                    0
                ],
                "published": "2024-04-03T01:09:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    1,
                    9,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "NL2KQL: From Natural Language to Kusto Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL2KQL: From Natural Language to Kusto Query"
                },
                "summary": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness."
                },
                "authors": [
                    {
                        "name": "Xinye Tang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Jeremias Eichelbaum"
                    },
                    {
                        "name": "Mahan Das"
                    },
                    {
                        "name": "Alex Klein"
                    },
                    {
                        "name": "Nihal Irmak Pakis"
                    },
                    {
                        "name": "William Blum"
                    },
                    {
                        "name": "Daniel L Mace"
                    },
                    {
                        "name": "Tanvi Raja"
                    },
                    {
                        "name": "Namrata Padmanabhan"
                    },
                    {
                        "name": "Ye Xing"
                    }
                ],
                "author_detail": {
                    "name": "Ye Xing"
                },
                "author": "Ye Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02933v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02933v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09709v1",
                "updated": "2025-01-16T18:00:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    0,
                    6,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:00:06Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    0,
                    6,
                    3,
                    16,
                    0
                ],
                "title": "CyberMentor: AI Powered Learning Tool Platform to Address Diverse\n  Student Needs in Cybersecurity Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberMentor: AI Powered Learning Tool Platform to Address Diverse\n  Student Needs in Cybersecurity Education"
                },
                "summary": "Many non-traditional students in cybersecurity programs often lack access to\nadvice from peers, family members and professors, which can hinder their\neducational experiences. Additionally, these students may not fully benefit\nfrom various LLM-powered AI assistants due to issues like content relevance,\nlocality of advice, minimum expertise, and timing. This paper addresses these\nchallenges by introducing an application designed to provide comprehensive\nsupport by answering questions related to knowledge, skills, and career\npreparation advice tailored to the needs of these students. We developed a\nlearning tool platform, CyberMentor, to address the diverse needs and pain\npoints of students majoring in cybersecurity. Powered by agentic workflow and\nGenerative Large Language Models (LLMs), the platform leverages\nRetrieval-Augmented Generation (RAG) for accurate and contextually relevant\ninformation retrieval to achieve accessibility and personalization. We\ndemonstrated its value in addressing knowledge requirements for cybersecurity\neducation and for career marketability, in tackling skill requirements for\nanalytical and programming assignments, and in delivering real time on demand\nlearning support. Using three use scenarios, we showcased CyberMentor in\nfacilitating knowledge acquisition and career preparation and providing\nseamless skill-based guidance and support. We also employed the LangChain\nprompt-based evaluation methodology to evaluate the platform's impact,\nconfirming its strong performance in helpfulness, correctness, and\ncompleteness. These results underscore the system's ability to support students\nin developing practical cybersecurity skills while improving equity and\nsustainability within higher education. Furthermore, CyberMentor's open-source\ndesign allows for adaptation across other disciplines, fostering educational\ninnovation and broadening its potential impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many non-traditional students in cybersecurity programs often lack access to\nadvice from peers, family members and professors, which can hinder their\neducational experiences. Additionally, these students may not fully benefit\nfrom various LLM-powered AI assistants due to issues like content relevance,\nlocality of advice, minimum expertise, and timing. This paper addresses these\nchallenges by introducing an application designed to provide comprehensive\nsupport by answering questions related to knowledge, skills, and career\npreparation advice tailored to the needs of these students. We developed a\nlearning tool platform, CyberMentor, to address the diverse needs and pain\npoints of students majoring in cybersecurity. Powered by agentic workflow and\nGenerative Large Language Models (LLMs), the platform leverages\nRetrieval-Augmented Generation (RAG) for accurate and contextually relevant\ninformation retrieval to achieve accessibility and personalization. We\ndemonstrated its value in addressing knowledge requirements for cybersecurity\neducation and for career marketability, in tackling skill requirements for\nanalytical and programming assignments, and in delivering real time on demand\nlearning support. Using three use scenarios, we showcased CyberMentor in\nfacilitating knowledge acquisition and career preparation and providing\nseamless skill-based guidance and support. We also employed the LangChain\nprompt-based evaluation methodology to evaluate the platform's impact,\nconfirming its strong performance in helpfulness, correctness, and\ncompleteness. These results underscore the system's ability to support students\nin developing practical cybersecurity skills while improving equity and\nsustainability within higher education. Furthermore, CyberMentor's open-source\ndesign allows for adaptation across other disciplines, fostering educational\ninnovation and broadening its potential impact."
                },
                "authors": [
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Nianjun Zhou"
                    },
                    {
                        "name": "Zhixiong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhixiong Chen"
                },
                "author": "Zhixiong Chen",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09706v1",
                "updated": "2025-01-16T17:58:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    58,
                    32,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T17:58:32Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    58,
                    32,
                    3,
                    16,
                    0
                ],
                "title": "Domain Adaptation of Foundation LLMs for e-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Adaptation of Foundation LLMs for e-Commerce"
                },
                "summary": "We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains."
                },
                "authors": [
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Michael Kozielski"
                    },
                    {
                        "name": "Tala Bazazo"
                    },
                    {
                        "name": "Pavel Petrushkov"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Patrycja Cieplicka"
                    },
                    {
                        "name": "Dominika Basaj"
                    },
                    {
                        "name": "Shahram Khadivi"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Khadivi"
                },
                "author": "Shahram Khadivi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09686v1",
                "updated": "2025-01-16T17:37:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    58,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T17:37:58Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    58,
                    3,
                    16,
                    0
                ],
                "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models"
                },
                "summary": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions."
                },
                "authors": [
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Qianyue Hao"
                    },
                    {
                        "name": "Zefang Zong"
                    },
                    {
                        "name": "Jingwei Wang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Xiaochong Lan"
                    },
                    {
                        "name": "Jiahui Gong"
                    },
                    {
                        "name": "Tianjian Ouyang"
                    },
                    {
                        "name": "Fanjin Meng"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Qinglong Yang"
                    },
                    {
                        "name": "Yiwen Song"
                    },
                    {
                        "name": "Sijian Ren"
                    },
                    {
                        "name": "Xinyuan Hu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "36 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09685v1",
                "updated": "2025-01-16T17:37:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    35,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T17:37:35Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    35,
                    3,
                    16,
                    0
                ],
                "title": "Reward-Guided Controlled Generation for Inference-Time Alignment in\n  Diffusion Models: Tutorial and Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Guided Controlled Generation for Inference-Time Alignment in\n  Diffusion Models: Tutorial and Review"
                },
                "summary": "This tutorial provides an in-depth guide on inference-time guidance and\nalignment methods for optimizing downstream reward functions in diffusion\nmodels. While diffusion models are renowned for their generative modeling\ncapabilities, practical applications in fields such as biology often require\nsample generation that maximizes specific metrics (e.g., stability, affinity in\nproteins, closeness to target structures). In these scenarios, diffusion models\ncan be adapted not only to generate realistic samples but also to explicitly\nmaximize desired measures at inference time without fine-tuning. This tutorial\nexplores the foundational aspects of such inference-time algorithms. We review\nthese methods from a unified perspective, demonstrating that current techniques\n-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,\nand classifier guidance -- aim to approximate soft optimal denoising processes\n(a.k.a. policies in RL) that combine pre-trained denoising processes with value\nfunctions serving as look-ahead functions that predict from intermediate states\nto terminal rewards. Within this framework, we present several novel algorithms\nnot yet covered in the literature. Furthermore, we discuss (1) fine-tuning\nmethods combined with inference-time techniques, (2) inference-time algorithms\nbased on search algorithms such as Monte Carlo tree search, which have received\nlimited attention in current research, and (3) connections between\ninference-time algorithms in language models and diffusion models. The code of\nthis tutorial on protein design is available at\nhttps://github.com/masa-ue/AlignInversePro",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This tutorial provides an in-depth guide on inference-time guidance and\nalignment methods for optimizing downstream reward functions in diffusion\nmodels. While diffusion models are renowned for their generative modeling\ncapabilities, practical applications in fields such as biology often require\nsample generation that maximizes specific metrics (e.g., stability, affinity in\nproteins, closeness to target structures). In these scenarios, diffusion models\ncan be adapted not only to generate realistic samples but also to explicitly\nmaximize desired measures at inference time without fine-tuning. This tutorial\nexplores the foundational aspects of such inference-time algorithms. We review\nthese methods from a unified perspective, demonstrating that current techniques\n-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,\nand classifier guidance -- aim to approximate soft optimal denoising processes\n(a.k.a. policies in RL) that combine pre-trained denoising processes with value\nfunctions serving as look-ahead functions that predict from intermediate states\nto terminal rewards. Within this framework, we present several novel algorithms\nnot yet covered in the literature. Furthermore, we discuss (1) fine-tuning\nmethods combined with inference-time techniques, (2) inference-time algorithms\nbased on search algorithms such as Monte Carlo tree search, which have received\nlimited attention in current research, and (3) connections between\ninference-time algorithms in language models and diffusion models. The code of\nthis tutorial on protein design is available at\nhttps://github.com/masa-ue/AlignInversePro"
                },
                "authors": [
                    {
                        "name": "Masatoshi Uehara"
                    },
                    {
                        "name": "Yulai Zhao"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Xiner Li"
                    },
                    {
                        "name": "Aviv Regev"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Tommaso Biancalani"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Biancalani"
                },
                "author": "Tommaso Biancalani",
                "arxiv_comment": "We plan to add more content/codes. Please let us know if there are\n  any comments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10329v2",
                "updated": "2025-01-16T17:10:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    10,
                    39,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-14T18:00:00Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    18,
                    0,
                    0,
                    4,
                    166,
                    0
                ],
                "title": "Exploring the AGN Fraction of a Sample of JWST's Little Red Dots at $5 <\n  z < 8$: Overmassive Black Holes Are Strongly Favored",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the AGN Fraction of a Sample of JWST's Little Red Dots at $5 <\n  z < 8$: Overmassive Black Holes Are Strongly Favored"
                },
                "summary": "JWST is revolutionizing our view of the early Universe by pushing the\nboundaries of detectable galaxies and black holes in redshift (upward) and mass\n(downward). The Little Red Dots (LRDs), detected by several surveys at $z > 4$,\npresent a significant interpretational challenge, as their Spectral Energy\nDistributions (SED) can mimic both AGN and stellar population templates. This\nstudy analyzes 19 LRDs from the JADES survey, utilizing NIRCam and MIRI\nphotometry. By performing SED fitting across a vast parameter space, we explore\na broad range of AGN fractions, defined as the ratio of the monochromatic\nluminosities (AGN, galaxy, and dust) over a specified wavelength range, 0.4 -\n0.7 $\\mu m$ rest-frame. We find that 17 of the 19 LRDs investigated are\nconsistent with having significant AGN contributions, with best-fitting AGN\nfractions ranging between 20% and 70%, while one galaxy shows a low AGN\ncontribution (2%) and another appears to be purely star-forming. Moreover,\nassuming these LRDs do indeed host AGN, we can place limits on their black hole\nmasses using the inferred AGN bolometric luminosities and adopting the\nEddington limit. We find that, independent of the specific AGN fraction\nadopted, the LRDs' black holes are significantly overmassive relative to their\nhost galaxies -- by $\\sim 1$ dex, and up to $\\sim 4$ dex in the most extreme\ncases -- compared to the local $M_{\\bullet} - M_{\\star}$ relation. The presence\nof overmassive black holes in the high-$z$ Universe may provide the strongest\nevidence yet of heavy black hole seeding occurring during the cosmic dark ages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST is revolutionizing our view of the early Universe by pushing the\nboundaries of detectable galaxies and black holes in redshift (upward) and mass\n(downward). The Little Red Dots (LRDs), detected by several surveys at $z > 4$,\npresent a significant interpretational challenge, as their Spectral Energy\nDistributions (SED) can mimic both AGN and stellar population templates. This\nstudy analyzes 19 LRDs from the JADES survey, utilizing NIRCam and MIRI\nphotometry. By performing SED fitting across a vast parameter space, we explore\na broad range of AGN fractions, defined as the ratio of the monochromatic\nluminosities (AGN, galaxy, and dust) over a specified wavelength range, 0.4 -\n0.7 $\\mu m$ rest-frame. We find that 17 of the 19 LRDs investigated are\nconsistent with having significant AGN contributions, with best-fitting AGN\nfractions ranging between 20% and 70%, while one galaxy shows a low AGN\ncontribution (2%) and another appears to be purely star-forming. Moreover,\nassuming these LRDs do indeed host AGN, we can place limits on their black hole\nmasses using the inferred AGN bolometric luminosities and adopting the\nEddington limit. We find that, independent of the specific AGN fraction\nadopted, the LRDs' black holes are significantly overmassive relative to their\nhost galaxies -- by $\\sim 1$ dex, and up to $\\sim 4$ dex in the most extreme\ncases -- compared to the local $M_{\\bullet} - M_{\\star}$ relation. The presence\nof overmassive black holes in the high-$z$ Universe may provide the strongest\nevidence yet of heavy black hole seeding occurring during the cosmic dark ages."
                },
                "authors": [
                    {
                        "name": "Emmanuel Durodola"
                    },
                    {
                        "name": "Fabio Pacucci"
                    },
                    {
                        "name": "Ryan C. Hickox"
                    }
                ],
                "author_detail": {
                    "name": "Ryan C. Hickox"
                },
                "author": "Ryan C. Hickox",
                "arxiv_comment": "Submitted to The Astrophysical Journal. 13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05728v2",
                "updated": "2025-01-16T17:09:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    9,
                    57,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-10T05:53:32Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    53,
                    32,
                    4,
                    10,
                    0
                ],
                "title": "Super-class guided Transformer for Zero-Shot Attribute Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-class guided Transformer for Zero-Shot Attribute Classification"
                },
                "summary": "Attribute classification is crucial for identifying specific characteristics\nwithin image regions. Vision-Language Models (VLMs) have been effective in\nzero-shot tasks by leveraging their general knowledge from large-scale\ndatasets. Recent studies demonstrate that transformer-based models with\nclass-wise queries can effectively address zero-shot multi-label\nclassification. However, poor utilization of the relationship between seen and\nunseen attributes makes the model lack generalizability. Additionally,\nattribute classification generally involves many attributes, making maintaining\nthe model's scalability difficult. To address these issues, we propose\nSuper-class guided transFormer (SugaFormer), a novel framework that leverages\nsuper-classes to enhance scalability and generalizability for zero-shot\nattribute classification. SugaFormer employs Super-class Query Initialization\n(SQI) to reduce the number of queries, utilizing common semantic information\nfrom super-classes, and incorporates Multi-context Decoding (MD) to handle\ndiverse visual cues. To strengthen generalizability, we introduce two knowledge\ntransfer strategies that utilize VLMs. During training, Super-class guided\nConsistency Regularization (SCR) aligns model's features with VLMs using\nsuper-class guided prompts, and during inference, Zero-shot Retrieval-based\nScore Enhancement (ZRSE) refines predictions for unseen attributes. Extensive\nexperiments demonstrate that SugaFormer achieves state-of-the-art performance\nacross three widely-used attribute classification benchmarks under zero-shot,\nand cross-dataset transfer settings. Our code is available at\nhttps://github.com/mlvlab/SugaFormer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute classification is crucial for identifying specific characteristics\nwithin image regions. Vision-Language Models (VLMs) have been effective in\nzero-shot tasks by leveraging their general knowledge from large-scale\ndatasets. Recent studies demonstrate that transformer-based models with\nclass-wise queries can effectively address zero-shot multi-label\nclassification. However, poor utilization of the relationship between seen and\nunseen attributes makes the model lack generalizability. Additionally,\nattribute classification generally involves many attributes, making maintaining\nthe model's scalability difficult. To address these issues, we propose\nSuper-class guided transFormer (SugaFormer), a novel framework that leverages\nsuper-classes to enhance scalability and generalizability for zero-shot\nattribute classification. SugaFormer employs Super-class Query Initialization\n(SQI) to reduce the number of queries, utilizing common semantic information\nfrom super-classes, and incorporates Multi-context Decoding (MD) to handle\ndiverse visual cues. To strengthen generalizability, we introduce two knowledge\ntransfer strategies that utilize VLMs. During training, Super-class guided\nConsistency Regularization (SCR) aligns model's features with VLMs using\nsuper-class guided prompts, and during inference, Zero-shot Retrieval-based\nScore Enhancement (ZRSE) refines predictions for unseen attributes. Extensive\nexperiments demonstrate that SugaFormer achieves state-of-the-art performance\nacross three widely-used attribute classification benchmarks under zero-shot,\nand cross-dataset transfer settings. Our code is available at\nhttps://github.com/mlvlab/SugaFormer."
                },
                "authors": [
                    {
                        "name": "Sehyung Kim"
                    },
                    {
                        "name": "Chanhyeong Yang"
                    },
                    {
                        "name": "Jihwan Park"
                    },
                    {
                        "name": "Taehoon Song"
                    },
                    {
                        "name": "Hyunwoo J. Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunwoo J. Kim"
                },
                "author": "Hyunwoo J. Kim",
                "arxiv_comment": "AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09672v1",
                "updated": "2025-01-16T17:08:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    8,
                    12,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T17:08:12Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    8,
                    12,
                    3,
                    16,
                    0
                ],
                "title": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP\n  Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP\n  Evaluation Benchmark"
                },
                "summary": "The proliferation of Vision-Language Models (VLMs) in the past several years\ncalls for rigorous and comprehensive evaluation methods and benchmarks. This\nwork analyzes existing VLM evaluation techniques, including automated metrics,\nAI-based assessments, and human evaluations across diverse tasks. We first\nintroduce Robin - a novel suite of VLMs that we built by combining Large\nLanguage Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use\nRobin to identify shortcomings of current evaluation approaches across scales.\nNext, to overcome the identified limitations, we introduce CHIRP - a new long\nform response benchmark we developed for more robust and complete VLM\nevaluation. We provide open access to the Robin training code, model suite, and\nCHIRP benchmark to promote reproducibility and advance VLM research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Vision-Language Models (VLMs) in the past several years\ncalls for rigorous and comprehensive evaluation methods and benchmarks. This\nwork analyzes existing VLM evaluation techniques, including automated metrics,\nAI-based assessments, and human evaluations across diverse tasks. We first\nintroduce Robin - a novel suite of VLMs that we built by combining Large\nLanguage Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use\nRobin to identify shortcomings of current evaluation approaches across scales.\nNext, to overcome the identified limitations, we introduce CHIRP - a new long\nform response benchmark we developed for more robust and complete VLM\nevaluation. We provide open access to the Robin training code, model suite, and\nCHIRP benchmark to promote reproducibility and advance VLM research."
                },
                "authors": [
                    {
                        "name": "Alexis Roger"
                    },
                    {
                        "name": "Prateek Humane"
                    },
                    {
                        "name": "Daniel Z. Kaplan"
                    },
                    {
                        "name": "Kshitij Gupta"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "George Adamopoulos"
                    },
                    {
                        "name": "Jonathan Siu Chi Lim"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Edwin Fennell"
                    },
                    {
                        "name": "Irina Rish"
                    }
                ],
                "author_detail": {
                    "name": "Irina Rish"
                },
                "author": "Irina Rish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09655v1",
                "updated": "2025-01-16T16:51:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    51,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:51:59Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    51,
                    59,
                    3,
                    16,
                    0
                ],
                "title": "A Survey of Research in Large Language Models for Electronic Design\n  Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Research in Large Language Models for Electronic Design\n  Automation"
                },
                "summary": "Within the rapidly evolving domain of Electronic Design Automation (EDA),\nLarge Language Models (LLMs) have emerged as transformative technologies,\noffering unprecedented capabilities for optimizing and automating various\naspects of electronic design. This survey provides a comprehensive exploration\nof LLM applications in EDA, focusing on advancements in model architectures,\nthe implications of varying model sizes, and innovative customization\ntechniques that enable tailored analytical insights. By examining the\nintersection of LLM capabilities and EDA requirements, the paper highlights the\nsignificant impact these models have on extracting nuanced understandings from\ncomplex datasets. Furthermore, it addresses the challenges and opportunities in\nintegrating LLMs into EDA workflows, paving the way for future research and\napplication in this dynamic field. Through this detailed analysis, the survey\naims to offer valuable insights to professionals in the EDA industry, AI\nresearchers, and anyone interested in the convergence of advanced AI\ntechnologies and electronic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the rapidly evolving domain of Electronic Design Automation (EDA),\nLarge Language Models (LLMs) have emerged as transformative technologies,\noffering unprecedented capabilities for optimizing and automating various\naspects of electronic design. This survey provides a comprehensive exploration\nof LLM applications in EDA, focusing on advancements in model architectures,\nthe implications of varying model sizes, and innovative customization\ntechniques that enable tailored analytical insights. By examining the\nintersection of LLM capabilities and EDA requirements, the paper highlights the\nsignificant impact these models have on extracting nuanced understandings from\ncomplex datasets. Furthermore, it addresses the challenges and opportunities in\nintegrating LLMs into EDA workflows, paving the way for future research and\napplication in this dynamic field. Through this detailed analysis, the survey\naims to offer valuable insights to professionals in the EDA industry, AI\nresearchers, and anyone interested in the convergence of advanced AI\ntechnologies and electronic design."
                },
                "authors": [
                    {
                        "name": "Jingyu Pan"
                    },
                    {
                        "name": "Guanglei Zhou"
                    },
                    {
                        "name": "Chen-Chia Chang"
                    },
                    {
                        "name": "Isaac Jacobson"
                    },
                    {
                        "name": "Jiang Hu"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "21 pages, 2 figures, 3 tables, accepted by TODAES",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09648v1",
                "updated": "2025-01-16T16:43:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    43,
                    5,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:43:05Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    43,
                    5,
                    3,
                    16,
                    0
                ],
                "title": "Statistical inference for interacting innovation processes and related\n  general results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for interacting innovation processes and related\n  general results"
                },
                "summary": "Given the importance of understanding how different innovation processes\naffect each other, we have introduced a model for a finite system of\ninteracting innovation processes. The present work focuses on the second-order\nasymptotic properties of the model and illustrates how to leverage the\ntheoretical results in order to make statistical inference on the intensity of\nthe interaction. We apply the proposed tools to two real data sets (from Reddit\nand Gutenberg).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the importance of understanding how different innovation processes\naffect each other, we have introduced a model for a finite system of\ninteracting innovation processes. The present work focuses on the second-order\nasymptotic properties of the model and illustrates how to leverage the\ntheoretical results in order to make statistical inference on the intensity of\nthe interaction. We apply the proposed tools to two real data sets (from Reddit\nand Gutenberg)."
                },
                "authors": [
                    {
                        "name": "Giacomo Aletti"
                    },
                    {
                        "name": "Irene Crimaldi"
                    },
                    {
                        "name": "Andrea Ghiglietti"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Ghiglietti"
                },
                "author": "Andrea Ghiglietti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09243v2",
                "updated": "2025-01-16T16:38:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    38,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-12-12T12:53:30Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    53,
                    30,
                    3,
                    347,
                    0
                ],
                "title": "SPRec: Leveraging Self-Play to Debias Preference Alignment for Large\n  Language Model-based Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPRec: Leveraging Self-Play to Debias Preference Alignment for Large\n  Language Model-based Recommendations"
                },
                "summary": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current LLM-based recommender systems primarily rely on\nsupervised fine-tuning (SFT) to train the model for recommendation tasks.\nHowever, relying solely on positive samples limits the model's ability to align\nwith user satisfaction and expectations. To address this, researchers have\nintroduced Direct Preference Optimization (DPO), which explicitly aligns\nrecommendations with user preferences using offline preference ranking data.\nDespite its advantages, our theoretical analysis reveals that DPO inherently\nbiases the model towards a few items, exacerbating the filter bubble issue and\nultimately degrading user experience. In this paper, we propose SPRec, a novel\nself-play recommendation framework designed to mitigate over-recommendation and\nimprove fairness without requiring additional data or manual intervention. In\neach self-play iteration, the model undergoes an SFT step followed by a DPO\nstep, treating offline interaction data as positive samples and the predicted\noutputs from the previous iteration as negative samples. This effectively\nre-weights the DPO loss function using the model's logits, adaptively\nsuppressing biased items. Extensive experiments on multiple real-world datasets\ndemonstrate SPRec's effectiveness in enhancing recommendation accuracy and\naddressing fairness concerns. The implementation is available via\nhttps://github.com/RegionCh/SPRec",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current LLM-based recommender systems primarily rely on\nsupervised fine-tuning (SFT) to train the model for recommendation tasks.\nHowever, relying solely on positive samples limits the model's ability to align\nwith user satisfaction and expectations. To address this, researchers have\nintroduced Direct Preference Optimization (DPO), which explicitly aligns\nrecommendations with user preferences using offline preference ranking data.\nDespite its advantages, our theoretical analysis reveals that DPO inherently\nbiases the model towards a few items, exacerbating the filter bubble issue and\nultimately degrading user experience. In this paper, we propose SPRec, a novel\nself-play recommendation framework designed to mitigate over-recommendation and\nimprove fairness without requiring additional data or manual intervention. In\neach self-play iteration, the model undergoes an SFT step followed by a DPO\nstep, treating offline interaction data as positive samples and the predicted\noutputs from the previous iteration as negative samples. This effectively\nre-weights the DPO loss function using the model's logits, adaptively\nsuppressing biased items. Extensive experiments on multiple real-world datasets\ndemonstrate SPRec's effectiveness in enhancing recommendation accuracy and\naddressing fairness concerns. The implementation is available via\nhttps://github.com/RegionCh/SPRec"
                },
                "authors": [
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09645v1",
                "updated": "2025-01-16T16:37:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    37,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:37:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    37,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through\n  Category-Bounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through\n  Category-Bounding"
                },
                "summary": "In today's assistant landscape, personalisation enhances interactions,\nfosters long-term relationships, and deepens engagement. However, many systems\nstruggle with retaining user preferences, leading to repetitive user requests\nand disengagement. Furthermore, the unregulated and opaque extraction of user\npreferences in industry applications raises significant concerns about privacy\nand trust, especially in regions with stringent regulations like Europe. In\nresponse to these challenges, we propose a long-term memory system for voice\nassistants, structured around predefined categories. This approach leverages\nLarge Language Models to efficiently extract, store, and retrieve preferences\nwithin these categories, ensuring both personalisation and transparency. We\nalso introduce a synthetic multi-turn, multi-session conversation dataset\n(CarMem), grounded in real industry data, tailored to an in-car voice assistant\nsetting. Benchmarked on the dataset, our system achieves an F1-score of .78 to\n.95 in preference extraction, depending on category granularity. Our\nmaintenance strategy reduces redundant preferences by 95% and contradictory\nones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,\nthe results demonstrate the system's suitability for industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's assistant landscape, personalisation enhances interactions,\nfosters long-term relationships, and deepens engagement. However, many systems\nstruggle with retaining user preferences, leading to repetitive user requests\nand disengagement. Furthermore, the unregulated and opaque extraction of user\npreferences in industry applications raises significant concerns about privacy\nand trust, especially in regions with stringent regulations like Europe. In\nresponse to these challenges, we propose a long-term memory system for voice\nassistants, structured around predefined categories. This approach leverages\nLarge Language Models to efficiently extract, store, and retrieve preferences\nwithin these categories, ensuring both personalisation and transparency. We\nalso introduce a synthetic multi-turn, multi-session conversation dataset\n(CarMem), grounded in real industry data, tailored to an in-car voice assistant\nsetting. Benchmarked on the dataset, our system achieves an F1-score of .78 to\n.95 in preference extraction, depending on category granularity. Our\nmaintenance strategy reduces redundant preferences by 95% and contradictory\nones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,\nthe results demonstrate the system's suitability for industrial applications."
                },
                "authors": [
                    {
                        "name": "Johannes Kirmayr"
                    },
                    {
                        "name": "Lukas Stappen"
                    },
                    {
                        "name": "Phillip Schneider"
                    },
                    {
                        "name": "Florian Matthes"
                    },
                    {
                        "name": "Elisabeth André"
                    }
                ],
                "author_detail": {
                    "name": "Elisabeth André"
                },
                "author": "Elisabeth André",
                "arxiv_comment": "Accepted for presentation at the International Conference on\n  Computational Linguistics (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16641v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16641v3",
                "updated": "2025-01-16T16:29:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    29,
                    31,
                    3,
                    16,
                    0
                ],
                "published": "2024-12-21T14:21:33Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    14,
                    21,
                    33,
                    5,
                    356,
                    0
                ],
                "title": "A Systems Thinking Approach to Algorithmic Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systems Thinking Approach to Algorithmic Fairness"
                },
                "summary": "Systems thinking provides us with a way to model the algorithmic fairness\nproblem by allowing us to encode prior knowledge and assumptions about where we\nbelieve bias might exist in the data generating process. We can then encode\nthese beliefs as a series of causal graphs, enabling us to link AI/ML systems\nto politics and the law. This allows us to combine techniques from machine\nlearning, causal inference, and system dynamics in order to capture different\nemergent aspects of the fairness problem. We can use systems thinking to help\npolicymakers on both sides of the political aisle to understand the complex\ntrade-offs that exist from different types of fairness policies, providing a\nsociotechnical foundation for designing AI policy that is aligned to their\npolitical agendas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systems thinking provides us with a way to model the algorithmic fairness\nproblem by allowing us to encode prior knowledge and assumptions about where we\nbelieve bias might exist in the data generating process. We can then encode\nthese beliefs as a series of causal graphs, enabling us to link AI/ML systems\nto politics and the law. This allows us to combine techniques from machine\nlearning, causal inference, and system dynamics in order to capture different\nemergent aspects of the fairness problem. We can use systems thinking to help\npolicymakers on both sides of the political aisle to understand the complex\ntrade-offs that exist from different types of fairness policies, providing a\nsociotechnical foundation for designing AI policy that is aligned to their\npolitical agendas."
                },
                "authors": [
                    {
                        "name": "Chris Lam"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lam"
                },
                "author": "Chris Lam",
                "arxiv_comment": "This paper has been submitted to the 2025 ACM FAccT conference for\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16641v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16641v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09636v1",
                "updated": "2025-01-16T16:25:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    25,
                    30,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:25:30Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    25,
                    30,
                    3,
                    16,
                    0
                ],
                "title": "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading"
                },
                "summary": "Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks."
                },
                "authors": [
                    {
                        "name": "Kuan-Ming Liu"
                    },
                    {
                        "name": "Ming-Chih Lo"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Chih Lo"
                },
                "arxiv_affiliation": "National Yang Ming Chiao Tung University, College of Computer Science",
                "author": "Ming-Chih Lo",
                "arxiv_comment": "Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging\n  Innovations in Finance, Social Media, and Crime Prevention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09631v1",
                "updated": "2025-01-16T16:19:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    19,
                    53,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:19:53Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    19,
                    53,
                    3,
                    16,
                    0
                ],
                "title": "Empowering Large Language Models in Wireless Communication: A Novel\n  Dataset and Fine-Tuning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Large Language Models in Wireless Communication: A Novel\n  Dataset and Fine-Tuning Framework"
                },
                "summary": "In this work, we develop a specialized dataset aimed at enhancing the\nevaluation and fine-tuning of large language models (LLMs) specifically for\nwireless communication applications. The dataset includes a diverse set of\nmulti-hop questions, including true/false and multiple-choice types, spanning\nvarying difficulty levels from easy to hard. By utilizing advanced language\nmodels for entity extraction and question generation, rigorous data curation\nprocesses are employed to maintain high quality and relevance. Additionally, we\nintroduce a Pointwise V-Information (PVI) based fine-tuning method, providing a\ndetailed theoretical analysis and justification for its use in quantifying the\ninformation content of training data with 2.24\\% and 1.31\\% performance boost\nfor different models compared to baselines, respectively. To demonstrate the\neffectiveness of the fine-tuned models with the proposed methodologies on\npractical tasks, we also consider different tasks, including summarizing\noptimization problems from technical papers and solving the mathematical\nproblems related to non-orthogonal multiple access (NOMA), which are generated\nby using the proposed multi-agent framework. Simulation results show\nsignificant performance gain in summarization tasks with 20.9\\% in the ROUGE-L\nmetrics. We also study the scaling laws of fine-tuning LLMs and the challenges\nLLMs face in the field of wireless communications, offering insights into their\nadaptation to wireless communication tasks. This dataset and fine-tuning\nmethodology aim to enhance the training and evaluation of LLMs, contributing to\nadvancements in LLMs for wireless communication research and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we develop a specialized dataset aimed at enhancing the\nevaluation and fine-tuning of large language models (LLMs) specifically for\nwireless communication applications. The dataset includes a diverse set of\nmulti-hop questions, including true/false and multiple-choice types, spanning\nvarying difficulty levels from easy to hard. By utilizing advanced language\nmodels for entity extraction and question generation, rigorous data curation\nprocesses are employed to maintain high quality and relevance. Additionally, we\nintroduce a Pointwise V-Information (PVI) based fine-tuning method, providing a\ndetailed theoretical analysis and justification for its use in quantifying the\ninformation content of training data with 2.24\\% and 1.31\\% performance boost\nfor different models compared to baselines, respectively. To demonstrate the\neffectiveness of the fine-tuned models with the proposed methodologies on\npractical tasks, we also consider different tasks, including summarizing\noptimization problems from technical papers and solving the mathematical\nproblems related to non-orthogonal multiple access (NOMA), which are generated\nby using the proposed multi-agent framework. Simulation results show\nsignificant performance gain in summarization tasks with 20.9\\% in the ROUGE-L\nmetrics. We also study the scaling laws of fine-tuning LLMs and the challenges\nLLMs face in the field of wireless communications, offering insights into their\nadaptation to wireless communication tasks. This dataset and fine-tuning\nmethodology aim to enhance the training and evaluation of LLMs, contributing to\nadvancements in LLMs for wireless communication research and applications."
                },
                "authors": [
                    {
                        "name": "Yushen Lin"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Wenqi Huang"
                    },
                    {
                        "name": "Kaidi Wang"
                    },
                    {
                        "name": "Zhiguo Ding"
                    },
                    {
                        "name": "Daniel K. C. So"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "13 pages, 13 figure, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09620v1",
                "updated": "2025-01-16T16:00:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    0,
                    37,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:00:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    0,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated significant\nprogress in performing complex tasks. While Reinforcement Learning from Human\nFeedback (RLHF) has been effective in aligning LLMs with human preferences, it\nis susceptible to spurious correlations in reward modeling. Consequently, it\noften introduces biases-such as length bias, sycophancy, conceptual bias, and\ndiscrimination that hinder the model's ability to capture true causal\nrelationships. To address this, we propose a novel causal reward modeling\napproach that integrates causal inference to mitigate these spurious\ncorrelations. Our method enforces counterfactual invariance, ensuring reward\npredictions remain consistent when irrelevant variables are altered. Through\nexperiments on both synthetic and real-world datasets, we show that our\napproach mitigates various types of spurious correlations effectively,\nresulting in more reliable and fair alignment of LLMs with human preferences.\nAs a drop-in enhancement to the existing RLHF workflow, our causal reward\nmodeling provides a practical way to improve the trustworthiness and fairness\nof LLM finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated significant\nprogress in performing complex tasks. While Reinforcement Learning from Human\nFeedback (RLHF) has been effective in aligning LLMs with human preferences, it\nis susceptible to spurious correlations in reward modeling. Consequently, it\noften introduces biases-such as length bias, sycophancy, conceptual bias, and\ndiscrimination that hinder the model's ability to capture true causal\nrelationships. To address this, we propose a novel causal reward modeling\napproach that integrates causal inference to mitigate these spurious\ncorrelations. Our method enforces counterfactual invariance, ensuring reward\npredictions remain consistent when irrelevant variables are altered. Through\nexperiments on both synthetic and real-world datasets, we show that our\napproach mitigates various types of spurious correlations effectively,\nresulting in more reliable and fair alignment of LLMs with human preferences.\nAs a drop-in enhancement to the existing RLHF workflow, our causal reward\nmodeling provides a practical way to improve the trustworthiness and fairness\nof LLM finetuning."
                },
                "authors": [
                    {
                        "name": "Chaoqi Wang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Yibo Jiang"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Jiayi Liu"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Xiangjun Fan"
                    },
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Sinong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sinong Wang"
                },
                "author": "Sinong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01818v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01818v3",
                "updated": "2025-01-16T15:58:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    58,
                    24,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-04T11:46:34Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    11,
                    46,
                    34,
                    0,
                    338,
                    0
                ],
                "title": "Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto"
                },
                "summary": "Increasing interest in ensuring the safety of next-generation Artificial\nIntelligence (AI) systems calls for novel approaches to embedding morality into\nautonomous agents. This goal differs qualitatively from traditional\ntask-specific AI methodologies. In this paper, we provide a systematization of\nexisting approaches to the problem of introducing morality in machines -\nmodelled as a continuum. Our analysis suggests that popular techniques lie at\nthe extremes of this continuum - either being fully hard-coded into top-down,\nexplicit rules, or entirely learned in a bottom-up, implicit fashion with no\ndirect statement of any moral principle (this includes learning from human\nfeedback, as applied to the training and finetuning of large language models,\nor LLMs). Given the relative strengths and weaknesses of each type of\nmethodology, we argue that more hybrid solutions are needed to create adaptable\nand robust, yet controllable and interpretable agentic systems. To that end,\nthis paper discusses both the ethical foundations (including deontology,\nconsequentialism and virtue ethics) and implementations of morally aligned AI\nsystems.\n  We present a series of case studies that rely on intrinsic rewards, moral\nconstraints or textual instructions, applied to either pure-Reinforcement\nLearning or LLM-based agents. By analysing these diverse implementations under\none framework, we compare their relative strengths and shortcomings in\ndeveloping morally aligned AI systems. We then discuss strategies for\nevaluating the effectiveness of moral learning agents. Finally, we present open\nresearch questions and implications for the future of AI safety and ethics\nwhich are emerging from this hybrid framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing interest in ensuring the safety of next-generation Artificial\nIntelligence (AI) systems calls for novel approaches to embedding morality into\nautonomous agents. This goal differs qualitatively from traditional\ntask-specific AI methodologies. In this paper, we provide a systematization of\nexisting approaches to the problem of introducing morality in machines -\nmodelled as a continuum. Our analysis suggests that popular techniques lie at\nthe extremes of this continuum - either being fully hard-coded into top-down,\nexplicit rules, or entirely learned in a bottom-up, implicit fashion with no\ndirect statement of any moral principle (this includes learning from human\nfeedback, as applied to the training and finetuning of large language models,\nor LLMs). Given the relative strengths and weaknesses of each type of\nmethodology, we argue that more hybrid solutions are needed to create adaptable\nand robust, yet controllable and interpretable agentic systems. To that end,\nthis paper discusses both the ethical foundations (including deontology,\nconsequentialism and virtue ethics) and implementations of morally aligned AI\nsystems.\n  We present a series of case studies that rely on intrinsic rewards, moral\nconstraints or textual instructions, applied to either pure-Reinforcement\nLearning or LLM-based agents. By analysing these diverse implementations under\none framework, we compare their relative strengths and shortcomings in\ndeveloping morally aligned AI systems. We then discuss strategies for\nevaluating the effectiveness of moral learning agents. Finally, we present open\nresearch questions and implications for the future of AI safety and ethics\nwhich are emerging from this hybrid framework."
                },
                "authors": [
                    {
                        "name": "Elizaveta Tennant"
                    },
                    {
                        "name": "Stephen Hailes"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01818v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01818v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08603v2",
                "updated": "2025-01-16T15:57:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    57,
                    3,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-15T06:00:50Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    0,
                    50,
                    2,
                    15,
                    0
                ],
                "title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design"
                },
                "summary": "Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard\ncombinatorial optimization (CO) problems) is a common practice but requires\nextensive domain knowledge. Recently, Large Language Model (LLM)-based\nautomatic heuristics design (AHD) methods have shown promise in generating\nhigh-quality heuristics without manual intervention. Existing LLM-based AHD\nmethods employ a population to maintain a fixed number of top-performing\nLLM-generated heuristics and introduce evolutionary computation (EC) to enhance\nthe population iteratively. However, the population-based procedure brings\ngreedy properties, often resulting in convergence to local optima. Instead, to\nmore comprehensively explore the space of heuristics, we propose using Monte\nCarlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all\nLLM-generated heuristics in a tree structure. With a novel thought-alignment\nprocess and an exploration-decay technique, the proposed MCTS-AHD method\ndelivers significantly higher-quality heuristics on various complex tasks. Our\ncode is available at https://github.com/zz1358m/MCTS-AHD-master.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard\ncombinatorial optimization (CO) problems) is a common practice but requires\nextensive domain knowledge. Recently, Large Language Model (LLM)-based\nautomatic heuristics design (AHD) methods have shown promise in generating\nhigh-quality heuristics without manual intervention. Existing LLM-based AHD\nmethods employ a population to maintain a fixed number of top-performing\nLLM-generated heuristics and introduce evolutionary computation (EC) to enhance\nthe population iteratively. However, the population-based procedure brings\ngreedy properties, often resulting in convergence to local optima. Instead, to\nmore comprehensively explore the space of heuristics, we propose using Monte\nCarlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all\nLLM-generated heuristics in a tree structure. With a novel thought-alignment\nprocess and an exploration-decay technique, the proposed MCTS-AHD method\ndelivers significantly higher-quality heuristics on various complex tasks. Our\ncode is available at https://github.com/zz1358m/MCTS-AHD-master."
                },
                "authors": [
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Zhuoliang Xie"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17962v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17962v4",
                "updated": "2025-01-16T15:47:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    47,
                    58,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-25T22:44:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    44,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Lanxiao Huang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17962v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17962v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18713v2",
                "updated": "2025-01-16T15:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    32,
                    6,
                    3,
                    16,
                    0
                ],
                "published": "2024-02-28T21:23:15Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    21,
                    23,
                    15,
                    2,
                    59,
                    0
                ],
                "title": "Identifying Assumptions and Research Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Assumptions and Research Dynamics"
                },
                "summary": "A representative researcher has repeated opportunities for empirical\nresearch. To process findings, she must impose an \"identifying assumption.\" She\nconducts research when the assumption is sufficiently plausible (taking into\naccount both current beliefs and the quality of the opportunity), and updates\nbeliefs as if the assumption were perfectly valid. We study the dynamics of\nthis learning process. While the rate of research cannot always increase over\ntime, research slowdown is possible. We characterize environments in which the\nrate is constant. Long-run beliefs can exhibit history-dependence and \"false\ncertitude.\" We apply the model to stylized examples of empirical methodologies:\nexperiments, various causal-inference techniques, and \"calibration.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A representative researcher has repeated opportunities for empirical\nresearch. To process findings, she must impose an \"identifying assumption.\" She\nconducts research when the assumption is sufficiently plausible (taking into\naccount both current beliefs and the quality of the opportunity), and updates\nbeliefs as if the assumption were perfectly valid. We study the dynamics of\nthis learning process. While the rate of research cannot always increase over\ntime, research slowdown is possible. We characterize environments in which the\nrate is constant. Long-run beliefs can exhibit history-dependence and \"false\ncertitude.\" We apply the model to stylized examples of empirical methodologies:\nexperiments, various causal-inference techniques, and \"calibration.\""
                },
                "authors": [
                    {
                        "name": "Andrew Ellis"
                    },
                    {
                        "name": "Ran Spiegler"
                    }
                ],
                "author_detail": {
                    "name": "Ran Spiegler"
                },
                "author": "Ran Spiegler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07227v2",
                "updated": "2025-01-16T15:30:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    30,
                    54,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-13T11:28:49Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    28,
                    49,
                    0,
                    13,
                    0
                ],
                "title": "MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning"
                },
                "summary": "Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction."
                },
                "authors": [
                    {
                        "name": "Tieyuan Chen"
                    },
                    {
                        "name": "Huabin Liu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Tianyao He"
                    },
                    {
                        "name": "Chaofan Gan"
                    },
                    {
                        "name": "Huanyu He"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "arxiv_comment": "IEEE TPAMI Submission. continuous work of arXiv:2409.17647 (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09605v1",
                "updated": "2025-01-16T15:25:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    25,
                    44,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T15:25:44Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    25,
                    44,
                    3,
                    16,
                    0
                ],
                "title": "Managed-Retention Memory: A New Class of Memory for the AI Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managed-Retention Memory: A New Class of Memory for the AI Era"
                },
                "summary": "AI clusters today are one of the major uses of High Bandwidth Memory (HBM).\nHowever, HBM is suboptimal for AI workloads for several reasons. Analysis shows\nHBM is overprovisioned on write performance, but underprovisioned on density\nand read bandwidth, and also has significant energy per bit overheads. It is\nalso expensive, with lower yield than DRAM due to manufacturing complexity. We\npropose a new memory class: Managed-Retention Memory (MRM), which is more\noptimized to store key data structures for AI inference workloads. We believe\nthat MRM may finally provide a path to viability for technologies that were\noriginally proposed to support Storage Class Memory (SCM). These technologies\ntraditionally offered long-term persistence (10+ years) but provided poor IO\nperformance and/or endurance. MRM makes different trade-offs, and by\nunderstanding the workload IO patterns, MRM foregoes long-term data retention\nand write performance for better potential performance on the metrics important\nfor these workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI clusters today are one of the major uses of High Bandwidth Memory (HBM).\nHowever, HBM is suboptimal for AI workloads for several reasons. Analysis shows\nHBM is overprovisioned on write performance, but underprovisioned on density\nand read bandwidth, and also has significant energy per bit overheads. It is\nalso expensive, with lower yield than DRAM due to manufacturing complexity. We\npropose a new memory class: Managed-Retention Memory (MRM), which is more\noptimized to store key data structures for AI inference workloads. We believe\nthat MRM may finally provide a path to viability for technologies that were\noriginally proposed to support Storage Class Memory (SCM). These technologies\ntraditionally offered long-term persistence (10+ years) but provided poor IO\nperformance and/or endurance. MRM makes different trade-offs, and by\nunderstanding the workload IO patterns, MRM foregoes long-term data retention\nand write performance for better potential performance on the metrics important\nfor these workloads."
                },
                "authors": [
                    {
                        "name": "Sergey Legtchenko"
                    },
                    {
                        "name": "Ioan Stefanovici"
                    },
                    {
                        "name": "Richard Black"
                    },
                    {
                        "name": "Antony Rowstron"
                    },
                    {
                        "name": "Junyi Liu"
                    },
                    {
                        "name": "Paolo Costa"
                    },
                    {
                        "name": "Burcu Canakci"
                    },
                    {
                        "name": "Dushyanth Narayanan"
                    },
                    {
                        "name": "Xingbo Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xingbo Wu"
                },
                "author": "Xingbo Wu",
                "arxiv_comment": "8 pages (5 content + 3 refs); 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09604v1",
                "updated": "2025-01-16T15:24:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    24,
                    41,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T15:24:41Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    24,
                    41,
                    3,
                    16,
                    0
                ],
                "title": "From Scarcity to Capability: Empowering Fake News Detection in\n  Low-Resource Languages with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Scarcity to Capability: Empowering Fake News Detection in\n  Low-Resource Languages with LLMs"
                },
                "summary": "The rapid spread of fake news presents a significant global challenge,\nparticularly in low-resource languages like Bangla, which lack adequate\ndatasets and detection tools. Although manual fact-checking is accurate, it is\nexpensive and slow to prevent the dissemination of fake news. Addressing this\ngap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news\ndetection. This version includes 11,700 additional, meticulously curated fake\nnews articles validated from credible sources, creating a proportional dataset\nof 47,000 authentic and 13,000 fake news items across 13 categories. In\naddition, we created a manually curated independent test set of 460 fake and\n540 authentic news items for rigorous evaluation. We invest efforts in\ncollecting fake news from credible sources and manually verified while\npreserving the linguistic richness. We develop a benchmark system utilizing\ntransformer-based architectures, including fine-tuned Bidirectional Encoder\nRepresentations from Transformers variants (F1-87\\%) and Large Language Models\nwith Quantized Low-Rank Approximation (F1-89\\%), that significantly outperforms\ntraditional methods. BanFakeNews-2.0 offers a valuable resource to advance\nresearch and application in fake news detection for low-resourced languages. We\npublicly release our dataset and model on Github to foster research in this\ndirection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of fake news presents a significant global challenge,\nparticularly in low-resource languages like Bangla, which lack adequate\ndatasets and detection tools. Although manual fact-checking is accurate, it is\nexpensive and slow to prevent the dissemination of fake news. Addressing this\ngap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news\ndetection. This version includes 11,700 additional, meticulously curated fake\nnews articles validated from credible sources, creating a proportional dataset\nof 47,000 authentic and 13,000 fake news items across 13 categories. In\naddition, we created a manually curated independent test set of 460 fake and\n540 authentic news items for rigorous evaluation. We invest efforts in\ncollecting fake news from credible sources and manually verified while\npreserving the linguistic richness. We develop a benchmark system utilizing\ntransformer-based architectures, including fine-tuned Bidirectional Encoder\nRepresentations from Transformers variants (F1-87\\%) and Large Language Models\nwith Quantized Low-Rank Approximation (F1-89\\%), that significantly outperforms\ntraditional methods. BanFakeNews-2.0 offers a valuable resource to advance\nresearch and application in fake news detection for low-resourced languages. We\npublicly release our dataset and model on Github to foster research in this\ndirection."
                },
                "authors": [
                    {
                        "name": "Hrithik Majumdar Shibu"
                    },
                    {
                        "name": "Shrestha Datta"
                    },
                    {
                        "name": "Md. Sumon Miah"
                    },
                    {
                        "name": "Nasrullah Sami"
                    },
                    {
                        "name": "Mahruba Sharmin Chowdhury"
                    },
                    {
                        "name": "Md. Saiful Islam"
                    }
                ],
                "author_detail": {
                    "name": "Md. Saiful Islam"
                },
                "author": "Md. Saiful Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10761v2",
                "updated": "2025-01-16T15:16:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    16,
                    4,
                    3,
                    16,
                    0
                ],
                "published": "2023-10-16T18:48:57Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    18,
                    48,
                    57,
                    0,
                    289,
                    0
                ],
                "title": "Simulation Based Composite Likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation Based Composite Likelihood"
                },
                "summary": "Inference for high-dimensional hidden Markov models is challenging due to the\nexponential-in-dimension computational cost of calculating the likelihood. To\naddress this issue, we introduce an innovative composite likelihood approach\ncalled \"Simulation Based Composite Likelihood\" (SimBa-CL). With SimBa-CL, we\napproximate the likelihood by the product of its marginals, which we estimate\nusing Monte Carlo sampling. In a similar vein to approximate Bayesian\ncomputation (ABC), SimBa-CL requires multiple simulations from the model, but,\nin contrast to ABC, it provides a likelihood approximation that guides the\noptimization of the parameters. Leveraging automatic differentiation libraries,\nit is simple to calculate gradients and Hessians to not only speed up\noptimization but also to build approximate confidence sets. We present\nextensive empirical results which validate our theory and demonstrate its\nadvantage over SMC, and apply SimBa-CL to real-world Aphtovirus data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for high-dimensional hidden Markov models is challenging due to the\nexponential-in-dimension computational cost of calculating the likelihood. To\naddress this issue, we introduce an innovative composite likelihood approach\ncalled \"Simulation Based Composite Likelihood\" (SimBa-CL). With SimBa-CL, we\napproximate the likelihood by the product of its marginals, which we estimate\nusing Monte Carlo sampling. In a similar vein to approximate Bayesian\ncomputation (ABC), SimBa-CL requires multiple simulations from the model, but,\nin contrast to ABC, it provides a likelihood approximation that guides the\noptimization of the parameters. Leveraging automatic differentiation libraries,\nit is simple to calculate gradients and Hessians to not only speed up\noptimization but also to build approximate confidence sets. We present\nextensive empirical results which validate our theory and demonstrate its\nadvantage over SMC, and apply SimBa-CL to real-world Aphtovirus data."
                },
                "authors": [
                    {
                        "name": "Lorenzo Rimella"
                    },
                    {
                        "name": "Chris Jewell"
                    },
                    {
                        "name": "Paul Fearnhead"
                    }
                ],
                "author_detail": {
                    "name": "Paul Fearnhead"
                },
                "author": "Paul Fearnhead",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09588v1",
                "updated": "2025-01-16T15:11:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T15:11:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "Atleus: Accelerating Transformers on the Edge Enabled by 3D\n  Heterogeneous Manycore Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atleus: Accelerating Transformers on the Edge Enabled by 3D\n  Heterogeneous Manycore Architectures"
                },
                "summary": "Transformer architectures have become the standard neural network model for\nvarious machine learning applications including natural language processing and\ncomputer vision. However, the compute and memory requirements introduced by\ntransformer models make them challenging to adopt for edge applications.\nFurthermore, fine-tuning pre-trained transformers (e.g., foundation models) is\na common task to enhance the model's predictive performance on specific\ntasks/applications. Existing transformer accelerators are oblivious to\ncomplexities introduced by fine-tuning. In this paper, we propose the design of\na three-dimensional (3D) heterogeneous architecture referred to as Atleus that\nincorporates heterogeneous computing resources specifically optimized to\naccelerate transformer models for the dual purposes of fine-tuning and\ninference. Specifically, Atleus utilizes non-volatile memory and systolic array\nfor accelerating transformer computational kernels using an integrated 3D\nplatform. Moreover, we design a suitable NoC to achieve high performance and\nenergy efficiency. Finally, Atleus adopts an effective quantization scheme to\nsupport model compression. Experimental results demonstrate that Atleus\noutperforms existing state-of-the-art by up to 56x and 64.5x in terms of\nperformance and energy efficiency respectively",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer architectures have become the standard neural network model for\nvarious machine learning applications including natural language processing and\ncomputer vision. However, the compute and memory requirements introduced by\ntransformer models make them challenging to adopt for edge applications.\nFurthermore, fine-tuning pre-trained transformers (e.g., foundation models) is\na common task to enhance the model's predictive performance on specific\ntasks/applications. Existing transformer accelerators are oblivious to\ncomplexities introduced by fine-tuning. In this paper, we propose the design of\na three-dimensional (3D) heterogeneous architecture referred to as Atleus that\nincorporates heterogeneous computing resources specifically optimized to\naccelerate transformer models for the dual purposes of fine-tuning and\ninference. Specifically, Atleus utilizes non-volatile memory and systolic array\nfor accelerating transformer computational kernels using an integrated 3D\nplatform. Moreover, we design a suitable NoC to achieve high performance and\nenergy efficiency. Finally, Atleus adopts an effective quantization scheme to\nsupport model compression. Experimental results demonstrate that Atleus\noutperforms existing state-of-the-art by up to 56x and 64.5x in terms of\nperformance and energy efficiency respectively"
                },
                "authors": [
                    {
                        "name": "Pratyush Dhingra"
                    },
                    {
                        "name": "Janardhan Rao Doppa"
                    },
                    {
                        "name": "Partha Pratim Pande"
                    }
                ],
                "author_detail": {
                    "name": "Partha Pratim Pande"
                },
                "author": "Partha Pratim Pande",
                "arxiv_comment": "Accepted for Publication in IEEE Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems (TCAD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01957v2",
                "updated": "2025-01-16T15:00:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    0,
                    16,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-03T18:59:52Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    52,
                    4,
                    3,
                    0
                ],
                "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction."
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "https://github.com/VITA-MLLM/VITA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04791v2",
                "updated": "2025-01-16T14:50:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    14,
                    50,
                    4,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-08T19:14:22Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    19,
                    14,
                    22,
                    2,
                    8,
                    0
                ],
                "title": "Approximating non-Gaussian Bayesian partitions with normalising flows:\n  statistics, inference and application to cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating non-Gaussian Bayesian partitions with normalising flows:\n  statistics, inference and application to cosmology"
                },
                "summary": "Subject of this paper is the simplification of Markov chain Monte Carlo\nsampling as used in Bayesian statistical inference by means of normalising\nflows, a machine learning method which is able to construct an invertible and\ndifferentiable transformation between Gaussian and non-Gaussian random\ndistributions. We use normalising flows to compute Bayesian partition functions\nfor non-Gaussian distributions and show how normalising flows can be employed\nin finding analytical expressions for posterior distributions beyond the\nGaussian limit. Flows offer advantages for the numerical evaluation of the\npartition function itself, as well as for cumulants and for the information\nentropy. We demonstrate how normalising flows in conjunction with Bayes\npartitions can be used in inference problems in cosmology and apply them to the\nposterior distribution for the matter density $\\Omega_m$ and a dark energy\nequation of state parameter $w_0$ on the basis of supernova data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subject of this paper is the simplification of Markov chain Monte Carlo\nsampling as used in Bayesian statistical inference by means of normalising\nflows, a machine learning method which is able to construct an invertible and\ndifferentiable transformation between Gaussian and non-Gaussian random\ndistributions. We use normalising flows to compute Bayesian partition functions\nfor non-Gaussian distributions and show how normalising flows can be employed\nin finding analytical expressions for posterior distributions beyond the\nGaussian limit. Flows offer advantages for the numerical evaluation of the\npartition function itself, as well as for cumulants and for the information\nentropy. We demonstrate how normalising flows in conjunction with Bayes\npartitions can be used in inference problems in cosmology and apply them to the\nposterior distribution for the matter density $\\Omega_m$ and a dark energy\nequation of state parameter $w_0$ on the basis of supernova data."
                },
                "authors": [
                    {
                        "name": "Tobias Röspel"
                    },
                    {
                        "name": "Adrian Schlosser"
                    },
                    {
                        "name": "Björn Malte Schäfer"
                    }
                ],
                "author_detail": {
                    "name": "Björn Malte Schäfer"
                },
                "author": "Björn Malte Schäfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06315v2",
                "updated": "2025-01-16T14:26:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    14,
                    26,
                    10,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-10T14:32:10Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    32,
                    10,
                    0,
                    162,
                    0
                ],
                "title": "Inference of the Mass Composition of Cosmic Rays with energies from\n  $\\mathbf{10^{18.5}}$ to $\\mathbf{10^{20}}$ eV using the Pierre Auger\n  Observatory and Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of the Mass Composition of Cosmic Rays with energies from\n  $\\mathbf{10^{18.5}}$ to $\\mathbf{10^{20}}$ eV using the Pierre Auger\n  Observatory and Deep Learning"
                },
                "summary": "We present measurements of the atmospheric depth of the shower maximum\n$X_\\mathrm{max}$, inferred for the first time on an event-by-event level using\nthe Surface Detector of the Pierre Auger Observatory. Using deep learning, we\nwere able to extend measurements of the $X_\\mathrm{max}$ distributions up to\nenergies of 100 EeV ($10^{20}$ eV), not yet revealed by current measurements,\nproviding new insights into the mass composition of cosmic rays at extreme\nenergies. Gaining a 10-fold increase in statistics compared to the Fluorescence\nDetector data, we find evidence that the rate of change of the average\n$X_\\mathrm{max}$ with the logarithm of energy features three breaks at\n$6.5\\pm0.6~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, $11\\pm\n2~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, and\n$31\\pm5~(\\mathrm{stat})\\pm3~(\\mathrm{sys})$ EeV, in the vicinity to the three\nprominent features (ankle, instep, suppression) of the cosmic-ray flux. The\nenergy evolution of the mean and standard deviation of the measured\n$X_\\mathrm{max}$ distributions indicates that the mass composition becomes\nincreasingly heavier and purer, thus being incompatible with a large fraction\nof light nuclei between 50 EeV and 100 EeV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present measurements of the atmospheric depth of the shower maximum\n$X_\\mathrm{max}$, inferred for the first time on an event-by-event level using\nthe Surface Detector of the Pierre Auger Observatory. Using deep learning, we\nwere able to extend measurements of the $X_\\mathrm{max}$ distributions up to\nenergies of 100 EeV ($10^{20}$ eV), not yet revealed by current measurements,\nproviding new insights into the mass composition of cosmic rays at extreme\nenergies. Gaining a 10-fold increase in statistics compared to the Fluorescence\nDetector data, we find evidence that the rate of change of the average\n$X_\\mathrm{max}$ with the logarithm of energy features three breaks at\n$6.5\\pm0.6~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, $11\\pm\n2~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, and\n$31\\pm5~(\\mathrm{stat})\\pm3~(\\mathrm{sys})$ EeV, in the vicinity to the three\nprominent features (ankle, instep, suppression) of the cosmic-ray flux. The\nenergy evolution of the mean and standard deviation of the measured\n$X_\\mathrm{max}$ distributions indicates that the mass composition becomes\nincreasingly heavier and purer, thus being incompatible with a large fraction\nof light nuclei between 50 EeV and 100 EeV."
                },
                "authors": [
                    {
                        "name": "The Pierre Auger Collaboration"
                    },
                    {
                        "name": "A. Abdul Halim"
                    },
                    {
                        "name": "P. Abreu"
                    },
                    {
                        "name": "M. Aglietta"
                    },
                    {
                        "name": "I. Allekotte"
                    },
                    {
                        "name": "K. Almeida Cheminant"
                    },
                    {
                        "name": "A. Almela"
                    },
                    {
                        "name": "R. Aloisio"
                    },
                    {
                        "name": "J. Alvarez-Muñiz"
                    },
                    {
                        "name": "J. Ammerman Yebra"
                    },
                    {
                        "name": "G. A. Anastasi"
                    },
                    {
                        "name": "L. Anchordoqui"
                    },
                    {
                        "name": "B. Andrada"
                    },
                    {
                        "name": "L. Andrade Dourado"
                    },
                    {
                        "name": "S. Andringa"
                    },
                    {
                        "name": "L. Apollonio"
                    },
                    {
                        "name": "C. Aramo"
                    },
                    {
                        "name": "P. R. Araújo Ferreira"
                    },
                    {
                        "name": "E. Arnone"
                    },
                    {
                        "name": "J. C. Arteaga Velázquez"
                    },
                    {
                        "name": "P. Assis"
                    },
                    {
                        "name": "G. Avila"
                    },
                    {
                        "name": "E. Avocone"
                    },
                    {
                        "name": "A. Bakalova"
                    },
                    {
                        "name": "F. Barbato"
                    },
                    {
                        "name": "A. Bartz Mocellin"
                    },
                    {
                        "name": "C. Berat"
                    },
                    {
                        "name": "M. E. Bertaina"
                    },
                    {
                        "name": "G. Bhatta"
                    },
                    {
                        "name": "M. Bianciotto"
                    },
                    {
                        "name": "P. L. Biermann"
                    },
                    {
                        "name": "V. Binet"
                    },
                    {
                        "name": "K. Bismark"
                    },
                    {
                        "name": "T. Bister"
                    },
                    {
                        "name": "J. Biteau"
                    },
                    {
                        "name": "J. Blazek"
                    },
                    {
                        "name": "C. Bleve"
                    },
                    {
                        "name": "J. Blümer"
                    },
                    {
                        "name": "M. Boháčová"
                    },
                    {
                        "name": "D. Boncioli"
                    },
                    {
                        "name": "C. Bonifazi"
                    },
                    {
                        "name": "L. Bonneau Arbeletche"
                    },
                    {
                        "name": "N. Borodai"
                    },
                    {
                        "name": "J. Brack"
                    },
                    {
                        "name": "P. G. Brichetto Orchera"
                    },
                    {
                        "name": "F. L. Briechle"
                    },
                    {
                        "name": "A. Bueno"
                    },
                    {
                        "name": "S. Buitink"
                    },
                    {
                        "name": "M. Buscemi"
                    },
                    {
                        "name": "M. Büsken"
                    },
                    {
                        "name": "A. Bwembya"
                    },
                    {
                        "name": "K. S. Caballero-Mora"
                    },
                    {
                        "name": "S. Cabana-Freire"
                    },
                    {
                        "name": "L. Caccianiga"
                    },
                    {
                        "name": "F. Campuzano"
                    },
                    {
                        "name": "R. Caruso"
                    },
                    {
                        "name": "A. Castellina"
                    },
                    {
                        "name": "F. Catalani"
                    },
                    {
                        "name": "G. Cataldi"
                    },
                    {
                        "name": "L. Cazon"
                    },
                    {
                        "name": "M. Cerda"
                    },
                    {
                        "name": "B. Čermáková"
                    },
                    {
                        "name": "A. Cermenati"
                    },
                    {
                        "name": "J. A. Chinellato"
                    },
                    {
                        "name": "J. Chudoba"
                    },
                    {
                        "name": "L. Chytka"
                    },
                    {
                        "name": "R. W. Clay"
                    },
                    {
                        "name": "A. C. Cobos Cerutti"
                    },
                    {
                        "name": "R. Colalillo"
                    },
                    {
                        "name": "M. R. Coluccia"
                    },
                    {
                        "name": "R. Conceição"
                    },
                    {
                        "name": "A. Condorelli"
                    },
                    {
                        "name": "G. Consolati"
                    },
                    {
                        "name": "M. Conte"
                    },
                    {
                        "name": "F. Convenga"
                    },
                    {
                        "name": "D. Correia dos Santos"
                    },
                    {
                        "name": "P. J. Costa"
                    },
                    {
                        "name": "C. E. Covault"
                    },
                    {
                        "name": "M. Cristinziani"
                    },
                    {
                        "name": "C. S. Cruz Sanchez"
                    },
                    {
                        "name": "S. Dasso"
                    },
                    {
                        "name": "K. Daumiller"
                    },
                    {
                        "name": "B. R. Dawson"
                    },
                    {
                        "name": "R. M. de Almeida"
                    },
                    {
                        "name": "B. de Errico"
                    },
                    {
                        "name": "J. de Jesús"
                    },
                    {
                        "name": "S. J. de Jong"
                    },
                    {
                        "name": "J. R. T. de Mello Neto"
                    },
                    {
                        "name": "I. De Mitri"
                    },
                    {
                        "name": "J. de Oliveira"
                    },
                    {
                        "name": "D. de Oliveira Franco"
                    },
                    {
                        "name": "F. de Palma"
                    },
                    {
                        "name": "V. de Souza"
                    },
                    {
                        "name": "E. De Vito"
                    },
                    {
                        "name": "A. Del Popolo"
                    },
                    {
                        "name": "O. Deligny"
                    },
                    {
                        "name": "N. Denner"
                    },
                    {
                        "name": "L. Deval"
                    },
                    {
                        "name": "A. di Matteo"
                    },
                    {
                        "name": "J. A. do"
                    },
                    {
                        "name": "M. Dobre"
                    },
                    {
                        "name": "C. Dobrigkeit"
                    },
                    {
                        "name": "J. C. D'Olivo"
                    },
                    {
                        "name": "L. M. Domingues Mendes"
                    },
                    {
                        "name": "Q. Dorosti"
                    },
                    {
                        "name": "J. C. dos Anjos"
                    },
                    {
                        "name": "R. C. dos Anjos"
                    },
                    {
                        "name": "J. Ebr"
                    },
                    {
                        "name": "F. Ellwanger"
                    },
                    {
                        "name": "M. Emam"
                    },
                    {
                        "name": "R. Engel"
                    },
                    {
                        "name": "I. Epicoco"
                    },
                    {
                        "name": "M. Erdmann"
                    },
                    {
                        "name": "A. Etchegoyen"
                    },
                    {
                        "name": "C. Evoli"
                    },
                    {
                        "name": "H. Falcke"
                    },
                    {
                        "name": "G. Farrar"
                    },
                    {
                        "name": "A. C. Fauth"
                    },
                    {
                        "name": "T. Fehler"
                    },
                    {
                        "name": "F. Feldbusch"
                    },
                    {
                        "name": "F. Fenu"
                    },
                    {
                        "name": "A. Fernandes"
                    },
                    {
                        "name": "B. Fick"
                    },
                    {
                        "name": "J. M. Figueira"
                    },
                    {
                        "name": "P. Filip"
                    },
                    {
                        "name": "A. Filipčič"
                    },
                    {
                        "name": "T. Fitoussi"
                    },
                    {
                        "name": "B. Flaggs"
                    },
                    {
                        "name": "T. Fodran"
                    },
                    {
                        "name": "T. Fujii"
                    },
                    {
                        "name": "A. Fuster"
                    },
                    {
                        "name": "C. Galea"
                    },
                    {
                        "name": "B. García"
                    },
                    {
                        "name": "C. Gaudu"
                    },
                    {
                        "name": "A. Gherghel-Lascu"
                    },
                    {
                        "name": "P. L. Ghia"
                    },
                    {
                        "name": "U. Giaccari"
                    },
                    {
                        "name": "J. Glombitza"
                    },
                    {
                        "name": "F. Gobbi"
                    },
                    {
                        "name": "F. Gollan"
                    },
                    {
                        "name": "G. Golup"
                    },
                    {
                        "name": "M. Gómez Berisso"
                    },
                    {
                        "name": "P. F. Gómez Vitale"
                    },
                    {
                        "name": "J. P. Gongora"
                    },
                    {
                        "name": "J. M. González"
                    },
                    {
                        "name": "N. González"
                    },
                    {
                        "name": "D. Góra"
                    },
                    {
                        "name": "A. Gorgi"
                    },
                    {
                        "name": "M. Gottowik"
                    },
                    {
                        "name": "F. Guarino"
                    },
                    {
                        "name": "G. P. Guedes"
                    },
                    {
                        "name": "E. Guido"
                    },
                    {
                        "name": "L. Gülzow"
                    },
                    {
                        "name": "S. Hahn"
                    },
                    {
                        "name": "P. Hamal"
                    },
                    {
                        "name": "M. R. Hampel"
                    },
                    {
                        "name": "P. Hansen"
                    },
                    {
                        "name": "D. Harari"
                    },
                    {
                        "name": "V. M. Harvey"
                    },
                    {
                        "name": "A. Haungs"
                    },
                    {
                        "name": "T. Hebbeker"
                    },
                    {
                        "name": "C. Hojvat"
                    },
                    {
                        "name": "J. R. Hörandel"
                    },
                    {
                        "name": "P. Horvath"
                    },
                    {
                        "name": "M. Hrabovský"
                    },
                    {
                        "name": "T. Huege"
                    },
                    {
                        "name": "A. Insolia"
                    },
                    {
                        "name": "P. G. Isar"
                    },
                    {
                        "name": "P. Janecek"
                    },
                    {
                        "name": "V. Jilek"
                    },
                    {
                        "name": "J. A. Johnsen"
                    },
                    {
                        "name": "J. Jurysek"
                    },
                    {
                        "name": "K. -H. Kampert"
                    },
                    {
                        "name": "B. Keilhauer"
                    },
                    {
                        "name": "A. Khakurdikar"
                    },
                    {
                        "name": "V. V. Kizakke Covilakam"
                    },
                    {
                        "name": "H. O. Klages"
                    },
                    {
                        "name": "M. Kleifges"
                    },
                    {
                        "name": "F. Knapp"
                    },
                    {
                        "name": "J. Köhler"
                    },
                    {
                        "name": "F. Krieger"
                    },
                    {
                        "name": "N. Kunka"
                    },
                    {
                        "name": "B. L. Lago"
                    },
                    {
                        "name": "N. Langner"
                    },
                    {
                        "name": "M. A. Leigui de Oliveira"
                    },
                    {
                        "name": "Y. Lema-Capeans"
                    },
                    {
                        "name": "A. Letessier-Selvon"
                    },
                    {
                        "name": "I. Lhenry-Yvon"
                    },
                    {
                        "name": "L. Lopes"
                    },
                    {
                        "name": "L. Lu"
                    },
                    {
                        "name": "Q. Luce"
                    },
                    {
                        "name": "J. P. Lundquist"
                    },
                    {
                        "name": "A. Machado Payeras"
                    },
                    {
                        "name": "M. Majercakova"
                    },
                    {
                        "name": "D. Mandat"
                    },
                    {
                        "name": "B. C. Manning"
                    },
                    {
                        "name": "P. Mantsch"
                    },
                    {
                        "name": "F. M. Mariani"
                    },
                    {
                        "name": "A. G. Mariazzi"
                    },
                    {
                        "name": "I. C. Mariş"
                    },
                    {
                        "name": "G. Marsella"
                    },
                    {
                        "name": "D. Martello"
                    },
                    {
                        "name": "S. Martinelli"
                    },
                    {
                        "name": "O. Martínez Bravo"
                    },
                    {
                        "name": "M. A. Martins"
                    },
                    {
                        "name": "H. -J. Mathes"
                    },
                    {
                        "name": "J. Matthews"
                    },
                    {
                        "name": "G. Matthiae"
                    },
                    {
                        "name": "E. Mayotte"
                    },
                    {
                        "name": "S. Mayotte"
                    },
                    {
                        "name": "P. O. Mazur"
                    },
                    {
                        "name": "G. Medina-Tanco"
                    },
                    {
                        "name": "J. Meinert"
                    },
                    {
                        "name": "D. Melo"
                    },
                    {
                        "name": "A. Menshikov"
                    },
                    {
                        "name": "C. Merx"
                    },
                    {
                        "name": "S. Michal"
                    },
                    {
                        "name": "M. I. Micheletti"
                    },
                    {
                        "name": "L. Miramonti"
                    },
                    {
                        "name": "S. Mollerach"
                    },
                    {
                        "name": "F. Montanet"
                    },
                    {
                        "name": "L. Morejon"
                    },
                    {
                        "name": "K. Mulrey"
                    },
                    {
                        "name": "R. Mussa"
                    },
                    {
                        "name": "W. M. Namasaka"
                    },
                    {
                        "name": "S. Negi"
                    },
                    {
                        "name": "L. Nellen"
                    },
                    {
                        "name": "K. Nguyen"
                    },
                    {
                        "name": "G. Nicora"
                    },
                    {
                        "name": "M. Niechciol"
                    },
                    {
                        "name": "D. Nitz"
                    },
                    {
                        "name": "D. Nosek"
                    },
                    {
                        "name": "V. Novotny"
                    },
                    {
                        "name": "L. Nožka"
                    },
                    {
                        "name": "A. Nucita"
                    },
                    {
                        "name": "L. A. Núñez"
                    },
                    {
                        "name": "C. Oliveira"
                    },
                    {
                        "name": "M. Palatka"
                    },
                    {
                        "name": "J. Pallotta"
                    },
                    {
                        "name": "S. Panja"
                    },
                    {
                        "name": "G. Parente"
                    },
                    {
                        "name": "T. Paulsen"
                    },
                    {
                        "name": "J. Pawlowsky"
                    },
                    {
                        "name": "M. Pech"
                    },
                    {
                        "name": "J. Pękala"
                    },
                    {
                        "name": "R. Pelayo"
                    },
                    {
                        "name": "V. Pelgrims"
                    },
                    {
                        "name": "L. A. S. Pereira"
                    },
                    {
                        "name": "E. E. Pereira Martins"
                    },
                    {
                        "name": "C. Pérez Bertolli"
                    },
                    {
                        "name": "L. Perrone"
                    },
                    {
                        "name": "S. Petrera"
                    },
                    {
                        "name": "C. Petrucci"
                    },
                    {
                        "name": "T. Pierog"
                    },
                    {
                        "name": "M. Pimenta"
                    },
                    {
                        "name": "M. Platino"
                    },
                    {
                        "name": "B. Pont"
                    },
                    {
                        "name": "M. Pothast"
                    },
                    {
                        "name": "M. Pourmohammad Shahvar"
                    },
                    {
                        "name": "P. Privitera"
                    },
                    {
                        "name": "M. Prouza"
                    },
                    {
                        "name": "S. Querchfeld"
                    },
                    {
                        "name": "J. Rautenberg"
                    },
                    {
                        "name": "D. Ravignani"
                    },
                    {
                        "name": "J. V. Reginatto Akim"
                    },
                    {
                        "name": "M. Reininghaus"
                    },
                    {
                        "name": "A. Reuzki"
                    },
                    {
                        "name": "J. Ridky"
                    },
                    {
                        "name": "F. Riehn"
                    },
                    {
                        "name": "M. Risse"
                    },
                    {
                        "name": "V. Rizi"
                    },
                    {
                        "name": "W. Rodrigues de Carvalho"
                    },
                    {
                        "name": "E. Rodriguez"
                    },
                    {
                        "name": "J. Rodriguez Rojo"
                    },
                    {
                        "name": "M. J. Roncoroni"
                    },
                    {
                        "name": "S. Rossoni"
                    },
                    {
                        "name": "M. Roth"
                    },
                    {
                        "name": "E. Roulet"
                    },
                    {
                        "name": "A. C. Rovero"
                    },
                    {
                        "name": "A. Saftoiu"
                    },
                    {
                        "name": "M. Saharan"
                    },
                    {
                        "name": "F. Salamida"
                    },
                    {
                        "name": "H. Salazar"
                    },
                    {
                        "name": "G. Salina"
                    },
                    {
                        "name": "J. D. Sanabria Gomez"
                    },
                    {
                        "name": "F. Sánchez"
                    },
                    {
                        "name": "E. M. Santos"
                    },
                    {
                        "name": "E. Santos"
                    },
                    {
                        "name": "F. Sarazin"
                    },
                    {
                        "name": "R. Sarmento"
                    },
                    {
                        "name": "R. Sato"
                    },
                    {
                        "name": "P. Savina"
                    },
                    {
                        "name": "C. M. Schäfer"
                    },
                    {
                        "name": "V. Scherini"
                    },
                    {
                        "name": "H. Schieler"
                    },
                    {
                        "name": "M. Schimassek"
                    },
                    {
                        "name": "M. Schimp"
                    },
                    {
                        "name": "D. Schmidt"
                    },
                    {
                        "name": "O. Scholten"
                    },
                    {
                        "name": "H. Schoorlemmer"
                    },
                    {
                        "name": "P. Schovánek"
                    },
                    {
                        "name": "F. G. Schröder"
                    },
                    {
                        "name": "J. Schulte"
                    },
                    {
                        "name": "T. Schulz"
                    },
                    {
                        "name": "S. J. Sciutto"
                    },
                    {
                        "name": "M. Scornavacche"
                    },
                    {
                        "name": "A. Sedoski"
                    },
                    {
                        "name": "A. Segreto"
                    },
                    {
                        "name": "S. Sehgal"
                    },
                    {
                        "name": "S. U. Shivashankara"
                    },
                    {
                        "name": "G. Sigl"
                    },
                    {
                        "name": "K. Simkova"
                    },
                    {
                        "name": "F. Simon"
                    },
                    {
                        "name": "R. Smau"
                    },
                    {
                        "name": "R. Šmída"
                    },
                    {
                        "name": "P. Sommers"
                    },
                    {
                        "name": "R. Squartini"
                    },
                    {
                        "name": "M. Stadelmaier"
                    },
                    {
                        "name": "S. Stanič"
                    },
                    {
                        "name": "J. Stasielak"
                    },
                    {
                        "name": "P. Stassi"
                    },
                    {
                        "name": "S. Strähnz"
                    },
                    {
                        "name": "M. Straub"
                    },
                    {
                        "name": "T. Suomijärvi"
                    },
                    {
                        "name": "A. D. Supanitsky"
                    },
                    {
                        "name": "Z. Svozilikova"
                    },
                    {
                        "name": "Z. Szadkowski"
                    },
                    {
                        "name": "F. Tairli"
                    },
                    {
                        "name": "A. Tapia"
                    },
                    {
                        "name": "C. Taricco"
                    },
                    {
                        "name": "C. Timmermans"
                    },
                    {
                        "name": "O. Tkachenko"
                    },
                    {
                        "name": "P. Tobiska"
                    },
                    {
                        "name": "C. J. Todero Peixoto"
                    },
                    {
                        "name": "B. Tomé"
                    },
                    {
                        "name": "Z. Torrès"
                    },
                    {
                        "name": "A. Travaini"
                    },
                    {
                        "name": "P. Travnicek"
                    },
                    {
                        "name": "M. Tueros"
                    },
                    {
                        "name": "M. Unger"
                    },
                    {
                        "name": "R. Uzeiroska"
                    },
                    {
                        "name": "L. Vaclavek"
                    },
                    {
                        "name": "M. Vacula"
                    },
                    {
                        "name": "J. F. Valdés Galicia"
                    },
                    {
                        "name": "L. Valore"
                    },
                    {
                        "name": "E. Varela"
                    },
                    {
                        "name": "V. Vašíčková"
                    },
                    {
                        "name": "A. Vásquez-Ramírez"
                    },
                    {
                        "name": "D. Veberič"
                    },
                    {
                        "name": "I. D. Vergara Quispe"
                    },
                    {
                        "name": "V. Verzi"
                    },
                    {
                        "name": "J. Vicha"
                    },
                    {
                        "name": "J. Vink"
                    },
                    {
                        "name": "S. Vorobiov"
                    },
                    {
                        "name": "C. Watanabe"
                    },
                    {
                        "name": "A. A. Watson"
                    },
                    {
                        "name": "A. Weindl"
                    },
                    {
                        "name": "L. Wiencke"
                    },
                    {
                        "name": "H. Wilczyński"
                    },
                    {
                        "name": "D. Wittkowski"
                    },
                    {
                        "name": "B. Wundheiler"
                    },
                    {
                        "name": "B. Yue"
                    },
                    {
                        "name": "A. Yushkov"
                    },
                    {
                        "name": "O. Zapparrata"
                    },
                    {
                        "name": "E. Zas"
                    },
                    {
                        "name": "D. Zavrtanik"
                    },
                    {
                        "name": "M. Zavrtanik"
                    }
                ],
                "author_detail": {
                    "name": "M. Zavrtanik"
                },
                "author": "M. Zavrtanik",
                "arxiv_doi": "10.1103/PhysRevLett.134.021001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevLett.134.021001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.06315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Version accepted for publication in Phys. Rev. Lett., 9 pages, 3\n  figures, 1 table",
                "arxiv_journal_ref": "Phys. Rev. Lett. 134, 021001 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07315v2",
                "updated": "2025-01-16T14:04:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    14,
                    4,
                    25,
                    3,
                    16,
                    0
                ],
                "published": "2024-10-09T18:00:01Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    18,
                    0,
                    1,
                    2,
                    283,
                    0
                ],
                "title": "Advancing Tools for Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Tools for Simulation-Based Inference"
                },
                "summary": "We study the benefit of modern simulation-based inference to constrain\nparticle interactions at the LHC. We explore ways to incorporate known physics\nstructures into likelihood estimation, specifically morphing-aware estimation\nand derivative learning. Technically, we introduce a new and more efficient\nsmearing algorithm, illustrate how uncertainties can be approximated through\nrepulsive ensembles, and show how equivariant networks can improve likelihood\nestimation. After illustrating these aspects for a toy model, we target\ndi-boson production at the LHC and find that our improvements significantly\nincrease numerical control and stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the benefit of modern simulation-based inference to constrain\nparticle interactions at the LHC. We explore ways to incorporate known physics\nstructures into likelihood estimation, specifically morphing-aware estimation\nand derivative learning. Technically, we introduce a new and more efficient\nsmearing algorithm, illustrate how uncertainties can be approximated through\nrepulsive ensembles, and show how equivariant networks can improve likelihood\nestimation. After illustrating these aspects for a toy model, we target\ndi-boson production at the LHC and find that our improvements significantly\nincrease numerical control and stability."
                },
                "authors": [
                    {
                        "name": "Henning Bahl"
                    },
                    {
                        "name": "Victor Bresó"
                    },
                    {
                        "name": "Giovanni De Crescenzo"
                    },
                    {
                        "name": "Tilman Plehn"
                    }
                ],
                "author_detail": {
                    "name": "Tilman Plehn"
                },
                "author": "Tilman Plehn",
                "arxiv_comment": "26 pages, 13 figures; v2: extended results section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09534v1",
                "updated": "2025-01-16T13:36:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    36,
                    24,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:36:24Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    36,
                    24,
                    3,
                    16,
                    0
                ],
                "title": "AI in Support of Diversity and Inclusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in Support of Diversity and Inclusion"
                },
                "summary": "In this paper, we elaborate on how AI can support diversity and inclusion and\nexemplify research projects conducted in that direction. We start by looking at\nthe challenges and progress in making large language models (LLMs) more\ntransparent, inclusive, and aware of social biases. Even though LLMs like\nChatGPT have impressive abilities, they struggle to understand different\ncultural contexts and engage in meaningful, human like conversations. A key\nissue is that biases in language processing, especially in machine translation,\ncan reinforce inequality. Tackling these biases requires a multidisciplinary\napproach to ensure AI promotes diversity, fairness, and inclusion. We also\nhighlight AI's role in identifying biased content in media, which is important\nfor improving representation. By detecting unequal portrayals of social groups,\nAI can help challenge stereotypes and create more inclusive technologies.\nTransparent AI algorithms, which clearly explain their decisions, are essential\nfor building trust and reducing bias in AI systems. We also stress AI systems\nneed diverse and inclusive training data. Projects like the Child Growth\nMonitor show how using a wide range of data can help address real world\nproblems like malnutrition and poverty. We present a project that demonstrates\nhow AI can be applied to monitor the role of search engines in spreading\ndisinformation about the LGBTQ+ community. Moreover, we discuss the SignON\nproject as an example of how technology can bridge communication gaps between\nhearing and deaf people, emphasizing the importance of collaboration and mutual\ntrust in developing inclusive AI. Overall, with this paper, we advocate for AI\nsystems that are not only effective but also socially responsible, promoting\nfair and inclusive interactions between humans and machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we elaborate on how AI can support diversity and inclusion and\nexemplify research projects conducted in that direction. We start by looking at\nthe challenges and progress in making large language models (LLMs) more\ntransparent, inclusive, and aware of social biases. Even though LLMs like\nChatGPT have impressive abilities, they struggle to understand different\ncultural contexts and engage in meaningful, human like conversations. A key\nissue is that biases in language processing, especially in machine translation,\ncan reinforce inequality. Tackling these biases requires a multidisciplinary\napproach to ensure AI promotes diversity, fairness, and inclusion. We also\nhighlight AI's role in identifying biased content in media, which is important\nfor improving representation. By detecting unequal portrayals of social groups,\nAI can help challenge stereotypes and create more inclusive technologies.\nTransparent AI algorithms, which clearly explain their decisions, are essential\nfor building trust and reducing bias in AI systems. We also stress AI systems\nneed diverse and inclusive training data. Projects like the Child Growth\nMonitor show how using a wide range of data can help address real world\nproblems like malnutrition and poverty. We present a project that demonstrates\nhow AI can be applied to monitor the role of search engines in spreading\ndisinformation about the LGBTQ+ community. Moreover, we discuss the SignON\nproject as an example of how technology can bridge communication gaps between\nhearing and deaf people, emphasizing the importance of collaboration and mutual\ntrust in developing inclusive AI. Overall, with this paper, we advocate for AI\nsystems that are not only effective but also socially responsible, promoting\nfair and inclusive interactions between humans and machines."
                },
                "authors": [
                    {
                        "name": "Çiçek Güven"
                    },
                    {
                        "name": "Afra Alishahi"
                    },
                    {
                        "name": "Henry Brighton"
                    },
                    {
                        "name": "Gonzalo Nápoles"
                    },
                    {
                        "name": "Juan Sebastian Olier"
                    },
                    {
                        "name": "Marie Šafář"
                    },
                    {
                        "name": "Eric Postma"
                    },
                    {
                        "name": "Dimitar Shterionov"
                    },
                    {
                        "name": "Mirella De Sisto"
                    },
                    {
                        "name": "Eva Vanmassenhove"
                    }
                ],
                "author_detail": {
                    "name": "Eva Vanmassenhove"
                },
                "author": "Eva Vanmassenhove",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09532v1",
                "updated": "2025-01-16T13:34:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    34,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:34:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    34,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention\n  Mixture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention\n  Mixture"
                },
                "summary": "The success of VLMs often relies on the dynamic high-resolution schema that\nadaptively augments the input images to multiple crops, so that the details of\nthe images can be retained. However, such approaches result in a large number\nof redundant visual tokens, thus significantly reducing the efficiency of the\nVLMs. To improve the VLMs' efficiency without introducing extra training costs,\nmany research works are proposed to reduce the visual tokens by filtering the\nuninformative visual tokens or aggregating their information. Some approaches\npropose to reduce the visual tokens according to the self-attention of VLMs,\nwhich are biased, to result in inaccurate responses. The token reduction\napproaches solely rely on visual cues are text-agnostic, and fail to focus on\nthe areas that are most relevant to the question, especially when the queried\nobjects are non-salient to the image. In this work, we first conduct\nexperiments to show that the original text embeddings are aligned with the\nvisual tokens, without bias on the tailed visual tokens. We then propose a\nself-adaptive cross-modality attention mixture mechanism that dynamically\nleverages the effectiveness of visual saliency and text-to-image similarity in\nthe pre-LLM layers to select the visual tokens that are informative. Extensive\nexperiments demonstrate that the proposed approach achieves state-of-the-art\ntraining-free VLM acceleration performance, especially when the reduction rate\nis sufficiently large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of VLMs often relies on the dynamic high-resolution schema that\nadaptively augments the input images to multiple crops, so that the details of\nthe images can be retained. However, such approaches result in a large number\nof redundant visual tokens, thus significantly reducing the efficiency of the\nVLMs. To improve the VLMs' efficiency without introducing extra training costs,\nmany research works are proposed to reduce the visual tokens by filtering the\nuninformative visual tokens or aggregating their information. Some approaches\npropose to reduce the visual tokens according to the self-attention of VLMs,\nwhich are biased, to result in inaccurate responses. The token reduction\napproaches solely rely on visual cues are text-agnostic, and fail to focus on\nthe areas that are most relevant to the question, especially when the queried\nobjects are non-salient to the image. In this work, we first conduct\nexperiments to show that the original text embeddings are aligned with the\nvisual tokens, without bias on the tailed visual tokens. We then propose a\nself-adaptive cross-modality attention mixture mechanism that dynamically\nleverages the effectiveness of visual saliency and text-to-image similarity in\nthe pre-LLM layers to select the visual tokens that are informative. Extensive\nexperiments demonstrate that the proposed approach achieves state-of-the-art\ntraining-free VLM acceleration performance, especially when the reduction rate\nis sufficiently large."
                },
                "authors": [
                    {
                        "name": "Jiayi Han"
                    },
                    {
                        "name": "Liang Du"
                    },
                    {
                        "name": "Yiwen Wu"
                    },
                    {
                        "name": "Xiangguo Zhou"
                    },
                    {
                        "name": "Hongwei Du"
                    },
                    {
                        "name": "Weibo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Weibo Zheng"
                },
                "author": "Weibo Zheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09527v1",
                "updated": "2025-01-16T13:23:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    23,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:23:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    23,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "Confidence Estimation for Error Detection in Text-to-SQL Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Estimation for Error Detection in Text-to-SQL Systems"
                },
                "summary": "Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying the retrieval and synthesis of information. Despite the\nsuccess of large language models (LLMs) in converting natural language\nquestions into SQL queries, their broader adoption is limited by two main\nchallenges: achieving robust generalization across diverse queries and ensuring\ninterpretative confidence in their predictions. To tackle these issues, our\nresearch investigates the integration of selective classifiers into Text-to-SQL\nsystems. We analyse the trade-off between coverage and risk using entropy based\nconfidence estimation with selective classifiers and assess its impact on the\noverall performance of Text-to-SQL models. Additionally, we explore the models'\ninitial calibration and improve it with calibration techniques for better model\nalignment between confidence and accuracy. Our experimental results show that\nencoder-decoder T5 is better calibrated than in-context-learning GPT 4 and\ndecoder-only Llama 3, thus the designated external entropy-based selective\nclassifier has better performance. The study also reveal that, in terms of\nerror detection, selective classifier with a higher probability detects errors\nassociated with irrelevant questions rather than incorrect query generations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying the retrieval and synthesis of information. Despite the\nsuccess of large language models (LLMs) in converting natural language\nquestions into SQL queries, their broader adoption is limited by two main\nchallenges: achieving robust generalization across diverse queries and ensuring\ninterpretative confidence in their predictions. To tackle these issues, our\nresearch investigates the integration of selective classifiers into Text-to-SQL\nsystems. We analyse the trade-off between coverage and risk using entropy based\nconfidence estimation with selective classifiers and assess its impact on the\noverall performance of Text-to-SQL models. Additionally, we explore the models'\ninitial calibration and improve it with calibration techniques for better model\nalignment between confidence and accuracy. Our experimental results show that\nencoder-decoder T5 is better calibrated than in-context-learning GPT 4 and\ndecoder-only Llama 3, thus the designated external entropy-based selective\nclassifier has better performance. The study also reveal that, in terms of\nerror detection, selective classifier with a higher probability detects errors\nassociated with irrelevant questions rather than incorrect query generations."
                },
                "authors": [
                    {
                        "name": "Oleg Somov"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "arxiv_comment": "15 pages, 11 figures, to be published in AAAI 2025 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09521v1",
                "updated": "2025-01-16T13:16:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    16,
                    37,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:16:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    16,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "Augmenting a Large Language Model with a Combination of Text and Visual\n  Data for Conversational Visualization of Global Geospatial Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting a Large Language Model with a Combination of Text and Visual\n  Data for Conversational Visualization of Global Geospatial Data"
                },
                "summary": "We present a method for augmenting a Large Language Model (LLM) with a\ncombination of text and visual data to enable accurate question answering in\nvisualization of scientific data, making conversational visualization possible.\nLLMs struggle with tasks like visual data interaction, as they lack contextual\nvisual information. We address this problem by merging a text description of a\nvisualization and dataset with snapshots of the visualization. We extract their\nessential features into a structured text file, highly compact, yet descriptive\nenough to appropriately augment the LLM with contextual information, without\nany fine-tuning. This approach can be applied to any visualization that is\nalready finally rendered, as long as it is associated with some textual\ndescription.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a method for augmenting a Large Language Model (LLM) with a\ncombination of text and visual data to enable accurate question answering in\nvisualization of scientific data, making conversational visualization possible.\nLLMs struggle with tasks like visual data interaction, as they lack contextual\nvisual information. We address this problem by merging a text description of a\nvisualization and dataset with snapshots of the visualization. We extract their\nessential features into a structured text file, highly compact, yet descriptive\nenough to appropriately augment the LLM with contextual information, without\nany fine-tuning. This approach can be applied to any visualization that is\nalready finally rendered, as long as it is associated with some textual\ndescription."
                },
                "authors": [
                    {
                        "name": "Omar Mena"
                    },
                    {
                        "name": "Alexandre Kouyoumdjian"
                    },
                    {
                        "name": "Lonni Besançon"
                    },
                    {
                        "name": "Michael Gleicher"
                    },
                    {
                        "name": "Ivan Viola"
                    },
                    {
                        "name": "Anders Ynnerman"
                    }
                ],
                "author_detail": {
                    "name": "Anders Ynnerman"
                },
                "author": "Anders Ynnerman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09517v1",
                "updated": "2025-01-16T13:04:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    4,
                    8,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:04:08Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    4,
                    8,
                    3,
                    16,
                    0
                ],
                "title": "Recovering latent linkage structures and spillover effects with\n  structural breaks in panel data models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering latent linkage structures and spillover effects with\n  structural breaks in panel data models"
                },
                "summary": "This paper introduces a framework to analyze time-varying spillover effects\nin panel data. We consider panel models where a unit's outcome depends not only\non its own characteristics (private effects) but also on the characteristics of\nother units (spillover effects). The linkage of units is allowed to be latent\nand may shift at an unknown breakpoint. We propose a novel procedure to\nestimate the breakpoint, linkage structure, spillover and private effects. We\naddress the high-dimensionality of spillover effect parameters using penalized\nestimation, and estimate the breakpoint with refinement. We establish the\nsuper-consistency of the breakpoint estimator, ensuring that inferences about\nother parameters can proceed as if the breakpoint were known. The private\neffect parameters are estimated using a double machine learning method. The\nproposed method is applied to estimate the cross-country R&D spillovers, and we\nfind that the R&D spillovers become sparser after the financial crisis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a framework to analyze time-varying spillover effects\nin panel data. We consider panel models where a unit's outcome depends not only\non its own characteristics (private effects) but also on the characteristics of\nother units (spillover effects). The linkage of units is allowed to be latent\nand may shift at an unknown breakpoint. We propose a novel procedure to\nestimate the breakpoint, linkage structure, spillover and private effects. We\naddress the high-dimensionality of spillover effect parameters using penalized\nestimation, and estimate the breakpoint with refinement. We establish the\nsuper-consistency of the breakpoint estimator, ensuring that inferences about\nother parameters can proceed as if the breakpoint were known. The private\neffect parameters are estimated using a double machine learning method. The\nproposed method is applied to estimate the cross-country R&D spillovers, and we\nfind that the R&D spillovers become sparser after the financial crisis."
                },
                "authors": [
                    {
                        "name": "Ryo Okui"
                    },
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Wendun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wendun Wang"
                },
                "author": "Wendun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17032v2",
                "updated": "2025-01-16T12:54:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    54,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2024-05-27T10:39:18Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    10,
                    39,
                    18,
                    0,
                    148,
                    0
                ],
                "title": "Exact phylodynamic likelihood via structured Markov genealogy processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact phylodynamic likelihood via structured Markov genealogy processes"
                },
                "summary": "We consider genealogies arising from a Markov population process in which\nindividuals are categorized into a discrete collection of compartments, with\nthe requirement that individuals within the same compartment are statistically\nexchangeable. When equipped with a sampling process, each such population\nprocess induces a time-evolving tree-valued process defined as the genealogy of\nall sampled individuals. We provide a construction of this genealogy process\nand derive exact expressions for the likelihood of an observed genealogy in\nterms of filter equations. These filter equations can be numerically solved\nusing standard Monte Carlo integration methods. Thus, we obtain statistically\nefficient likelihood-based inference for essentially arbitrary compartment\nmodels based on an observed genealogy of individuals sampled from the\npopulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider genealogies arising from a Markov population process in which\nindividuals are categorized into a discrete collection of compartments, with\nthe requirement that individuals within the same compartment are statistically\nexchangeable. When equipped with a sampling process, each such population\nprocess induces a time-evolving tree-valued process defined as the genealogy of\nall sampled individuals. We provide a construction of this genealogy process\nand derive exact expressions for the likelihood of an observed genealogy in\nterms of filter equations. These filter equations can be numerically solved\nusing standard Monte Carlo integration methods. Thus, we obtain statistically\nefficient likelihood-based inference for essentially arbitrary compartment\nmodels based on an observed genealogy of individuals sampled from the\npopulation."
                },
                "authors": [
                    {
                        "name": "Aaron A. King"
                    },
                    {
                        "name": "Qianying Lin"
                    },
                    {
                        "name": "Edward L. Ionides"
                    }
                ],
                "author_detail": {
                    "name": "Edward L. Ionides"
                },
                "author": "Edward L. Ionides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13187v3",
                "updated": "2025-01-16T12:46:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    46,
                    53,
                    3,
                    16,
                    0
                ],
                "published": "2024-10-17T03:32:02Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    3,
                    32,
                    2,
                    3,
                    291,
                    0
                ],
                "title": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code\n  Processing"
                },
                "summary": "Large Language Models (LLMs) have been widely used in code completion, and\nresearchers are focusing on scaling up LLMs to improve their accuracy. However,\nlarger LLMs have lower inference efficiency, affecting developers' experience\nand productivity. In this paper, we propose a lightweight and effective LLM for\ncode completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B\nachieves higher code completion accuracy while having smaller scales (i.e., 7\nbillion parameters). We attribute the superiority of aiXcoder-7B to three key\nfactors: (1) Multi-objective training. We employ three training objectives, one\nof which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers\nthe syntax structures in code and effectively improves the performance of LLMs\nfor code. (2) Diverse data sampling strategies. They consider inter-file\nrelationships and enhance the capability of LLMs in understanding cross-file\ncontexts. (3) Extensive high-quality data. We establish a rigorous data\ncollection pipeline and consume a total of 1.2 trillion unique tokens for\ntraining aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a\nbroad distribution of code. We evaluate aiXcoder-7B in five popular code\ncompletion benchmarks and a new benchmark collected by this paper. The results\nshow that aiXcoder-7B outperforms the latest six LLMs with similar sizes and\neven surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),\npositioning aiXcoder-7B as a lightweight and effective LLM for academia and\nindustry. Finally, we summarize three valuable insights for helping\npractitioners train the next generations of LLMs for code. aiXcoder-7B has been\nopen-souced and gained significant attention. Until January 2025, aiXcoder-7B\nhas received 2,226 GitHub Stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used in code completion, and\nresearchers are focusing on scaling up LLMs to improve their accuracy. However,\nlarger LLMs have lower inference efficiency, affecting developers' experience\nand productivity. In this paper, we propose a lightweight and effective LLM for\ncode completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B\nachieves higher code completion accuracy while having smaller scales (i.e., 7\nbillion parameters). We attribute the superiority of aiXcoder-7B to three key\nfactors: (1) Multi-objective training. We employ three training objectives, one\nof which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers\nthe syntax structures in code and effectively improves the performance of LLMs\nfor code. (2) Diverse data sampling strategies. They consider inter-file\nrelationships and enhance the capability of LLMs in understanding cross-file\ncontexts. (3) Extensive high-quality data. We establish a rigorous data\ncollection pipeline and consume a total of 1.2 trillion unique tokens for\ntraining aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a\nbroad distribution of code. We evaluate aiXcoder-7B in five popular code\ncompletion benchmarks and a new benchmark collected by this paper. The results\nshow that aiXcoder-7B outperforms the latest six LLMs with similar sizes and\neven surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),\npositioning aiXcoder-7B as a lightweight and effective LLM for academia and\nindustry. Finally, we summarize three valuable insights for helping\npractitioners train the next generations of LLMs for code. aiXcoder-7B has been\nopen-souced and gained significant attention. Until January 2025, aiXcoder-7B\nhas received 2,226 GitHub Stars."
                },
                "authors": [
                    {
                        "name": "Siyuan Jiang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "He Zong"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Shukai Hu"
                    },
                    {
                        "name": "Erlu Li"
                    },
                    {
                        "name": "Jiazheng Ding"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Wei Ning"
                    },
                    {
                        "name": "Gen Wang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "arxiv_comment": "(1) Accepted by the 47th International Conference on Software\n  Engineering (ICSE 2025). (2) aiXcoder-7B is available at\n  https://github.com/aixcoder-plugin/aiXcoder-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09495v1",
                "updated": "2025-01-16T12:16:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    16,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T12:16:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    16,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "Resolving the $χ_{\\rm eff}$-$q$ correlation among Coalescing Binary\n  Black Holes and Evidence for AGN-driven Hierarchical Mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving the $χ_{\\rm eff}$-$q$ correlation among Coalescing Binary\n  Black Holes and Evidence for AGN-driven Hierarchical Mergers"
                },
                "summary": "The origin of the correlation between the effective spins ($\\chi_{\\rm eff}$)\nand mass ratios ($q$) of LIGO-Virgo-KAGRA's binary black holes (BBHs) is still\nan open question. Motivated by recent identification of two subpopulations of\nthe BBHs, in this work we investigate the potential $\\chi_{\\rm eff}-q$\ncorrelation for each subpopulation. Surprisingly, the $\\chi_{\\rm eff}$-$q$\ncorrelation vanishes for the low-mass subpopulation if we introduce a second\n$\\chi_{\\rm eff}$ distribution for the high-mass subpopulation likely\noriginating from hierarchical mergers. The first subpopulation has a narrow\n$\\chi_{\\rm eff}$ distribution peaking at $\\sim0.05$, whose primary-mass\nfunction cuts off at $\\sim 45M_{\\odot}$, in agreement with first-generation\nBBHs. The second $\\chi_{\\rm eff}$ distribution is broad and peaks at\n$\\mu_{\\chi,2}=0.35^{+0.18}_{-0.22}$, consistent with the expectation of\nhierarchical mergers formed in the disks of active galactic nucleus (AGNs). We\ninfer $\\mu_{\\chi,2}>0$ at 98.7\\% credible level, and a symmetric $\\chi_{\\rm\neff}$ distribution for the second subpopulation is disfavored by\n$\\mathcal{B}\\sim5$. However, negative values of $\\chi_{\\rm eff}$ are also\nmeasured, indicating that the hierarchical mergers may take place within both\nstar clusters and AGN disks. We find a Bayes factor of $\\ln\\mathcal{B}=5.2$ for\ntwo distinct $\\chi_{\\rm eff}$ distributions relative to single $\\chi_{\\rm eff}$\ndistribution that conditioned on mass ratios. Therefore we conclude that the\n$\\chi_{\\rm eff}$-$q$ correlation in the entire population can be explained as\nthe superposition of two subpopulations. Additionally, we suggest to use a\nflexible mass function to reduce the bias in $\\chi_{\\rm eff}$-$q$ correlation\nthat may be introduced by model mis-specification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The origin of the correlation between the effective spins ($\\chi_{\\rm eff}$)\nand mass ratios ($q$) of LIGO-Virgo-KAGRA's binary black holes (BBHs) is still\nan open question. Motivated by recent identification of two subpopulations of\nthe BBHs, in this work we investigate the potential $\\chi_{\\rm eff}-q$\ncorrelation for each subpopulation. Surprisingly, the $\\chi_{\\rm eff}$-$q$\ncorrelation vanishes for the low-mass subpopulation if we introduce a second\n$\\chi_{\\rm eff}$ distribution for the high-mass subpopulation likely\noriginating from hierarchical mergers. The first subpopulation has a narrow\n$\\chi_{\\rm eff}$ distribution peaking at $\\sim0.05$, whose primary-mass\nfunction cuts off at $\\sim 45M_{\\odot}$, in agreement with first-generation\nBBHs. The second $\\chi_{\\rm eff}$ distribution is broad and peaks at\n$\\mu_{\\chi,2}=0.35^{+0.18}_{-0.22}$, consistent with the expectation of\nhierarchical mergers formed in the disks of active galactic nucleus (AGNs). We\ninfer $\\mu_{\\chi,2}>0$ at 98.7\\% credible level, and a symmetric $\\chi_{\\rm\neff}$ distribution for the second subpopulation is disfavored by\n$\\mathcal{B}\\sim5$. However, negative values of $\\chi_{\\rm eff}$ are also\nmeasured, indicating that the hierarchical mergers may take place within both\nstar clusters and AGN disks. We find a Bayes factor of $\\ln\\mathcal{B}=5.2$ for\ntwo distinct $\\chi_{\\rm eff}$ distributions relative to single $\\chi_{\\rm eff}$\ndistribution that conditioned on mass ratios. Therefore we conclude that the\n$\\chi_{\\rm eff}$-$q$ correlation in the entire population can be explained as\nthe superposition of two subpopulations. Additionally, we suggest to use a\nflexible mass function to reduce the bias in $\\chi_{\\rm eff}$-$q$ correlation\nthat may be introduced by model mis-specification."
                },
                "authors": [
                    {
                        "name": "Yin-Jie Li"
                    },
                    {
                        "name": "Yuan-Zhu Wang"
                    },
                    {
                        "name": "Shao-Peng Tang"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Yi-Zhong Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Zhong Fan"
                },
                "author": "Yi-Zhong Fan",
                "arxiv_comment": "13 pages, 7 figures, comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09494v1",
                "updated": "2025-01-16T12:07:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    7,
                    24,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T12:07:24Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    7,
                    24,
                    3,
                    16,
                    0
                ],
                "title": "On the robustness of exoplanet atmospheric detections: insights from\n  extensive simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the robustness of exoplanet atmospheric detections: insights from\n  extensive simulations"
                },
                "summary": "The classical picture of our Solar System being the archetypal outcome of\nplanet formation has been rendered obsolete by the astonishing diversity of\nextrasolar-system architectures. From rare hot-Jupiters to abundant\nsuper-Earths and sub-Neptunes, most detected exoplanets have no analogs in our\nsystem, and their interior and atmospheric compositions remain largely unknown.\nFortunately, new methodologies enable us to analyze exoplanet atmospheres,\ninferring their compositions, temperatures, dynamics, and even formation\npathways. Specifically, ground-based high-resolution Doppler spectroscopy\n(HRDS) can disentangle spectral-line profiles of weak exo-atmospheric signals\nfrom the dominating features of Earth's atmosphere in the observed flux. For\nover a decade, HRDS has focused on hot Jupiters (close-orbiting gas giants) due\nto their high signal-to-noise ratio, which makes them ideal laboratories for\nadvancing our knowledge. However, there have been concerns regarding potential\nbiases in exo-atmospheric-detection methods, hindering comparative planetology.\nHere we propose a modeling framework based on extensive simulations of HRDS\nexo-atmospheric observations to systematically explore in-silico underlying\nbiases in commonly-used pipelines, particularly under the presence of\nobservational noise. Our findings show that exo-atmospheric\ndetection-significances are highly contingent on details of the\nanalysis-pipeline used, with different techniques responding differently to\nnoise: A given technique may fail to recover a true-signal that is detected by\nanother. Noise effects in the computed significances are non-trivial and\npipeline-dependent. Statistical analyses provide a complementary tool to\ncontextualize signal-significances, which will gain in relevance as we move\ntowards studying the atmospheres of smaller, potentially habitable exoplanets\nwith even weaker signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classical picture of our Solar System being the archetypal outcome of\nplanet formation has been rendered obsolete by the astonishing diversity of\nextrasolar-system architectures. From rare hot-Jupiters to abundant\nsuper-Earths and sub-Neptunes, most detected exoplanets have no analogs in our\nsystem, and their interior and atmospheric compositions remain largely unknown.\nFortunately, new methodologies enable us to analyze exoplanet atmospheres,\ninferring their compositions, temperatures, dynamics, and even formation\npathways. Specifically, ground-based high-resolution Doppler spectroscopy\n(HRDS) can disentangle spectral-line profiles of weak exo-atmospheric signals\nfrom the dominating features of Earth's atmosphere in the observed flux. For\nover a decade, HRDS has focused on hot Jupiters (close-orbiting gas giants) due\nto their high signal-to-noise ratio, which makes them ideal laboratories for\nadvancing our knowledge. However, there have been concerns regarding potential\nbiases in exo-atmospheric-detection methods, hindering comparative planetology.\nHere we propose a modeling framework based on extensive simulations of HRDS\nexo-atmospheric observations to systematically explore in-silico underlying\nbiases in commonly-used pipelines, particularly under the presence of\nobservational noise. Our findings show that exo-atmospheric\ndetection-significances are highly contingent on details of the\nanalysis-pipeline used, with different techniques responding differently to\nnoise: A given technique may fail to recover a true-signal that is detected by\nanother. Noise effects in the computed significances are non-trivial and\npipeline-dependent. Statistical analyses provide a complementary tool to\ncontextualize signal-significances, which will gain in relevance as we move\ntowards studying the atmospheres of smaller, potentially habitable exoplanets\nwith even weaker signals."
                },
                "authors": [
                    {
                        "name": "A. Sánchez-López"
                    },
                    {
                        "name": "Ana P. Millán"
                    }
                ],
                "author_detail": {
                    "name": "Ana P. Millán"
                },
                "author": "Ana P. Millán",
                "arxiv_comment": "25 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09493v1",
                "updated": "2025-01-16T12:06:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    6,
                    56,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T12:06:56Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    6,
                    56,
                    3,
                    16,
                    0
                ],
                "title": "Evaluating Conversational Recommender Systems with Large Language\n  Models: A User-Centric Evaluation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Conversational Recommender Systems with Large Language\n  Models: A User-Centric Evaluation Framework"
                },
                "summary": "Conversational recommender systems (CRS) involve both recommendation and\ndialogue tasks, which makes their evaluation a unique challenge. Although past\nresearch has analyzed various factors that may affect user satisfaction with\nCRS interactions from the perspective of user studies, few evaluation metrics\nfor CRS have been proposed. Recent studies have shown that LLMs can align with\nhuman preferences, and several LLM-based text quality evaluation measures have\nbeen introduced. However, the application of LLMs in CRS evaluation remains\nrelatively limited. To address this research gap and advance the development of\nuser-centric conversational recommender systems, this study proposes an\nautomated LLM-based CRS evaluation framework, building upon existing research\nin human-computer interaction and psychology. The framework evaluates CRS from\nfour dimensions: dialogue behavior, language expression, recommendation items,\nand response content. We use this framework to evaluate four different\nconversational recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational recommender systems (CRS) involve both recommendation and\ndialogue tasks, which makes their evaluation a unique challenge. Although past\nresearch has analyzed various factors that may affect user satisfaction with\nCRS interactions from the perspective of user studies, few evaluation metrics\nfor CRS have been proposed. Recent studies have shown that LLMs can align with\nhuman preferences, and several LLM-based text quality evaluation measures have\nbeen introduced. However, the application of LLMs in CRS evaluation remains\nrelatively limited. To address this research gap and advance the development of\nuser-centric conversational recommender systems, this study proposes an\nautomated LLM-based CRS evaluation framework, building upon existing research\nin human-computer interaction and psychology. The framework evaluates CRS from\nfour dimensions: dialogue behavior, language expression, recommendation items,\nand response content. We use this framework to evaluate four different\nconversational recommender systems."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "name": "Zhenhua Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Dong"
                },
                "author": "Zhenhua Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16485v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16485v3",
                "updated": "2025-01-16T11:59:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    59,
                    2,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-23T14:00:18Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    14,
                    0,
                    18,
                    1,
                    205,
                    0
                ],
                "title": "Learning Constraint Network from Demonstrations via Positive-Unlabeled\n  Learning with Memory Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Constraint Network from Demonstrations via Positive-Unlabeled\n  Learning with Memory Replay"
                },
                "summary": "Planning for a wide range of real-world tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. The majority of prior works\nlimit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint\nfrom demonstration. From a PU learning view, We treat all data in\ndemonstrations as positive (feasible) data, and learn a (sub)-optimal policy to\ngenerate high-reward-winning but potentially infeasible trajectories, which\nserve as unlabeled data containing both feasible and infeasible states. Under\nan assumption on data distribution, a feasible-infeasible classifier (i.e.,\nconstraint model) is learned from the two datasets through a postprocessing PU\nlearning technique. The entire method employs an iterative framework\nalternating between updating the policy, which generates and selects\nhigher-reward policies, and updating the constraint model. Additionally, a\nmemory buffer is introduced to record and reuse samples from previous\niterations to prevent forgetting. The effectiveness of the proposed method is\nvalidated in two Mujoco environments, successfully inferring continuous\nnonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning for a wide range of real-world tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. The majority of prior works\nlimit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint\nfrom demonstration. From a PU learning view, We treat all data in\ndemonstrations as positive (feasible) data, and learn a (sub)-optimal policy to\ngenerate high-reward-winning but potentially infeasible trajectories, which\nserve as unlabeled data containing both feasible and infeasible states. Under\nan assumption on data distribution, a feasible-infeasible classifier (i.e.,\nconstraint model) is learned from the two datasets through a postprocessing PU\nlearning technique. The entire method employs an iterative framework\nalternating between updating the policy, which generates and selects\nhigher-reward policies, and updating the constraint model. Additionally, a\nmemory buffer is introduced to record and reuse samples from previous\niterations to prevent forgetting. The effectiveness of the proposed method is\nvalidated in two Mujoco environments, successfully inferring continuous\nnonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety."
                },
                "authors": [
                    {
                        "name": "Baiyu Peng"
                    },
                    {
                        "name": "Aude Billard"
                    }
                ],
                "author_detail": {
                    "name": "Aude Billard"
                },
                "author": "Aude Billard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16485v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16485v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09483v1",
                "updated": "2025-01-16T11:40:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    40,
                    48,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T11:40:48Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    40,
                    48,
                    3,
                    16,
                    0
                ],
                "title": "Semiparametrics via parametrics and contiguity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametrics via parametrics and contiguity"
                },
                "summary": "Inference on the parametric part of a semiparametric model is no trivial\ntask. On the other hand, if one approximates the infinite dimensional part of\nthe semiparametric model by a parametric function, one obtains a parametric\nmodel that is in some sense close to the semiparametric model; and inference\nmay proceed by the method of maximum likelihood. Under regularity conditions,\nand assuming that the approximating parametric model in fact generated the\ndata, the ensuing maximum likelihood estimator is asymptotically normal and\nefficient (in the approximating parametric model). Thus one obtains a sequence\nof asymptotically normal and efficient estimators in a sequence of growing\nparametric models that approximate the semiparametric model and, intuitively,\nthe limiting {`}semiparametric{'} estimator should be asymptotically normal and\nefficient as well. In this paper we make this intuition rigorous. Consequently,\nwe are able to move much of the semiparametric analysis back into classical\nparametric terrain, and then translate our parametric results back to the\nsemiparametric world by way of contiguity. Our approach departs from the sieve\nliterature by being more specific about the approximating parametric models, by\nworking under these when treating the parametric models, and by taking\nadvantage of the mutual contiguity between the parametric and semiparametric\nmodels to lift conclusions about the former to conclusions about the latter. We\nillustrate our theory with two canonical examples of semiparametric models,\nnamely the partially linear regression model and the Cox regression model. An\nupshot of our theory is a new, relatively simple, and rather parametric proof\nof the efficiency of the Cox partial likelihood estimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on the parametric part of a semiparametric model is no trivial\ntask. On the other hand, if one approximates the infinite dimensional part of\nthe semiparametric model by a parametric function, one obtains a parametric\nmodel that is in some sense close to the semiparametric model; and inference\nmay proceed by the method of maximum likelihood. Under regularity conditions,\nand assuming that the approximating parametric model in fact generated the\ndata, the ensuing maximum likelihood estimator is asymptotically normal and\nefficient (in the approximating parametric model). Thus one obtains a sequence\nof asymptotically normal and efficient estimators in a sequence of growing\nparametric models that approximate the semiparametric model and, intuitively,\nthe limiting {`}semiparametric{'} estimator should be asymptotically normal and\nefficient as well. In this paper we make this intuition rigorous. Consequently,\nwe are able to move much of the semiparametric analysis back into classical\nparametric terrain, and then translate our parametric results back to the\nsemiparametric world by way of contiguity. Our approach departs from the sieve\nliterature by being more specific about the approximating parametric models, by\nworking under these when treating the parametric models, and by taking\nadvantage of the mutual contiguity between the parametric and semiparametric\nmodels to lift conclusions about the former to conclusions about the latter. We\nillustrate our theory with two canonical examples of semiparametric models,\nnamely the partially linear regression model and the Cox regression model. An\nupshot of our theory is a new, relatively simple, and rather parametric proof\nof the efficiency of the Cox partial likelihood estimator."
                },
                "authors": [
                    {
                        "name": "Adam Lee"
                    },
                    {
                        "name": "Emil A. Stoltenberg"
                    },
                    {
                        "name": "Per A. Mykland"
                    }
                ],
                "author_detail": {
                    "name": "Per A. Mykland"
                },
                "author": "Per A. Mykland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09475v1",
                "updated": "2025-01-16T11:27:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    27,
                    25,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T11:27:25Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    27,
                    25,
                    3,
                    16,
                    0
                ],
                "title": "Guided Debugging of Auto-Translated Code Using Differential Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Debugging of Auto-Translated Code Using Differential Testing"
                },
                "summary": "Large Language Models (LLMs) hold great promise in the task of code\ntranslation. However, the lack of explainability complicates the identification\nof the inevitable translation errors. In this paper, we propose tHinter, a\ndebugging tool to locate translation errors in auto-translated code. The core\nidea of tHinter is that correctly translated, the source and translated code\nshould present the same functionalities, giving the same output for the same\ninput. Hence, lines in the translated code responsible for output differences\nare possibly translation errors. First, tHinter employs fuzzing to generate\ndiverse test cases that thoroughly explore the translated code. Then, tHinter\nrelies on a heuristic algorithm to pinpoint translation errors from coverage\ninformation and differential testing execution results of those test cases.\nThis heuristic algorithm is designed to leverage both the statistics and the\nexpertise of developers. Comprehensive experiments with real code show its\neffectiveness. It reduces 71% lines developers need to review during debugging\nand increases the likelihood of the LLM fixing translation errors in a single\nquery by 59%. Developers generally consider it satisfactory and helpful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold great promise in the task of code\ntranslation. However, the lack of explainability complicates the identification\nof the inevitable translation errors. In this paper, we propose tHinter, a\ndebugging tool to locate translation errors in auto-translated code. The core\nidea of tHinter is that correctly translated, the source and translated code\nshould present the same functionalities, giving the same output for the same\ninput. Hence, lines in the translated code responsible for output differences\nare possibly translation errors. First, tHinter employs fuzzing to generate\ndiverse test cases that thoroughly explore the translated code. Then, tHinter\nrelies on a heuristic algorithm to pinpoint translation errors from coverage\ninformation and differential testing execution results of those test cases.\nThis heuristic algorithm is designed to leverage both the statistics and the\nexpertise of developers. Comprehensive experiments with real code show its\neffectiveness. It reduces 71% lines developers need to review during debugging\nand increases the likelihood of the LLM fixing translation errors in a single\nquery by 59%. Developers generally consider it satisfactory and helpful."
                },
                "authors": [
                    {
                        "name": "Shengnan Wu"
                    },
                    {
                        "name": "Xinyu Sun"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yangfan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yangfan Zhou"
                },
                "author": "Yangfan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22944v2",
                "updated": "2025-01-16T11:26:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    26,
                    2,
                    3,
                    16,
                    0
                ],
                "published": "2024-10-30T12:01:48Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    1,
                    48,
                    2,
                    304,
                    0
                ],
                "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification"
                },
                "summary": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments."
                },
                "authors": [
                    {
                        "name": "Tom A. Lamb"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Alasdair Paren"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Francesco Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pinto"
                },
                "author": "Francesco Pinto",
                "arxiv_comment": "28pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09465v1",
                "updated": "2025-01-16T10:56:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    56,
                    45,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T10:56:45Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    56,
                    45,
                    3,
                    16,
                    0
                ],
                "title": "RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and\n  Offloading for Edge Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and\n  Offloading for Edge Object Detection"
                },
                "summary": "Object detection plays a crucial role in smart video analysis, with\napplications ranging from autonomous driving and security to smart cities.\nHowever, achieving real-time object detection on edge devices presents\nsignificant challenges due to their limited computational resources and the\nhigh demands of deep neural network (DNN)-based detection models, particularly\nwhen processing high-resolution video. Conventional strategies, such as input\ndown-sampling and network up-scaling, often compromise detection accuracy for\nfaster performance or lead to higher inference latency. To address these\nissues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven\nPartitioning and Edge Offloading framework designed to optimize the\naccuracy-latency trade-off in resource-constrained edge environments. Our\napproach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that\npartitions video frames into non-uniform blocks based on object distribution\nand the computational characteristics of DNNs. Furthermore, a parallel edge\noffloading scheme is implemented to distribute these blocks across multiple\nedge servers for concurrent processing. Experimental evaluations show that\nRE-POSE significantly enhances detection accuracy and reduces inference\nlatency, surpassing existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection plays a crucial role in smart video analysis, with\napplications ranging from autonomous driving and security to smart cities.\nHowever, achieving real-time object detection on edge devices presents\nsignificant challenges due to their limited computational resources and the\nhigh demands of deep neural network (DNN)-based detection models, particularly\nwhen processing high-resolution video. Conventional strategies, such as input\ndown-sampling and network up-scaling, often compromise detection accuracy for\nfaster performance or lead to higher inference latency. To address these\nissues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven\nPartitioning and Edge Offloading framework designed to optimize the\naccuracy-latency trade-off in resource-constrained edge environments. Our\napproach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that\npartitions video frames into non-uniform blocks based on object distribution\nand the computational characteristics of DNNs. Furthermore, a parallel edge\noffloading scheme is implemented to distribute these blocks across multiple\nedge servers for concurrent processing. Experimental evaluations show that\nRE-POSE significantly enhances detection accuracy and reduces inference\nlatency, surpassing existing methods."
                },
                "authors": [
                    {
                        "name": "Jianrui Shi"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "Zeyang Cui"
                    },
                    {
                        "name": "Xiaoming Shen"
                    },
                    {
                        "name": "Minhang Zeng"
                    },
                    {
                        "name": "Xiaojie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Liu"
                },
                "author": "Xiaojie Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09464v1",
                "updated": "2025-01-16T10:55:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    55,
                    5,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T10:55:05Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    55,
                    5,
                    3,
                    16,
                    0
                ],
                "title": "Pruning for Sparse Diffusion Models based on Gradient Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning for Sparse Diffusion Models based on Gradient Flow"
                },
                "summary": "Diffusion Models (DMs) have impressive capabilities among generation models,\nbut are limited to slower inference speeds and higher computational costs.\nPrevious works utilize one-shot structure pruning to derive lightweight DMs\nfrom pre-trained ones, but this approach often leads to a significant drop in\ngeneration quality and may result in the removal of crucial weights. Thus we\npropose a iterative pruning method based on gradient flow, including the\ngradient flow pruning process and the gradient flow pruning criterion. We\nemploy a progressive soft pruning strategy to maintain the continuity of the\nmask matrix and guide it along the gradient flow of the energy function based\non the pruning criterion in sparse space, thereby avoiding the sudden\ninformation loss typically caused by one-shot pruning. Gradient-flow based\ncriterion prune parameters whose removal increases the gradient norm of loss\nfunction and can enable fast convergence for a pruned model in iterative\npruning stage. Our extensive experiments on widely used datasets demonstrate\nthat our method achieves superior performance in efficiency and consistency\nwith pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models (DMs) have impressive capabilities among generation models,\nbut are limited to slower inference speeds and higher computational costs.\nPrevious works utilize one-shot structure pruning to derive lightweight DMs\nfrom pre-trained ones, but this approach often leads to a significant drop in\ngeneration quality and may result in the removal of crucial weights. Thus we\npropose a iterative pruning method based on gradient flow, including the\ngradient flow pruning process and the gradient flow pruning criterion. We\nemploy a progressive soft pruning strategy to maintain the continuity of the\nmask matrix and guide it along the gradient flow of the energy function based\non the pruning criterion in sparse space, thereby avoiding the sudden\ninformation loss typically caused by one-shot pruning. Gradient-flow based\ncriterion prune parameters whose removal increases the gradient norm of loss\nfunction and can enable fast convergence for a pruned model in iterative\npruning stage. Our extensive experiments on widely used datasets demonstrate\nthat our method achieves superior performance in efficiency and consistency\nwith pre-trained models."
                },
                "authors": [
                    {
                        "name": "Ben Wan"
                    },
                    {
                        "name": "Tianyi Zheng"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Yuxiao Wang"
                    },
                    {
                        "name": "Jia Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jia Wang"
                },
                "author": "Jia Wang",
                "arxiv_comment": "5 pages, 1 figure, accepted by ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19185v2",
                "updated": "2025-01-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    54,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-27T14:03:49Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    14,
                    3,
                    49,
                    3,
                    179,
                    0
                ],
                "title": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a\n  supervised-friendly fashion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a\n  supervised-friendly fashion"
                },
                "summary": "Reinforcement Learning (RL) has been used to finetune Large Language Models\n(LLMs) using a reward model trained from preference data, to better align with\nhuman judgment. The recently introduced direct alignment methods, which are\noften simpler, more stable, and computationally lighter, can more directly\nachieve this. However, these approaches cannot optimize arbitrary rewards, and\nthe preference-based ones are not the only rewards of interest for LLMs (eg.,\nunit tests for code generation or textual entailment for summarization, among\nothers). RL-finetuning is usually done with a variation of policy gradient,\nwhich calls for on-policy or near-on-policy samples, requiring costly\ngenerations. We introduce Contrastive Policy Gradient, or CoPG, a simple and\nmathematically principled new RL algorithm that can estimate the optimal policy\neven from off-policy data. It can be seen as an off-policy policy gradient\napproach that does not rely on important sampling techniques and highlights the\nimportance of using (the right) state baseline. We show this approach to\ngeneralize the direct alignment method IPO (identity preference optimization)\nand classic policy gradient. We experiment with the proposed CoPG on a toy\nbandit problem to illustrate its properties, as well as for finetuning LLMs on\na summarization task, using a learned reward function considered as ground\ntruth for the purpose of the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has been used to finetune Large Language Models\n(LLMs) using a reward model trained from preference data, to better align with\nhuman judgment. The recently introduced direct alignment methods, which are\noften simpler, more stable, and computationally lighter, can more directly\nachieve this. However, these approaches cannot optimize arbitrary rewards, and\nthe preference-based ones are not the only rewards of interest for LLMs (eg.,\nunit tests for code generation or textual entailment for summarization, among\nothers). RL-finetuning is usually done with a variation of policy gradient,\nwhich calls for on-policy or near-on-policy samples, requiring costly\ngenerations. We introduce Contrastive Policy Gradient, or CoPG, a simple and\nmathematically principled new RL algorithm that can estimate the optimal policy\neven from off-policy data. It can be seen as an off-policy policy gradient\napproach that does not rely on important sampling techniques and highlights the\nimportance of using (the right) state baseline. We show this approach to\ngeneralize the direct alignment method IPO (identity preference optimization)\nand classic policy gradient. We experiment with the proposed CoPG on a toy\nbandit problem to illustrate its properties, as well as for finetuning LLMs on\na summarization task, using a learned reward function considered as ground\ntruth for the purpose of the experiments."
                },
                "authors": [
                    {
                        "name": "Yannis Flet-Berliac"
                    },
                    {
                        "name": "Nathan Grinsztajn"
                    },
                    {
                        "name": "Florian Strub"
                    },
                    {
                        "name": "Bill Wu"
                    },
                    {
                        "name": "Eugene Choi"
                    },
                    {
                        "name": "Chris Cremer"
                    },
                    {
                        "name": "Arash Ahmadian"
                    },
                    {
                        "name": "Yash Chandak"
                    },
                    {
                        "name": "Mohammad Gheshlaghi Azar"
                    },
                    {
                        "name": "Olivier Pietquin"
                    },
                    {
                        "name": "Matthieu Geist"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Geist"
                },
                "author": "Matthieu Geist",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09457v1",
                "updated": "2025-01-16T10:33:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    33,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T10:33:42Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    33,
                    42,
                    3,
                    16,
                    0
                ],
                "title": "\"A Great Start, But...\": Evaluating LLM-Generated Mind Maps for\n  Information Mapping in Video-Based Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"A Great Start, But...\": Evaluating LLM-Generated Mind Maps for\n  Information Mapping in Video-Based Design"
                },
                "summary": "Extracting concepts and understanding relationships from videos is essential\nin Video-Based Design (VBD), where videos serve as a primary medium for\nexploration but require significant effort in managing meta-information. Mind\nmaps, with their ability to visually organize complex data, offer a promising\napproach for structuring and analysing video content. Recent advancements in\nLarge Language Models (LLMs) provide new opportunities for meta-information\nprocessing and visual understanding in VBD, yet their application remains\nunderexplored. This study recruited 28 VBD practitioners to investigate the use\nof prompt-tuned LLMs for generating mind maps from ethnographic videos.\nComparing LLM-generated mind maps with those created by professional designers,\nwe evaluated rated scores, design effectiveness, and user experience across two\ncontexts. Findings reveal that LLMs effectively capture central concepts but\nstruggle with hierarchical organization and contextual grounding. We discuss\ntrust, customization, and workflow integration as key factors to guide future\nresearch on LLM-supported information mapping in VBD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting concepts and understanding relationships from videos is essential\nin Video-Based Design (VBD), where videos serve as a primary medium for\nexploration but require significant effort in managing meta-information. Mind\nmaps, with their ability to visually organize complex data, offer a promising\napproach for structuring and analysing video content. Recent advancements in\nLarge Language Models (LLMs) provide new opportunities for meta-information\nprocessing and visual understanding in VBD, yet their application remains\nunderexplored. This study recruited 28 VBD practitioners to investigate the use\nof prompt-tuned LLMs for generating mind maps from ethnographic videos.\nComparing LLM-generated mind maps with those created by professional designers,\nwe evaluated rated scores, design effectiveness, and user experience across two\ncontexts. Findings reveal that LLMs effectively capture central concepts but\nstruggle with hierarchical organization and contextual grounding. We discuss\ntrust, customization, and workflow integration as key factors to guide future\nresearch on LLM-supported information mapping in VBD."
                },
                "authors": [
                    {
                        "name": "Tianhao He"
                    },
                    {
                        "name": "Karthi Saravanan"
                    },
                    {
                        "name": "Evangelos Niforatos"
                    },
                    {
                        "name": "Gerd Kortuem"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Kortuem"
                },
                "author": "Gerd Kortuem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01622v2",
                "updated": "2025-01-16T10:30:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    30,
                    40,
                    3,
                    16,
                    0
                ],
                "published": "2024-08-03T01:09:48Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    1,
                    9,
                    48,
                    5,
                    216,
                    0
                ],
                "title": "Positive-Unlabeled Constraint Learning for Inferring Nonlinear\n  Continuous Constraints Functions from Expert Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positive-Unlabeled Constraint Learning for Inferring Nonlinear\n  Continuous Constraints Functions from Expert Demonstrations"
                },
                "summary": "Planning for diverse real-world robotic tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. This paper presents a novel\ntwo-step Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a\ncontinuous constraint function from demonstrations, without requiring prior\nknowledge of the true constraint parameterization or environmental model as\nexisting works. We treat all data in demonstrations as positive (feasible)\ndata, and learn a control policy to generate potentially infeasible\ntrajectories, which serve as unlabeled data. The proposed two-step learning\nframework first identifies reliable infeasible data using a distance metric,\nand secondly learns a binary feasibility classifier (i.e., constraint function)\nfrom the feasible demonstrations and reliable infeasible data. The proposed\nmethod is flexible to learn complex-shaped constraint boundary and will not\nmistakenly classify demonstrations as infeasible as previous methods. The\neffectiveness of the proposed method is verified in four constrained\nenvironments, using a networked policy or a dynamical system policy. It\nsuccessfully infers the continuous nonlinear constraints and outperforms other\nbaseline methods in terms of constraint accuracy and policy safety. This work\nhas been published in IEEE Robotics and Automation Letters (RA-L). Please refer\nto the final version at https://doi.org/10.1109/LRA.2024.3522756",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning for diverse real-world robotic tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. This paper presents a novel\ntwo-step Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a\ncontinuous constraint function from demonstrations, without requiring prior\nknowledge of the true constraint parameterization or environmental model as\nexisting works. We treat all data in demonstrations as positive (feasible)\ndata, and learn a control policy to generate potentially infeasible\ntrajectories, which serve as unlabeled data. The proposed two-step learning\nframework first identifies reliable infeasible data using a distance metric,\nand secondly learns a binary feasibility classifier (i.e., constraint function)\nfrom the feasible demonstrations and reliable infeasible data. The proposed\nmethod is flexible to learn complex-shaped constraint boundary and will not\nmistakenly classify demonstrations as infeasible as previous methods. The\neffectiveness of the proposed method is verified in four constrained\nenvironments, using a networked policy or a dynamical system policy. It\nsuccessfully infers the continuous nonlinear constraints and outperforms other\nbaseline methods in terms of constraint accuracy and policy safety. This work\nhas been published in IEEE Robotics and Automation Letters (RA-L). Please refer\nto the final version at https://doi.org/10.1109/LRA.2024.3522756"
                },
                "authors": [
                    {
                        "name": "Baiyu Peng"
                    },
                    {
                        "name": "Aude Billard"
                    }
                ],
                "author_detail": {
                    "name": "Aude Billard"
                },
                "author": "Aude Billard",
                "arxiv_doi": "10.1109/LRA.2024.3522756",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3522756",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.01622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Robotics and Automation Letters, vol. 10, no. 2, pp.\n  1593-1600, Feb. 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04174v2",
                "updated": "2025-01-16T10:30:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    30,
                    35,
                    3,
                    16,
                    0
                ],
                "published": "2024-01-08T19:00:05Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    19,
                    0,
                    5,
                    0,
                    8,
                    0
                ],
                "title": "Optimal, fast, and robust inference of reionization-era cosmology with\n  the 21cmPIE-INN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal, fast, and robust inference of reionization-era cosmology with\n  the 21cmPIE-INN"
                },
                "summary": "Modern machine learning will allow for simulation-based inference from\nreionization-era 21cm observations at the Square Kilometre Array. Our framework\ncombines a convolutional summary network and a conditional invertible network\nthrough a physics-inspired latent representation. It allows for an optimal and\nextremely fast determination of the posteriors of astrophysical and\ncosmological parameters. The sensitivity to non-Gaussian information makes our\nmethod a promising alternative to the established power spectra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern machine learning will allow for simulation-based inference from\nreionization-era 21cm observations at the Square Kilometre Array. Our framework\ncombines a convolutional summary network and a conditional invertible network\nthrough a physics-inspired latent representation. It allows for an optimal and\nextremely fast determination of the posteriors of astrophysical and\ncosmological parameters. The sensitivity to non-Gaussian information makes our\nmethod a promising alternative to the established power spectra."
                },
                "authors": [
                    {
                        "name": "Benedikt Schosser"
                    },
                    {
                        "name": "Caroline Heneka"
                    },
                    {
                        "name": "Tilman Plehn"
                    }
                ],
                "author_detail": {
                    "name": "Tilman Plehn"
                },
                "author": "Tilman Plehn",
                "arxiv_comment": "15+10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.04174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11775v2",
                "updated": "2025-01-16T10:20:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    20,
                    3,
                    3,
                    16,
                    0
                ],
                "published": "2024-08-21T17:00:05Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    0,
                    5,
                    2,
                    234,
                    0
                ],
                "title": "Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support: For 3GPP Standards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support: For 3GPP Standards"
                },
                "summary": "Recent studies show that large language models (LLMs) struggle with technical\nstandards in telecommunications. We propose a fine-tuned retrieval-augmented\ngeneration (RAG) system based on the Phi-2 small language model (SLM) to serve\nas an oracle for communication networks. Our developed system leverages\nforward-looking semantic chunking to adaptively determine parsing breakpoints\nbased on embedding similarity, enabling effective processing of diverse\ndocument formats. To handle the challenge of multiple similar contexts in\ntechnical standards, we employ a re-ranking algorithm to prioritize the most\nrelevant retrieved chunks. Recognizing the limitations of Phi-2's small context\nwindow, we implement a recent technique, namely SelfExtend, to expand the\ncontext window during inference, which not only boosts the performance but also\ncan accommodate a wider range of user queries and design requirements from\ncustomers to specialized technicians. For fine-tuning, we utilize the low-rank\nadaptation (LoRA) technique to enhance computational efficiency during training\nand enable effective fine-tuning on small datasets. Our comprehensive\nexperiments demonstrate substantial improvements over existing\nquestion-answering approaches in the telecom domain, achieving performance that\nexceeds larger language models such as GPT-4 (which is about 880 times larger\nin size). This work presents a novel approach to leveraging SLMs for\ncommunication networks, offering a balance of efficiency and performance. This\nwork can serve as a foundation towards agentic language models for networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that large language models (LLMs) struggle with technical\nstandards in telecommunications. We propose a fine-tuned retrieval-augmented\ngeneration (RAG) system based on the Phi-2 small language model (SLM) to serve\nas an oracle for communication networks. Our developed system leverages\nforward-looking semantic chunking to adaptively determine parsing breakpoints\nbased on embedding similarity, enabling effective processing of diverse\ndocument formats. To handle the challenge of multiple similar contexts in\ntechnical standards, we employ a re-ranking algorithm to prioritize the most\nrelevant retrieved chunks. Recognizing the limitations of Phi-2's small context\nwindow, we implement a recent technique, namely SelfExtend, to expand the\ncontext window during inference, which not only boosts the performance but also\ncan accommodate a wider range of user queries and design requirements from\ncustomers to specialized technicians. For fine-tuning, we utilize the low-rank\nadaptation (LoRA) technique to enhance computational efficiency during training\nand enable effective fine-tuning on small datasets. Our comprehensive\nexperiments demonstrate substantial improvements over existing\nquestion-answering approaches in the telecom domain, achieving performance that\nexceeds larger language models such as GPT-4 (which is about 880 times larger\nin size). This work presents a novel approach to leveraging SLMs for\ncommunication networks, offering a balance of efficiency and performance. This\nwork can serve as a foundation towards agentic language models for networks."
                },
                "authors": [
                    {
                        "name": "Omar Erak"
                    },
                    {
                        "name": "Nouf Alabbasi"
                    },
                    {
                        "name": "Omar Alhussein"
                    },
                    {
                        "name": "Ismail Lotfi"
                    },
                    {
                        "name": "Amr Hussein"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "submitted to Proc. IEEE Globecom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09438v1",
                "updated": "2025-01-16T10:09:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    9,
                    55,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T10:09:55Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    9,
                    55,
                    3,
                    16,
                    0
                ],
                "title": "Testing Born's rule via photoionization of helium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Born's rule via photoionization of helium"
                },
                "summary": "It is shown how state-of-the-art attosecond photoionization experiments can\ntest Born's rule -- a postulate of quantum mechanics -- via the so-called\nSorkin test. A simulation of the Sorkin test under consideration of typical\nexperimental noise and data acquisition efficiencies infers an achievable\nmeasurement precision in the range of the best Sorkin tests to date. The\nimplementation of further fundamental tests of quantum mechanics is discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is shown how state-of-the-art attosecond photoionization experiments can\ntest Born's rule -- a postulate of quantum mechanics -- via the so-called\nSorkin test. A simulation of the Sorkin test under consideration of typical\nexperimental noise and data acquisition efficiencies infers an achievable\nmeasurement precision in the range of the best Sorkin tests to date. The\nimplementation of further fundamental tests of quantum mechanics is discussed."
                },
                "authors": [
                    {
                        "name": "Peter Robert Förderer"
                    },
                    {
                        "name": "Andreas Buchleitner"
                    },
                    {
                        "name": "David Busto"
                    },
                    {
                        "name": "Christoph Dittel"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Dittel"
                },
                "author": "Christoph Dittel",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11005v2",
                "updated": "2025-01-16T10:05:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    5,
                    17,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-25T20:23:15Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    23,
                    15,
                    1,
                    177,
                    0
                ],
                "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation\n  Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a standard architectural\npattern for incorporating domain-specific knowledge into user-facing chat\napplications powered by Large Language Models (LLMs). RAG systems are\ncharacterized by (1) a document retriever that queries a domain-specific corpus\nfor context information relevant to an input query, and (2) an LLM that\ngenerates a response based on the provided query and context. However,\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\nunified evaluation criteria and annotated datasets. In response, we introduce\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\nexamples. It covers five unique industry-specific domains and various RAG task\ntypes. RAGBench examples are sourced from industry corpora such as user\nmanuals, making it particularly relevant for industry applications. Further, we\nformalize the TRACe evaluation framework: a set of explainable and actionable\nRAG evaluation metrics applicable across all RAG domains. We release the\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\nenabling actionable feedback for continuous improvement of production\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\nRAG evaluation task. We identify areas where existing approaches fall short and\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\nevaluation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a standard architectural\npattern for incorporating domain-specific knowledge into user-facing chat\napplications powered by Large Language Models (LLMs). RAG systems are\ncharacterized by (1) a document retriever that queries a domain-specific corpus\nfor context information relevant to an input query, and (2) an LLM that\ngenerates a response based on the provided query and context. However,\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\nunified evaluation criteria and annotated datasets. In response, we introduce\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\nexamples. It covers five unique industry-specific domains and various RAG task\ntypes. RAGBench examples are sourced from industry corpora such as user\nmanuals, making it particularly relevant for industry applications. Further, we\nformalize the TRACe evaluation framework: a set of explainable and actionable\nRAG evaluation metrics applicable across all RAG domains. We release the\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\nenabling actionable feedback for continuous improvement of production\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\nRAG evaluation task. We identify areas where existing approaches fall short and\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\nevaluation systems."
                },
                "authors": [
                    {
                        "name": "Robert Friel"
                    },
                    {
                        "name": "Masha Belyi"
                    },
                    {
                        "name": "Atindriyo Sanyal"
                    }
                ],
                "author_detail": {
                    "name": "Atindriyo Sanyal"
                },
                "author": "Atindriyo Sanyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09431v1",
                "updated": "2025-01-16T09:59:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    59,
                    45,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:59:45Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    59,
                    45,
                    3,
                    16,
                    0
                ],
                "title": "A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and\n  Mitigation Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and\n  Mitigation Strategy"
                },
                "summary": "While large language models (LLMs) present significant potential for\nsupporting numerous real-world applications and delivering positive social\nimpacts, they still face significant challenges in terms of the inherent risk\nof privacy leakage, hallucinated outputs, and value misalignment, and can be\nmaliciously used for generating toxic content and unethical purposes after been\njailbroken. Therefore, in this survey, we present a comprehensive review of\nrecent advancements aimed at mitigating these issues, organized across the four\nphases of LLM development and usage: data collecting and pre-training,\nfine-tuning and alignment, prompting and reasoning, and post-processing and\nauditing. We elaborate on the recent advances for enhancing the performance of\nLLMs in terms of privacy protection, hallucination reduction, value alignment,\ntoxicity elimination, and jailbreak defenses. In contrast to previous surveys\nthat focus on a single dimension of responsible LLMs, this survey presents a\nunified framework that encompasses these diverse dimensions, providing a\ncomprehensive view of enhancing LLMs to better serve real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) present significant potential for\nsupporting numerous real-world applications and delivering positive social\nimpacts, they still face significant challenges in terms of the inherent risk\nof privacy leakage, hallucinated outputs, and value misalignment, and can be\nmaliciously used for generating toxic content and unethical purposes after been\njailbroken. Therefore, in this survey, we present a comprehensive review of\nrecent advancements aimed at mitigating these issues, organized across the four\nphases of LLM development and usage: data collecting and pre-training,\nfine-tuning and alignment, prompting and reasoning, and post-processing and\nauditing. We elaborate on the recent advances for enhancing the performance of\nLLMs in terms of privacy protection, hallucination reduction, value alignment,\ntoxicity elimination, and jailbreak defenses. In contrast to previous surveys\nthat focus on a single dimension of responsible LLMs, this survey presents a\nunified framework that encompasses these diverse dimensions, providing a\ncomprehensive view of enhancing LLMs to better serve real-world applications."
                },
                "authors": [
                    {
                        "name": "Huandong Wang"
                    },
                    {
                        "name": "Wenjie Fu"
                    },
                    {
                        "name": "Yingzhou Tang"
                    },
                    {
                        "name": "Zhilong Chen"
                    },
                    {
                        "name": "Yuxi Huang"
                    },
                    {
                        "name": "Jinghua Piao"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Tao Jiang"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04997v2",
                "updated": "2025-01-16T09:58:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    58,
                    54,
                    3,
                    16,
                    0
                ],
                "published": "2024-01-10T08:28:56Z",
                "published_parsed": [
                    2024,
                    1,
                    10,
                    8,
                    28,
                    56,
                    2,
                    10,
                    0
                ],
                "title": "Tapping the Potential of Large Language Models as Recommender Systems: A\n  Comprehensive Framework and Empirical Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapping the Potential of Large Language Models as Recommender Systems: A\n  Comprehensive Framework and Empirical Analysis"
                },
                "summary": "Recently, Large Language Models~(LLMs) such as ChatGPT have showcased\nremarkable abilities in solving general tasks, demonstrating the potential for\napplications in recommender systems. To assess how effectively LLMs can be used\nin recommendation tasks, our study primarily focuses on employing LLMs as\nrecommender systems through prompting engineering. We propose a general\nframework for utilizing LLMs in recommendation tasks, focusing on the\ncapabilities of LLMs as recommenders. To conduct our analysis, we formalize the\ninput of LLMs for recommendation into natural language prompts with two key\naspects, and explain how our framework can be generalized to various\nrecommendation scenarios. As for the use of LLMs as recommenders, we analyze\nthe impact of public availability, tuning strategies, model architecture,\nparameter scale, and context length on recommendation results based on the\nclassification of LLMs. As for prompt engineering, we further analyze the\nimpact of four important components of prompts, \\ie task descriptions, user\ninterest modeling, candidate items construction and prompting strategies. In\neach section, we first define and categorize concepts in line with the existing\nliterature. Then, we propose inspiring research questions followed by detailed\nexperiments on two public datasets, in order to systematically analyze the\nimpact of different factors on performance. Based on our empirical analysis, we\nfinally summarize promising directions to shed lights on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models~(LLMs) such as ChatGPT have showcased\nremarkable abilities in solving general tasks, demonstrating the potential for\napplications in recommender systems. To assess how effectively LLMs can be used\nin recommendation tasks, our study primarily focuses on employing LLMs as\nrecommender systems through prompting engineering. We propose a general\nframework for utilizing LLMs in recommendation tasks, focusing on the\ncapabilities of LLMs as recommenders. To conduct our analysis, we formalize the\ninput of LLMs for recommendation into natural language prompts with two key\naspects, and explain how our framework can be generalized to various\nrecommendation scenarios. As for the use of LLMs as recommenders, we analyze\nthe impact of public availability, tuning strategies, model architecture,\nparameter scale, and context length on recommendation results based on the\nclassification of LLMs. As for prompt engineering, we further analyze the\nimpact of four important components of prompts, \\ie task descriptions, user\ninterest modeling, candidate items construction and prompting strategies. In\neach section, we first define and categorize concepts in line with the existing\nliterature. Then, we propose inspiring research questions followed by detailed\nexperiments on two public datasets, in order to systematically analyze the\nimpact of different factors on performance. Based on our empirical analysis, we\nfinally summarize promising directions to shed lights on future research."
                },
                "authors": [
                    {
                        "name": "Lanling Xu"
                    },
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Bingqian Li"
                    },
                    {
                        "name": "Jinpeng Wang"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "52 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.04997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09426v1",
                "updated": "2025-01-16T09:57:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    57,
                    12,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:57:12Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    57,
                    12,
                    3,
                    16,
                    0
                ],
                "title": "AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral\n  Therapy in Psychological Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral\n  Therapy in Psychological Counseling"
                },
                "summary": "Traditional in-person psychological counseling remains primarily niche, often\nchosen by individuals with psychological issues, while online automated\ncounseling offers a potential solution for those hesitant to seek help due to\nfeelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and\nwidely used approach in psychological counseling. The advent of large language\nmodels (LLMs) and agent technology enables automatic CBT diagnosis and\ntreatment. However, current LLM-based CBT systems use agents with a fixed\nstructure, limiting their self-optimization capabilities, or providing hollow,\nunhelpful suggestions due to redundant response patterns. In this work, we\nutilize Quora-like and YiXinLi single-round consultation models to build a\ngeneral agent framework that generates high-quality responses for single-turn\npsychological consultation scenarios. We use a bilingual dataset to evaluate\nthe quality of single-response consultations generated by each framework. Then,\nwe incorporate dynamic routing and supervisory mechanisms inspired by real\npsychological counseling to construct a CBT-oriented autonomous multi-agent\nframework, demonstrating its general applicability. Experimental results\nindicate that AutoCBT can provide higher-quality automated psychological\ncounseling services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional in-person psychological counseling remains primarily niche, often\nchosen by individuals with psychological issues, while online automated\ncounseling offers a potential solution for those hesitant to seek help due to\nfeelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and\nwidely used approach in psychological counseling. The advent of large language\nmodels (LLMs) and agent technology enables automatic CBT diagnosis and\ntreatment. However, current LLM-based CBT systems use agents with a fixed\nstructure, limiting their self-optimization capabilities, or providing hollow,\nunhelpful suggestions due to redundant response patterns. In this work, we\nutilize Quora-like and YiXinLi single-round consultation models to build a\ngeneral agent framework that generates high-quality responses for single-turn\npsychological consultation scenarios. We use a bilingual dataset to evaluate\nthe quality of single-response consultations generated by each framework. Then,\nwe incorporate dynamic routing and supervisory mechanisms inspired by real\npsychological counseling to construct a CBT-oriented autonomous multi-agent\nframework, demonstrating its general applicability. Experimental results\nindicate that AutoCBT can provide higher-quality automated psychological\ncounseling services."
                },
                "authors": [
                    {
                        "name": "Ancheng Xu"
                    },
                    {
                        "name": "Di Yang"
                    },
                    {
                        "name": "Renhao Li"
                    },
                    {
                        "name": "Jingwei Zhu"
                    },
                    {
                        "name": "Minghuan Tan"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Wanxin Qiu"
                    },
                    {
                        "name": "Mingchen Ma"
                    },
                    {
                        "name": "Haihong Wu"
                    },
                    {
                        "name": "Bingyu Li"
                    },
                    {
                        "name": "Feng Sha"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Qiang Qu"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09412v1",
                "updated": "2025-01-16T09:38:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    38,
                    39,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:38:39Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    38,
                    39,
                    3,
                    16,
                    0
                ],
                "title": "FASP: Fast and Accurate Structured Pruning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FASP: Fast and Accurate Structured Pruning of Large Language Models"
                },
                "summary": "The rapid increase in the size of large language models (LLMs) has\nsignificantly escalated their computational and memory demands, posing\nchallenges for efficient deployment, especially on resource-constrained\ndevices. Structured pruning has emerged as an effective model compression\nmethod that can reduce these demands while preserving performance. In this\npaper, we introduce FASP (Fast and Accurate Structured Pruning), a novel\nstructured pruning framework for LLMs that emphasizes both speed and accuracy.\nFASP employs a distinctive pruning structure that interlinks sequential layers,\nallowing for the removal of columns in one layer while simultaneously\neliminating corresponding rows in the preceding layer without incurring\nadditional performance loss. The pruning metric, inspired by Wanda, is\ncomputationally efficient and effectively selects components to prune.\nAdditionally, we propose a restoration mechanism that enhances model fidelity\nby adjusting the remaining weights post-pruning. We evaluate FASP on the OPT\nand LLaMA model families, demonstrating superior performance in terms of\nperplexity and accuracy on downstream tasks compared to state-of-the-art\nmethods. Our approach achieves significant speed-ups, pruning models such as\nOPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090\nGPU, making it a highly practical solution for optimizing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in the size of large language models (LLMs) has\nsignificantly escalated their computational and memory demands, posing\nchallenges for efficient deployment, especially on resource-constrained\ndevices. Structured pruning has emerged as an effective model compression\nmethod that can reduce these demands while preserving performance. In this\npaper, we introduce FASP (Fast and Accurate Structured Pruning), a novel\nstructured pruning framework for LLMs that emphasizes both speed and accuracy.\nFASP employs a distinctive pruning structure that interlinks sequential layers,\nallowing for the removal of columns in one layer while simultaneously\neliminating corresponding rows in the preceding layer without incurring\nadditional performance loss. The pruning metric, inspired by Wanda, is\ncomputationally efficient and effectively selects components to prune.\nAdditionally, we propose a restoration mechanism that enhances model fidelity\nby adjusting the remaining weights post-pruning. We evaluate FASP on the OPT\nand LLaMA model families, demonstrating superior performance in terms of\nperplexity and accuracy on downstream tasks compared to state-of-the-art\nmethods. Our approach achieves significant speed-ups, pruning models such as\nOPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090\nGPU, making it a highly practical solution for optimizing LLMs."
                },
                "authors": [
                    {
                        "name": "Hanyu Hu"
                    },
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaoming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Yuan"
                },
                "author": "Xiaoming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09410v1",
                "updated": "2025-01-16T09:36:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    36,
                    32,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:36:32Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    36,
                    32,
                    3,
                    16,
                    0
                ],
                "title": "MoE$^2$: Optimizing Collaborative Inference for Edge Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE$^2$: Optimizing Collaborative Inference for Edge Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. Exploiting the heterogeneous\ncapabilities of edge LLMs is crucial for diverse emerging applications, as it\nenables greater cost-effectiveness and reduced latency. In this work, we\nintroduce \\textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative\ninference framework for edge LLMs. We formulate the joint gating and expert\nselection problem to optimize inference performance under energy and latency\nconstraints. Unlike conventional MoE problems, LLM expert selection is\nsignificantly more challenging due to the combinatorial nature and the\nheterogeneity of edge LLMs across various attributes. To this end, we propose a\ntwo-level expert selection mechanism through which we uncover an\noptimality-preserving property of gating parameters across expert selections.\nThis property enables the decomposition of the training and selection\nprocesses, significantly reducing complexity. Furthermore, we leverage the\nobjective's monotonicity and design a discrete monotonic optimization algorithm\nfor optimal expert selection. We implement edge servers with NVIDIA Jetson AGX\nOrins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results\nvalidate that performance improvements of various LLM models and show that our\nMoE$^2$ method can achieve optimal trade-offs among different delay and energy\nbudgets, and outperforms baselines under various system resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. Exploiting the heterogeneous\ncapabilities of edge LLMs is crucial for diverse emerging applications, as it\nenables greater cost-effectiveness and reduced latency. In this work, we\nintroduce \\textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative\ninference framework for edge LLMs. We formulate the joint gating and expert\nselection problem to optimize inference performance under energy and latency\nconstraints. Unlike conventional MoE problems, LLM expert selection is\nsignificantly more challenging due to the combinatorial nature and the\nheterogeneity of edge LLMs across various attributes. To this end, we propose a\ntwo-level expert selection mechanism through which we uncover an\noptimality-preserving property of gating parameters across expert selections.\nThis property enables the decomposition of the training and selection\nprocesses, significantly reducing complexity. Furthermore, we leverage the\nobjective's monotonicity and design a discrete monotonic optimization algorithm\nfor optimal expert selection. We implement edge servers with NVIDIA Jetson AGX\nOrins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results\nvalidate that performance improvements of various LLM models and show that our\nMoE$^2$ method can achieve optimal trade-offs among different delay and energy\nbudgets, and outperforms baselines under various system resource constraints."
                },
                "authors": [
                    {
                        "name": "Lyudong Jin"
                    },
                    {
                        "name": "Yanning Zhang"
                    },
                    {
                        "name": "Yanhan Li"
                    },
                    {
                        "name": "Shurong Wang"
                    },
                    {
                        "name": "Howard H. Yang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Meng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Zhang"
                },
                "author": "Meng Zhang",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Networking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04635v2",
                "updated": "2025-01-16T09:30:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    30,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-08T17:29:46Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    29,
                    46,
                    2,
                    8,
                    0
                ],
                "title": "Knowledge Retrieval Based on Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Retrieval Based on Generative AI"
                },
                "summary": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI. The system's\neffectiveness is assessed through a two-stage evaluation: automatic and\nassisted performance evaluations. The automatic evaluation calculates accuracy\nby comparing the model's auto-generated labels with ground truth answers,\nmeasuring performance under standardized conditions without human intervention.\nThe assisted performance evaluation involves 20 finance-related multiple-choice\nquestions answered by 20 participants without financial backgrounds. Initially,\nparticipants answer independently. Later, they receive system-generated\nreference information to assist in answering, examining whether the system\nimproves accuracy when assistance is provided. The main contributions of this\nresearch are: (1) Enhanced LLM Capability: By integrating BGE-M3 and\nBGE-reranker, the system retrieves and reorders highly relevant results,\nreduces hallucinations, and dynamically accesses authorized or public knowledge\nsources. (2) Improved Data Privacy: A customized RAG architecture enables local\noperation of the LLM, eliminating the need to send private data to external\nservers. This approach enhances data security, reduces reliance on commercial\nservices, lowers operational costs, and mitigates privacy risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI. The system's\neffectiveness is assessed through a two-stage evaluation: automatic and\nassisted performance evaluations. The automatic evaluation calculates accuracy\nby comparing the model's auto-generated labels with ground truth answers,\nmeasuring performance under standardized conditions without human intervention.\nThe assisted performance evaluation involves 20 finance-related multiple-choice\nquestions answered by 20 participants without financial backgrounds. Initially,\nparticipants answer independently. Later, they receive system-generated\nreference information to assist in answering, examining whether the system\nimproves accuracy when assistance is provided. The main contributions of this\nresearch are: (1) Enhanced LLM Capability: By integrating BGE-M3 and\nBGE-reranker, the system retrieves and reorders highly relevant results,\nreduces hallucinations, and dynamically accesses authorized or public knowledge\nsources. (2) Improved Data Privacy: A customized RAG architecture enables local\noperation of the LLM, eliminating the need to send private data to external\nservers. This approach enhances data security, reduces reliance on commercial\nservices, lowers operational costs, and mitigates privacy risks."
                },
                "authors": [
                    {
                        "name": "Te-Lun Yang"
                    },
                    {
                        "name": "Jyi-Shane Liu"
                    },
                    {
                        "name": "Yuen-Hsien Tseng"
                    },
                    {
                        "name": "Jyh-Shing Roger Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jyh-Shing Roger Jang"
                },
                "author": "Jyh-Shing Roger Jang",
                "arxiv_comment": "8 pages, 13 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04088v3",
                "updated": "2025-01-16T09:07:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    7,
                    51,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-06T13:58:41Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    13,
                    58,
                    41,
                    3,
                    158,
                    0
                ],
                "title": "Deterministic Uncertainty Propagation for Improved Model-Based Offline\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deterministic Uncertainty Propagation for Improved Model-Based Offline\n  Reinforcement Learning"
                },
                "summary": "Current approaches to model-based offline reinforcement learning often\nincorporate uncertainty-based reward penalization to address the distributional\nshift problem. These approaches, commonly known as pessimistic value iteration,\nuse Monte Carlo sampling to estimate the Bellman target to perform temporal\ndifference-based policy evaluation. We find out that the randomness caused by\nthis sampling step significantly delays convergence. We present a theoretical\nresult demonstrating the strong dependency of suboptimality on the number of\nMonte Carlo samples taken per Bellman target calculation. Our main contribution\nis a deterministic approximation to the Bellman target that uses progressive\nmoment matching, a method developed originally for deterministic variational\ninference. The resulting algorithm, which we call Moment Matching Offline\nModel-Based Policy Optimization (MOMBO), propagates the uncertainty of the next\nstate through a nonlinear Q-network in a deterministic fashion by approximating\nthe distributions of hidden layer activations by a normal distribution. We show\nthat it is possible to provide tighter guarantees for the suboptimality of\nMOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO\nto converge faster than these approaches in a large set of benchmark tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current approaches to model-based offline reinforcement learning often\nincorporate uncertainty-based reward penalization to address the distributional\nshift problem. These approaches, commonly known as pessimistic value iteration,\nuse Monte Carlo sampling to estimate the Bellman target to perform temporal\ndifference-based policy evaluation. We find out that the randomness caused by\nthis sampling step significantly delays convergence. We present a theoretical\nresult demonstrating the strong dependency of suboptimality on the number of\nMonte Carlo samples taken per Bellman target calculation. Our main contribution\nis a deterministic approximation to the Bellman target that uses progressive\nmoment matching, a method developed originally for deterministic variational\ninference. The resulting algorithm, which we call Moment Matching Offline\nModel-Based Policy Optimization (MOMBO), propagates the uncertainty of the next\nstate through a nonlinear Q-network in a deterministic fashion by approximating\nthe distributions of hidden layer activations by a normal distribution. We show\nthat it is possible to provide tighter guarantees for the suboptimality of\nMOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO\nto converge faster than these approaches in a large set of benchmark tasks."
                },
                "authors": [
                    {
                        "name": "Abdullah Akgül"
                    },
                    {
                        "name": "Manuel Haußmann"
                    },
                    {
                        "name": "Melih Kandemir"
                    }
                ],
                "author_detail": {
                    "name": "Melih Kandemir"
                },
                "author": "Melih Kandemir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09384v1",
                "updated": "2025-01-16T08:52:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    50,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:50Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    50,
                    3,
                    16,
                    0
                ],
                "title": "Evaluating LLM Abilities to Understand Tabular Electronic Health\n  Records: A Comprehensive Study of Patient Data Extraction and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Abilities to Understand Tabular Electronic Health\n  Records: A Comprehensive Study of Patient Data Extraction and Retrieval"
                },
                "summary": "Electronic Health Record (EHR) tables pose unique challenges among which is\nthe presence of hidden contextual dependencies between medical features with a\nhigh level of data dimensionality and sparsity. This study presents the first\ninvestigation into the abilities of LLMs to comprehend EHRs for patient data\nextraction and retrieval. We conduct extensive experiments using the MIMICSQL\ndataset to explore the impact of the prompt structure, instruction, context,\nand demonstration, of two backbone LLMs, Llama2 and Meditron, based on task\nperformance. Through quantitative and qualitative analyses, our findings show\nthat optimal feature selection and serialization methods can enhance task\nperformance by up to 26.79% compared to naive approaches. Similarly, in-context\nlearning setups with relevant example selection improve data extraction\nperformance by 5.95%. Based on our study findings, we propose guidelines that\nwe believe would help the design of LLM-based models to support health search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Record (EHR) tables pose unique challenges among which is\nthe presence of hidden contextual dependencies between medical features with a\nhigh level of data dimensionality and sparsity. This study presents the first\ninvestigation into the abilities of LLMs to comprehend EHRs for patient data\nextraction and retrieval. We conduct extensive experiments using the MIMICSQL\ndataset to explore the impact of the prompt structure, instruction, context,\nand demonstration, of two backbone LLMs, Llama2 and Meditron, based on task\nperformance. Through quantitative and qualitative analyses, our findings show\nthat optimal feature selection and serialization methods can enhance task\nperformance by up to 26.79% compared to naive approaches. Similarly, in-context\nlearning setups with relevant example selection improve data extraction\nperformance by 5.95%. Based on our study findings, we propose guidelines that\nwe believe would help the design of LLM-based models to support health search."
                },
                "authors": [
                    {
                        "name": "Jesus Lovon"
                    },
                    {
                        "name": "Martin Mouysset"
                    },
                    {
                        "name": "Jo Oleiwan"
                    },
                    {
                        "name": "Jose G. Moreno"
                    },
                    {
                        "name": "Christine Damase-Michel"
                    },
                    {
                        "name": "Lynda Tamine"
                    }
                ],
                "author_detail": {
                    "name": "Lynda Tamine"
                },
                "arxiv_affiliation": "IRIT-IRIS",
                "author": "Lynda Tamine",
                "arxiv_comment": "To be published as full paper in the Proceedings of the European\n  Conference on Information Retrieval (ECIR) 2025. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07124v2",
                "updated": "2025-01-16T08:49:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    49,
                    10,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-13T08:26:43Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    26,
                    43,
                    0,
                    13,
                    0
                ],
                "title": "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch"
                },
                "summary": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch."
                },
                "authors": [
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Bowen Tan"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Tianhua Tao"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Suqi Sun"
                    },
                    {
                        "name": "Omkar Pangarkar"
                    },
                    {
                        "name": "Richard Fan"
                    },
                    {
                        "name": "Yi Gu"
                    },
                    {
                        "name": "Victor Miller"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Liping Tang"
                    },
                    {
                        "name": "Nikhil Ranjan"
                    },
                    {
                        "name": "Yonghao Zhuang"
                    },
                    {
                        "name": "Guowei He"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Robin Algayres"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12112v3",
                "updated": "2025-01-16T08:44:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    44,
                    22,
                    3,
                    16,
                    0
                ],
                "published": "2024-08-22T03:54:08Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    54,
                    8,
                    3,
                    235,
                    0
                ],
                "title": "Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards"
                },
                "summary": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches."
                },
                "authors": [
                    {
                        "name": "Shresth Verma"
                    },
                    {
                        "name": "Niclas Boehmer"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Milind Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Milind Tambe"
                },
                "author": "Milind Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13340v2",
                "updated": "2025-01-16T08:34:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    34,
                    36,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-19T08:46:29Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    8,
                    46,
                    29,
                    2,
                    171,
                    0
                ],
                "title": "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond\n  Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond\n  Words"
                },
                "summary": "Speech encompasses a wealth of information, including but not limited to\ncontent, paralinguistic, and environmental information. This comprehensive\nnature of speech significantly impacts communication and is crucial for\nhuman-computer interaction. Chat-Oriented Large Language Models (LLMs), known\nfor their general-purpose assistance capabilities, have evolved to handle\nmulti-modal inputs, including speech. Although these models can be adept at\nrecognizing and analyzing speech, they often fall short of generating\nappropriate responses. We argue that this is due to the lack of principles on\ntask definition and model development, which requires open-source datasets and\nmetrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a\nbenchmark dataset aimed at multidimensional evaluation of spoken dialogue\nunderstanding and generation. SD-Eval focuses on paralinguistic and\nenvironmental information and includes 7,303 utterances, amounting to 8.76\nhours of speech data. The data is aggregated from eight public datasets,\nrepresenting four perspectives: emotion, accent, age, and background sound. To\nassess the SD-Eval benchmark dataset, we implement three different models and\nconstruct a training set following a process similar to that of SD-Eval. The\ntraining set contains 1,052.72 hours of speech data and 724.4k utterances. We\nalso conduct a comprehensive evaluation using objective evaluation methods\n(e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the\ngenerated responses. Models conditioned with paralinguistic and environmental\ninformation outperform their counterparts in both objective and subjective\nmeasures. Moreover, experiments demonstrate that LLM-based metrics show a\nhigher correlation with human evaluation compared to traditional metrics. We\nopen-source SD-Eval at https://github.com/amphionspace/SD-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech encompasses a wealth of information, including but not limited to\ncontent, paralinguistic, and environmental information. This comprehensive\nnature of speech significantly impacts communication and is crucial for\nhuman-computer interaction. Chat-Oriented Large Language Models (LLMs), known\nfor their general-purpose assistance capabilities, have evolved to handle\nmulti-modal inputs, including speech. Although these models can be adept at\nrecognizing and analyzing speech, they often fall short of generating\nappropriate responses. We argue that this is due to the lack of principles on\ntask definition and model development, which requires open-source datasets and\nmetrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a\nbenchmark dataset aimed at multidimensional evaluation of spoken dialogue\nunderstanding and generation. SD-Eval focuses on paralinguistic and\nenvironmental information and includes 7,303 utterances, amounting to 8.76\nhours of speech data. The data is aggregated from eight public datasets,\nrepresenting four perspectives: emotion, accent, age, and background sound. To\nassess the SD-Eval benchmark dataset, we implement three different models and\nconstruct a training set following a process similar to that of SD-Eval. The\ntraining set contains 1,052.72 hours of speech data and 724.4k utterances. We\nalso conduct a comprehensive evaluation using objective evaluation methods\n(e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the\ngenerated responses. Models conditioned with paralinguistic and environmental\ninformation outperform their counterparts in both objective and subjective\nmeasures. Moreover, experiments demonstrate that LLM-based metrics show a\nhigher correlation with human evaluation compared to traditional metrics. We\nopen-source SD-Eval at https://github.com/amphionspace/SD-Eval."
                },
                "authors": [
                    {
                        "name": "Junyi Ao"
                    },
                    {
                        "name": "Yuancheng Wang"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Dekun Chen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Zhizheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhizheng Wu"
                },
                "author": "Zhizheng Wu",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09368v1",
                "updated": "2025-01-16T08:27:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    27,
                    40,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:27:40Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    27,
                    40,
                    3,
                    16,
                    0
                ],
                "title": "Aligning Instruction Tuning with Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Instruction Tuning with Pre-training"
                },
                "summary": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose *Aligning Instruction Tuning\nwith Pre-training* (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose *Aligning Instruction Tuning\nwith Pre-training* (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09367v1",
                "updated": "2025-01-16T08:25:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    25,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:25:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    25,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PICE: A Semantic-Driven Progressive Inference System for LLM Serving in\n  Cloud-Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PICE: A Semantic-Driven Progressive Inference System for LLM Serving in\n  Cloud-Edge Networks"
                },
                "summary": "Large language models (LLMs), while driving a new wave of interactive AI\napplications across numerous domains, suffer from high inference costs and\nheavy cloud dependency. Motivated by the redundancy phenomenon in linguistics,\nwe propose a progressive inference paradigm over cloud and edge, i.e., firstly\ngenerating the sketch of the answer by LLMs at cloud, and then conducting\nparallel extension to fill in details by small models (SLMs) at edge.\nProgressive inference offers potential benefits to improve throughput and\nreduce inference latency while facing key implementation challenges, including\ndecreased response quality from SLMs, a tradeoff between the brevity and\ncomprehensiveness of sketches, as well as increased latency caused by network\ntransmission and edge inference. In this work, we propose and implement PICE,\nan LLM serving system with semantic-level cloud-edge collaboration, enhancing\ninference throughput and quality through dynamic inference task scheduling,\nensemble learning, and parallel edge inference. Extensive testbed experiments\nillustrate that our approach achieves $1.5-2\\times$ throughput enhancement and\nup to 43% latency reduction, while also potentially enhancing the quality\ncompared to SOTA systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), while driving a new wave of interactive AI\napplications across numerous domains, suffer from high inference costs and\nheavy cloud dependency. Motivated by the redundancy phenomenon in linguistics,\nwe propose a progressive inference paradigm over cloud and edge, i.e., firstly\ngenerating the sketch of the answer by LLMs at cloud, and then conducting\nparallel extension to fill in details by small models (SLMs) at edge.\nProgressive inference offers potential benefits to improve throughput and\nreduce inference latency while facing key implementation challenges, including\ndecreased response quality from SLMs, a tradeoff between the brevity and\ncomprehensiveness of sketches, as well as increased latency caused by network\ntransmission and edge inference. In this work, we propose and implement PICE,\nan LLM serving system with semantic-level cloud-edge collaboration, enhancing\ninference throughput and quality through dynamic inference task scheduling,\nensemble learning, and parallel edge inference. Extensive testbed experiments\nillustrate that our approach achieves $1.5-2\\times$ throughput enhancement and\nup to 43% latency reduction, while also potentially enhancing the quality\ncompared to SOTA systems."
                },
                "authors": [
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Haisheng Tan"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Dongping Yong"
                    },
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Xiang-Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang-Yang Li"
                },
                "author": "Xiang-Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03659v3",
                "updated": "2025-01-16T08:20:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    20,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-07T09:47:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    47,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting"
                },
                "summary": "Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency. visualizations are available at\nhttps://dehazegs.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency. visualizations are available at\nhttps://dehazegs.github.io/"
                },
                "authors": [
                    {
                        "name": "Jinze Yu"
                    },
                    {
                        "name": "Yiqun Wang"
                    },
                    {
                        "name": "Zhengda Lu"
                    },
                    {
                        "name": "Jianwei Guo"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongxing Qin"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "9 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05777v2",
                "updated": "2025-01-16T08:20:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    20,
                    11,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-10T08:18:37Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    18,
                    37,
                    4,
                    10,
                    0
                ],
                "title": "StructSR: Refuse Spurious Details in Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructSR: Refuse Spurious Details in Real-World Image Super-Resolution"
                },
                "summary": "Diffusion-based models have shown great promise in real-world image\nsuper-resolution (Real-ISR), but often generate content with structural errors\nand spurious texture details due to the empirical priors and illusions of these\nmodels. To address this issue, we introduce StructSR, a simple, effective, and\nplug-and-play method that enhances structural fidelity and suppresses spurious\ndetails for diffusion-based Real-ISR. StructSR operates without the need for\nadditional fine-tuning, external model priors, or high-level semantic\nknowledge. At its core is the Structure-Aware Screening (SAS) mechanism, which\nidentifies the image with the highest structural similarity to the\nlow-resolution (LR) input in the early inference stage, allowing us to leverage\nit as a historical structure knowledge to suppress the generation of spurious\ndetails. By intervening in the diffusion inference process, StructSR seamlessly\nintegrates with existing diffusion-based Real-ISR models. Our experimental\nresults demonstrate that StructSR significantly improves the fidelity of\nstructure and texture, improving the PSNR and SSIM metrics by an average of\n5.27% and 9.36% on a synthetic dataset (DIV2K-Val) and 4.13% and 8.64% on two\nreal-world datasets (RealSR and DRealSR) when integrated with four\nstate-of-the-art diffusion-based Real-ISR methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based models have shown great promise in real-world image\nsuper-resolution (Real-ISR), but often generate content with structural errors\nand spurious texture details due to the empirical priors and illusions of these\nmodels. To address this issue, we introduce StructSR, a simple, effective, and\nplug-and-play method that enhances structural fidelity and suppresses spurious\ndetails for diffusion-based Real-ISR. StructSR operates without the need for\nadditional fine-tuning, external model priors, or high-level semantic\nknowledge. At its core is the Structure-Aware Screening (SAS) mechanism, which\nidentifies the image with the highest structural similarity to the\nlow-resolution (LR) input in the early inference stage, allowing us to leverage\nit as a historical structure knowledge to suppress the generation of spurious\ndetails. By intervening in the diffusion inference process, StructSR seamlessly\nintegrates with existing diffusion-based Real-ISR models. Our experimental\nresults demonstrate that StructSR significantly improves the fidelity of\nstructure and texture, improving the PSNR and SSIM metrics by an average of\n5.27% and 9.36% on a synthetic dataset (DIV2K-Val) and 4.13% and 8.64% on two\nreal-world datasets (RealSR and DRealSR) when integrated with four\nstate-of-the-art diffusion-based Real-ISR methods."
                },
                "authors": [
                    {
                        "name": "Yachao Li"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Tianyu Ding"
                    },
                    {
                        "name": "Sheng-Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Sheng-Jun Huang"
                },
                "author": "Sheng-Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02623v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02623v3",
                "updated": "2025-01-16T08:18:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    18,
                    1,
                    3,
                    16,
                    0
                ],
                "published": "2024-11-04T21:31:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    21,
                    31,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "Learning to Assist Humans without Inferring Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Assist Humans without Inferring Rewards"
                },
                "summary": "Assistive agents should make humans' lives easier. Classically, such\nassistance is studied through the lens of inverse reinforcement learning, where\nan assistive agent (e.g., a chatbot, a robot) infers a human's intention and\nthen selects actions to help the human reach that goal. This approach requires\ninferring intentions, which can be difficult in high-dimensional settings. We\nbuild upon prior work that studies assistance through the lens of empowerment:\nan assistive agent aims to maximize the influence of the human's actions such\nthat they exert a greater control over the environmental outcomes and can solve\ntasks in fewer steps. We lift the major limitation of prior work in this\narea--scalability to high-dimensional settings--with contrastive successor\nrepresentations. We formally prove that these representations estimate a\nsimilar notion of empowerment to that studied by prior work and provide a\nready-made mechanism for optimizing it. Empirically, our proposed method\noutperforms prior methods on synthetic benchmarks, and scales to Overcooked, a\ncooperative game setting. Theoretically, our work connects ideas from\ninformation theory, neuroscience, and reinforcement learning, and charts a path\nfor representations to play a critical role in solving assistive problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistive agents should make humans' lives easier. Classically, such\nassistance is studied through the lens of inverse reinforcement learning, where\nan assistive agent (e.g., a chatbot, a robot) infers a human's intention and\nthen selects actions to help the human reach that goal. This approach requires\ninferring intentions, which can be difficult in high-dimensional settings. We\nbuild upon prior work that studies assistance through the lens of empowerment:\nan assistive agent aims to maximize the influence of the human's actions such\nthat they exert a greater control over the environmental outcomes and can solve\ntasks in fewer steps. We lift the major limitation of prior work in this\narea--scalability to high-dimensional settings--with contrastive successor\nrepresentations. We formally prove that these representations estimate a\nsimilar notion of empowerment to that studied by prior work and provide a\nready-made mechanism for optimizing it. Empirically, our proposed method\noutperforms prior methods on synthetic benchmarks, and scales to Overcooked, a\ncooperative game setting. Theoretically, our work connects ideas from\ninformation theory, neuroscience, and reinforcement learning, and charts a path\nfor representations to play a critical role in solving assistive problems."
                },
                "authors": [
                    {
                        "name": "Vivek Myers"
                    },
                    {
                        "name": "Evan Ellis"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Benjamin Eysenbach"
                    },
                    {
                        "name": "Anca Dragan"
                    }
                ],
                "author_detail": {
                    "name": "Anca Dragan"
                },
                "author": "Anca Dragan",
                "arxiv_comment": "Conference on Neural Information Processing Systems (NeurIPS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02623v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02623v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09355v1",
                "updated": "2025-01-16T08:06:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    6,
                    2,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:06:02Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    6,
                    2,
                    3,
                    16,
                    0
                ],
                "title": "YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents\n  in Augmented Reality Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents\n  in Augmented Reality Tasks"
                },
                "summary": "Multimodal AI Agents are AI models that have the capability of interactively\nand cooperatively assisting human users to solve day-to-day tasks. Augmented\nReality (AR) head worn devices can uniquely improve the user experience of\nsolving procedural day-to-day tasks by providing egocentric multimodal (audio\nand video) observational capabilities to AI Agents. Such AR capabilities can\nhelp AI Agents see and listen to actions that users take which can relate to\nmultimodal capabilities of human users. Existing AI Agents, either Large\nLanguage Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive\nin nature, which means that models cannot take an action without reading or\nlistening to the human user's prompts. Proactivity of AI Agents on the other\nhand can help the human user detect and correct any mistakes in agent observed\ntasks, encourage users when they do tasks correctly or simply engage in\nconversation with the user - akin to a human teaching or assisting a user. Our\nproposed YET to Intervene (YETI) multimodal agent focuses on the research\nquestion of identifying circumstances that may require the agent to intervene\nproactively. This allows the agent to understand when it can intervene in a\nconversation with human users that can help the user correct mistakes on tasks,\nlike cooking, using AR. Our YETI Agent learns scene understanding signals based\non interpretable notions of Structural Similarity (SSIM) on consecutive video\nframes. We also define the alignment signal which the AI Agent can learn to\nidentify if the video frames corresponding to the user's actions on the task\nare consistent with expected actions. These signals are used by our AI Agent to\ndetermine when it should proactively intervene. We compare our results on the\ninstances of proactive intervention in the HoloAssist multimodal benchmark for\nan expert agent guiding a user to complete procedural tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal AI Agents are AI models that have the capability of interactively\nand cooperatively assisting human users to solve day-to-day tasks. Augmented\nReality (AR) head worn devices can uniquely improve the user experience of\nsolving procedural day-to-day tasks by providing egocentric multimodal (audio\nand video) observational capabilities to AI Agents. Such AR capabilities can\nhelp AI Agents see and listen to actions that users take which can relate to\nmultimodal capabilities of human users. Existing AI Agents, either Large\nLanguage Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive\nin nature, which means that models cannot take an action without reading or\nlistening to the human user's prompts. Proactivity of AI Agents on the other\nhand can help the human user detect and correct any mistakes in agent observed\ntasks, encourage users when they do tasks correctly or simply engage in\nconversation with the user - akin to a human teaching or assisting a user. Our\nproposed YET to Intervene (YETI) multimodal agent focuses on the research\nquestion of identifying circumstances that may require the agent to intervene\nproactively. This allows the agent to understand when it can intervene in a\nconversation with human users that can help the user correct mistakes on tasks,\nlike cooking, using AR. Our YETI Agent learns scene understanding signals based\non interpretable notions of Structural Similarity (SSIM) on consecutive video\nframes. We also define the alignment signal which the AI Agent can learn to\nidentify if the video frames corresponding to the user's actions on the task\nare consistent with expected actions. These signals are used by our AI Agent to\ndetermine when it should proactively intervene. We compare our results on the\ninstances of proactive intervention in the HoloAssist multimodal benchmark for\nan expert agent guiding a user to complete procedural tasks."
                },
                "authors": [
                    {
                        "name": "Saptarashmi Bandyopadhyay"
                    },
                    {
                        "name": "Vikas Bahirwani"
                    },
                    {
                        "name": "Lavisha Aggarwal"
                    },
                    {
                        "name": "Bhanu Guda"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Andrea Colaco"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Colaco"
                },
                "author": "Andrea Colaco",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.10; I.2.11; I.2.1; I.2.7; I.4.8; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09345v1",
                "updated": "2025-01-16T07:58:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    58,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T07:58:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    58,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "Rational Tuning of LLM Cascades via Probabilistic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rational Tuning of LLM Cascades via Probabilistic Modeling"
                },
                "summary": "Understanding the reliability of large language models (LLMs) has recently\ngarnered significant attention. Given LLMs' propensity to hallucinate, as well\nas their high sensitivity to prompt design, it is already challenging to\npredict the performance of an individual LLM. However, the problem becomes more\ncomplex for compound LLM systems such as cascades, where in addition to each\nmodel's standalone performance, we must understand how the error rates of\ndifferent models interact. In this paper, we present a probabilistic model for\nthe joint performance distribution of a sequence of LLMs, which enables a\nframework for rationally tuning the confidence thresholds of a LLM cascade\nusing continuous optimization. Compared to selecting confidence thresholds\nusing grid search, our parametric Markov-copula model significantly improves\nruntime scaling with respect to the length of the cascade and the desired\nresolution of the cost-error curve, turning them from intractable into\nlow-order polynomial. In addition, the optimal thresholds computed using our\ncontinuous optimization-based algorithm increasingly outperform those found via\ngrid search as cascade length grows, improving the area under the cost-error\ncurve by 1.9% on average for cascades consisting of at least three models.\nOverall, our Markov-copula model provides a rational basis for tuning LLM\ncascade performance and points to the potential of probabilistic methods in\nanalyzing LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the reliability of large language models (LLMs) has recently\ngarnered significant attention. Given LLMs' propensity to hallucinate, as well\nas their high sensitivity to prompt design, it is already challenging to\npredict the performance of an individual LLM. However, the problem becomes more\ncomplex for compound LLM systems such as cascades, where in addition to each\nmodel's standalone performance, we must understand how the error rates of\ndifferent models interact. In this paper, we present a probabilistic model for\nthe joint performance distribution of a sequence of LLMs, which enables a\nframework for rationally tuning the confidence thresholds of a LLM cascade\nusing continuous optimization. Compared to selecting confidence thresholds\nusing grid search, our parametric Markov-copula model significantly improves\nruntime scaling with respect to the length of the cascade and the desired\nresolution of the cost-error curve, turning them from intractable into\nlow-order polynomial. In addition, the optimal thresholds computed using our\ncontinuous optimization-based algorithm increasingly outperform those found via\ngrid search as cascade length grows, improving the area under the cost-error\ncurve by 1.9% on average for cascades consisting of at least three models.\nOverall, our Markov-copula model provides a rational basis for tuning LLM\ncascade performance and points to the potential of probabilistic methods in\nanalyzing LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael J. Zellinger"
                    },
                    {
                        "name": "Matt Thomson"
                    }
                ],
                "author_detail": {
                    "name": "Matt Thomson"
                },
                "author": "Matt Thomson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06158v2",
                "updated": "2025-01-16T07:52:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    52,
                    41,
                    3,
                    16,
                    0
                ],
                "published": "2024-11-09T12:06:28Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    12,
                    6,
                    28,
                    5,
                    314,
                    0
                ],
                "title": "Effective and General Distance Computation for Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and General Distance Computation for Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate K Nearest Neighbor (AKNN) search in high-dimensional spaces is a\ncritical yet challenging problem. In AKNN search, distance computation is the\ncore task that dominates the runtime. Existing approaches typically use\napproximate distances to improve computational efficiency, often at the cost of\nreduced search accuracy. To address this issue, the state-of-the-art method,\nADsampling, employs random projections to estimate approximate distances and\nintroduces an additional distance correction process to mitigate accuracy loss.\nHowever, ADsampling has limitations in both effectiveness and generality,\nprimarily due to its heavy reliance on random projections for distance\napproximation and correction. Motivated by this, we leverage data distribution\nto improve distance approximation via orthogonal projection, thereby addressing\nthe effectiveness limitation of ADsampling; we also adopt a data-driven\napproach to distance correction, decoupling the correction process from the\ndistance approximation process, thereby overcoming the generality limitation of\nADsampling. Extensive experiments demonstrate the superiority and effectiveness\nof our method. In particular, compared to ADsampling, our method achieves a\nspeedup of 1.6 to 2.1 times on real-world datasets while providing higher\naccuracy. In addition, our method shows superior performance in Ant Group image\nsearch scenarios and has been integrated into their search engine VSAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate K Nearest Neighbor (AKNN) search in high-dimensional spaces is a\ncritical yet challenging problem. In AKNN search, distance computation is the\ncore task that dominates the runtime. Existing approaches typically use\napproximate distances to improve computational efficiency, often at the cost of\nreduced search accuracy. To address this issue, the state-of-the-art method,\nADsampling, employs random projections to estimate approximate distances and\nintroduces an additional distance correction process to mitigate accuracy loss.\nHowever, ADsampling has limitations in both effectiveness and generality,\nprimarily due to its heavy reliance on random projections for distance\napproximation and correction. Motivated by this, we leverage data distribution\nto improve distance approximation via orthogonal projection, thereby addressing\nthe effectiveness limitation of ADsampling; we also adopt a data-driven\napproach to distance correction, decoupling the correction process from the\ndistance approximation process, thereby overcoming the generality limitation of\nADsampling. Extensive experiments demonstrate the superiority and effectiveness\nof our method. In particular, compared to ADsampling, our method achieves a\nspeedup of 1.6 to 2.1 times on real-world datasets while providing higher\naccuracy. In addition, our method shows superior performance in Ant Group image\nsearch scenarios and has been integrated into their search engine VSAG."
                },
                "authors": [
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Accepted by ICDE2025 code available at:\n  github.com/mingyu-hkustgz/Res-Infer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09164v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09164v4",
                "updated": "2025-01-16T07:50:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    50,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-12T10:59:32Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    10,
                    59,
                    32,
                    4,
                    194,
                    0
                ],
                "title": "TPIA: Towards Target-specific Prompt Injection Attack against\n  Code-oriented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPIA: Towards Target-specific Prompt Injection Attack against\n  Code-oriented Large Language Models"
                },
                "summary": "Recently, code-oriented large language models (Code LLMs) have been widely\nexploited to simplify and facilitate programming. With these tools, developers\ncan easily generate the desired complete functional code based on incomplete\ncode snippets and natural language prompts. Unfortunately, a few pioneering\nworks revealed that these Code LLMs are vulnerable to backdoor and adversarial\nattacks. The former poisons the training data or model parameters, hijacking\nthe LLMs to generate malicious code snippets when encountering the trigger. The\nlatter crafts malicious adversarial input codes to reduce the quality of the\ngenerated codes. However, both attacks have some inherent limitations: backdoor\nattacks rely on the adversary's capability of controlling the model training\nprocess; adversarial attacks struggle with fulfilling specific malicious\npurposes. This paper presents a novel attack paradigm against Code LLMs, namely\ntarget-specific prompt injection attack (TPIA). TPIA generates non-functional\nperturbations containing the information of malicious instructions and inserts\nthem into the victim's code context by spreading them into potentially used\ndependencies (e.g., packages or RAG's knowledge base). It induces the Code LLMs\nto generate attacker-specified malicious code snippets at the target location.\nIn general, we compress the attacker-specified malicious objective into the\nperturbation by adversarial optimization based on greedy token search. We\ncollect 13 representative malicious objectives to design 31 threat cases for\nthree popular programming languages. We show that our TPIA can successfully\nattack three representative open-source Code LLMs (with an ASR of up to 97.9%)\nand two mainstream commercial Code LLM-integrated applications (with an ASR of\nover 90%) in all threat cases, using only a 12-token perturbation. Our work\nalerts a new practical threat of using Code LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, code-oriented large language models (Code LLMs) have been widely\nexploited to simplify and facilitate programming. With these tools, developers\ncan easily generate the desired complete functional code based on incomplete\ncode snippets and natural language prompts. Unfortunately, a few pioneering\nworks revealed that these Code LLMs are vulnerable to backdoor and adversarial\nattacks. The former poisons the training data or model parameters, hijacking\nthe LLMs to generate malicious code snippets when encountering the trigger. The\nlatter crafts malicious adversarial input codes to reduce the quality of the\ngenerated codes. However, both attacks have some inherent limitations: backdoor\nattacks rely on the adversary's capability of controlling the model training\nprocess; adversarial attacks struggle with fulfilling specific malicious\npurposes. This paper presents a novel attack paradigm against Code LLMs, namely\ntarget-specific prompt injection attack (TPIA). TPIA generates non-functional\nperturbations containing the information of malicious instructions and inserts\nthem into the victim's code context by spreading them into potentially used\ndependencies (e.g., packages or RAG's knowledge base). It induces the Code LLMs\nto generate attacker-specified malicious code snippets at the target location.\nIn general, we compress the attacker-specified malicious objective into the\nperturbation by adversarial optimization based on greedy token search. We\ncollect 13 representative malicious objectives to design 31 threat cases for\nthree popular programming languages. We show that our TPIA can successfully\nattack three representative open-source Code LLMs (with an ASR of up to 97.9%)\nand two mainstream commercial Code LLM-integrated applications (with an ASR of\nover 90%) in all threat cases, using only a 12-token perturbation. Our work\nalerts a new practical threat of using Code LLMs."
                },
                "authors": [
                    {
                        "name": "Yuchen Yang"
                    },
                    {
                        "name": "Hongwei Yao"
                    },
                    {
                        "name": "Bingrun Yang"
                    },
                    {
                        "name": "Yiling He"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    },
                    {
                        "name": "Chun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chun Chen"
                },
                "author": "Chun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09164v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09164v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03337v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03337v4",
                "updated": "2025-01-16T07:40:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    40,
                    27,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-22T07:19:12Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    19,
                    12,
                    0,
                    204,
                    0
                ],
                "title": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements"
                },
                "summary": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI"
                },
                "authors": [
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Yazhe Niu"
                    },
                    {
                        "name": "Shuai Hu"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "29 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03337v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03337v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.15081v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.15081v8",
                "updated": "2025-01-16T07:34:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    34,
                    31,
                    3,
                    16,
                    0
                ],
                "published": "2022-11-28T05:54:24Z",
                "published_parsed": [
                    2022,
                    11,
                    28,
                    5,
                    54,
                    24,
                    0,
                    332,
                    0
                ],
                "title": "Mitigating Overfitting in Graph Neural Networks via Feature and\n  Hyperplane Perturbation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Overfitting in Graph Neural Networks via Feature and\n  Hyperplane Perturbation"
                },
                "summary": "Graph neural networks (GNNs) are commonly used in semi-supervised settings.\nPrevious research has primarily focused on finding appropriate graph filters\n(e.g. aggregation methods) to perform well on both homophilic and heterophilic\ngraphs. While these methods are effective, they can still suffer from the\nsparsity of node features, where the initial data contain few non-zero\nelements. This can lead to overfitting in certain dimensions in the first\nprojection matrix, as training samples may not cover the entire range of graph\nfilters (hyperplanes). To address this, we propose a novel data augmentation\nstrategy. Specifically, by flipping both the initial features and hyperplane,\nwe create additional space for training, which leads to more precise updates of\nthe learnable parameters and improved robustness for unseen features during\ninference. To the best of our knowledge, this is the first attempt to mitigate\nthe overfitting caused by the initial features. Extensive experiments on\nreal-world datasets show that our proposed technique increases node\nclassification accuracy by up to 46.5% relatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are commonly used in semi-supervised settings.\nPrevious research has primarily focused on finding appropriate graph filters\n(e.g. aggregation methods) to perform well on both homophilic and heterophilic\ngraphs. While these methods are effective, they can still suffer from the\nsparsity of node features, where the initial data contain few non-zero\nelements. This can lead to overfitting in certain dimensions in the first\nprojection matrix, as training samples may not cover the entire range of graph\nfilters (hyperplanes). To address this, we propose a novel data augmentation\nstrategy. Specifically, by flipping both the initial features and hyperplane,\nwe create additional space for training, which leads to more precise updates of\nthe learnable parameters and improved robustness for unseen features during\ninference. To the best of our knowledge, this is the first attempt to mitigate\nthe overfitting caused by the initial features. Extensive experiments on\nreal-world datasets show that our proposed technique increases node\nclassification accuracy by up to 46.5% relatively."
                },
                "authors": [
                    {
                        "name": "Yoonhyuk Choi"
                    },
                    {
                        "name": "Jiho Choi"
                    },
                    {
                        "name": "Taewook Ko"
                    },
                    {
                        "name": "Chong-Kwon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Chong-Kwon Kim"
                },
                "author": "Chong-Kwon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.15081v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.15081v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09329v1",
                "updated": "2025-01-16T06:59:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    59,
                    29,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T06:59:29Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    59,
                    29,
                    3,
                    16,
                    0
                ],
                "title": "Exploring the Neutrino Mass Hierarchy from Isotopic Ratios of Supernova\n  Nucleosynthesis Products in Presolar Grains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Neutrino Mass Hierarchy from Isotopic Ratios of Supernova\n  Nucleosynthesis Products in Presolar Grains"
                },
                "summary": "We study the nucleosynthesis in a core-collapse supernova model including\nnewly calculated neutrino-induced reaction rates with both collective and\nMikheyev-Smirnov-Wolfenstein (MSW) neutrino-flavor oscillations considered. We\nshow that the measurement of a pair of $^{11}$B/$^{10}$B and\n$^{138}$La/$^{139}$La or $^6$Li/$^7$Li and $^{138}$La/$^{139}$La in presolar\ngrains that are inferred to have originated from core-collapse supernovae could\nconstrain the neutrino mass hierarchy. The new shell-model and the model of\nquasi-particle random phase approximation in the estimate of three important\nneutrino-induced reactions, $\\nu+^{16}$O, $\\nu+^{20}$Ne, and $\\nu+^{138}$Ba are\napplied in our reaction network. The new rates decrease the calculated\n$^{7}$Li/$^{6}$Li ratio by a factor of five compared with the previous study.\nMore interestingly, these new rates result in a clear separation of the\nisotopic ratio of $^{11}$B/$^{10}$B between normal and inverted mass\nhierarchies in the O/Ne, O/C, and C/He layers where $^{138}$La abundance\ndepends strongly on the mass hierarchy. In these layers, the sensitivity of the\ncalculated abundances of $^{10,11}$B and $^{6,7}$Li to the nuclear reaction\nuncertainties is also tiny. Therefore, we propose that the $^{11}$B/$^{10}$B\nvs. $^{138}$La/$^{139}$La and $^6$Li/$^7$Li vs. $^{138}$La/$^{139}$La in type X\nsilicon carbide grains sampled material from C/He layer can be used as a new\nprobe to constrain the neutrino mass hierarchy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the nucleosynthesis in a core-collapse supernova model including\nnewly calculated neutrino-induced reaction rates with both collective and\nMikheyev-Smirnov-Wolfenstein (MSW) neutrino-flavor oscillations considered. We\nshow that the measurement of a pair of $^{11}$B/$^{10}$B and\n$^{138}$La/$^{139}$La or $^6$Li/$^7$Li and $^{138}$La/$^{139}$La in presolar\ngrains that are inferred to have originated from core-collapse supernovae could\nconstrain the neutrino mass hierarchy. The new shell-model and the model of\nquasi-particle random phase approximation in the estimate of three important\nneutrino-induced reactions, $\\nu+^{16}$O, $\\nu+^{20}$Ne, and $\\nu+^{138}$Ba are\napplied in our reaction network. The new rates decrease the calculated\n$^{7}$Li/$^{6}$Li ratio by a factor of five compared with the previous study.\nMore interestingly, these new rates result in a clear separation of the\nisotopic ratio of $^{11}$B/$^{10}$B between normal and inverted mass\nhierarchies in the O/Ne, O/C, and C/He layers where $^{138}$La abundance\ndepends strongly on the mass hierarchy. In these layers, the sensitivity of the\ncalculated abundances of $^{10,11}$B and $^{6,7}$Li to the nuclear reaction\nuncertainties is also tiny. Therefore, we propose that the $^{11}$B/$^{10}$B\nvs. $^{138}$La/$^{139}$La and $^6$Li/$^7$Li vs. $^{138}$La/$^{139}$La in type X\nsilicon carbide grains sampled material from C/He layer can be used as a new\nprobe to constrain the neutrino mass hierarchy."
                },
                "authors": [
                    {
                        "name": "Xingqun Yao"
                    },
                    {
                        "name": "Toshitaka Kajino"
                    },
                    {
                        "name": "Yudong Luo"
                    },
                    {
                        "name": "Takehito Hayakawa"
                    },
                    {
                        "name": "Toshio Suzuki"
                    },
                    {
                        "name": "Heamin Ko"
                    },
                    {
                        "name": "Myung-Ki Cheoun"
                    },
                    {
                        "name": "Seiya Hayakawa"
                    },
                    {
                        "name": "Hidetoshi Yamaguchi"
                    },
                    {
                        "name": "Silvio Cherubini"
                    }
                ],
                "author_detail": {
                    "name": "Silvio Cherubini"
                },
                "author": "Silvio Cherubini",
                "arxiv_comment": "21 pages, 4 figures, accepted by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09320v1",
                "updated": "2025-01-16T06:22:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    22,
                    35,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T06:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    22,
                    35,
                    3,
                    16,
                    0
                ],
                "title": "Cooperative Decentralized Backdoor Attacks on Vertical Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Decentralized Backdoor Attacks on Vertical Federated\n  Learning"
                },
                "summary": "Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance."
                },
                "authors": [
                    {
                        "name": "Seohyun Lee"
                    },
                    {
                        "name": "Wenzhi Fang"
                    },
                    {
                        "name": "Anindya Bijoy Das"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "David J. Love"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "arxiv_comment": "This paper is currently under review in the IEEE/ACM Transactions on\n  Networking Special Issue on AI and Networking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09316v1",
                "updated": "2025-01-16T06:14:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    14,
                    58,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T06:14:58Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    14,
                    58,
                    3,
                    16,
                    0
                ],
                "title": "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs"
                },
                "summary": "Despite significant advancements in general-purpose AI agents, several\nchallenges still hinder their practical application in real-world scenarios.\nFirst, the limited planning capabilities of Large Language Models (LLM)\nrestrict AI agents from effectively solving complex tasks that require\nlong-horizon planning. Second, general-purpose AI agents struggle to\nefficiently utilize domain-specific knowledge and human expertise. In this\npaper, we introduce the Standard Operational Procedure-guided Agent\n(SOP-agent), a novel framework for constructing domain-specific agents through\npseudocode-style Standard Operational Procedures (SOPs) written in natural\nlanguage. Formally, we represent a SOP as a decision graph, which is traversed\nto guide the agent in completing tasks specified by the SOP. We conduct\nextensive experiments across tasks in multiple domains, including\ndecision-making, search and reasoning, code generation, data cleaning, and\ngrounded customer service. The SOP-agent demonstrates excellent versatility,\nachieving performance superior to general-purpose agent frameworks and\ncomparable to domain-specific agent systems. Additionally, we introduce the\nGrounded Customer Service Benchmark, the first benchmark designed to evaluate\nthe grounded decision-making capabilities of AI agents in customer service\nscenarios based on SOPs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in general-purpose AI agents, several\nchallenges still hinder their practical application in real-world scenarios.\nFirst, the limited planning capabilities of Large Language Models (LLM)\nrestrict AI agents from effectively solving complex tasks that require\nlong-horizon planning. Second, general-purpose AI agents struggle to\nefficiently utilize domain-specific knowledge and human expertise. In this\npaper, we introduce the Standard Operational Procedure-guided Agent\n(SOP-agent), a novel framework for constructing domain-specific agents through\npseudocode-style Standard Operational Procedures (SOPs) written in natural\nlanguage. Formally, we represent a SOP as a decision graph, which is traversed\nto guide the agent in completing tasks specified by the SOP. We conduct\nextensive experiments across tasks in multiple domains, including\ndecision-making, search and reasoning, code generation, data cleaning, and\ngrounded customer service. The SOP-agent demonstrates excellent versatility,\nachieving performance superior to general-purpose agent frameworks and\ncomparable to domain-specific agent systems. Additionally, we introduce the\nGrounded Customer Service Benchmark, the first benchmark designed to evaluate\nthe grounded decision-making capabilities of AI agents in customer service\nscenarios based on SOPs."
                },
                "authors": [
                    {
                        "name": "Anbang Ye"
                    },
                    {
                        "name": "Qianran Ma"
                    },
                    {
                        "name": "Jia Chen"
                    },
                    {
                        "name": "Muqi Li"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Fujiao Liu"
                    },
                    {
                        "name": "Siqi Mai"
                    },
                    {
                        "name": "Meichen Lu"
                    },
                    {
                        "name": "Haitao Bao"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "arxiv_comment": "35 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15862v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15862v4",
                "updated": "2025-01-16T06:07:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    7,
                    12,
                    3,
                    16,
                    0
                ],
                "published": "2024-11-24T14:38:59Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    14,
                    38,
                    59,
                    6,
                    329,
                    0
                ],
                "title": "Do LLMs Really Think Step-by-step In Implicit Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Really Think Step-by-step In Implicit Reasoning?"
                },
                "summary": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yijiong Yu"
                },
                "author": "Yijiong Yu",
                "arxiv_comment": "The code is in\n  https://github.com/yuyijiong/if_step_by_step_implicit_CoT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15862v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15862v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09310v1",
                "updated": "2025-01-16T05:54:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    54,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T05:54:59Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    54,
                    59,
                    3,
                    16,
                    0
                ],
                "title": "A Study of In-Context-Learning-Based Text-to-SQL Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of In-Context-Learning-Based Text-to-SQL Errors"
                },
                "summary": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead."
                },
                "authors": [
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Ruoyi Qiao"
                    },
                    {
                        "name": "Jiazhen Zou"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Yuchen Shao"
                    },
                    {
                        "name": "Yueling Zhang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    }
                ],
                "author_detail": {
                    "name": "Geguang Pu"
                },
                "author": "Geguang Pu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09308v1",
                "updated": "2025-01-16T05:46:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    46,
                    23,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T05:46:23Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    46,
                    23,
                    3,
                    16,
                    0
                ],
                "title": "Rapid, Comprehensive Search of Crystalline Phases from X-ray Diffraction\n  in Seconds via GPU-Accelerated Bayesian Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid, Comprehensive Search of Crystalline Phases from X-ray Diffraction\n  in Seconds via GPU-Accelerated Bayesian Variational Inference"
                },
                "summary": "In analysis of X-ray diffraction data, identifying the crystalline phase is\nimportant for interpreting the material. The typical method is identifying the\ncrystalline phase from the coincidence of the main diffraction peaks. This\nmethod identifies crystalline phases by matching them as individual crystalline\nphases rather than as combinations of crystalline phases, in the same way as\nthe greedy method. If multiple candidates are obtained, the researcher must\nsubjectively select the crystalline phases. Thus, the identification results\ndepend on the researcher's experience and knowledge of materials science. To\nsolve this problem, we have developed a Bayesian estimation method to identify\nthe combination of crystalline phases, taking the entire profile into account.\nThis method estimates the Bayesian posterior probability of crystalline phase\ncombinations by performing an approximate exhaustive search of all possible\ncombinations. It is a method for identifying crystalline phases that takes into\naccount all peak shapes and phase combinations. However, it takes a few hours\nto obtain the analysis results. The aim of this study is to develop a Bayesian\nmethod for crystalline phase identification that can provide results in\nseconds, which is a practical calculation time. We introduce variational sparse\nestimation and GPU computing. Our method is able to provide results within 10\nseconds even when analysing $2^{50}$ candidate crystalline phase combinations.\nFurthermore, the crystalline phases identified by our method are consistent\nwith the results of previous studies that used a high-precision algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In analysis of X-ray diffraction data, identifying the crystalline phase is\nimportant for interpreting the material. The typical method is identifying the\ncrystalline phase from the coincidence of the main diffraction peaks. This\nmethod identifies crystalline phases by matching them as individual crystalline\nphases rather than as combinations of crystalline phases, in the same way as\nthe greedy method. If multiple candidates are obtained, the researcher must\nsubjectively select the crystalline phases. Thus, the identification results\ndepend on the researcher's experience and knowledge of materials science. To\nsolve this problem, we have developed a Bayesian estimation method to identify\nthe combination of crystalline phases, taking the entire profile into account.\nThis method estimates the Bayesian posterior probability of crystalline phase\ncombinations by performing an approximate exhaustive search of all possible\ncombinations. It is a method for identifying crystalline phases that takes into\naccount all peak shapes and phase combinations. However, it takes a few hours\nto obtain the analysis results. The aim of this study is to develop a Bayesian\nmethod for crystalline phase identification that can provide results in\nseconds, which is a practical calculation time. We introduce variational sparse\nestimation and GPU computing. Our method is able to provide results within 10\nseconds even when analysing $2^{50}$ candidate crystalline phase combinations.\nFurthermore, the crystalline phases identified by our method are consistent\nwith the results of previous studies that used a high-precision algorithm."
                },
                "authors": [
                    {
                        "name": "Ryo Murakami"
                    },
                    {
                        "name": "Kenji Nagata"
                    },
                    {
                        "name": "Yoshitaka Matsushita"
                    },
                    {
                        "name": "Masahiko Demura"
                    }
                ],
                "author_detail": {
                    "name": "Masahiko Demura"
                },
                "author": "Masahiko Demura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09307v1",
                "updated": "2025-01-16T05:40:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    40,
                    37,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T05:40:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    40,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "RoboReflect: Robotic Reflective Reasoning for Grasping\n  Ambiguous-Condition Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboReflect: Robotic Reflective Reasoning for Grasping\n  Ambiguous-Condition Objects"
                },
                "summary": "As robotic technology rapidly develops, robots are being employed in an\nincreasing number of fields. However, due to the complexity of deployment\nenvironments or the prevalence of ambiguous-condition objects, the practical\napplication of robotics still faces many challenges, leading to frequent\nerrors. Traditional methods and some LLM-based approaches, although improved,\nstill require substantial human intervention and struggle with autonomous error\ncorrection in complex scenarios.In this work, we propose RoboReflect, a novel\nframework leveraging large vision-language models (LVLMs) to enable\nself-reflection and autonomous error correction in robotic grasping tasks.\nRoboReflect allows robots to automatically adjust their strategies based on\nunsuccessful attempts until successful execution is achieved.The corrected\nstrategies are saved in a memory for future task reference.We evaluate\nRoboReflect through extensive testing on eight common objects prone to\nambiguous conditions of three categories.Our results demonstrate that\nRoboReflect not only outperforms existing grasp pose estimation methods like\nAnyGrasp and high-level action planning techniques using GPT-4V but also\nsignificantly enhances the robot's ability to adapt and correct errors\nindependently. These findings underscore the critical importance of autonomous\nselfreflection in robotic systems while effectively addressing the challenges\nposed by ambiguous environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robotic technology rapidly develops, robots are being employed in an\nincreasing number of fields. However, due to the complexity of deployment\nenvironments or the prevalence of ambiguous-condition objects, the practical\napplication of robotics still faces many challenges, leading to frequent\nerrors. Traditional methods and some LLM-based approaches, although improved,\nstill require substantial human intervention and struggle with autonomous error\ncorrection in complex scenarios.In this work, we propose RoboReflect, a novel\nframework leveraging large vision-language models (LVLMs) to enable\nself-reflection and autonomous error correction in robotic grasping tasks.\nRoboReflect allows robots to automatically adjust their strategies based on\nunsuccessful attempts until successful execution is achieved.The corrected\nstrategies are saved in a memory for future task reference.We evaluate\nRoboReflect through extensive testing on eight common objects prone to\nambiguous conditions of three categories.Our results demonstrate that\nRoboReflect not only outperforms existing grasp pose estimation methods like\nAnyGrasp and high-level action planning techniques using GPT-4V but also\nsignificantly enhances the robot's ability to adapt and correct errors\nindependently. These findings underscore the critical importance of autonomous\nselfreflection in robotic systems while effectively addressing the challenges\nposed by ambiguous environments."
                },
                "authors": [
                    {
                        "name": "Zhen Luo"
                    },
                    {
                        "name": "Yixuan Yang"
                    },
                    {
                        "name": "Chang Cai"
                    },
                    {
                        "name": "Yanfu Zhang"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09300v1",
                "updated": "2025-01-16T05:27:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    27,
                    27,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T05:27:27Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    27,
                    27,
                    3,
                    16,
                    0
                ],
                "title": "A long-term study of Mrk 50 : Appearance and disappearance of soft\n  excess",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A long-term study of Mrk 50 : Appearance and disappearance of soft\n  excess"
                },
                "summary": "We present an extensive temporal and spectral study of the Seyfert 1 AGN Mrk\n50 using 15 years (2007-2022) of multiwavelength observations from XMM-Newton,\nSwift, and NuSTAR for the first time. From the timing analysis, we found that\nthe source exhibited variability of $\\sim$20 % during the 2007 observation,\nwhich reduced to below 10 % in the subsequent observations and became\nnon-variable in the observations from 2010 onward. From the spectral study, we\nfound that the spectra are nearly featureless. Non-detection of absorption in\nthe low-energy domain during the 15 years of observation infers the absence of\nobscuration around the central engine, rendering the nucleus a `bare' type. A\nprominent soft X-ray excess below 2 keV was detected in the source spectrum\nduring the observations between 2007 and 2010, which vanished during the later\nobservations. To describe the nature of the soft excess, we use two physical\nmodels, such as warm Comptonization and blurred reflection from the ionized\naccretion disk. Both the physical models explain the nature and origin of the\nsoft excess in this source. Our analysis found that Mrk~50 accretes at\nsub-Eddington accretion rate ($\\lambda_{Edd}=0.13-0.02$) during all the\nobservations used in this work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an extensive temporal and spectral study of the Seyfert 1 AGN Mrk\n50 using 15 years (2007-2022) of multiwavelength observations from XMM-Newton,\nSwift, and NuSTAR for the first time. From the timing analysis, we found that\nthe source exhibited variability of $\\sim$20 % during the 2007 observation,\nwhich reduced to below 10 % in the subsequent observations and became\nnon-variable in the observations from 2010 onward. From the spectral study, we\nfound that the spectra are nearly featureless. Non-detection of absorption in\nthe low-energy domain during the 15 years of observation infers the absence of\nobscuration around the central engine, rendering the nucleus a `bare' type. A\nprominent soft X-ray excess below 2 keV was detected in the source spectrum\nduring the observations between 2007 and 2010, which vanished during the later\nobservations. To describe the nature of the soft excess, we use two physical\nmodels, such as warm Comptonization and blurred reflection from the ionized\naccretion disk. Both the physical models explain the nature and origin of the\nsoft excess in this source. Our analysis found that Mrk~50 accretes at\nsub-Eddington accretion rate ($\\lambda_{Edd}=0.13-0.02$) during all the\nobservations used in this work."
                },
                "authors": [
                    {
                        "name": "Narendranath Layek"
                    },
                    {
                        "name": "Prantik Nandi"
                    },
                    {
                        "name": "Sachindra Naik"
                    },
                    {
                        "name": "Arghajit Jana"
                    }
                ],
                "author_detail": {
                    "name": "Arghajit Jana"
                },
                "author": "Arghajit Jana",
                "arxiv_comment": "27 pages,11 figures,8 tables Accepted for publication in APJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09298v1",
                "updated": "2025-01-16T05:07:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    7,
                    5,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T05:07:05Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    7,
                    5,
                    3,
                    16,
                    0
                ],
                "title": "Physics-informed deep learning for infectious disease forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-informed deep learning for infectious disease forecasting"
                },
                "summary": "Accurate forecasting of contagious illnesses has become increasingly\nimportant to public health policymaking, and better prediction could prevent\nthe loss of millions of lives. To better prepare for future pandemics, it is\nessential to improve forecasting methods and capabilities. In this work, we\npropose a new infectious disease forecasting model based on physics-informed\nneural networks (PINNs), an emerging area of scientific machine learning. The\nproposed PINN model incorporates dynamical systems representations of disease\ntransmission into the loss function, thereby assimilating epidemiological\ntheory and data using neural networks (NNs). Our approach is designed to\nprevent model overfitting, which often occurs when training deep learning\nmodels with observation data alone. In addition, we employ an additional\nsub-network to account for mobility, vaccination, and other covariates that\ninfluence the transmission rate, a key parameter in the compartment model. To\ndemonstrate the capability of the proposed model, we examine the performance of\nthe model using state-level COVID-19 data in California. Our simulation results\nshow that predictions of PINN model on the number of cases, deaths, and\nhospitalizations are consistent with existing benchmarks. In particular, the\nPINN model outperforms the basic NN model and naive baseline forecast. We also\nshow that the performance of the PINN model is comparable to a sophisticated\nGaussian infection state space with time dependence (GISST) forecasting model\nthat integrates the compartment model with a data observation model and a\nregression model for inferring parameters in the compartment model.\nNonetheless, the PINN model offers a simpler structure and is easier to\nimplement. Our results show that the proposed forecaster could potentially\nserve as a new computational tool to enhance the current capacity of infectious\ndisease forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate forecasting of contagious illnesses has become increasingly\nimportant to public health policymaking, and better prediction could prevent\nthe loss of millions of lives. To better prepare for future pandemics, it is\nessential to improve forecasting methods and capabilities. In this work, we\npropose a new infectious disease forecasting model based on physics-informed\nneural networks (PINNs), an emerging area of scientific machine learning. The\nproposed PINN model incorporates dynamical systems representations of disease\ntransmission into the loss function, thereby assimilating epidemiological\ntheory and data using neural networks (NNs). Our approach is designed to\nprevent model overfitting, which often occurs when training deep learning\nmodels with observation data alone. In addition, we employ an additional\nsub-network to account for mobility, vaccination, and other covariates that\ninfluence the transmission rate, a key parameter in the compartment model. To\ndemonstrate the capability of the proposed model, we examine the performance of\nthe model using state-level COVID-19 data in California. Our simulation results\nshow that predictions of PINN model on the number of cases, deaths, and\nhospitalizations are consistent with existing benchmarks. In particular, the\nPINN model outperforms the basic NN model and naive baseline forecast. We also\nshow that the performance of the PINN model is comparable to a sophisticated\nGaussian infection state space with time dependence (GISST) forecasting model\nthat integrates the compartment model with a data observation model and a\nregression model for inferring parameters in the compartment model.\nNonetheless, the PINN model offers a simpler structure and is easier to\nimplement. Our results show that the proposed forecaster could potentially\nserve as a new computational tool to enhance the current capacity of infectious\ndisease forecasting."
                },
                "authors": [
                    {
                        "name": "Ying Qian"
                    },
                    {
                        "name": "Éric Marty"
                    },
                    {
                        "name": "Avranil Basu"
                    },
                    {
                        "name": "Eamon B. O'Dea"
                    },
                    {
                        "name": "Xianqiao Wang"
                    },
                    {
                        "name": "Spencer Fox"
                    },
                    {
                        "name": "Pejman Rohani"
                    },
                    {
                        "name": "John M. Drake"
                    },
                    {
                        "name": "He Li"
                    }
                ],
                "author_detail": {
                    "name": "He Li"
                },
                "author": "He Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09292v1",
                "updated": "2025-01-16T04:56:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    56,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:56:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    56,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy."
                },
                "authors": [
                    {
                        "name": "Kaustubh D. Dhole"
                    }
                ],
                "author_detail": {
                    "name": "Kaustubh D. Dhole"
                },
                "author": "Kaustubh D. Dhole",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09291v1",
                "updated": "2025-01-16T04:53:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    53,
                    29,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:53:29Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    53,
                    29,
                    3,
                    16,
                    0
                ],
                "title": "LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport"
                },
                "summary": "Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap."
                },
                "authors": [
                    {
                        "name": "Kyeongha Rho"
                    },
                    {
                        "name": "Hyeongkeun Lee"
                    },
                    {
                        "name": "Valentio Iverson"
                    },
                    {
                        "name": "Joon Son Chung"
                    }
                ],
                "author_detail": {
                    "name": "Joon Son Chung"
                },
                "author": "Joon Son Chung",
                "arxiv_comment": "5 pages, 2 figures; Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v3",
                "updated": "2025-01-16T04:08:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    8,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 11,632\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 11,632\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09274v1",
                "updated": "2025-01-16T03:44:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    44,
                    16,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T03:44:16Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    44,
                    16,
                    3,
                    16,
                    0
                ],
                "title": "Large Language Model is Secretly a Protein Sequence Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model is Secretly a Protein Sequence Optimizer"
                },
                "summary": "We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes."
                },
                "authors": [
                    {
                        "name": "Yinkai Wang"
                    },
                    {
                        "name": "Jiaxing He"
                    },
                    {
                        "name": "Yuanqi Du"
                    },
                    {
                        "name": "Xiaohui Chen"
                    },
                    {
                        "name": "Jianan Canal Li"
                    },
                    {
                        "name": "Li-Ping Liu"
                    },
                    {
                        "name": "Xiaolin Xu"
                    },
                    {
                        "name": "Soha Hassoun"
                    }
                ],
                "author_detail": {
                    "name": "Soha Hassoun"
                },
                "author": "Soha Hassoun",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09265v1",
                "updated": "2025-01-16T03:30:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    30,
                    47,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T03:30:47Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    30,
                    47,
                    3,
                    16,
                    0
                ],
                "title": "Perspective Transition of Large Language Models for Solving Subjective\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perspective Transition of Large Language Models for Solving Subjective\n  Tasks"
                },
                "summary": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems."
                },
                "authors": [
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Yuanchi Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Fuwen Luo"
                    },
                    {
                        "name": "Yile Wang"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15477v2",
                "updated": "2025-01-16T03:26:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    26,
                    36,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-16T23:01:10Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    23,
                    1,
                    10,
                    6,
                    168,
                    0
                ],
                "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for\n  Multi-label Social Media Text Classification in Disaster Informatics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for\n  Multi-label Social Media Text Classification in Disaster Informatics"
                },
                "summary": "In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios."
                },
                "authors": [
                    {
                        "name": "Kai Yin"
                    },
                    {
                        "name": "Chengkai Liu"
                    },
                    {
                        "name": "Ali Mostafavi"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_comment": "Relevant source code and data is available:\n  https://github.com/KaiYin97/CrsisLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06848v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06848v3",
                "updated": "2025-01-16T03:18:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    18,
                    14,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-12T15:34:24Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    15,
                    34,
                    24,
                    6,
                    12,
                    0
                ],
                "title": "A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models"
                },
                "summary": "Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we propose Feynman Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models, even\nwith off-the-shelf rewards, can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering ."
                },
                "authors": [
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Zachary Horvitz"
                    },
                    {
                        "name": "Ryan Teehan"
                    },
                    {
                        "name": "Mengye Ren"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Kathleen McKeown"
                    },
                    {
                        "name": "Rajesh Ranganath"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Ranganath"
                },
                "author": "Rajesh Ranganath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06848v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06848v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.09757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09757v1",
                "updated": "2025-01-16T18:59:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    53,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:59:53Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    53,
                    3,
                    16,
                    0
                ],
                "title": "Distilling Multi-modal Large Language Models for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Multi-modal Large Language Models for Autonomous Driving"
                },
                "summary": "Autonomous driving demands safe motion planning, especially in critical\n\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\nlarge language models (LLMs) as planners to improve generalizability to rare\nevents. However, using LLMs at test time introduces high computational costs.\nTo address this, we propose DiMA, an end-to-end autonomous driving system that\nmaintains the efficiency of an LLM-free (or vision-based) planner while\nleveraging the world knowledge of an LLM. DiMA distills the information from a\nmulti-modal LLM to a vision-based end-to-end planner through a set of specially\ndesigned surrogate tasks. Under a joint training strategy, a scene encoder\ncommon to both networks produces structured representations that are\nsemantically grounded as well as aligned to the final planning objective.\nNotably, the LLM is optional at inference, enabling robust planning without\ncompromising on efficiency. Training with DiMA results in a 37% reduction in\nthe L2 trajectory error and an 80% reduction in the collision rate of the\nvision-based planner, as well as a 44% trajectory error reduction in longtail\nscenarios. DiMA also achieves state-of-the-art performance on the nuScenes\nplanning benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving demands safe motion planning, especially in critical\n\"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage\nlarge language models (LLMs) as planners to improve generalizability to rare\nevents. However, using LLMs at test time introduces high computational costs.\nTo address this, we propose DiMA, an end-to-end autonomous driving system that\nmaintains the efficiency of an LLM-free (or vision-based) planner while\nleveraging the world knowledge of an LLM. DiMA distills the information from a\nmulti-modal LLM to a vision-based end-to-end planner through a set of specially\ndesigned surrogate tasks. Under a joint training strategy, a scene encoder\ncommon to both networks produces structured representations that are\nsemantically grounded as well as aligned to the final planning objective.\nNotably, the LLM is optional at inference, enabling robust planning without\ncompromising on efficiency. Training with DiMA results in a 37% reduction in\nthe L2 trajectory error and an 80% reduction in the collision rate of the\nvision-based planner, as well as a 44% trajectory error reduction in longtail\nscenarios. DiMA also achieves state-of-the-art performance on the nuScenes\nplanning benchmark."
                },
                "authors": [
                    {
                        "name": "Deepti Hegde"
                    },
                    {
                        "name": "Rajeev Yasarla"
                    },
                    {
                        "name": "Hong Cai"
                    },
                    {
                        "name": "Shizhong Han"
                    },
                    {
                        "name": "Apratim Bhattacharyya"
                    },
                    {
                        "name": "Shweta Mahajan"
                    },
                    {
                        "name": "Litian Liu"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Vishal M. Patel"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09754v1",
                "updated": "2025-01-16T18:59:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    3,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:59:03Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    59,
                    3,
                    3,
                    16,
                    0
                ],
                "title": "Lost in Translation, Found in Context: Sign Language Translation with\n  Contextual Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Translation, Found in Context: Sign Language Translation with\n  Contextual Cues"
                },
                "summary": "Our objective is to translate continuous sign language into spoken language\ntext. Inspired by the way human interpreters rely on context for accurate\ntranslation, we incorporate additional contextual cues together with the\nsigning video, into a new translation framework. Specifically, besides visual\nsign recognition features that encode the input video, we integrate\ncomplementary textual information from (i) captions describing the background\nshow, (ii) translation of previous sentences, as well as (iii) pseudo-glosses\ntranscribing the signing. These are automatically extracted and inputted along\nwith the visual features to a pre-trained large language model (LLM), which we\nfine-tune to generate spoken language translations in text form. Through\nextensive ablation studies, we show the positive contribution of each input cue\nto the translation performance. We train and evaluate our approach on BOBSL --\nthe largest British Sign Language dataset currently available. We show that our\ncontextual approach significantly enhances the quality of the translations\ncompared to previously reported results on BOBSL, and also to state-of-the-art\nmethods that we implement as baselines. Furthermore, we demonstrate the\ngenerality of our approach by applying it also to How2Sign, an American Sign\nLanguage dataset, and achieve competitive results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our objective is to translate continuous sign language into spoken language\ntext. Inspired by the way human interpreters rely on context for accurate\ntranslation, we incorporate additional contextual cues together with the\nsigning video, into a new translation framework. Specifically, besides visual\nsign recognition features that encode the input video, we integrate\ncomplementary textual information from (i) captions describing the background\nshow, (ii) translation of previous sentences, as well as (iii) pseudo-glosses\ntranscribing the signing. These are automatically extracted and inputted along\nwith the visual features to a pre-trained large language model (LLM), which we\nfine-tune to generate spoken language translations in text form. Through\nextensive ablation studies, we show the positive contribution of each input cue\nto the translation performance. We train and evaluate our approach on BOBSL --\nthe largest British Sign Language dataset currently available. We show that our\ncontextual approach significantly enhances the quality of the translations\ncompared to previously reported results on BOBSL, and also to state-of-the-art\nmethods that we implement as baselines. Furthermore, we demonstrate the\ngenerality of our approach by applying it also to How2Sign, an American Sign\nLanguage dataset, and achieve competitive results."
                },
                "authors": [
                    {
                        "name": "Youngjoon Jang"
                    },
                    {
                        "name": "Haran Raajesh"
                    },
                    {
                        "name": "Liliane Momeni"
                    },
                    {
                        "name": "Gül Varol"
                    },
                    {
                        "name": "Andrew Zisserman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zisserman"
                },
                "author": "Andrew Zisserman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09749v1",
                "updated": "2025-01-16T18:57:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    57,
                    20,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:57:20Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    57,
                    20,
                    3,
                    16,
                    0
                ],
                "title": "Enhancing Lexicon-Based Text Embeddings with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lexicon-Based Text Embeddings with Large Language Models"
                },
                "summary": "Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR)."
                },
                "authors": [
                    {
                        "name": "Yibin Lei"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Andrew Yates"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Yates"
                },
                "author": "Andrew Yates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08965v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08965v3",
                "updated": "2025-01-16T18:56:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    56,
                    27,
                    3,
                    16,
                    0
                ],
                "published": "2024-05-14T21:12:01Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    21,
                    12,
                    1,
                    1,
                    135,
                    0
                ],
                "title": "Meaning-Typed Programming: Language-level Abstractions and Runtime for\n  GenAI Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meaning-Typed Programming: Language-level Abstractions and Runtime for\n  GenAI Applications"
                },
                "summary": "Software is rapidly evolving from being programmed with traditional logical\ncode, to neuro-integrated applications that leverage generative AI and large\nlanguage models (LLMs) for application functionality. This shift increases the\ncomplexity of building applications, as developers now must reasoning about,\nprogram, and prompt LLMs. Despite efforts to create tools to assist with prompt\nengineering, these solutions often introduce additional layers of complexity to\nthe development of neuro-integrated applications. This paper proposes\nmeaning-typed programming (MTP), a novel approach to simplify the creation of\nneuro-integrated applications by introducing new language-level abstractions\nthat hide the complexities of LLM integration. Our key insight is that typical\nconventional code already possesses a high level of semantic richness that can\nbe automatically reasoned about, as it is designed to be readable and\nmaintainable by humans. Leveraging this insight, we conceptualize LLMs as\nmeaning-typed code constructs and introduce a by abstraction at the language\nlevel, MT-IR, a new meaning-based intermediate representation at the compiler\nlevel, and MT Runtime, an automated run-time engine for LLM integration and\noperations. We implement MTP in a production-grade Python super-set language\ncalled Jac and perform an extensive evaluation. Our results demonstrate that\nMTP not only simplifies the development process but also meets or exceeds the\nefficacy of state-of-the-art manual and tool-assisted prompt engineering\ntechniques in terms of accuracy and usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software is rapidly evolving from being programmed with traditional logical\ncode, to neuro-integrated applications that leverage generative AI and large\nlanguage models (LLMs) for application functionality. This shift increases the\ncomplexity of building applications, as developers now must reasoning about,\nprogram, and prompt LLMs. Despite efforts to create tools to assist with prompt\nengineering, these solutions often introduce additional layers of complexity to\nthe development of neuro-integrated applications. This paper proposes\nmeaning-typed programming (MTP), a novel approach to simplify the creation of\nneuro-integrated applications by introducing new language-level abstractions\nthat hide the complexities of LLM integration. Our key insight is that typical\nconventional code already possesses a high level of semantic richness that can\nbe automatically reasoned about, as it is designed to be readable and\nmaintainable by humans. Leveraging this insight, we conceptualize LLMs as\nmeaning-typed code constructs and introduce a by abstraction at the language\nlevel, MT-IR, a new meaning-based intermediate representation at the compiler\nlevel, and MT Runtime, an automated run-time engine for LLM integration and\noperations. We implement MTP in a production-grade Python super-set language\ncalled Jac and perform an extensive evaluation. Our results demonstrate that\nMTP not only simplifies the development process but also meets or exceeds the\nefficacy of state-of-the-art manual and tool-assisted prompt engineering\ntechniques in terms of accuracy and usability."
                },
                "authors": [
                    {
                        "name": "Jason Mars"
                    },
                    {
                        "name": "Yiping Kang"
                    },
                    {
                        "name": "Jayanaka L. Dantanarayana"
                    },
                    {
                        "name": "Kugesan Sivasothynathan"
                    },
                    {
                        "name": "Christopher Clarke"
                    },
                    {
                        "name": "Baichuan Li"
                    },
                    {
                        "name": "Krisztian Flautner"
                    },
                    {
                        "name": "Lingjia Tang"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Tang"
                },
                "author": "Lingjia Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08965v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08965v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09745v1",
                "updated": "2025-01-16T18:55:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    55,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:55:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    55,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Suggesting Code Edits in Interactive Machine Learning Notebooks Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suggesting Code Edits in Interactive Machine Learning Notebooks Using\n  Large Language Models"
                },
                "summary": "Machine learning developers frequently use interactive computational\nnotebooks, such as Jupyter notebooks, to host code for data processing and\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\nlearning pipelines and interactively observing outputs, however, maintaining\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\ndue to the length and complexity of the notebooks. Moreover, there is no\nexisting benchmark related to developer edits on Jupyter notebooks. To address\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\nperform the first study of the using LLMs to predict code edits in Jupyter\nnotebooks. Our dataset captures granular details of cell-level and line-level\nmodifications, offering a foundation for understanding real-world maintenance\npatterns in machine learning workflows. We observed that the edits on Jupyter\nnotebooks are highly localized, with changes averaging only 166 lines of code\nin repositories. While larger models outperform smaller counterparts in code\nediting, all models have low accuracy on our dataset even after finetuning,\ndemonstrating the complexity of real-world machine learning maintenance tasks.\nOur findings emphasize the critical role of contextual information in improving\nmodel performance and point toward promising avenues for advancing large\nlanguage models' capabilities in engineering machine learning code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning developers frequently use interactive computational\nnotebooks, such as Jupyter notebooks, to host code for data processing and\nmodel training. Jupyter notebooks provide a convenient tool for writing machine\nlearning pipelines and interactively observing outputs, however, maintaining\nJupyter notebooks, e.g., to add new features or fix bugs, can be challenging\ndue to the length and complexity of the notebooks. Moreover, there is no\nexisting benchmark related to developer edits on Jupyter notebooks. To address\nthis, we present the first dataset of 48,398 Jupyter notebook edits derived\nfrom 20,095 revisions of 792 machine learning repositories on GitHub, and\nperform the first study of the using LLMs to predict code edits in Jupyter\nnotebooks. Our dataset captures granular details of cell-level and line-level\nmodifications, offering a foundation for understanding real-world maintenance\npatterns in machine learning workflows. We observed that the edits on Jupyter\nnotebooks are highly localized, with changes averaging only 166 lines of code\nin repositories. While larger models outperform smaller counterparts in code\nediting, all models have low accuracy on our dataset even after finetuning,\ndemonstrating the complexity of real-world machine learning maintenance tasks.\nOur findings emphasize the critical role of contextual information in improving\nmodel performance and point toward promising avenues for advancing large\nlanguage models' capabilities in engineering machine learning code."
                },
                "authors": [
                    {
                        "name": "Bihui Jin"
                    },
                    {
                        "name": "Jiayue Wang"
                    },
                    {
                        "name": "Pengyu Nie"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Nie"
                },
                "author": "Pengyu Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09732v1",
                "updated": "2025-01-16T18:30:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    30,
                    37,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:30:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    30,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\n  Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\n  Steps"
                },
                "summary": "Generative models have made significant impacts across various domains,\nlargely due to their ability to scale during training by increasing data,\ncomputational resources, and model size, a phenomenon characterized by the\nscaling laws. Recent research has begun to explore inference-time scaling\nbehavior in Large Language Models (LLMs), revealing how performance can further\nimprove with additional computation during inference. Unlike LLMs, diffusion\nmodels inherently possess the flexibility to adjust inference-time computation\nvia the number of denoising steps, although the performance gains typically\nflatten after a few dozen. In this work, we explore the inference-time scaling\nbehavior of diffusion models beyond increasing denoising steps and investigate\nhow the generation performance can further improve with increased computation.\nSpecifically, we consider a search problem aimed at identifying better noises\nfor the diffusion sampling process. We structure the design space along two\naxes: the verifiers used to provide feedback, and the algorithms used to find\nbetter noise candidates. Through extensive experiments on class-conditioned and\ntext-conditioned image generation benchmarks, our findings reveal that\nincreasing inference-time compute leads to substantial improvements in the\nquality of samples generated by diffusion models, and with the complicated\nnature of images, combinations of the components in the framework can be\nspecifically chosen to conform with different application scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have made significant impacts across various domains,\nlargely due to their ability to scale during training by increasing data,\ncomputational resources, and model size, a phenomenon characterized by the\nscaling laws. Recent research has begun to explore inference-time scaling\nbehavior in Large Language Models (LLMs), revealing how performance can further\nimprove with additional computation during inference. Unlike LLMs, diffusion\nmodels inherently possess the flexibility to adjust inference-time computation\nvia the number of denoising steps, although the performance gains typically\nflatten after a few dozen. In this work, we explore the inference-time scaling\nbehavior of diffusion models beyond increasing denoising steps and investigate\nhow the generation performance can further improve with increased computation.\nSpecifically, we consider a search problem aimed at identifying better noises\nfor the diffusion sampling process. We structure the design space along two\naxes: the verifiers used to provide feedback, and the algorithms used to find\nbetter noise candidates. Through extensive experiments on class-conditioned and\ntext-conditioned image generation benchmarks, our findings reveal that\nincreasing inference-time compute leads to substantial improvements in the\nquality of samples generated by diffusion models, and with the complicated\nnature of images, combinations of the components in the framework can be\nspecifically chosen to conform with different application scenario."
                },
                "authors": [
                    {
                        "name": "Nanye Ma"
                    },
                    {
                        "name": "Shangyuan Tong"
                    },
                    {
                        "name": "Haolin Jia"
                    },
                    {
                        "name": "Hexiang Hu"
                    },
                    {
                        "name": "Yu-Chuan Su"
                    },
                    {
                        "name": "Mingda Zhang"
                    },
                    {
                        "name": "Xuan Yang"
                    },
                    {
                        "name": "Yandong Li"
                    },
                    {
                        "name": "Tommi Jaakkola"
                    },
                    {
                        "name": "Xuhui Jia"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02933v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02933v3",
                "updated": "2025-01-16T18:08:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    8,
                    22,
                    3,
                    16,
                    0
                ],
                "published": "2024-04-03T01:09:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    1,
                    9,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "NL2KQL: From Natural Language to Kusto Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL2KQL: From Natural Language to Kusto Query"
                },
                "summary": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness."
                },
                "authors": [
                    {
                        "name": "Xinye Tang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Jeremias Eichelbaum"
                    },
                    {
                        "name": "Mahan Das"
                    },
                    {
                        "name": "Alex Klein"
                    },
                    {
                        "name": "Nihal Irmak Pakis"
                    },
                    {
                        "name": "William Blum"
                    },
                    {
                        "name": "Daniel L Mace"
                    },
                    {
                        "name": "Tanvi Raja"
                    },
                    {
                        "name": "Namrata Padmanabhan"
                    },
                    {
                        "name": "Ye Xing"
                    }
                ],
                "author_detail": {
                    "name": "Ye Xing"
                },
                "author": "Ye Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02933v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02933v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09716v1",
                "updated": "2025-01-16T18:05:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    5,
                    28,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:05:28Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    5,
                    28,
                    3,
                    16,
                    0
                ],
                "title": "Intelligent OLSR Routing Protocol Optimization for VANETs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent OLSR Routing Protocol Optimization for VANETs"
                },
                "summary": "Recent advances in wireless technologies have given rise to the emergence of\nvehicular ad hoc networks (VANETs). In such networks, the limited coverage of\nWiFi and the high mobility of the nodes generate frequent topology changes and\nnetwork fragmentations. For these reasons, and taking into account that there\nis no central manager entity, routing packets through the network is a\nchallenging task. Therefore, offering an efficient routing strategy is crucial\nto the deployment of VANETs. This paper deals with the optimal parameter\nsetting of the optimized link state routing (OLSR), which is a well-known\nmobile ad hoc network routing protocol, by defining an optimization problem.\nThis way, a series of representative metaheuristic algorithms (particle swarm\noptimization, differential evolution, genetic algorithm, and simulated\nannealing) are studied in this paper to find automatically optimal\nconfigurations of this routing protocol. In addition, a set of realistic VANET\nscenarios (based in the city of M\\'alaga) have been defined to accurately\nevaluate the performance of the network under our automatic OLSR. In the\nexperiments, our tuned OLSR configurations result in better quality of service\n(QoS) than the standard request for comments (RFC 3626), as well as several\nhuman experts, making it amenable for utilization in VANET configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in wireless technologies have given rise to the emergence of\nvehicular ad hoc networks (VANETs). In such networks, the limited coverage of\nWiFi and the high mobility of the nodes generate frequent topology changes and\nnetwork fragmentations. For these reasons, and taking into account that there\nis no central manager entity, routing packets through the network is a\nchallenging task. Therefore, offering an efficient routing strategy is crucial\nto the deployment of VANETs. This paper deals with the optimal parameter\nsetting of the optimized link state routing (OLSR), which is a well-known\nmobile ad hoc network routing protocol, by defining an optimization problem.\nThis way, a series of representative metaheuristic algorithms (particle swarm\noptimization, differential evolution, genetic algorithm, and simulated\nannealing) are studied in this paper to find automatically optimal\nconfigurations of this routing protocol. In addition, a set of realistic VANET\nscenarios (based in the city of M\\'alaga) have been defined to accurately\nevaluate the performance of the network under our automatic OLSR. In the\nexperiments, our tuned OLSR configurations result in better quality of service\n(QoS) than the standard request for comments (RFC 3626), as well as several\nhuman experts, making it amenable for utilization in VANET configurations."
                },
                "authors": [
                    {
                        "name": "Jamal Toutouh"
                    },
                    {
                        "name": "José García-Nieto"
                    },
                    {
                        "name": "Enrique Alba"
                    }
                ],
                "author_detail": {
                    "name": "Enrique Alba"
                },
                "author": "Enrique Alba",
                "arxiv_doi": "10.1109/TVT.2012.2188552",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2012.2188552",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.09716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Vehicular Technology, vol. 61, no. 4, pp.\n  1884-1894, May 2012",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09709v1",
                "updated": "2025-01-16T18:00:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    0,
                    6,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T18:00:06Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    18,
                    0,
                    6,
                    3,
                    16,
                    0
                ],
                "title": "CyberMentor: AI Powered Learning Tool Platform to Address Diverse\n  Student Needs in Cybersecurity Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberMentor: AI Powered Learning Tool Platform to Address Diverse\n  Student Needs in Cybersecurity Education"
                },
                "summary": "Many non-traditional students in cybersecurity programs often lack access to\nadvice from peers, family members and professors, which can hinder their\neducational experiences. Additionally, these students may not fully benefit\nfrom various LLM-powered AI assistants due to issues like content relevance,\nlocality of advice, minimum expertise, and timing. This paper addresses these\nchallenges by introducing an application designed to provide comprehensive\nsupport by answering questions related to knowledge, skills, and career\npreparation advice tailored to the needs of these students. We developed a\nlearning tool platform, CyberMentor, to address the diverse needs and pain\npoints of students majoring in cybersecurity. Powered by agentic workflow and\nGenerative Large Language Models (LLMs), the platform leverages\nRetrieval-Augmented Generation (RAG) for accurate and contextually relevant\ninformation retrieval to achieve accessibility and personalization. We\ndemonstrated its value in addressing knowledge requirements for cybersecurity\neducation and for career marketability, in tackling skill requirements for\nanalytical and programming assignments, and in delivering real time on demand\nlearning support. Using three use scenarios, we showcased CyberMentor in\nfacilitating knowledge acquisition and career preparation and providing\nseamless skill-based guidance and support. We also employed the LangChain\nprompt-based evaluation methodology to evaluate the platform's impact,\nconfirming its strong performance in helpfulness, correctness, and\ncompleteness. These results underscore the system's ability to support students\nin developing practical cybersecurity skills while improving equity and\nsustainability within higher education. Furthermore, CyberMentor's open-source\ndesign allows for adaptation across other disciplines, fostering educational\ninnovation and broadening its potential impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many non-traditional students in cybersecurity programs often lack access to\nadvice from peers, family members and professors, which can hinder their\neducational experiences. Additionally, these students may not fully benefit\nfrom various LLM-powered AI assistants due to issues like content relevance,\nlocality of advice, minimum expertise, and timing. This paper addresses these\nchallenges by introducing an application designed to provide comprehensive\nsupport by answering questions related to knowledge, skills, and career\npreparation advice tailored to the needs of these students. We developed a\nlearning tool platform, CyberMentor, to address the diverse needs and pain\npoints of students majoring in cybersecurity. Powered by agentic workflow and\nGenerative Large Language Models (LLMs), the platform leverages\nRetrieval-Augmented Generation (RAG) for accurate and contextually relevant\ninformation retrieval to achieve accessibility and personalization. We\ndemonstrated its value in addressing knowledge requirements for cybersecurity\neducation and for career marketability, in tackling skill requirements for\nanalytical and programming assignments, and in delivering real time on demand\nlearning support. Using three use scenarios, we showcased CyberMentor in\nfacilitating knowledge acquisition and career preparation and providing\nseamless skill-based guidance and support. We also employed the LangChain\nprompt-based evaluation methodology to evaluate the platform's impact,\nconfirming its strong performance in helpfulness, correctness, and\ncompleteness. These results underscore the system's ability to support students\nin developing practical cybersecurity skills while improving equity and\nsustainability within higher education. Furthermore, CyberMentor's open-source\ndesign allows for adaptation across other disciplines, fostering educational\ninnovation and broadening its potential impact."
                },
                "authors": [
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Nianjun Zhou"
                    },
                    {
                        "name": "Zhixiong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhixiong Chen"
                },
                "author": "Zhixiong Chen",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09706v1",
                "updated": "2025-01-16T17:58:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    58,
                    32,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T17:58:32Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    58,
                    32,
                    3,
                    16,
                    0
                ],
                "title": "Domain Adaptation of Foundation LLMs for e-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Adaptation of Foundation LLMs for e-Commerce"
                },
                "summary": "We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains."
                },
                "authors": [
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Michael Kozielski"
                    },
                    {
                        "name": "Tala Bazazo"
                    },
                    {
                        "name": "Pavel Petrushkov"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Patrycja Cieplicka"
                    },
                    {
                        "name": "Dominika Basaj"
                    },
                    {
                        "name": "Shahram Khadivi"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Khadivi"
                },
                "author": "Shahram Khadivi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09686v1",
                "updated": "2025-01-16T17:37:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    58,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T17:37:58Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    37,
                    58,
                    3,
                    16,
                    0
                ],
                "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\n  Large Language Models"
                },
                "summary": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions."
                },
                "authors": [
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Qianyue Hao"
                    },
                    {
                        "name": "Zefang Zong"
                    },
                    {
                        "name": "Jingwei Wang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Xiaochong Lan"
                    },
                    {
                        "name": "Jiahui Gong"
                    },
                    {
                        "name": "Tianjian Ouyang"
                    },
                    {
                        "name": "Fanjin Meng"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Qinglong Yang"
                    },
                    {
                        "name": "Yiwen Song"
                    },
                    {
                        "name": "Sijian Ren"
                    },
                    {
                        "name": "Xinyuan Hu"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "36 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09674v1",
                "updated": "2025-01-16T17:11:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    11,
                    21,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T17:11:21Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    11,
                    21,
                    3,
                    16,
                    0
                ],
                "title": "Authenticated Delegation and Authorized AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authenticated Delegation and Authorized AI Agents"
                },
                "summary": "The rapid deployment of autonomous AI agents creates urgent challenges around\nauthorization, accountability, and access control in digital spaces. New\nstandards are needed to know whom AI agents act on behalf of and guide their\nuse appropriately, protecting online spaces while unlocking the value of task\ndelegation to autonomous agents. We introduce a novel framework for\nauthenticated, authorized, and auditable delegation of authority to AI agents,\nwhere human users can securely delegate and restrict the permissions and scope\nof agents while maintaining clear chains of accountability. This framework\nbuilds on existing identification and access management protocols, extending\nOAuth 2.0 and OpenID Connect with agent-specific credentials and metadata,\nmaintaining compatibility with established authentication and web\ninfrastructure. Further, we propose a framework for translating flexible,\nnatural language permissions into auditable access control configurations,\nenabling robust scoping of AI agent capabilities across diverse interaction\nmodalities. Taken together, this practical approach facilitates immediate\ndeployment of AI agents while addressing key security and accountability\nconcerns, working toward ensuring agentic AI systems perform only appropriate\nactions and providing a tool for digital service providers to enable AI agent\ninteractions without risking harm from scalable interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of autonomous AI agents creates urgent challenges around\nauthorization, accountability, and access control in digital spaces. New\nstandards are needed to know whom AI agents act on behalf of and guide their\nuse appropriately, protecting online spaces while unlocking the value of task\ndelegation to autonomous agents. We introduce a novel framework for\nauthenticated, authorized, and auditable delegation of authority to AI agents,\nwhere human users can securely delegate and restrict the permissions and scope\nof agents while maintaining clear chains of accountability. This framework\nbuilds on existing identification and access management protocols, extending\nOAuth 2.0 and OpenID Connect with agent-specific credentials and metadata,\nmaintaining compatibility with established authentication and web\ninfrastructure. Further, we propose a framework for translating flexible,\nnatural language permissions into auditable access control configurations,\nenabling robust scoping of AI agent capabilities across diverse interaction\nmodalities. Taken together, this practical approach facilitates immediate\ndeployment of AI agents while addressing key security and accountability\nconcerns, working toward ensuring agentic AI systems perform only appropriate\nactions and providing a tool for digital service providers to enable AI agent\ninteractions without risking harm from scalable interaction."
                },
                "authors": [
                    {
                        "name": "Tobin South"
                    },
                    {
                        "name": "Samuele Marro"
                    },
                    {
                        "name": "Thomas Hardjono"
                    },
                    {
                        "name": "Robert Mahari"
                    },
                    {
                        "name": "Cedric Deslandes Whitney"
                    },
                    {
                        "name": "Dazza Greenwood"
                    },
                    {
                        "name": "Alan Chan"
                    },
                    {
                        "name": "Alex Pentland"
                    }
                ],
                "author_detail": {
                    "name": "Alex Pentland"
                },
                "author": "Alex Pentland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M01, 68T01, 68U35, 94A60, 68P20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09672v1",
                "updated": "2025-01-16T17:08:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    8,
                    12,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T17:08:12Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    8,
                    12,
                    3,
                    16,
                    0
                ],
                "title": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP\n  Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP\n  Evaluation Benchmark"
                },
                "summary": "The proliferation of Vision-Language Models (VLMs) in the past several years\ncalls for rigorous and comprehensive evaluation methods and benchmarks. This\nwork analyzes existing VLM evaluation techniques, including automated metrics,\nAI-based assessments, and human evaluations across diverse tasks. We first\nintroduce Robin - a novel suite of VLMs that we built by combining Large\nLanguage Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use\nRobin to identify shortcomings of current evaluation approaches across scales.\nNext, to overcome the identified limitations, we introduce CHIRP - a new long\nform response benchmark we developed for more robust and complete VLM\nevaluation. We provide open access to the Robin training code, model suite, and\nCHIRP benchmark to promote reproducibility and advance VLM research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Vision-Language Models (VLMs) in the past several years\ncalls for rigorous and comprehensive evaluation methods and benchmarks. This\nwork analyzes existing VLM evaluation techniques, including automated metrics,\nAI-based assessments, and human evaluations across diverse tasks. We first\nintroduce Robin - a novel suite of VLMs that we built by combining Large\nLanguage Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use\nRobin to identify shortcomings of current evaluation approaches across scales.\nNext, to overcome the identified limitations, we introduce CHIRP - a new long\nform response benchmark we developed for more robust and complete VLM\nevaluation. We provide open access to the Robin training code, model suite, and\nCHIRP benchmark to promote reproducibility and advance VLM research."
                },
                "authors": [
                    {
                        "name": "Alexis Roger"
                    },
                    {
                        "name": "Prateek Humane"
                    },
                    {
                        "name": "Daniel Z. Kaplan"
                    },
                    {
                        "name": "Kshitij Gupta"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "George Adamopoulos"
                    },
                    {
                        "name": "Jonathan Siu Chi Lim"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Edwin Fennell"
                    },
                    {
                        "name": "Irina Rish"
                    }
                ],
                "author_detail": {
                    "name": "Irina Rish"
                },
                "author": "Irina Rish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09655v1",
                "updated": "2025-01-16T16:51:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    51,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:51:59Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    51,
                    59,
                    3,
                    16,
                    0
                ],
                "title": "A Survey of Research in Large Language Models for Electronic Design\n  Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Research in Large Language Models for Electronic Design\n  Automation"
                },
                "summary": "Within the rapidly evolving domain of Electronic Design Automation (EDA),\nLarge Language Models (LLMs) have emerged as transformative technologies,\noffering unprecedented capabilities for optimizing and automating various\naspects of electronic design. This survey provides a comprehensive exploration\nof LLM applications in EDA, focusing on advancements in model architectures,\nthe implications of varying model sizes, and innovative customization\ntechniques that enable tailored analytical insights. By examining the\nintersection of LLM capabilities and EDA requirements, the paper highlights the\nsignificant impact these models have on extracting nuanced understandings from\ncomplex datasets. Furthermore, it addresses the challenges and opportunities in\nintegrating LLMs into EDA workflows, paving the way for future research and\napplication in this dynamic field. Through this detailed analysis, the survey\naims to offer valuable insights to professionals in the EDA industry, AI\nresearchers, and anyone interested in the convergence of advanced AI\ntechnologies and electronic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the rapidly evolving domain of Electronic Design Automation (EDA),\nLarge Language Models (LLMs) have emerged as transformative technologies,\noffering unprecedented capabilities for optimizing and automating various\naspects of electronic design. This survey provides a comprehensive exploration\nof LLM applications in EDA, focusing on advancements in model architectures,\nthe implications of varying model sizes, and innovative customization\ntechniques that enable tailored analytical insights. By examining the\nintersection of LLM capabilities and EDA requirements, the paper highlights the\nsignificant impact these models have on extracting nuanced understandings from\ncomplex datasets. Furthermore, it addresses the challenges and opportunities in\nintegrating LLMs into EDA workflows, paving the way for future research and\napplication in this dynamic field. Through this detailed analysis, the survey\naims to offer valuable insights to professionals in the EDA industry, AI\nresearchers, and anyone interested in the convergence of advanced AI\ntechnologies and electronic design."
                },
                "authors": [
                    {
                        "name": "Jingyu Pan"
                    },
                    {
                        "name": "Guanglei Zhou"
                    },
                    {
                        "name": "Chen-Chia Chang"
                    },
                    {
                        "name": "Isaac Jacobson"
                    },
                    {
                        "name": "Jiang Hu"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "21 pages, 2 figures, 3 tables, accepted by TODAES",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09243v2",
                "updated": "2025-01-16T16:38:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    38,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-12-12T12:53:30Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    53,
                    30,
                    3,
                    347,
                    0
                ],
                "title": "SPRec: Leveraging Self-Play to Debias Preference Alignment for Large\n  Language Model-based Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPRec: Leveraging Self-Play to Debias Preference Alignment for Large\n  Language Model-based Recommendations"
                },
                "summary": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current LLM-based recommender systems primarily rely on\nsupervised fine-tuning (SFT) to train the model for recommendation tasks.\nHowever, relying solely on positive samples limits the model's ability to align\nwith user satisfaction and expectations. To address this, researchers have\nintroduced Direct Preference Optimization (DPO), which explicitly aligns\nrecommendations with user preferences using offline preference ranking data.\nDespite its advantages, our theoretical analysis reveals that DPO inherently\nbiases the model towards a few items, exacerbating the filter bubble issue and\nultimately degrading user experience. In this paper, we propose SPRec, a novel\nself-play recommendation framework designed to mitigate over-recommendation and\nimprove fairness without requiring additional data or manual intervention. In\neach self-play iteration, the model undergoes an SFT step followed by a DPO\nstep, treating offline interaction data as positive samples and the predicted\noutputs from the previous iteration as negative samples. This effectively\nre-weights the DPO loss function using the model's logits, adaptively\nsuppressing biased items. Extensive experiments on multiple real-world datasets\ndemonstrate SPRec's effectiveness in enhancing recommendation accuracy and\naddressing fairness concerns. The implementation is available via\nhttps://github.com/RegionCh/SPRec",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current LLM-based recommender systems primarily rely on\nsupervised fine-tuning (SFT) to train the model for recommendation tasks.\nHowever, relying solely on positive samples limits the model's ability to align\nwith user satisfaction and expectations. To address this, researchers have\nintroduced Direct Preference Optimization (DPO), which explicitly aligns\nrecommendations with user preferences using offline preference ranking data.\nDespite its advantages, our theoretical analysis reveals that DPO inherently\nbiases the model towards a few items, exacerbating the filter bubble issue and\nultimately degrading user experience. In this paper, we propose SPRec, a novel\nself-play recommendation framework designed to mitigate over-recommendation and\nimprove fairness without requiring additional data or manual intervention. In\neach self-play iteration, the model undergoes an SFT step followed by a DPO\nstep, treating offline interaction data as positive samples and the predicted\noutputs from the previous iteration as negative samples. This effectively\nre-weights the DPO loss function using the model's logits, adaptively\nsuppressing biased items. Extensive experiments on multiple real-world datasets\ndemonstrate SPRec's effectiveness in enhancing recommendation accuracy and\naddressing fairness concerns. The implementation is available via\nhttps://github.com/RegionCh/SPRec"
                },
                "authors": [
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09645v1",
                "updated": "2025-01-16T16:37:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    37,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:37:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    37,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through\n  Category-Bounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through\n  Category-Bounding"
                },
                "summary": "In today's assistant landscape, personalisation enhances interactions,\nfosters long-term relationships, and deepens engagement. However, many systems\nstruggle with retaining user preferences, leading to repetitive user requests\nand disengagement. Furthermore, the unregulated and opaque extraction of user\npreferences in industry applications raises significant concerns about privacy\nand trust, especially in regions with stringent regulations like Europe. In\nresponse to these challenges, we propose a long-term memory system for voice\nassistants, structured around predefined categories. This approach leverages\nLarge Language Models to efficiently extract, store, and retrieve preferences\nwithin these categories, ensuring both personalisation and transparency. We\nalso introduce a synthetic multi-turn, multi-session conversation dataset\n(CarMem), grounded in real industry data, tailored to an in-car voice assistant\nsetting. Benchmarked on the dataset, our system achieves an F1-score of .78 to\n.95 in preference extraction, depending on category granularity. Our\nmaintenance strategy reduces redundant preferences by 95% and contradictory\nones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,\nthe results demonstrate the system's suitability for industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's assistant landscape, personalisation enhances interactions,\nfosters long-term relationships, and deepens engagement. However, many systems\nstruggle with retaining user preferences, leading to repetitive user requests\nand disengagement. Furthermore, the unregulated and opaque extraction of user\npreferences in industry applications raises significant concerns about privacy\nand trust, especially in regions with stringent regulations like Europe. In\nresponse to these challenges, we propose a long-term memory system for voice\nassistants, structured around predefined categories. This approach leverages\nLarge Language Models to efficiently extract, store, and retrieve preferences\nwithin these categories, ensuring both personalisation and transparency. We\nalso introduce a synthetic multi-turn, multi-session conversation dataset\n(CarMem), grounded in real industry data, tailored to an in-car voice assistant\nsetting. Benchmarked on the dataset, our system achieves an F1-score of .78 to\n.95 in preference extraction, depending on category granularity. Our\nmaintenance strategy reduces redundant preferences by 95% and contradictory\nones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,\nthe results demonstrate the system's suitability for industrial applications."
                },
                "authors": [
                    {
                        "name": "Johannes Kirmayr"
                    },
                    {
                        "name": "Lukas Stappen"
                    },
                    {
                        "name": "Phillip Schneider"
                    },
                    {
                        "name": "Florian Matthes"
                    },
                    {
                        "name": "Elisabeth André"
                    }
                ],
                "author_detail": {
                    "name": "Elisabeth André"
                },
                "author": "Elisabeth André",
                "arxiv_comment": "Accepted for presentation at the International Conference on\n  Computational Linguistics (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09636v1",
                "updated": "2025-01-16T16:25:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    25,
                    30,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:25:30Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    25,
                    30,
                    3,
                    16,
                    0
                ],
                "title": "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading"
                },
                "summary": "Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning and large language models (LLMs) have\nfacilitated the deployment of the mixture-of-experts (MoE) mechanism in the\nstock investment domain. While these models have demonstrated promising trading\nperformance, they are often unimodal, neglecting the wealth of information\navailable in other modalities, such as textual data. Moreover, the traditional\nneural network-based router selection mechanism fails to consider contextual\nand real-world nuances, resulting in suboptimal expert selection. To address\nthese limitations, we propose LLMoE, a novel framework that employs LLMs as the\nrouter within the MoE architecture. Specifically, we replace the conventional\nneural network-based router with LLMs, leveraging their extensive world\nknowledge and reasoning capabilities to select experts based on historical\nprice data and stock news. This approach provides a more effective and\ninterpretable selection mechanism. Our experiments on multimodal real-world\nstock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models\nand other deep neural network approaches. Additionally, the flexible\narchitecture of LLMoE allows for easy adaptation to various downstream tasks."
                },
                "authors": [
                    {
                        "name": "Kuan-Ming Liu"
                    },
                    {
                        "name": "Ming-Chih Lo"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Chih Lo"
                },
                "arxiv_affiliation": "National Yang Ming Chiao Tung University, College of Computer Science",
                "author": "Ming-Chih Lo",
                "arxiv_comment": "Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging\n  Innovations in Finance, Social Media, and Crime Prevention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09631v1",
                "updated": "2025-01-16T16:19:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    19,
                    53,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:19:53Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    19,
                    53,
                    3,
                    16,
                    0
                ],
                "title": "Empowering Large Language Models in Wireless Communication: A Novel\n  Dataset and Fine-Tuning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Large Language Models in Wireless Communication: A Novel\n  Dataset and Fine-Tuning Framework"
                },
                "summary": "In this work, we develop a specialized dataset aimed at enhancing the\nevaluation and fine-tuning of large language models (LLMs) specifically for\nwireless communication applications. The dataset includes a diverse set of\nmulti-hop questions, including true/false and multiple-choice types, spanning\nvarying difficulty levels from easy to hard. By utilizing advanced language\nmodels for entity extraction and question generation, rigorous data curation\nprocesses are employed to maintain high quality and relevance. Additionally, we\nintroduce a Pointwise V-Information (PVI) based fine-tuning method, providing a\ndetailed theoretical analysis and justification for its use in quantifying the\ninformation content of training data with 2.24\\% and 1.31\\% performance boost\nfor different models compared to baselines, respectively. To demonstrate the\neffectiveness of the fine-tuned models with the proposed methodologies on\npractical tasks, we also consider different tasks, including summarizing\noptimization problems from technical papers and solving the mathematical\nproblems related to non-orthogonal multiple access (NOMA), which are generated\nby using the proposed multi-agent framework. Simulation results show\nsignificant performance gain in summarization tasks with 20.9\\% in the ROUGE-L\nmetrics. We also study the scaling laws of fine-tuning LLMs and the challenges\nLLMs face in the field of wireless communications, offering insights into their\nadaptation to wireless communication tasks. This dataset and fine-tuning\nmethodology aim to enhance the training and evaluation of LLMs, contributing to\nadvancements in LLMs for wireless communication research and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we develop a specialized dataset aimed at enhancing the\nevaluation and fine-tuning of large language models (LLMs) specifically for\nwireless communication applications. The dataset includes a diverse set of\nmulti-hop questions, including true/false and multiple-choice types, spanning\nvarying difficulty levels from easy to hard. By utilizing advanced language\nmodels for entity extraction and question generation, rigorous data curation\nprocesses are employed to maintain high quality and relevance. Additionally, we\nintroduce a Pointwise V-Information (PVI) based fine-tuning method, providing a\ndetailed theoretical analysis and justification for its use in quantifying the\ninformation content of training data with 2.24\\% and 1.31\\% performance boost\nfor different models compared to baselines, respectively. To demonstrate the\neffectiveness of the fine-tuned models with the proposed methodologies on\npractical tasks, we also consider different tasks, including summarizing\noptimization problems from technical papers and solving the mathematical\nproblems related to non-orthogonal multiple access (NOMA), which are generated\nby using the proposed multi-agent framework. Simulation results show\nsignificant performance gain in summarization tasks with 20.9\\% in the ROUGE-L\nmetrics. We also study the scaling laws of fine-tuning LLMs and the challenges\nLLMs face in the field of wireless communications, offering insights into their\nadaptation to wireless communication tasks. This dataset and fine-tuning\nmethodology aim to enhance the training and evaluation of LLMs, contributing to\nadvancements in LLMs for wireless communication research and applications."
                },
                "authors": [
                    {
                        "name": "Yushen Lin"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Wenqi Huang"
                    },
                    {
                        "name": "Kaidi Wang"
                    },
                    {
                        "name": "Zhiguo Ding"
                    },
                    {
                        "name": "Daniel K. C. So"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "13 pages, 13 figure, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10729v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10729v3",
                "updated": "2025-01-16T16:04:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    4,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-15T20:04:06Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    20,
                    4,
                    6,
                    5,
                    167,
                    0
                ],
                "title": "A Comprehensive Survey of Foundation Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Foundation Models in Medicine"
                },
                "summary": "Foundation models (FMs) are large-scale deep learning models trained on\nmassive datasets, often using self-supervised learning techniques. These models\nserve as a versatile base for a wide range of downstream tasks, including those\nin medicine and healthcare. FMs have demonstrated remarkable success across\nmultiple healthcare domains. However, existing surveys in this field do not\ncomprehensively cover all areas where FMs have made significant strides. In\nthis survey, we present a comprehensive review of FMs in medicine, focusing on\ntheir evolution, learning strategies, flagship models, applications, and\nassociated challenges. We examine how prominent FMs, such as the BERT and GPT\nfamilies, are transforming various aspects of healthcare, including clinical\nlarge language models, medical image analysis, and omics research.\nAdditionally, we provide a detailed taxonomy of FM-enabled healthcare\napplications, spanning clinical natural language processing, medical computer\nvision, graph learning, and other biology- and omics- related tasks. Despite\nthe transformative potentials of FMs, they also pose unique challenges. This\nsurvey delves into these challenges and highlights open research questions and\nlessons learned to guide researchers and practitioners. Our goal is to provide\nvaluable insights into the capabilities of FMs in health, facilitating\nresponsible deployment and mitigating associated risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) are large-scale deep learning models trained on\nmassive datasets, often using self-supervised learning techniques. These models\nserve as a versatile base for a wide range of downstream tasks, including those\nin medicine and healthcare. FMs have demonstrated remarkable success across\nmultiple healthcare domains. However, existing surveys in this field do not\ncomprehensively cover all areas where FMs have made significant strides. In\nthis survey, we present a comprehensive review of FMs in medicine, focusing on\ntheir evolution, learning strategies, flagship models, applications, and\nassociated challenges. We examine how prominent FMs, such as the BERT and GPT\nfamilies, are transforming various aspects of healthcare, including clinical\nlarge language models, medical image analysis, and omics research.\nAdditionally, we provide a detailed taxonomy of FM-enabled healthcare\napplications, spanning clinical natural language processing, medical computer\nvision, graph learning, and other biology- and omics- related tasks. Despite\nthe transformative potentials of FMs, they also pose unique challenges. This\nsurvey delves into these challenges and highlights open research questions and\nlessons learned to guide researchers and practitioners. Our goal is to provide\nvaluable insights into the capabilities of FMs in health, facilitating\nresponsible deployment and mitigating associated risks."
                },
                "authors": [
                    {
                        "name": "Wasif Khan"
                    },
                    {
                        "name": "Seowung Leem"
                    },
                    {
                        "name": "Kyle B. See"
                    },
                    {
                        "name": "Joshua K. Wong"
                    },
                    {
                        "name": "Shaoting Zhang"
                    },
                    {
                        "name": "Ruogu Fang"
                    }
                ],
                "author_detail": {
                    "name": "Ruogu Fang"
                },
                "author": "Ruogu Fang",
                "arxiv_comment": "Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10729v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10729v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09620v1",
                "updated": "2025-01-16T16:00:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    0,
                    37,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T16:00:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    16,
                    0,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated significant\nprogress in performing complex tasks. While Reinforcement Learning from Human\nFeedback (RLHF) has been effective in aligning LLMs with human preferences, it\nis susceptible to spurious correlations in reward modeling. Consequently, it\noften introduces biases-such as length bias, sycophancy, conceptual bias, and\ndiscrimination that hinder the model's ability to capture true causal\nrelationships. To address this, we propose a novel causal reward modeling\napproach that integrates causal inference to mitigate these spurious\ncorrelations. Our method enforces counterfactual invariance, ensuring reward\npredictions remain consistent when irrelevant variables are altered. Through\nexperiments on both synthetic and real-world datasets, we show that our\napproach mitigates various types of spurious correlations effectively,\nresulting in more reliable and fair alignment of LLMs with human preferences.\nAs a drop-in enhancement to the existing RLHF workflow, our causal reward\nmodeling provides a practical way to improve the trustworthiness and fairness\nof LLM finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated significant\nprogress in performing complex tasks. While Reinforcement Learning from Human\nFeedback (RLHF) has been effective in aligning LLMs with human preferences, it\nis susceptible to spurious correlations in reward modeling. Consequently, it\noften introduces biases-such as length bias, sycophancy, conceptual bias, and\ndiscrimination that hinder the model's ability to capture true causal\nrelationships. To address this, we propose a novel causal reward modeling\napproach that integrates causal inference to mitigate these spurious\ncorrelations. Our method enforces counterfactual invariance, ensuring reward\npredictions remain consistent when irrelevant variables are altered. Through\nexperiments on both synthetic and real-world datasets, we show that our\napproach mitigates various types of spurious correlations effectively,\nresulting in more reliable and fair alignment of LLMs with human preferences.\nAs a drop-in enhancement to the existing RLHF workflow, our causal reward\nmodeling provides a practical way to improve the trustworthiness and fairness\nof LLM finetuning."
                },
                "authors": [
                    {
                        "name": "Chaoqi Wang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Yibo Jiang"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Jiayi Liu"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Xiangjun Fan"
                    },
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Sinong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sinong Wang"
                },
                "author": "Sinong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01818v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01818v3",
                "updated": "2025-01-16T15:58:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    58,
                    24,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-04T11:46:34Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    11,
                    46,
                    34,
                    0,
                    338,
                    0
                ],
                "title": "Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto"
                },
                "summary": "Increasing interest in ensuring the safety of next-generation Artificial\nIntelligence (AI) systems calls for novel approaches to embedding morality into\nautonomous agents. This goal differs qualitatively from traditional\ntask-specific AI methodologies. In this paper, we provide a systematization of\nexisting approaches to the problem of introducing morality in machines -\nmodelled as a continuum. Our analysis suggests that popular techniques lie at\nthe extremes of this continuum - either being fully hard-coded into top-down,\nexplicit rules, or entirely learned in a bottom-up, implicit fashion with no\ndirect statement of any moral principle (this includes learning from human\nfeedback, as applied to the training and finetuning of large language models,\nor LLMs). Given the relative strengths and weaknesses of each type of\nmethodology, we argue that more hybrid solutions are needed to create adaptable\nand robust, yet controllable and interpretable agentic systems. To that end,\nthis paper discusses both the ethical foundations (including deontology,\nconsequentialism and virtue ethics) and implementations of morally aligned AI\nsystems.\n  We present a series of case studies that rely on intrinsic rewards, moral\nconstraints or textual instructions, applied to either pure-Reinforcement\nLearning or LLM-based agents. By analysing these diverse implementations under\none framework, we compare their relative strengths and shortcomings in\ndeveloping morally aligned AI systems. We then discuss strategies for\nevaluating the effectiveness of moral learning agents. Finally, we present open\nresearch questions and implications for the future of AI safety and ethics\nwhich are emerging from this hybrid framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing interest in ensuring the safety of next-generation Artificial\nIntelligence (AI) systems calls for novel approaches to embedding morality into\nautonomous agents. This goal differs qualitatively from traditional\ntask-specific AI methodologies. In this paper, we provide a systematization of\nexisting approaches to the problem of introducing morality in machines -\nmodelled as a continuum. Our analysis suggests that popular techniques lie at\nthe extremes of this continuum - either being fully hard-coded into top-down,\nexplicit rules, or entirely learned in a bottom-up, implicit fashion with no\ndirect statement of any moral principle (this includes learning from human\nfeedback, as applied to the training and finetuning of large language models,\nor LLMs). Given the relative strengths and weaknesses of each type of\nmethodology, we argue that more hybrid solutions are needed to create adaptable\nand robust, yet controllable and interpretable agentic systems. To that end,\nthis paper discusses both the ethical foundations (including deontology,\nconsequentialism and virtue ethics) and implementations of morally aligned AI\nsystems.\n  We present a series of case studies that rely on intrinsic rewards, moral\nconstraints or textual instructions, applied to either pure-Reinforcement\nLearning or LLM-based agents. By analysing these diverse implementations under\none framework, we compare their relative strengths and shortcomings in\ndeveloping morally aligned AI systems. We then discuss strategies for\nevaluating the effectiveness of moral learning agents. Finally, we present open\nresearch questions and implications for the future of AI safety and ethics\nwhich are emerging from this hybrid framework."
                },
                "authors": [
                    {
                        "name": "Elizaveta Tennant"
                    },
                    {
                        "name": "Stephen Hailes"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01818v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01818v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08603v2",
                "updated": "2025-01-16T15:57:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    57,
                    3,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-15T06:00:50Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    0,
                    50,
                    2,
                    15,
                    0
                ],
                "title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design"
                },
                "summary": "Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard\ncombinatorial optimization (CO) problems) is a common practice but requires\nextensive domain knowledge. Recently, Large Language Model (LLM)-based\nautomatic heuristics design (AHD) methods have shown promise in generating\nhigh-quality heuristics without manual intervention. Existing LLM-based AHD\nmethods employ a population to maintain a fixed number of top-performing\nLLM-generated heuristics and introduce evolutionary computation (EC) to enhance\nthe population iteratively. However, the population-based procedure brings\ngreedy properties, often resulting in convergence to local optima. Instead, to\nmore comprehensively explore the space of heuristics, we propose using Monte\nCarlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all\nLLM-generated heuristics in a tree structure. With a novel thought-alignment\nprocess and an exploration-decay technique, the proposed MCTS-AHD method\ndelivers significantly higher-quality heuristics on various complex tasks. Our\ncode is available at https://github.com/zz1358m/MCTS-AHD-master.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard\ncombinatorial optimization (CO) problems) is a common practice but requires\nextensive domain knowledge. Recently, Large Language Model (LLM)-based\nautomatic heuristics design (AHD) methods have shown promise in generating\nhigh-quality heuristics without manual intervention. Existing LLM-based AHD\nmethods employ a population to maintain a fixed number of top-performing\nLLM-generated heuristics and introduce evolutionary computation (EC) to enhance\nthe population iteratively. However, the population-based procedure brings\ngreedy properties, often resulting in convergence to local optima. Instead, to\nmore comprehensively explore the space of heuristics, we propose using Monte\nCarlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all\nLLM-generated heuristics in a tree structure. With a novel thought-alignment\nprocess and an exploration-decay technique, the proposed MCTS-AHD method\ndelivers significantly higher-quality heuristics on various complex tasks. Our\ncode is available at https://github.com/zz1358m/MCTS-AHD-master."
                },
                "authors": [
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Zhuoliang Xie"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17962v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17962v4",
                "updated": "2025-01-16T15:47:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    47,
                    58,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-25T22:44:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    44,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Lanxiao Huang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17962v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17962v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09604v1",
                "updated": "2025-01-16T15:24:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    24,
                    41,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T15:24:41Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    24,
                    41,
                    3,
                    16,
                    0
                ],
                "title": "From Scarcity to Capability: Empowering Fake News Detection in\n  Low-Resource Languages with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Scarcity to Capability: Empowering Fake News Detection in\n  Low-Resource Languages with LLMs"
                },
                "summary": "The rapid spread of fake news presents a significant global challenge,\nparticularly in low-resource languages like Bangla, which lack adequate\ndatasets and detection tools. Although manual fact-checking is accurate, it is\nexpensive and slow to prevent the dissemination of fake news. Addressing this\ngap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news\ndetection. This version includes 11,700 additional, meticulously curated fake\nnews articles validated from credible sources, creating a proportional dataset\nof 47,000 authentic and 13,000 fake news items across 13 categories. In\naddition, we created a manually curated independent test set of 460 fake and\n540 authentic news items for rigorous evaluation. We invest efforts in\ncollecting fake news from credible sources and manually verified while\npreserving the linguistic richness. We develop a benchmark system utilizing\ntransformer-based architectures, including fine-tuned Bidirectional Encoder\nRepresentations from Transformers variants (F1-87\\%) and Large Language Models\nwith Quantized Low-Rank Approximation (F1-89\\%), that significantly outperforms\ntraditional methods. BanFakeNews-2.0 offers a valuable resource to advance\nresearch and application in fake news detection for low-resourced languages. We\npublicly release our dataset and model on Github to foster research in this\ndirection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of fake news presents a significant global challenge,\nparticularly in low-resource languages like Bangla, which lack adequate\ndatasets and detection tools. Although manual fact-checking is accurate, it is\nexpensive and slow to prevent the dissemination of fake news. Addressing this\ngap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news\ndetection. This version includes 11,700 additional, meticulously curated fake\nnews articles validated from credible sources, creating a proportional dataset\nof 47,000 authentic and 13,000 fake news items across 13 categories. In\naddition, we created a manually curated independent test set of 460 fake and\n540 authentic news items for rigorous evaluation. We invest efforts in\ncollecting fake news from credible sources and manually verified while\npreserving the linguistic richness. We develop a benchmark system utilizing\ntransformer-based architectures, including fine-tuned Bidirectional Encoder\nRepresentations from Transformers variants (F1-87\\%) and Large Language Models\nwith Quantized Low-Rank Approximation (F1-89\\%), that significantly outperforms\ntraditional methods. BanFakeNews-2.0 offers a valuable resource to advance\nresearch and application in fake news detection for low-resourced languages. We\npublicly release our dataset and model on Github to foster research in this\ndirection."
                },
                "authors": [
                    {
                        "name": "Hrithik Majumdar Shibu"
                    },
                    {
                        "name": "Shrestha Datta"
                    },
                    {
                        "name": "Md. Sumon Miah"
                    },
                    {
                        "name": "Nasrullah Sami"
                    },
                    {
                        "name": "Mahruba Sharmin Chowdhury"
                    },
                    {
                        "name": "Md. Saiful Islam"
                    }
                ],
                "author_detail": {
                    "name": "Md. Saiful Islam"
                },
                "author": "Md. Saiful Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01957v2",
                "updated": "2025-01-16T15:00:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    0,
                    16,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-03T18:59:52Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    52,
                    4,
                    3,
                    0
                ],
                "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction."
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "https://github.com/VITA-MLLM/VITA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09534v1",
                "updated": "2025-01-16T13:36:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    36,
                    24,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:36:24Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    36,
                    24,
                    3,
                    16,
                    0
                ],
                "title": "AI in Support of Diversity and Inclusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in Support of Diversity and Inclusion"
                },
                "summary": "In this paper, we elaborate on how AI can support diversity and inclusion and\nexemplify research projects conducted in that direction. We start by looking at\nthe challenges and progress in making large language models (LLMs) more\ntransparent, inclusive, and aware of social biases. Even though LLMs like\nChatGPT have impressive abilities, they struggle to understand different\ncultural contexts and engage in meaningful, human like conversations. A key\nissue is that biases in language processing, especially in machine translation,\ncan reinforce inequality. Tackling these biases requires a multidisciplinary\napproach to ensure AI promotes diversity, fairness, and inclusion. We also\nhighlight AI's role in identifying biased content in media, which is important\nfor improving representation. By detecting unequal portrayals of social groups,\nAI can help challenge stereotypes and create more inclusive technologies.\nTransparent AI algorithms, which clearly explain their decisions, are essential\nfor building trust and reducing bias in AI systems. We also stress AI systems\nneed diverse and inclusive training data. Projects like the Child Growth\nMonitor show how using a wide range of data can help address real world\nproblems like malnutrition and poverty. We present a project that demonstrates\nhow AI can be applied to monitor the role of search engines in spreading\ndisinformation about the LGBTQ+ community. Moreover, we discuss the SignON\nproject as an example of how technology can bridge communication gaps between\nhearing and deaf people, emphasizing the importance of collaboration and mutual\ntrust in developing inclusive AI. Overall, with this paper, we advocate for AI\nsystems that are not only effective but also socially responsible, promoting\nfair and inclusive interactions between humans and machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we elaborate on how AI can support diversity and inclusion and\nexemplify research projects conducted in that direction. We start by looking at\nthe challenges and progress in making large language models (LLMs) more\ntransparent, inclusive, and aware of social biases. Even though LLMs like\nChatGPT have impressive abilities, they struggle to understand different\ncultural contexts and engage in meaningful, human like conversations. A key\nissue is that biases in language processing, especially in machine translation,\ncan reinforce inequality. Tackling these biases requires a multidisciplinary\napproach to ensure AI promotes diversity, fairness, and inclusion. We also\nhighlight AI's role in identifying biased content in media, which is important\nfor improving representation. By detecting unequal portrayals of social groups,\nAI can help challenge stereotypes and create more inclusive technologies.\nTransparent AI algorithms, which clearly explain their decisions, are essential\nfor building trust and reducing bias in AI systems. We also stress AI systems\nneed diverse and inclusive training data. Projects like the Child Growth\nMonitor show how using a wide range of data can help address real world\nproblems like malnutrition and poverty. We present a project that demonstrates\nhow AI can be applied to monitor the role of search engines in spreading\ndisinformation about the LGBTQ+ community. Moreover, we discuss the SignON\nproject as an example of how technology can bridge communication gaps between\nhearing and deaf people, emphasizing the importance of collaboration and mutual\ntrust in developing inclusive AI. Overall, with this paper, we advocate for AI\nsystems that are not only effective but also socially responsible, promoting\nfair and inclusive interactions between humans and machines."
                },
                "authors": [
                    {
                        "name": "Çiçek Güven"
                    },
                    {
                        "name": "Afra Alishahi"
                    },
                    {
                        "name": "Henry Brighton"
                    },
                    {
                        "name": "Gonzalo Nápoles"
                    },
                    {
                        "name": "Juan Sebastian Olier"
                    },
                    {
                        "name": "Marie Šafář"
                    },
                    {
                        "name": "Eric Postma"
                    },
                    {
                        "name": "Dimitar Shterionov"
                    },
                    {
                        "name": "Mirella De Sisto"
                    },
                    {
                        "name": "Eva Vanmassenhove"
                    }
                ],
                "author_detail": {
                    "name": "Eva Vanmassenhove"
                },
                "author": "Eva Vanmassenhove",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09532v1",
                "updated": "2025-01-16T13:34:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    34,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:34:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    34,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention\n  Mixture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention\n  Mixture"
                },
                "summary": "The success of VLMs often relies on the dynamic high-resolution schema that\nadaptively augments the input images to multiple crops, so that the details of\nthe images can be retained. However, such approaches result in a large number\nof redundant visual tokens, thus significantly reducing the efficiency of the\nVLMs. To improve the VLMs' efficiency without introducing extra training costs,\nmany research works are proposed to reduce the visual tokens by filtering the\nuninformative visual tokens or aggregating their information. Some approaches\npropose to reduce the visual tokens according to the self-attention of VLMs,\nwhich are biased, to result in inaccurate responses. The token reduction\napproaches solely rely on visual cues are text-agnostic, and fail to focus on\nthe areas that are most relevant to the question, especially when the queried\nobjects are non-salient to the image. In this work, we first conduct\nexperiments to show that the original text embeddings are aligned with the\nvisual tokens, without bias on the tailed visual tokens. We then propose a\nself-adaptive cross-modality attention mixture mechanism that dynamically\nleverages the effectiveness of visual saliency and text-to-image similarity in\nthe pre-LLM layers to select the visual tokens that are informative. Extensive\nexperiments demonstrate that the proposed approach achieves state-of-the-art\ntraining-free VLM acceleration performance, especially when the reduction rate\nis sufficiently large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of VLMs often relies on the dynamic high-resolution schema that\nadaptively augments the input images to multiple crops, so that the details of\nthe images can be retained. However, such approaches result in a large number\nof redundant visual tokens, thus significantly reducing the efficiency of the\nVLMs. To improve the VLMs' efficiency without introducing extra training costs,\nmany research works are proposed to reduce the visual tokens by filtering the\nuninformative visual tokens or aggregating their information. Some approaches\npropose to reduce the visual tokens according to the self-attention of VLMs,\nwhich are biased, to result in inaccurate responses. The token reduction\napproaches solely rely on visual cues are text-agnostic, and fail to focus on\nthe areas that are most relevant to the question, especially when the queried\nobjects are non-salient to the image. In this work, we first conduct\nexperiments to show that the original text embeddings are aligned with the\nvisual tokens, without bias on the tailed visual tokens. We then propose a\nself-adaptive cross-modality attention mixture mechanism that dynamically\nleverages the effectiveness of visual saliency and text-to-image similarity in\nthe pre-LLM layers to select the visual tokens that are informative. Extensive\nexperiments demonstrate that the proposed approach achieves state-of-the-art\ntraining-free VLM acceleration performance, especially when the reduction rate\nis sufficiently large."
                },
                "authors": [
                    {
                        "name": "Jiayi Han"
                    },
                    {
                        "name": "Liang Du"
                    },
                    {
                        "name": "Yiwen Wu"
                    },
                    {
                        "name": "Xiangguo Zhou"
                    },
                    {
                        "name": "Hongwei Du"
                    },
                    {
                        "name": "Weibo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Weibo Zheng"
                },
                "author": "Weibo Zheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09530v1",
                "updated": "2025-01-16T13:28:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    28,
                    40,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:28:40Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    28,
                    40,
                    3,
                    16,
                    0
                ],
                "title": "Make yourself comfortable: Nudging urban heat and noise mitigation with\n  smartwatch-based Just-in-time Adaptive Interventions (JITAI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make yourself comfortable: Nudging urban heat and noise mitigation with\n  smartwatch-based Just-in-time Adaptive Interventions (JITAI)"
                },
                "summary": "Humans can play a more active role in improving their comfort in the built\nenvironment if given the right information at the right place and time. This\npaper outlines the use of Just-in-Time Adaptive Interventions (JITAI)\nimplemented in the context of the built environment to provide information that\nhelps humans minimize the impact of heat and noise on their daily lives. This\nframework builds upon the open-source Cozie iOS smartwatch platform. It\nincludes data collection through micro-surveys and intervention messages\ntriggered by environmental, contextual, and personal history conditions. An\neight-month deployment of the method was completed in Singapore with 103\nparticipants who submitted over 12,000 micro-surveys and delivered over 3,600\nJITAI intervention messages. A weekly survey conducted during two deployment\nphases revealed an overall increase in perceived usefulness ranging from 8-19%\nover the first three weeks of data collection. For noise-related interventions,\nparticipants showed an overall increase in location changes ranging from 4-11%\nand a 2-17% increase in earphone use to mitigate noise distractions. For\nthermal comfort-related interventions, participants demonstrated a 3-13%\nincrease in adjustments to their location or thermostat to feel more\ncomfortable. The analysis found evidence that personality traits (such as\nconscientiousness), gender, and environmental preferences could be factors in\ndetermining the perceived helpfulness of JITAIs and influencing behavior\nchange. These findings underscore the importance of tailoring intervention\nstrategies to individual traits and environmental conditions, setting the stage\nfor future research to refine the delivery, timing, and content of intervention\nmessages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can play a more active role in improving their comfort in the built\nenvironment if given the right information at the right place and time. This\npaper outlines the use of Just-in-Time Adaptive Interventions (JITAI)\nimplemented in the context of the built environment to provide information that\nhelps humans minimize the impact of heat and noise on their daily lives. This\nframework builds upon the open-source Cozie iOS smartwatch platform. It\nincludes data collection through micro-surveys and intervention messages\ntriggered by environmental, contextual, and personal history conditions. An\neight-month deployment of the method was completed in Singapore with 103\nparticipants who submitted over 12,000 micro-surveys and delivered over 3,600\nJITAI intervention messages. A weekly survey conducted during two deployment\nphases revealed an overall increase in perceived usefulness ranging from 8-19%\nover the first three weeks of data collection. For noise-related interventions,\nparticipants showed an overall increase in location changes ranging from 4-11%\nand a 2-17% increase in earphone use to mitigate noise distractions. For\nthermal comfort-related interventions, participants demonstrated a 3-13%\nincrease in adjustments to their location or thermostat to feel more\ncomfortable. The analysis found evidence that personality traits (such as\nconscientiousness), gender, and environmental preferences could be factors in\ndetermining the perceived helpfulness of JITAIs and influencing behavior\nchange. These findings underscore the importance of tailoring intervention\nstrategies to individual traits and environmental conditions, setting the stage\nfor future research to refine the delivery, timing, and content of intervention\nmessages."
                },
                "authors": [
                    {
                        "name": "Clayton Miller"
                    },
                    {
                        "name": "Yun Xuan Chua"
                    },
                    {
                        "name": "Matias Quintana"
                    },
                    {
                        "name": "Binyu Lei"
                    },
                    {
                        "name": "Filip Biljecki"
                    },
                    {
                        "name": "Mario Frei"
                    }
                ],
                "author_detail": {
                    "name": "Mario Frei"
                },
                "author": "Mario Frei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09527v1",
                "updated": "2025-01-16T13:23:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    23,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:23:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    23,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "Confidence Estimation for Error Detection in Text-to-SQL Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Estimation for Error Detection in Text-to-SQL Systems"
                },
                "summary": "Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying the retrieval and synthesis of information. Despite the\nsuccess of large language models (LLMs) in converting natural language\nquestions into SQL queries, their broader adoption is limited by two main\nchallenges: achieving robust generalization across diverse queries and ensuring\ninterpretative confidence in their predictions. To tackle these issues, our\nresearch investigates the integration of selective classifiers into Text-to-SQL\nsystems. We analyse the trade-off between coverage and risk using entropy based\nconfidence estimation with selective classifiers and assess its impact on the\noverall performance of Text-to-SQL models. Additionally, we explore the models'\ninitial calibration and improve it with calibration techniques for better model\nalignment between confidence and accuracy. Our experimental results show that\nencoder-decoder T5 is better calibrated than in-context-learning GPT 4 and\ndecoder-only Llama 3, thus the designated external entropy-based selective\nclassifier has better performance. The study also reveal that, in terms of\nerror detection, selective classifier with a higher probability detects errors\nassociated with irrelevant questions rather than incorrect query generations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL enables users to interact with databases through natural\nlanguage, simplifying the retrieval and synthesis of information. Despite the\nsuccess of large language models (LLMs) in converting natural language\nquestions into SQL queries, their broader adoption is limited by two main\nchallenges: achieving robust generalization across diverse queries and ensuring\ninterpretative confidence in their predictions. To tackle these issues, our\nresearch investigates the integration of selective classifiers into Text-to-SQL\nsystems. We analyse the trade-off between coverage and risk using entropy based\nconfidence estimation with selective classifiers and assess its impact on the\noverall performance of Text-to-SQL models. Additionally, we explore the models'\ninitial calibration and improve it with calibration techniques for better model\nalignment between confidence and accuracy. Our experimental results show that\nencoder-decoder T5 is better calibrated than in-context-learning GPT 4 and\ndecoder-only Llama 3, thus the designated external entropy-based selective\nclassifier has better performance. The study also reveal that, in terms of\nerror detection, selective classifier with a higher probability detects errors\nassociated with irrelevant questions rather than incorrect query generations."
                },
                "authors": [
                    {
                        "name": "Oleg Somov"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "arxiv_comment": "15 pages, 11 figures, to be published in AAAI 2025 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09521v1",
                "updated": "2025-01-16T13:16:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    16,
                    37,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T13:16:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    13,
                    16,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "Augmenting a Large Language Model with a Combination of Text and Visual\n  Data for Conversational Visualization of Global Geospatial Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting a Large Language Model with a Combination of Text and Visual\n  Data for Conversational Visualization of Global Geospatial Data"
                },
                "summary": "We present a method for augmenting a Large Language Model (LLM) with a\ncombination of text and visual data to enable accurate question answering in\nvisualization of scientific data, making conversational visualization possible.\nLLMs struggle with tasks like visual data interaction, as they lack contextual\nvisual information. We address this problem by merging a text description of a\nvisualization and dataset with snapshots of the visualization. We extract their\nessential features into a structured text file, highly compact, yet descriptive\nenough to appropriately augment the LLM with contextual information, without\nany fine-tuning. This approach can be applied to any visualization that is\nalready finally rendered, as long as it is associated with some textual\ndescription.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a method for augmenting a Large Language Model (LLM) with a\ncombination of text and visual data to enable accurate question answering in\nvisualization of scientific data, making conversational visualization possible.\nLLMs struggle with tasks like visual data interaction, as they lack contextual\nvisual information. We address this problem by merging a text description of a\nvisualization and dataset with snapshots of the visualization. We extract their\nessential features into a structured text file, highly compact, yet descriptive\nenough to appropriately augment the LLM with contextual information, without\nany fine-tuning. This approach can be applied to any visualization that is\nalready finally rendered, as long as it is associated with some textual\ndescription."
                },
                "authors": [
                    {
                        "name": "Omar Mena"
                    },
                    {
                        "name": "Alexandre Kouyoumdjian"
                    },
                    {
                        "name": "Lonni Besançon"
                    },
                    {
                        "name": "Michael Gleicher"
                    },
                    {
                        "name": "Ivan Viola"
                    },
                    {
                        "name": "Anders Ynnerman"
                    }
                ],
                "author_detail": {
                    "name": "Anders Ynnerman"
                },
                "author": "Anders Ynnerman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13187v3",
                "updated": "2025-01-16T12:46:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    46,
                    53,
                    3,
                    16,
                    0
                ],
                "published": "2024-10-17T03:32:02Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    3,
                    32,
                    2,
                    3,
                    291,
                    0
                ],
                "title": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code\n  Processing"
                },
                "summary": "Large Language Models (LLMs) have been widely used in code completion, and\nresearchers are focusing on scaling up LLMs to improve their accuracy. However,\nlarger LLMs have lower inference efficiency, affecting developers' experience\nand productivity. In this paper, we propose a lightweight and effective LLM for\ncode completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B\nachieves higher code completion accuracy while having smaller scales (i.e., 7\nbillion parameters). We attribute the superiority of aiXcoder-7B to three key\nfactors: (1) Multi-objective training. We employ three training objectives, one\nof which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers\nthe syntax structures in code and effectively improves the performance of LLMs\nfor code. (2) Diverse data sampling strategies. They consider inter-file\nrelationships and enhance the capability of LLMs in understanding cross-file\ncontexts. (3) Extensive high-quality data. We establish a rigorous data\ncollection pipeline and consume a total of 1.2 trillion unique tokens for\ntraining aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a\nbroad distribution of code. We evaluate aiXcoder-7B in five popular code\ncompletion benchmarks and a new benchmark collected by this paper. The results\nshow that aiXcoder-7B outperforms the latest six LLMs with similar sizes and\neven surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),\npositioning aiXcoder-7B as a lightweight and effective LLM for academia and\nindustry. Finally, we summarize three valuable insights for helping\npractitioners train the next generations of LLMs for code. aiXcoder-7B has been\nopen-souced and gained significant attention. Until January 2025, aiXcoder-7B\nhas received 2,226 GitHub Stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used in code completion, and\nresearchers are focusing on scaling up LLMs to improve their accuracy. However,\nlarger LLMs have lower inference efficiency, affecting developers' experience\nand productivity. In this paper, we propose a lightweight and effective LLM for\ncode completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B\nachieves higher code completion accuracy while having smaller scales (i.e., 7\nbillion parameters). We attribute the superiority of aiXcoder-7B to three key\nfactors: (1) Multi-objective training. We employ three training objectives, one\nof which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers\nthe syntax structures in code and effectively improves the performance of LLMs\nfor code. (2) Diverse data sampling strategies. They consider inter-file\nrelationships and enhance the capability of LLMs in understanding cross-file\ncontexts. (3) Extensive high-quality data. We establish a rigorous data\ncollection pipeline and consume a total of 1.2 trillion unique tokens for\ntraining aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a\nbroad distribution of code. We evaluate aiXcoder-7B in five popular code\ncompletion benchmarks and a new benchmark collected by this paper. The results\nshow that aiXcoder-7B outperforms the latest six LLMs with similar sizes and\neven surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),\npositioning aiXcoder-7B as a lightweight and effective LLM for academia and\nindustry. Finally, we summarize three valuable insights for helping\npractitioners train the next generations of LLMs for code. aiXcoder-7B has been\nopen-souced and gained significant attention. Until January 2025, aiXcoder-7B\nhas received 2,226 GitHub Stars."
                },
                "authors": [
                    {
                        "name": "Siyuan Jiang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "He Zong"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Shukai Hu"
                    },
                    {
                        "name": "Erlu Li"
                    },
                    {
                        "name": "Jiazheng Ding"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Wei Ning"
                    },
                    {
                        "name": "Gen Wang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "arxiv_comment": "(1) Accepted by the 47th International Conference on Software\n  Engineering (ICSE 2025). (2) aiXcoder-7B is available at\n  https://github.com/aixcoder-plugin/aiXcoder-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09509v1",
                "updated": "2025-01-16T12:43:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    43,
                    23,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T12:43:23Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    43,
                    23,
                    3,
                    16,
                    0
                ],
                "title": "Power-Efficient RAN Intelligent Controllers Through Optimized KPI\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Power-Efficient RAN Intelligent Controllers Through Optimized KPI\n  Monitoring"
                },
                "summary": "The Open Radio Access Network (RAN) paradigm envisions a more flexible,\ninteroperable, and intelligent RAN ecosystem via new open interfaces and\nelements like the RAN Intelligent Controller (RIC). However, the impact of\nthese elements on Open RAN's power consumption remains heavily unexplored. This\nwork for the first time evaluates the impact of Key Performance Indicator (KPI)\nmonitoring on RIC's power consumption using real traffic and power\nmeasurements. By analyzing various RIC-RAN communication scenarios, we identify\nthat RIC's power consumption can become a scalability bottleneck, particularly\nin large-scale deployments, even when RIC is limited to its core operational\nfunctionalities and without incorporating application-specific processes. In\nthis context, also for the first time we explore potential power savings\nthrough the elimination of redundant KPI transmissions, extending existing\ntechniques for identical subscription removal and KPI selection, achieving\nsignificant power consumption gains exceeding 87\\% of the overall RIC power\nconsumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Radio Access Network (RAN) paradigm envisions a more flexible,\ninteroperable, and intelligent RAN ecosystem via new open interfaces and\nelements like the RAN Intelligent Controller (RIC). However, the impact of\nthese elements on Open RAN's power consumption remains heavily unexplored. This\nwork for the first time evaluates the impact of Key Performance Indicator (KPI)\nmonitoring on RIC's power consumption using real traffic and power\nmeasurements. By analyzing various RIC-RAN communication scenarios, we identify\nthat RIC's power consumption can become a scalability bottleneck, particularly\nin large-scale deployments, even when RIC is limited to its core operational\nfunctionalities and without incorporating application-specific processes. In\nthis context, also for the first time we explore potential power savings\nthrough the elimination of redundant KPI transmissions, extending existing\ntechniques for identical subscription removal and KPI selection, achieving\nsignificant power consumption gains exceeding 87\\% of the overall RIC power\nconsumption."
                },
                "authors": [
                    {
                        "name": "João Paulo S. H. Lima"
                    },
                    {
                        "name": "George N. Katsaros"
                    },
                    {
                        "name": "Konstantinos Nikitopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Nikitopoulos"
                },
                "author": "Konstantinos Nikitopoulos",
                "arxiv_comment": "Accepted for publication and presentation at IEEE WCNC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M12, 90B18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.3; D.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09493v1",
                "updated": "2025-01-16T12:06:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    6,
                    56,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T12:06:56Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    6,
                    56,
                    3,
                    16,
                    0
                ],
                "title": "Evaluating Conversational Recommender Systems with Large Language\n  Models: A User-Centric Evaluation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Conversational Recommender Systems with Large Language\n  Models: A User-Centric Evaluation Framework"
                },
                "summary": "Conversational recommender systems (CRS) involve both recommendation and\ndialogue tasks, which makes their evaluation a unique challenge. Although past\nresearch has analyzed various factors that may affect user satisfaction with\nCRS interactions from the perspective of user studies, few evaluation metrics\nfor CRS have been proposed. Recent studies have shown that LLMs can align with\nhuman preferences, and several LLM-based text quality evaluation measures have\nbeen introduced. However, the application of LLMs in CRS evaluation remains\nrelatively limited. To address this research gap and advance the development of\nuser-centric conversational recommender systems, this study proposes an\nautomated LLM-based CRS evaluation framework, building upon existing research\nin human-computer interaction and psychology. The framework evaluates CRS from\nfour dimensions: dialogue behavior, language expression, recommendation items,\nand response content. We use this framework to evaluate four different\nconversational recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational recommender systems (CRS) involve both recommendation and\ndialogue tasks, which makes their evaluation a unique challenge. Although past\nresearch has analyzed various factors that may affect user satisfaction with\nCRS interactions from the perspective of user studies, few evaluation metrics\nfor CRS have been proposed. Recent studies have shown that LLMs can align with\nhuman preferences, and several LLM-based text quality evaluation measures have\nbeen introduced. However, the application of LLMs in CRS evaluation remains\nrelatively limited. To address this research gap and advance the development of\nuser-centric conversational recommender systems, this study proposes an\nautomated LLM-based CRS evaluation framework, building upon existing research\nin human-computer interaction and psychology. The framework evaluates CRS from\nfour dimensions: dialogue behavior, language expression, recommendation items,\nand response content. We use this framework to evaluate four different\nconversational recommender systems."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Quanyu Dai"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "name": "Zhenhua Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Dong"
                },
                "author": "Zhenhua Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09482v1",
                "updated": "2025-01-16T11:35:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    35,
                    57,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T11:35:57Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    35,
                    57,
                    3,
                    16,
                    0
                ],
                "title": "Building Bridges across Papua New Guinea's Digital Divide in Growing the\n  ICT Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Bridges across Papua New Guinea's Digital Divide in Growing the\n  ICT Industry"
                },
                "summary": "Papua New Guinea (PNG) is an emerging tech society with an opportunity to\novercome geographic and social boundaries, in order to engage with the global\nmarket. However, the current tech landscape, dominated by Big Tech in Silicon\nValley and other multinational companies in the Global North, tends to overlook\nthe requirements of emerging economies such as PNG. This is becoming more\nobvious as issues such as algorithmic bias (in tech product deployments) and\nthe digital divide (as in the case of non-affordable commercial software) are\naffecting PNG users. The Open Source Software (OSS) movement, based on extant\nresearch, is seen as a way to level the playing field in the digitalization and\nadoption of Information and Communications Technologies (ICTs) in PNG. This\nperspectives paper documents the outcome of the second International Workshop\non BRIdging the Divides with Globally Engineered Software} (BRIDGES2023) in the\nhopes of proposing ideas for future research into ICT education, uplifting\nsoftware engineering (SE) capability, and OSS adoption in promoting a more\nequitable digital future for PNG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Papua New Guinea (PNG) is an emerging tech society with an opportunity to\novercome geographic and social boundaries, in order to engage with the global\nmarket. However, the current tech landscape, dominated by Big Tech in Silicon\nValley and other multinational companies in the Global North, tends to overlook\nthe requirements of emerging economies such as PNG. This is becoming more\nobvious as issues such as algorithmic bias (in tech product deployments) and\nthe digital divide (as in the case of non-affordable commercial software) are\naffecting PNG users. The Open Source Software (OSS) movement, based on extant\nresearch, is seen as a way to level the playing field in the digitalization and\nadoption of Information and Communications Technologies (ICTs) in PNG. This\nperspectives paper documents the outcome of the second International Workshop\non BRIdging the Divides with Globally Engineered Software} (BRIDGES2023) in the\nhopes of proposing ideas for future research into ICT education, uplifting\nsoftware engineering (SE) capability, and OSS adoption in promoting a more\nequitable digital future for PNG."
                },
                "authors": [
                    {
                        "name": "Marc Cheong"
                    },
                    {
                        "name": "Sankwi Abuzo"
                    },
                    {
                        "name": "Hideaki Hata"
                    },
                    {
                        "name": "Priscilla Kevin"
                    },
                    {
                        "name": "Winifred Kula"
                    },
                    {
                        "name": "Benson Mirou"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Raula Gaikovina Kula"
                    }
                ],
                "author_detail": {
                    "name": "Raula Gaikovina Kula"
                },
                "author": "Raula Gaikovina Kula",
                "arxiv_comment": "6 pages. Accepted by the ICSE 2025, Symposium on Software Engineering\n  in Global South (ICSE 2025, SEiGS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09475v1",
                "updated": "2025-01-16T11:27:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    27,
                    25,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T11:27:25Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    27,
                    25,
                    3,
                    16,
                    0
                ],
                "title": "Guided Debugging of Auto-Translated Code Using Differential Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Debugging of Auto-Translated Code Using Differential Testing"
                },
                "summary": "Large Language Models (LLMs) hold great promise in the task of code\ntranslation. However, the lack of explainability complicates the identification\nof the inevitable translation errors. In this paper, we propose tHinter, a\ndebugging tool to locate translation errors in auto-translated code. The core\nidea of tHinter is that correctly translated, the source and translated code\nshould present the same functionalities, giving the same output for the same\ninput. Hence, lines in the translated code responsible for output differences\nare possibly translation errors. First, tHinter employs fuzzing to generate\ndiverse test cases that thoroughly explore the translated code. Then, tHinter\nrelies on a heuristic algorithm to pinpoint translation errors from coverage\ninformation and differential testing execution results of those test cases.\nThis heuristic algorithm is designed to leverage both the statistics and the\nexpertise of developers. Comprehensive experiments with real code show its\neffectiveness. It reduces 71% lines developers need to review during debugging\nand increases the likelihood of the LLM fixing translation errors in a single\nquery by 59%. Developers generally consider it satisfactory and helpful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold great promise in the task of code\ntranslation. However, the lack of explainability complicates the identification\nof the inevitable translation errors. In this paper, we propose tHinter, a\ndebugging tool to locate translation errors in auto-translated code. The core\nidea of tHinter is that correctly translated, the source and translated code\nshould present the same functionalities, giving the same output for the same\ninput. Hence, lines in the translated code responsible for output differences\nare possibly translation errors. First, tHinter employs fuzzing to generate\ndiverse test cases that thoroughly explore the translated code. Then, tHinter\nrelies on a heuristic algorithm to pinpoint translation errors from coverage\ninformation and differential testing execution results of those test cases.\nThis heuristic algorithm is designed to leverage both the statistics and the\nexpertise of developers. Comprehensive experiments with real code show its\neffectiveness. It reduces 71% lines developers need to review during debugging\nand increases the likelihood of the LLM fixing translation errors in a single\nquery by 59%. Developers generally consider it satisfactory and helpful."
                },
                "authors": [
                    {
                        "name": "Shengnan Wu"
                    },
                    {
                        "name": "Xinyu Sun"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yangfan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yangfan Zhou"
                },
                "author": "Yangfan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22944v2",
                "updated": "2025-01-16T11:26:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    26,
                    2,
                    3,
                    16,
                    0
                ],
                "published": "2024-10-30T12:01:48Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    1,
                    48,
                    2,
                    304,
                    0
                ],
                "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification"
                },
                "summary": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments."
                },
                "authors": [
                    {
                        "name": "Tom A. Lamb"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Alasdair Paren"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Francesco Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pinto"
                },
                "author": "Francesco Pinto",
                "arxiv_comment": "28pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19185v2",
                "updated": "2025-01-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    54,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-27T14:03:49Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    14,
                    3,
                    49,
                    3,
                    179,
                    0
                ],
                "title": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a\n  supervised-friendly fashion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a\n  supervised-friendly fashion"
                },
                "summary": "Reinforcement Learning (RL) has been used to finetune Large Language Models\n(LLMs) using a reward model trained from preference data, to better align with\nhuman judgment. The recently introduced direct alignment methods, which are\noften simpler, more stable, and computationally lighter, can more directly\nachieve this. However, these approaches cannot optimize arbitrary rewards, and\nthe preference-based ones are not the only rewards of interest for LLMs (eg.,\nunit tests for code generation or textual entailment for summarization, among\nothers). RL-finetuning is usually done with a variation of policy gradient,\nwhich calls for on-policy or near-on-policy samples, requiring costly\ngenerations. We introduce Contrastive Policy Gradient, or CoPG, a simple and\nmathematically principled new RL algorithm that can estimate the optimal policy\neven from off-policy data. It can be seen as an off-policy policy gradient\napproach that does not rely on important sampling techniques and highlights the\nimportance of using (the right) state baseline. We show this approach to\ngeneralize the direct alignment method IPO (identity preference optimization)\nand classic policy gradient. We experiment with the proposed CoPG on a toy\nbandit problem to illustrate its properties, as well as for finetuning LLMs on\na summarization task, using a learned reward function considered as ground\ntruth for the purpose of the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has been used to finetune Large Language Models\n(LLMs) using a reward model trained from preference data, to better align with\nhuman judgment. The recently introduced direct alignment methods, which are\noften simpler, more stable, and computationally lighter, can more directly\nachieve this. However, these approaches cannot optimize arbitrary rewards, and\nthe preference-based ones are not the only rewards of interest for LLMs (eg.,\nunit tests for code generation or textual entailment for summarization, among\nothers). RL-finetuning is usually done with a variation of policy gradient,\nwhich calls for on-policy or near-on-policy samples, requiring costly\ngenerations. We introduce Contrastive Policy Gradient, or CoPG, a simple and\nmathematically principled new RL algorithm that can estimate the optimal policy\neven from off-policy data. It can be seen as an off-policy policy gradient\napproach that does not rely on important sampling techniques and highlights the\nimportance of using (the right) state baseline. We show this approach to\ngeneralize the direct alignment method IPO (identity preference optimization)\nand classic policy gradient. We experiment with the proposed CoPG on a toy\nbandit problem to illustrate its properties, as well as for finetuning LLMs on\na summarization task, using a learned reward function considered as ground\ntruth for the purpose of the experiments."
                },
                "authors": [
                    {
                        "name": "Yannis Flet-Berliac"
                    },
                    {
                        "name": "Nathan Grinsztajn"
                    },
                    {
                        "name": "Florian Strub"
                    },
                    {
                        "name": "Bill Wu"
                    },
                    {
                        "name": "Eugene Choi"
                    },
                    {
                        "name": "Chris Cremer"
                    },
                    {
                        "name": "Arash Ahmadian"
                    },
                    {
                        "name": "Yash Chandak"
                    },
                    {
                        "name": "Mohammad Gheshlaghi Azar"
                    },
                    {
                        "name": "Olivier Pietquin"
                    },
                    {
                        "name": "Matthieu Geist"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Geist"
                },
                "author": "Matthieu Geist",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09457v1",
                "updated": "2025-01-16T10:33:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    33,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T10:33:42Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    33,
                    42,
                    3,
                    16,
                    0
                ],
                "title": "\"A Great Start, But...\": Evaluating LLM-Generated Mind Maps for\n  Information Mapping in Video-Based Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"A Great Start, But...\": Evaluating LLM-Generated Mind Maps for\n  Information Mapping in Video-Based Design"
                },
                "summary": "Extracting concepts and understanding relationships from videos is essential\nin Video-Based Design (VBD), where videos serve as a primary medium for\nexploration but require significant effort in managing meta-information. Mind\nmaps, with their ability to visually organize complex data, offer a promising\napproach for structuring and analysing video content. Recent advancements in\nLarge Language Models (LLMs) provide new opportunities for meta-information\nprocessing and visual understanding in VBD, yet their application remains\nunderexplored. This study recruited 28 VBD practitioners to investigate the use\nof prompt-tuned LLMs for generating mind maps from ethnographic videos.\nComparing LLM-generated mind maps with those created by professional designers,\nwe evaluated rated scores, design effectiveness, and user experience across two\ncontexts. Findings reveal that LLMs effectively capture central concepts but\nstruggle with hierarchical organization and contextual grounding. We discuss\ntrust, customization, and workflow integration as key factors to guide future\nresearch on LLM-supported information mapping in VBD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting concepts and understanding relationships from videos is essential\nin Video-Based Design (VBD), where videos serve as a primary medium for\nexploration but require significant effort in managing meta-information. Mind\nmaps, with their ability to visually organize complex data, offer a promising\napproach for structuring and analysing video content. Recent advancements in\nLarge Language Models (LLMs) provide new opportunities for meta-information\nprocessing and visual understanding in VBD, yet their application remains\nunderexplored. This study recruited 28 VBD practitioners to investigate the use\nof prompt-tuned LLMs for generating mind maps from ethnographic videos.\nComparing LLM-generated mind maps with those created by professional designers,\nwe evaluated rated scores, design effectiveness, and user experience across two\ncontexts. Findings reveal that LLMs effectively capture central concepts but\nstruggle with hierarchical organization and contextual grounding. We discuss\ntrust, customization, and workflow integration as key factors to guide future\nresearch on LLM-supported information mapping in VBD."
                },
                "authors": [
                    {
                        "name": "Tianhao He"
                    },
                    {
                        "name": "Karthi Saravanan"
                    },
                    {
                        "name": "Evangelos Niforatos"
                    },
                    {
                        "name": "Gerd Kortuem"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Kortuem"
                },
                "author": "Gerd Kortuem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20770v2",
                "updated": "2025-01-16T10:24:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    24,
                    32,
                    3,
                    16,
                    0
                ],
                "published": "2024-12-30T07:41:01Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    41,
                    1,
                    0,
                    365,
                    0
                ],
                "title": "Humanoid Robot RHP Friends: Seamless Combination of Autonomous and\n  Teleoperated Tasks in a Nursing Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid Robot RHP Friends: Seamless Combination of Autonomous and\n  Teleoperated Tasks in a Nursing Context"
                },
                "summary": "This paper describes RHP Friends, a social humanoid robot developed to enable\nassistive robotic deployments in human-coexisting environments. As a use-case\napplication, we present its potential use in nursing by extending its\ncapabilities to operate human devices and tools according to the task and by\nenabling remote assistance operations. To meet a wide variety of tasks and\nsituations in environments designed by and for humans, we developed a system\nthat seamlessly integrates the slim and lightweight robot and several\ntechnologies: locomanipulation, multi-contact motion, teleoperation, and object\ndetection and tracking. We demonstrated the system's usage in a nursing\napplication. The robot efficiently performed the daily task of patient transfer\nand a non-routine task, represented by a request to operate a circuit breaker.\nThis demonstration, held at the 2023 International Robot Exhibition (IREX),\nconducted three times a day over three days.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes RHP Friends, a social humanoid robot developed to enable\nassistive robotic deployments in human-coexisting environments. As a use-case\napplication, we present its potential use in nursing by extending its\ncapabilities to operate human devices and tools according to the task and by\nenabling remote assistance operations. To meet a wide variety of tasks and\nsituations in environments designed by and for humans, we developed a system\nthat seamlessly integrates the slim and lightweight robot and several\ntechnologies: locomanipulation, multi-contact motion, teleoperation, and object\ndetection and tracking. We demonstrated the system's usage in a nursing\napplication. The robot efficiently performed the daily task of patient transfer\nand a non-routine task, represented by a request to operate a circuit breaker.\nThis demonstration, held at the 2023 International Robot Exhibition (IREX),\nconducted three times a day over three days."
                },
                "authors": [
                    {
                        "name": "Mehdi Benallegue"
                    },
                    {
                        "name": "Guillaume Lorthioir"
                    },
                    {
                        "name": "Antonin Dallard"
                    },
                    {
                        "name": "Rafael Cisneros-Limón"
                    },
                    {
                        "name": "Iori Kumagai"
                    },
                    {
                        "name": "Mitsuharu Morisawa"
                    },
                    {
                        "name": "Hiroshi Kaminaga"
                    },
                    {
                        "name": "Masaki Murooka"
                    },
                    {
                        "name": "Antoine Andre"
                    },
                    {
                        "name": "Pierre Gergondet"
                    },
                    {
                        "name": "Kenji Kaneko"
                    },
                    {
                        "name": "Guillaume Caron"
                    },
                    {
                        "name": "Fumio Kanehiro"
                    },
                    {
                        "name": "Abderrahmane Kheddar"
                    },
                    {
                        "name": "Soh Yukizaki"
                    },
                    {
                        "name": "Junichi Karasuyama"
                    },
                    {
                        "name": "Junichi Murakami"
                    },
                    {
                        "name": "Masayuki Kamon"
                    }
                ],
                "author_detail": {
                    "name": "Masayuki Kamon"
                },
                "arxiv_affiliation": "CNRS-AIST JRL, LIRMM",
                "author": "Masayuki Kamon",
                "arxiv_comment": "IEEE Robotics and Automation Magazine, In press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11775v2",
                "updated": "2025-01-16T10:20:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    20,
                    3,
                    3,
                    16,
                    0
                ],
                "published": "2024-08-21T17:00:05Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    0,
                    5,
                    2,
                    234,
                    0
                ],
                "title": "Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support: For 3GPP Standards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support: For 3GPP Standards"
                },
                "summary": "Recent studies show that large language models (LLMs) struggle with technical\nstandards in telecommunications. We propose a fine-tuned retrieval-augmented\ngeneration (RAG) system based on the Phi-2 small language model (SLM) to serve\nas an oracle for communication networks. Our developed system leverages\nforward-looking semantic chunking to adaptively determine parsing breakpoints\nbased on embedding similarity, enabling effective processing of diverse\ndocument formats. To handle the challenge of multiple similar contexts in\ntechnical standards, we employ a re-ranking algorithm to prioritize the most\nrelevant retrieved chunks. Recognizing the limitations of Phi-2's small context\nwindow, we implement a recent technique, namely SelfExtend, to expand the\ncontext window during inference, which not only boosts the performance but also\ncan accommodate a wider range of user queries and design requirements from\ncustomers to specialized technicians. For fine-tuning, we utilize the low-rank\nadaptation (LoRA) technique to enhance computational efficiency during training\nand enable effective fine-tuning on small datasets. Our comprehensive\nexperiments demonstrate substantial improvements over existing\nquestion-answering approaches in the telecom domain, achieving performance that\nexceeds larger language models such as GPT-4 (which is about 880 times larger\nin size). This work presents a novel approach to leveraging SLMs for\ncommunication networks, offering a balance of efficiency and performance. This\nwork can serve as a foundation towards agentic language models for networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that large language models (LLMs) struggle with technical\nstandards in telecommunications. We propose a fine-tuned retrieval-augmented\ngeneration (RAG) system based on the Phi-2 small language model (SLM) to serve\nas an oracle for communication networks. Our developed system leverages\nforward-looking semantic chunking to adaptively determine parsing breakpoints\nbased on embedding similarity, enabling effective processing of diverse\ndocument formats. To handle the challenge of multiple similar contexts in\ntechnical standards, we employ a re-ranking algorithm to prioritize the most\nrelevant retrieved chunks. Recognizing the limitations of Phi-2's small context\nwindow, we implement a recent technique, namely SelfExtend, to expand the\ncontext window during inference, which not only boosts the performance but also\ncan accommodate a wider range of user queries and design requirements from\ncustomers to specialized technicians. For fine-tuning, we utilize the low-rank\nadaptation (LoRA) technique to enhance computational efficiency during training\nand enable effective fine-tuning on small datasets. Our comprehensive\nexperiments demonstrate substantial improvements over existing\nquestion-answering approaches in the telecom domain, achieving performance that\nexceeds larger language models such as GPT-4 (which is about 880 times larger\nin size). This work presents a novel approach to leveraging SLMs for\ncommunication networks, offering a balance of efficiency and performance. This\nwork can serve as a foundation towards agentic language models for networks."
                },
                "authors": [
                    {
                        "name": "Omar Erak"
                    },
                    {
                        "name": "Nouf Alabbasi"
                    },
                    {
                        "name": "Omar Alhussein"
                    },
                    {
                        "name": "Ismail Lotfi"
                    },
                    {
                        "name": "Amr Hussein"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "submitted to Proc. IEEE Globecom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11005v2",
                "updated": "2025-01-16T10:05:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    5,
                    17,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-25T20:23:15Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    23,
                    15,
                    1,
                    177,
                    0
                ],
                "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation\n  Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a standard architectural\npattern for incorporating domain-specific knowledge into user-facing chat\napplications powered by Large Language Models (LLMs). RAG systems are\ncharacterized by (1) a document retriever that queries a domain-specific corpus\nfor context information relevant to an input query, and (2) an LLM that\ngenerates a response based on the provided query and context. However,\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\nunified evaluation criteria and annotated datasets. In response, we introduce\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\nexamples. It covers five unique industry-specific domains and various RAG task\ntypes. RAGBench examples are sourced from industry corpora such as user\nmanuals, making it particularly relevant for industry applications. Further, we\nformalize the TRACe evaluation framework: a set of explainable and actionable\nRAG evaluation metrics applicable across all RAG domains. We release the\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\nenabling actionable feedback for continuous improvement of production\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\nRAG evaluation task. We identify areas where existing approaches fall short and\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\nevaluation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a standard architectural\npattern for incorporating domain-specific knowledge into user-facing chat\napplications powered by Large Language Models (LLMs). RAG systems are\ncharacterized by (1) a document retriever that queries a domain-specific corpus\nfor context information relevant to an input query, and (2) an LLM that\ngenerates a response based on the provided query and context. However,\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\nunified evaluation criteria and annotated datasets. In response, we introduce\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\nexamples. It covers five unique industry-specific domains and various RAG task\ntypes. RAGBench examples are sourced from industry corpora such as user\nmanuals, making it particularly relevant for industry applications. Further, we\nformalize the TRACe evaluation framework: a set of explainable and actionable\nRAG evaluation metrics applicable across all RAG domains. We release the\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\nenabling actionable feedback for continuous improvement of production\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\nRAG evaluation task. We identify areas where existing approaches fall short and\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\nevaluation systems."
                },
                "authors": [
                    {
                        "name": "Robert Friel"
                    },
                    {
                        "name": "Masha Belyi"
                    },
                    {
                        "name": "Atindriyo Sanyal"
                    }
                ],
                "author_detail": {
                    "name": "Atindriyo Sanyal"
                },
                "author": "Atindriyo Sanyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09434v1",
                "updated": "2025-01-16T10:04:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    4,
                    19,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T10:04:19Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    4,
                    19,
                    3,
                    16,
                    0
                ],
                "title": "Agile System Development Lifecycle for AI Systems: Decision Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile System Development Lifecycle for AI Systems: Decision Architecture"
                },
                "summary": "Agile system development life cycle (SDLC) focuses on typical functional and\nnon-functional system requirements for developing traditional software systems.\nHowever, Artificial Intelligent (AI) systems are different in nature and have\ndistinct attributes such as (1) autonomy, (2) adaptiveness, (3) content\ngeneration, (4) decision-making, (5) predictability and (6) recommendation.\nAgile SDLC needs to be enhanced to support the AI system development and\nongoing post-deployment adaptation. The challenge is: how can agile SDLC be\nenhanced to support AI systems? The scope of this paper is limited to AI system\nenabled decision automation. Thus, this paper proposes the use of decision\nscience to enhance the agile SDLC to support the AI system development.\nDecision science is the study of decision-making, which seems useful to\nidentify, analyse and describe decisions and their architecture subject to\nautomation via AI systems. Specifically, this paper discusses the decision\narchitecture in detail within the overall context of agile SDLC for AI systems.\nThe application of the proposed approach is demonstrated with the help of an\nexample scenario of insurance claim processing. This initial work indicated the\nusability of a decision science to enhancing the agile SDLC for designing and\nimplementing the AI systems for decision-automation. This work provides an\ninitial foundation for further work in this new area of decision architecture\nand agile SDLC for AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile system development life cycle (SDLC) focuses on typical functional and\nnon-functional system requirements for developing traditional software systems.\nHowever, Artificial Intelligent (AI) systems are different in nature and have\ndistinct attributes such as (1) autonomy, (2) adaptiveness, (3) content\ngeneration, (4) decision-making, (5) predictability and (6) recommendation.\nAgile SDLC needs to be enhanced to support the AI system development and\nongoing post-deployment adaptation. The challenge is: how can agile SDLC be\nenhanced to support AI systems? The scope of this paper is limited to AI system\nenabled decision automation. Thus, this paper proposes the use of decision\nscience to enhance the agile SDLC to support the AI system development.\nDecision science is the study of decision-making, which seems useful to\nidentify, analyse and describe decisions and their architecture subject to\nautomation via AI systems. Specifically, this paper discusses the decision\narchitecture in detail within the overall context of agile SDLC for AI systems.\nThe application of the proposed approach is demonstrated with the help of an\nexample scenario of insurance claim processing. This initial work indicated the\nusability of a decision science to enhancing the agile SDLC for designing and\nimplementing the AI systems for decision-automation. This work provides an\ninitial foundation for further work in this new area of decision architecture\nand agile SDLC for AI systems."
                },
                "authors": [
                    {
                        "name": "Asif Q. Gill"
                    }
                ],
                "author_detail": {
                    "name": "Asif Q. Gill"
                },
                "author": "Asif Q. Gill",
                "arxiv_comment": "11, 4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09431v1",
                "updated": "2025-01-16T09:59:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    59,
                    45,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:59:45Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    59,
                    45,
                    3,
                    16,
                    0
                ],
                "title": "A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and\n  Mitigation Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and\n  Mitigation Strategy"
                },
                "summary": "While large language models (LLMs) present significant potential for\nsupporting numerous real-world applications and delivering positive social\nimpacts, they still face significant challenges in terms of the inherent risk\nof privacy leakage, hallucinated outputs, and value misalignment, and can be\nmaliciously used for generating toxic content and unethical purposes after been\njailbroken. Therefore, in this survey, we present a comprehensive review of\nrecent advancements aimed at mitigating these issues, organized across the four\nphases of LLM development and usage: data collecting and pre-training,\nfine-tuning and alignment, prompting and reasoning, and post-processing and\nauditing. We elaborate on the recent advances for enhancing the performance of\nLLMs in terms of privacy protection, hallucination reduction, value alignment,\ntoxicity elimination, and jailbreak defenses. In contrast to previous surveys\nthat focus on a single dimension of responsible LLMs, this survey presents a\nunified framework that encompasses these diverse dimensions, providing a\ncomprehensive view of enhancing LLMs to better serve real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) present significant potential for\nsupporting numerous real-world applications and delivering positive social\nimpacts, they still face significant challenges in terms of the inherent risk\nof privacy leakage, hallucinated outputs, and value misalignment, and can be\nmaliciously used for generating toxic content and unethical purposes after been\njailbroken. Therefore, in this survey, we present a comprehensive review of\nrecent advancements aimed at mitigating these issues, organized across the four\nphases of LLM development and usage: data collecting and pre-training,\nfine-tuning and alignment, prompting and reasoning, and post-processing and\nauditing. We elaborate on the recent advances for enhancing the performance of\nLLMs in terms of privacy protection, hallucination reduction, value alignment,\ntoxicity elimination, and jailbreak defenses. In contrast to previous surveys\nthat focus on a single dimension of responsible LLMs, this survey presents a\nunified framework that encompasses these diverse dimensions, providing a\ncomprehensive view of enhancing LLMs to better serve real-world applications."
                },
                "authors": [
                    {
                        "name": "Huandong Wang"
                    },
                    {
                        "name": "Wenjie Fu"
                    },
                    {
                        "name": "Yingzhou Tang"
                    },
                    {
                        "name": "Zhilong Chen"
                    },
                    {
                        "name": "Yuxi Huang"
                    },
                    {
                        "name": "Jinghua Piao"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Tao Jiang"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04997v2",
                "updated": "2025-01-16T09:58:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    58,
                    54,
                    3,
                    16,
                    0
                ],
                "published": "2024-01-10T08:28:56Z",
                "published_parsed": [
                    2024,
                    1,
                    10,
                    8,
                    28,
                    56,
                    2,
                    10,
                    0
                ],
                "title": "Tapping the Potential of Large Language Models as Recommender Systems: A\n  Comprehensive Framework and Empirical Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapping the Potential of Large Language Models as Recommender Systems: A\n  Comprehensive Framework and Empirical Analysis"
                },
                "summary": "Recently, Large Language Models~(LLMs) such as ChatGPT have showcased\nremarkable abilities in solving general tasks, demonstrating the potential for\napplications in recommender systems. To assess how effectively LLMs can be used\nin recommendation tasks, our study primarily focuses on employing LLMs as\nrecommender systems through prompting engineering. We propose a general\nframework for utilizing LLMs in recommendation tasks, focusing on the\ncapabilities of LLMs as recommenders. To conduct our analysis, we formalize the\ninput of LLMs for recommendation into natural language prompts with two key\naspects, and explain how our framework can be generalized to various\nrecommendation scenarios. As for the use of LLMs as recommenders, we analyze\nthe impact of public availability, tuning strategies, model architecture,\nparameter scale, and context length on recommendation results based on the\nclassification of LLMs. As for prompt engineering, we further analyze the\nimpact of four important components of prompts, \\ie task descriptions, user\ninterest modeling, candidate items construction and prompting strategies. In\neach section, we first define and categorize concepts in line with the existing\nliterature. Then, we propose inspiring research questions followed by detailed\nexperiments on two public datasets, in order to systematically analyze the\nimpact of different factors on performance. Based on our empirical analysis, we\nfinally summarize promising directions to shed lights on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models~(LLMs) such as ChatGPT have showcased\nremarkable abilities in solving general tasks, demonstrating the potential for\napplications in recommender systems. To assess how effectively LLMs can be used\nin recommendation tasks, our study primarily focuses on employing LLMs as\nrecommender systems through prompting engineering. We propose a general\nframework for utilizing LLMs in recommendation tasks, focusing on the\ncapabilities of LLMs as recommenders. To conduct our analysis, we formalize the\ninput of LLMs for recommendation into natural language prompts with two key\naspects, and explain how our framework can be generalized to various\nrecommendation scenarios. As for the use of LLMs as recommenders, we analyze\nthe impact of public availability, tuning strategies, model architecture,\nparameter scale, and context length on recommendation results based on the\nclassification of LLMs. As for prompt engineering, we further analyze the\nimpact of four important components of prompts, \\ie task descriptions, user\ninterest modeling, candidate items construction and prompting strategies. In\neach section, we first define and categorize concepts in line with the existing\nliterature. Then, we propose inspiring research questions followed by detailed\nexperiments on two public datasets, in order to systematically analyze the\nimpact of different factors on performance. Based on our empirical analysis, we\nfinally summarize promising directions to shed lights on future research."
                },
                "authors": [
                    {
                        "name": "Lanling Xu"
                    },
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Bingqian Li"
                    },
                    {
                        "name": "Jinpeng Wang"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "52 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.04997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09426v1",
                "updated": "2025-01-16T09:57:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    57,
                    12,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:57:12Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    57,
                    12,
                    3,
                    16,
                    0
                ],
                "title": "AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral\n  Therapy in Psychological Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral\n  Therapy in Psychological Counseling"
                },
                "summary": "Traditional in-person psychological counseling remains primarily niche, often\nchosen by individuals with psychological issues, while online automated\ncounseling offers a potential solution for those hesitant to seek help due to\nfeelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and\nwidely used approach in psychological counseling. The advent of large language\nmodels (LLMs) and agent technology enables automatic CBT diagnosis and\ntreatment. However, current LLM-based CBT systems use agents with a fixed\nstructure, limiting their self-optimization capabilities, or providing hollow,\nunhelpful suggestions due to redundant response patterns. In this work, we\nutilize Quora-like and YiXinLi single-round consultation models to build a\ngeneral agent framework that generates high-quality responses for single-turn\npsychological consultation scenarios. We use a bilingual dataset to evaluate\nthe quality of single-response consultations generated by each framework. Then,\nwe incorporate dynamic routing and supervisory mechanisms inspired by real\npsychological counseling to construct a CBT-oriented autonomous multi-agent\nframework, demonstrating its general applicability. Experimental results\nindicate that AutoCBT can provide higher-quality automated psychological\ncounseling services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional in-person psychological counseling remains primarily niche, often\nchosen by individuals with psychological issues, while online automated\ncounseling offers a potential solution for those hesitant to seek help due to\nfeelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and\nwidely used approach in psychological counseling. The advent of large language\nmodels (LLMs) and agent technology enables automatic CBT diagnosis and\ntreatment. However, current LLM-based CBT systems use agents with a fixed\nstructure, limiting their self-optimization capabilities, or providing hollow,\nunhelpful suggestions due to redundant response patterns. In this work, we\nutilize Quora-like and YiXinLi single-round consultation models to build a\ngeneral agent framework that generates high-quality responses for single-turn\npsychological consultation scenarios. We use a bilingual dataset to evaluate\nthe quality of single-response consultations generated by each framework. Then,\nwe incorporate dynamic routing and supervisory mechanisms inspired by real\npsychological counseling to construct a CBT-oriented autonomous multi-agent\nframework, demonstrating its general applicability. Experimental results\nindicate that AutoCBT can provide higher-quality automated psychological\ncounseling services."
                },
                "authors": [
                    {
                        "name": "Ancheng Xu"
                    },
                    {
                        "name": "Di Yang"
                    },
                    {
                        "name": "Renhao Li"
                    },
                    {
                        "name": "Jingwei Zhu"
                    },
                    {
                        "name": "Minghuan Tan"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Wanxin Qiu"
                    },
                    {
                        "name": "Mingchen Ma"
                    },
                    {
                        "name": "Haihong Wu"
                    },
                    {
                        "name": "Bingyu Li"
                    },
                    {
                        "name": "Feng Sha"
                    },
                    {
                        "name": "Chengming Li"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Qiang Qu"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09412v1",
                "updated": "2025-01-16T09:38:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    38,
                    39,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:38:39Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    38,
                    39,
                    3,
                    16,
                    0
                ],
                "title": "FASP: Fast and Accurate Structured Pruning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FASP: Fast and Accurate Structured Pruning of Large Language Models"
                },
                "summary": "The rapid increase in the size of large language models (LLMs) has\nsignificantly escalated their computational and memory demands, posing\nchallenges for efficient deployment, especially on resource-constrained\ndevices. Structured pruning has emerged as an effective model compression\nmethod that can reduce these demands while preserving performance. In this\npaper, we introduce FASP (Fast and Accurate Structured Pruning), a novel\nstructured pruning framework for LLMs that emphasizes both speed and accuracy.\nFASP employs a distinctive pruning structure that interlinks sequential layers,\nallowing for the removal of columns in one layer while simultaneously\neliminating corresponding rows in the preceding layer without incurring\nadditional performance loss. The pruning metric, inspired by Wanda, is\ncomputationally efficient and effectively selects components to prune.\nAdditionally, we propose a restoration mechanism that enhances model fidelity\nby adjusting the remaining weights post-pruning. We evaluate FASP on the OPT\nand LLaMA model families, demonstrating superior performance in terms of\nperplexity and accuracy on downstream tasks compared to state-of-the-art\nmethods. Our approach achieves significant speed-ups, pruning models such as\nOPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090\nGPU, making it a highly practical solution for optimizing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in the size of large language models (LLMs) has\nsignificantly escalated their computational and memory demands, posing\nchallenges for efficient deployment, especially on resource-constrained\ndevices. Structured pruning has emerged as an effective model compression\nmethod that can reduce these demands while preserving performance. In this\npaper, we introduce FASP (Fast and Accurate Structured Pruning), a novel\nstructured pruning framework for LLMs that emphasizes both speed and accuracy.\nFASP employs a distinctive pruning structure that interlinks sequential layers,\nallowing for the removal of columns in one layer while simultaneously\neliminating corresponding rows in the preceding layer without incurring\nadditional performance loss. The pruning metric, inspired by Wanda, is\ncomputationally efficient and effectively selects components to prune.\nAdditionally, we propose a restoration mechanism that enhances model fidelity\nby adjusting the remaining weights post-pruning. We evaluate FASP on the OPT\nand LLaMA model families, demonstrating superior performance in terms of\nperplexity and accuracy on downstream tasks compared to state-of-the-art\nmethods. Our approach achieves significant speed-ups, pruning models such as\nOPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090\nGPU, making it a highly practical solution for optimizing LLMs."
                },
                "authors": [
                    {
                        "name": "Hanyu Hu"
                    },
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaoming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Yuan"
                },
                "author": "Xiaoming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09410v1",
                "updated": "2025-01-16T09:36:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    36,
                    32,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:36:32Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    36,
                    32,
                    3,
                    16,
                    0
                ],
                "title": "MoE$^2$: Optimizing Collaborative Inference for Edge Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE$^2$: Optimizing Collaborative Inference for Edge Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. Exploiting the heterogeneous\ncapabilities of edge LLMs is crucial for diverse emerging applications, as it\nenables greater cost-effectiveness and reduced latency. In this work, we\nintroduce \\textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative\ninference framework for edge LLMs. We formulate the joint gating and expert\nselection problem to optimize inference performance under energy and latency\nconstraints. Unlike conventional MoE problems, LLM expert selection is\nsignificantly more challenging due to the combinatorial nature and the\nheterogeneity of edge LLMs across various attributes. To this end, we propose a\ntwo-level expert selection mechanism through which we uncover an\noptimality-preserving property of gating parameters across expert selections.\nThis property enables the decomposition of the training and selection\nprocesses, significantly reducing complexity. Furthermore, we leverage the\nobjective's monotonicity and design a discrete monotonic optimization algorithm\nfor optimal expert selection. We implement edge servers with NVIDIA Jetson AGX\nOrins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results\nvalidate that performance improvements of various LLM models and show that our\nMoE$^2$ method can achieve optimal trade-offs among different delay and energy\nbudgets, and outperforms baselines under various system resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. Exploiting the heterogeneous\ncapabilities of edge LLMs is crucial for diverse emerging applications, as it\nenables greater cost-effectiveness and reduced latency. In this work, we\nintroduce \\textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative\ninference framework for edge LLMs. We formulate the joint gating and expert\nselection problem to optimize inference performance under energy and latency\nconstraints. Unlike conventional MoE problems, LLM expert selection is\nsignificantly more challenging due to the combinatorial nature and the\nheterogeneity of edge LLMs across various attributes. To this end, we propose a\ntwo-level expert selection mechanism through which we uncover an\noptimality-preserving property of gating parameters across expert selections.\nThis property enables the decomposition of the training and selection\nprocesses, significantly reducing complexity. Furthermore, we leverage the\nobjective's monotonicity and design a discrete monotonic optimization algorithm\nfor optimal expert selection. We implement edge servers with NVIDIA Jetson AGX\nOrins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results\nvalidate that performance improvements of various LLM models and show that our\nMoE$^2$ method can achieve optimal trade-offs among different delay and energy\nbudgets, and outperforms baselines under various system resource constraints."
                },
                "authors": [
                    {
                        "name": "Lyudong Jin"
                    },
                    {
                        "name": "Yanning Zhang"
                    },
                    {
                        "name": "Yanhan Li"
                    },
                    {
                        "name": "Shurong Wang"
                    },
                    {
                        "name": "Howard H. Yang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Meng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Zhang"
                },
                "author": "Meng Zhang",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Networking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04635v2",
                "updated": "2025-01-16T09:30:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    30,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-08T17:29:46Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    29,
                    46,
                    2,
                    8,
                    0
                ],
                "title": "Knowledge Retrieval Based on Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Retrieval Based on Generative AI"
                },
                "summary": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI. The system's\neffectiveness is assessed through a two-stage evaluation: automatic and\nassisted performance evaluations. The automatic evaluation calculates accuracy\nby comparing the model's auto-generated labels with ground truth answers,\nmeasuring performance under standardized conditions without human intervention.\nThe assisted performance evaluation involves 20 finance-related multiple-choice\nquestions answered by 20 participants without financial backgrounds. Initially,\nparticipants answer independently. Later, they receive system-generated\nreference information to assist in answering, examining whether the system\nimproves accuracy when assistance is provided. The main contributions of this\nresearch are: (1) Enhanced LLM Capability: By integrating BGE-M3 and\nBGE-reranker, the system retrieves and reorders highly relevant results,\nreduces hallucinations, and dynamically accesses authorized or public knowledge\nsources. (2) Improved Data Privacy: A customized RAG architecture enables local\noperation of the LLM, eliminating the need to send private data to external\nservers. This approach enhances data security, reduces reliance on commercial\nservices, lowers operational costs, and mitigates privacy risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI. The system's\neffectiveness is assessed through a two-stage evaluation: automatic and\nassisted performance evaluations. The automatic evaluation calculates accuracy\nby comparing the model's auto-generated labels with ground truth answers,\nmeasuring performance under standardized conditions without human intervention.\nThe assisted performance evaluation involves 20 finance-related multiple-choice\nquestions answered by 20 participants without financial backgrounds. Initially,\nparticipants answer independently. Later, they receive system-generated\nreference information to assist in answering, examining whether the system\nimproves accuracy when assistance is provided. The main contributions of this\nresearch are: (1) Enhanced LLM Capability: By integrating BGE-M3 and\nBGE-reranker, the system retrieves and reorders highly relevant results,\nreduces hallucinations, and dynamically accesses authorized or public knowledge\nsources. (2) Improved Data Privacy: A customized RAG architecture enables local\noperation of the LLM, eliminating the need to send private data to external\nservers. This approach enhances data security, reduces reliance on commercial\nservices, lowers operational costs, and mitigates privacy risks."
                },
                "authors": [
                    {
                        "name": "Te-Lun Yang"
                    },
                    {
                        "name": "Jyi-Shane Liu"
                    },
                    {
                        "name": "Yuen-Hsien Tseng"
                    },
                    {
                        "name": "Jyh-Shing Roger Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jyh-Shing Roger Jang"
                },
                "author": "Jyh-Shing Roger Jang",
                "arxiv_comment": "8 pages, 13 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09400v1",
                "updated": "2025-01-16T09:12:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    12,
                    56,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:12:56Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    12,
                    56,
                    3,
                    16,
                    0
                ],
                "title": "Joint Antenna Selection and Beamforming Design for Active RIS-aided ISAC\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Antenna Selection and Beamforming Design for Active RIS-aided ISAC\n  Systems"
                },
                "summary": "Active reconfigurable intelligent surface (A-RIS) aided integrated sensing\nand communications (ISAC) system has been considered as a promising paradigm to\nimprove spectrum efficiency. However, massive energy-hungry radio frequency\n(RF) chains hinder its large-scale deployment. To address this issue, an\nA-RIS-aided ISAC system with antenna selection (AS) is proposed in this work,\nwhere a target is sensed while multiple communication users are served with\nspecifically selected antennas. Specifically, a cuckoo search-based scheme is\nfirst utilized to select the antennas associated with high-gain channels.\nSubsequently, with the properly selected antennas, the weighted sum-rate (WSR)\nof the system is optimized under the condition of radar probing power level,\npower budget for the A-RIS and transmitter. To solve the highly non-convex\noptimization problem, we develop an efficient algorithm based on weighted\nminimum mean square error (WMMSE) and fractional programming (FP). Simulation\nresults show that the proposed AS scheme and the algorithm are effective, which\nreduce the number of RF chains without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active reconfigurable intelligent surface (A-RIS) aided integrated sensing\nand communications (ISAC) system has been considered as a promising paradigm to\nimprove spectrum efficiency. However, massive energy-hungry radio frequency\n(RF) chains hinder its large-scale deployment. To address this issue, an\nA-RIS-aided ISAC system with antenna selection (AS) is proposed in this work,\nwhere a target is sensed while multiple communication users are served with\nspecifically selected antennas. Specifically, a cuckoo search-based scheme is\nfirst utilized to select the antennas associated with high-gain channels.\nSubsequently, with the properly selected antennas, the weighted sum-rate (WSR)\nof the system is optimized under the condition of radar probing power level,\npower budget for the A-RIS and transmitter. To solve the highly non-convex\noptimization problem, we develop an efficient algorithm based on weighted\nminimum mean square error (WMMSE) and fractional programming (FP). Simulation\nresults show that the proposed AS scheme and the algorithm are effective, which\nreduce the number of RF chains without significant performance degradation."
                },
                "authors": [
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Peichang Zhang"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Rouyang Guan"
                    },
                    {
                        "name": "Xiao-Peng Li"
                    },
                    {
                        "name": "Lei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Huang"
                },
                "author": "Lei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09394v1",
                "updated": "2025-01-16T09:06:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    6,
                    10,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T09:06:10Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    9,
                    6,
                    10,
                    3,
                    16,
                    0
                ],
                "title": "Quantum-Enhanced Transformers for Robust Acoustic Scene Classification\n  in IoT Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Enhanced Transformers for Robust Acoustic Scene Classification\n  in IoT Environments"
                },
                "summary": "The proliferation of Internet of Things (IoT) devices equipped with acoustic\nsensors necessitates robust acoustic scene classification (ASC) capabilities,\neven in noisy and data-limited environments. Traditional machine learning\nmethods often struggle to generalize effectively under such conditions. To\naddress this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene\nClassifier that leverages the power of quantum-inspired transformers. By\nintegrating quantum concepts like superposition and entanglement, Q-ASC\nachieves superior feature learning and enhanced noise resilience compared to\nclassical models. Furthermore, we introduce a Quantum Variational Autoencoder\n(QVAE) based data augmentation technique to mitigate the challenge of limited\nlabeled data in IoT deployments. Extensive evaluations on the Tampere\nUniversity of Technology (TUT) Acoustic Scenes 2016 benchmark dataset\ndemonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%\nunder challenging conditions, outperforming state-of-the-art methods by over 5%\nin the best case. This research paves the way for deploying intelligent\nacoustic sensing in IoT networks, with potential applications in smart homes,\nindustrial monitoring, and environmental surveillance, even in adverse acoustic\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Internet of Things (IoT) devices equipped with acoustic\nsensors necessitates robust acoustic scene classification (ASC) capabilities,\neven in noisy and data-limited environments. Traditional machine learning\nmethods often struggle to generalize effectively under such conditions. To\naddress this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene\nClassifier that leverages the power of quantum-inspired transformers. By\nintegrating quantum concepts like superposition and entanglement, Q-ASC\nachieves superior feature learning and enhanced noise resilience compared to\nclassical models. Furthermore, we introduce a Quantum Variational Autoencoder\n(QVAE) based data augmentation technique to mitigate the challenge of limited\nlabeled data in IoT deployments. Extensive evaluations on the Tampere\nUniversity of Technology (TUT) Acoustic Scenes 2016 benchmark dataset\ndemonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%\nunder challenging conditions, outperforming state-of-the-art methods by over 5%\nin the best case. This research paves the way for deploying intelligent\nacoustic sensing in IoT networks, with potential applications in smart homes,\nindustrial monitoring, and environmental surveillance, even in adverse acoustic\nenvironments."
                },
                "authors": [
                    {
                        "name": "Minh K. Quan"
                    },
                    {
                        "name": "Mayuri Wijayasundara"
                    },
                    {
                        "name": "Sujeeva Setunge"
                    },
                    {
                        "name": "Pubudu N. Pathirana"
                    }
                ],
                "author_detail": {
                    "name": "Pubudu N. Pathirana"
                },
                "author": "Pubudu N. Pathirana",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09384v1",
                "updated": "2025-01-16T08:52:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    50,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:50Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    50,
                    3,
                    16,
                    0
                ],
                "title": "Evaluating LLM Abilities to Understand Tabular Electronic Health\n  Records: A Comprehensive Study of Patient Data Extraction and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Abilities to Understand Tabular Electronic Health\n  Records: A Comprehensive Study of Patient Data Extraction and Retrieval"
                },
                "summary": "Electronic Health Record (EHR) tables pose unique challenges among which is\nthe presence of hidden contextual dependencies between medical features with a\nhigh level of data dimensionality and sparsity. This study presents the first\ninvestigation into the abilities of LLMs to comprehend EHRs for patient data\nextraction and retrieval. We conduct extensive experiments using the MIMICSQL\ndataset to explore the impact of the prompt structure, instruction, context,\nand demonstration, of two backbone LLMs, Llama2 and Meditron, based on task\nperformance. Through quantitative and qualitative analyses, our findings show\nthat optimal feature selection and serialization methods can enhance task\nperformance by up to 26.79% compared to naive approaches. Similarly, in-context\nlearning setups with relevant example selection improve data extraction\nperformance by 5.95%. Based on our study findings, we propose guidelines that\nwe believe would help the design of LLM-based models to support health search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Record (EHR) tables pose unique challenges among which is\nthe presence of hidden contextual dependencies between medical features with a\nhigh level of data dimensionality and sparsity. This study presents the first\ninvestigation into the abilities of LLMs to comprehend EHRs for patient data\nextraction and retrieval. We conduct extensive experiments using the MIMICSQL\ndataset to explore the impact of the prompt structure, instruction, context,\nand demonstration, of two backbone LLMs, Llama2 and Meditron, based on task\nperformance. Through quantitative and qualitative analyses, our findings show\nthat optimal feature selection and serialization methods can enhance task\nperformance by up to 26.79% compared to naive approaches. Similarly, in-context\nlearning setups with relevant example selection improve data extraction\nperformance by 5.95%. Based on our study findings, we propose guidelines that\nwe believe would help the design of LLM-based models to support health search."
                },
                "authors": [
                    {
                        "name": "Jesus Lovon"
                    },
                    {
                        "name": "Martin Mouysset"
                    },
                    {
                        "name": "Jo Oleiwan"
                    },
                    {
                        "name": "Jose G. Moreno"
                    },
                    {
                        "name": "Christine Damase-Michel"
                    },
                    {
                        "name": "Lynda Tamine"
                    }
                ],
                "author_detail": {
                    "name": "Lynda Tamine"
                },
                "arxiv_affiliation": "IRIT-IRIS",
                "author": "Lynda Tamine",
                "arxiv_comment": "To be published as full paper in the Proceedings of the European\n  Conference on Information Retrieval (ECIR) 2025. Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07124v2",
                "updated": "2025-01-16T08:49:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    49,
                    10,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-13T08:26:43Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    26,
                    43,
                    0,
                    13,
                    0
                ],
                "title": "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch"
                },
                "summary": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch."
                },
                "authors": [
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Bowen Tan"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Tianhua Tao"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Suqi Sun"
                    },
                    {
                        "name": "Omkar Pangarkar"
                    },
                    {
                        "name": "Richard Fan"
                    },
                    {
                        "name": "Yi Gu"
                    },
                    {
                        "name": "Victor Miller"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Liping Tang"
                    },
                    {
                        "name": "Nikhil Ranjan"
                    },
                    {
                        "name": "Yonghao Zhuang"
                    },
                    {
                        "name": "Guowei He"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Robin Algayres"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12112v3",
                "updated": "2025-01-16T08:44:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    44,
                    22,
                    3,
                    16,
                    0
                ],
                "published": "2024-08-22T03:54:08Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    54,
                    8,
                    3,
                    235,
                    0
                ],
                "title": "Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards"
                },
                "summary": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches."
                },
                "authors": [
                    {
                        "name": "Shresth Verma"
                    },
                    {
                        "name": "Niclas Boehmer"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Milind Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Milind Tambe"
                },
                "author": "Milind Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13340v2",
                "updated": "2025-01-16T08:34:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    34,
                    36,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-19T08:46:29Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    8,
                    46,
                    29,
                    2,
                    171,
                    0
                ],
                "title": "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond\n  Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond\n  Words"
                },
                "summary": "Speech encompasses a wealth of information, including but not limited to\ncontent, paralinguistic, and environmental information. This comprehensive\nnature of speech significantly impacts communication and is crucial for\nhuman-computer interaction. Chat-Oriented Large Language Models (LLMs), known\nfor their general-purpose assistance capabilities, have evolved to handle\nmulti-modal inputs, including speech. Although these models can be adept at\nrecognizing and analyzing speech, they often fall short of generating\nappropriate responses. We argue that this is due to the lack of principles on\ntask definition and model development, which requires open-source datasets and\nmetrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a\nbenchmark dataset aimed at multidimensional evaluation of spoken dialogue\nunderstanding and generation. SD-Eval focuses on paralinguistic and\nenvironmental information and includes 7,303 utterances, amounting to 8.76\nhours of speech data. The data is aggregated from eight public datasets,\nrepresenting four perspectives: emotion, accent, age, and background sound. To\nassess the SD-Eval benchmark dataset, we implement three different models and\nconstruct a training set following a process similar to that of SD-Eval. The\ntraining set contains 1,052.72 hours of speech data and 724.4k utterances. We\nalso conduct a comprehensive evaluation using objective evaluation methods\n(e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the\ngenerated responses. Models conditioned with paralinguistic and environmental\ninformation outperform their counterparts in both objective and subjective\nmeasures. Moreover, experiments demonstrate that LLM-based metrics show a\nhigher correlation with human evaluation compared to traditional metrics. We\nopen-source SD-Eval at https://github.com/amphionspace/SD-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech encompasses a wealth of information, including but not limited to\ncontent, paralinguistic, and environmental information. This comprehensive\nnature of speech significantly impacts communication and is crucial for\nhuman-computer interaction. Chat-Oriented Large Language Models (LLMs), known\nfor their general-purpose assistance capabilities, have evolved to handle\nmulti-modal inputs, including speech. Although these models can be adept at\nrecognizing and analyzing speech, they often fall short of generating\nappropriate responses. We argue that this is due to the lack of principles on\ntask definition and model development, which requires open-source datasets and\nmetrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a\nbenchmark dataset aimed at multidimensional evaluation of spoken dialogue\nunderstanding and generation. SD-Eval focuses on paralinguistic and\nenvironmental information and includes 7,303 utterances, amounting to 8.76\nhours of speech data. The data is aggregated from eight public datasets,\nrepresenting four perspectives: emotion, accent, age, and background sound. To\nassess the SD-Eval benchmark dataset, we implement three different models and\nconstruct a training set following a process similar to that of SD-Eval. The\ntraining set contains 1,052.72 hours of speech data and 724.4k utterances. We\nalso conduct a comprehensive evaluation using objective evaluation methods\n(e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the\ngenerated responses. Models conditioned with paralinguistic and environmental\ninformation outperform their counterparts in both objective and subjective\nmeasures. Moreover, experiments demonstrate that LLM-based metrics show a\nhigher correlation with human evaluation compared to traditional metrics. We\nopen-source SD-Eval at https://github.com/amphionspace/SD-Eval."
                },
                "authors": [
                    {
                        "name": "Junyi Ao"
                    },
                    {
                        "name": "Yuancheng Wang"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Dekun Chen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Zhizheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhizheng Wu"
                },
                "author": "Zhizheng Wu",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09368v1",
                "updated": "2025-01-16T08:27:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    27,
                    40,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:27:40Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    27,
                    40,
                    3,
                    16,
                    0
                ],
                "title": "Aligning Instruction Tuning with Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Instruction Tuning with Pre-training"
                },
                "summary": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose *Aligning Instruction Tuning\nwith Pre-training* (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose *Aligning Instruction Tuning\nwith Pre-training* (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09367v1",
                "updated": "2025-01-16T08:25:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    25,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:25:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    25,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PICE: A Semantic-Driven Progressive Inference System for LLM Serving in\n  Cloud-Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PICE: A Semantic-Driven Progressive Inference System for LLM Serving in\n  Cloud-Edge Networks"
                },
                "summary": "Large language models (LLMs), while driving a new wave of interactive AI\napplications across numerous domains, suffer from high inference costs and\nheavy cloud dependency. Motivated by the redundancy phenomenon in linguistics,\nwe propose a progressive inference paradigm over cloud and edge, i.e., firstly\ngenerating the sketch of the answer by LLMs at cloud, and then conducting\nparallel extension to fill in details by small models (SLMs) at edge.\nProgressive inference offers potential benefits to improve throughput and\nreduce inference latency while facing key implementation challenges, including\ndecreased response quality from SLMs, a tradeoff between the brevity and\ncomprehensiveness of sketches, as well as increased latency caused by network\ntransmission and edge inference. In this work, we propose and implement PICE,\nan LLM serving system with semantic-level cloud-edge collaboration, enhancing\ninference throughput and quality through dynamic inference task scheduling,\nensemble learning, and parallel edge inference. Extensive testbed experiments\nillustrate that our approach achieves $1.5-2\\times$ throughput enhancement and\nup to 43% latency reduction, while also potentially enhancing the quality\ncompared to SOTA systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), while driving a new wave of interactive AI\napplications across numerous domains, suffer from high inference costs and\nheavy cloud dependency. Motivated by the redundancy phenomenon in linguistics,\nwe propose a progressive inference paradigm over cloud and edge, i.e., firstly\ngenerating the sketch of the answer by LLMs at cloud, and then conducting\nparallel extension to fill in details by small models (SLMs) at edge.\nProgressive inference offers potential benefits to improve throughput and\nreduce inference latency while facing key implementation challenges, including\ndecreased response quality from SLMs, a tradeoff between the brevity and\ncomprehensiveness of sketches, as well as increased latency caused by network\ntransmission and edge inference. In this work, we propose and implement PICE,\nan LLM serving system with semantic-level cloud-edge collaboration, enhancing\ninference throughput and quality through dynamic inference task scheduling,\nensemble learning, and parallel edge inference. Extensive testbed experiments\nillustrate that our approach achieves $1.5-2\\times$ throughput enhancement and\nup to 43% latency reduction, while also potentially enhancing the quality\ncompared to SOTA systems."
                },
                "authors": [
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Haisheng Tan"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Dongping Yong"
                    },
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Xiang-Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang-Yang Li"
                },
                "author": "Xiang-Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21035v2",
                "updated": "2025-01-16T08:08:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    8,
                    57,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-17T08:19:11Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    8,
                    19,
                    11,
                    2,
                    199,
                    0
                ],
                "title": "Direct Unlearning Optimization for Robust and Safe Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Unlearning Optimization for Robust and Safe Text-to-Image Models"
                },
                "summary": "Recent advancements in text-to-image (T2I) models have unlocked a wide range\nof applications but also present significant risks, particularly in their\npotential to generate unsafe content. To mitigate this issue, researchers have\ndeveloped unlearning techniques to remove the model's ability to generate\npotentially harmful content. However, these methods are easily bypassed by\nadversarial attacks, making them unreliable for ensuring the safety of\ngenerated images. In this paper, we propose Direct Unlearning Optimization\n(DUO), a novel framework for removing Not Safe For Work (NSFW) content from T2I\nmodels while preserving their performance on unrelated topics. DUO employs a\npreference optimization approach using curated paired image data, ensuring that\nthe model learns to remove unsafe visual concepts while retaining unrelated\nfeatures. Furthermore, we introduce an output-preserving regularization term to\nmaintain the model's generative capabilities on safe content. Extensive\nexperiments demonstrate that DUO can robustly defend against various\nstate-of-the-art red teaming methods without significant performance\ndegradation on unrelated topics, as measured by FID and CLIP scores. Our work\ncontributes to the development of safer and more reliable T2I models, paving\nthe way for their responsible deployment in both closed-source and open-source\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in text-to-image (T2I) models have unlocked a wide range\nof applications but also present significant risks, particularly in their\npotential to generate unsafe content. To mitigate this issue, researchers have\ndeveloped unlearning techniques to remove the model's ability to generate\npotentially harmful content. However, these methods are easily bypassed by\nadversarial attacks, making them unreliable for ensuring the safety of\ngenerated images. In this paper, we propose Direct Unlearning Optimization\n(DUO), a novel framework for removing Not Safe For Work (NSFW) content from T2I\nmodels while preserving their performance on unrelated topics. DUO employs a\npreference optimization approach using curated paired image data, ensuring that\nthe model learns to remove unsafe visual concepts while retaining unrelated\nfeatures. Furthermore, we introduce an output-preserving regularization term to\nmaintain the model's generative capabilities on safe content. Extensive\nexperiments demonstrate that DUO can robustly defend against various\nstate-of-the-art red teaming methods without significant performance\ndegradation on unrelated topics, as measured by FID and CLIP scores. Our work\ncontributes to the development of safer and more reliable T2I models, paving\nthe way for their responsible deployment in both closed-source and open-source\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yong-Hyun Park"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Jin-Hwa Kim"
                    },
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Geonhui Jang"
                    },
                    {
                        "name": "Yonghyun Jeong"
                    },
                    {
                        "name": "Junghyo Jo"
                    },
                    {
                        "name": "Gayoung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gayoung Lee"
                },
                "author": "Gayoung Lee",
                "arxiv_comment": "This paper has been accepted for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09355v1",
                "updated": "2025-01-16T08:06:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    6,
                    2,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:06:02Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    6,
                    2,
                    3,
                    16,
                    0
                ],
                "title": "YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents\n  in Augmented Reality Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents\n  in Augmented Reality Tasks"
                },
                "summary": "Multimodal AI Agents are AI models that have the capability of interactively\nand cooperatively assisting human users to solve day-to-day tasks. Augmented\nReality (AR) head worn devices can uniquely improve the user experience of\nsolving procedural day-to-day tasks by providing egocentric multimodal (audio\nand video) observational capabilities to AI Agents. Such AR capabilities can\nhelp AI Agents see and listen to actions that users take which can relate to\nmultimodal capabilities of human users. Existing AI Agents, either Large\nLanguage Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive\nin nature, which means that models cannot take an action without reading or\nlistening to the human user's prompts. Proactivity of AI Agents on the other\nhand can help the human user detect and correct any mistakes in agent observed\ntasks, encourage users when they do tasks correctly or simply engage in\nconversation with the user - akin to a human teaching or assisting a user. Our\nproposed YET to Intervene (YETI) multimodal agent focuses on the research\nquestion of identifying circumstances that may require the agent to intervene\nproactively. This allows the agent to understand when it can intervene in a\nconversation with human users that can help the user correct mistakes on tasks,\nlike cooking, using AR. Our YETI Agent learns scene understanding signals based\non interpretable notions of Structural Similarity (SSIM) on consecutive video\nframes. We also define the alignment signal which the AI Agent can learn to\nidentify if the video frames corresponding to the user's actions on the task\nare consistent with expected actions. These signals are used by our AI Agent to\ndetermine when it should proactively intervene. We compare our results on the\ninstances of proactive intervention in the HoloAssist multimodal benchmark for\nan expert agent guiding a user to complete procedural tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal AI Agents are AI models that have the capability of interactively\nand cooperatively assisting human users to solve day-to-day tasks. Augmented\nReality (AR) head worn devices can uniquely improve the user experience of\nsolving procedural day-to-day tasks by providing egocentric multimodal (audio\nand video) observational capabilities to AI Agents. Such AR capabilities can\nhelp AI Agents see and listen to actions that users take which can relate to\nmultimodal capabilities of human users. Existing AI Agents, either Large\nLanguage Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive\nin nature, which means that models cannot take an action without reading or\nlistening to the human user's prompts. Proactivity of AI Agents on the other\nhand can help the human user detect and correct any mistakes in agent observed\ntasks, encourage users when they do tasks correctly or simply engage in\nconversation with the user - akin to a human teaching or assisting a user. Our\nproposed YET to Intervene (YETI) multimodal agent focuses on the research\nquestion of identifying circumstances that may require the agent to intervene\nproactively. This allows the agent to understand when it can intervene in a\nconversation with human users that can help the user correct mistakes on tasks,\nlike cooking, using AR. Our YETI Agent learns scene understanding signals based\non interpretable notions of Structural Similarity (SSIM) on consecutive video\nframes. We also define the alignment signal which the AI Agent can learn to\nidentify if the video frames corresponding to the user's actions on the task\nare consistent with expected actions. These signals are used by our AI Agent to\ndetermine when it should proactively intervene. We compare our results on the\ninstances of proactive intervention in the HoloAssist multimodal benchmark for\nan expert agent guiding a user to complete procedural tasks."
                },
                "authors": [
                    {
                        "name": "Saptarashmi Bandyopadhyay"
                    },
                    {
                        "name": "Vikas Bahirwani"
                    },
                    {
                        "name": "Lavisha Aggarwal"
                    },
                    {
                        "name": "Bhanu Guda"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Andrea Colaco"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Colaco"
                },
                "author": "Andrea Colaco",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.10; I.2.11; I.2.1; I.2.7; I.4.8; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09351v1",
                "updated": "2025-01-16T08:04:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    4,
                    2,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:04:02Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    4,
                    2,
                    3,
                    16,
                    0
                ],
                "title": "RIS-Aided Fluid Antenna Array-Mounted UAV Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Aided Fluid Antenna Array-Mounted UAV Networks"
                },
                "summary": "This paper investigates reconfigurable intelligent surface (RIS)-assisted\nunmanned aerial vehicle (UAV) downlink networks with fluid antennas (FA), where\nRIS enables non-line-of-sight (NLoS) transmissions. Moreover, the FA is\nequipped on the UAV offering dynamic antenna position adjustment, enhancing\nspatial diversity besides UAV deployment. We aim at total downlink rate\nmaximization while ensuring minimum user rate requirement. We consider joint\noptimization of active UAV beamforming, passive RIS beamforming, UAV deployment\nand FA position adjustment. To address the complex problem, we propose\nbeamfomring for RIS/UAV and FA-UAV deployment (BRAUD) scheme by employing\nalternative optimization, successive convex approximation (SCA) and sequential\nrank-one constraint relaxation (SROCR) method for the decomposed subproblems.\nSimulation results demonstrate the effectiveness of RIS-FA-UAV, achieving the\nhighest rate among existing architectures without FA/UAV/RIS deployment and\nwithout proper beamforming. Moreover, BRAUD achieves the highest rate among\nbenchmarks of drop-rank method, heuristic optimizations and conventional\nzero-forcing beamforming as well as random method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates reconfigurable intelligent surface (RIS)-assisted\nunmanned aerial vehicle (UAV) downlink networks with fluid antennas (FA), where\nRIS enables non-line-of-sight (NLoS) transmissions. Moreover, the FA is\nequipped on the UAV offering dynamic antenna position adjustment, enhancing\nspatial diversity besides UAV deployment. We aim at total downlink rate\nmaximization while ensuring minimum user rate requirement. We consider joint\noptimization of active UAV beamforming, passive RIS beamforming, UAV deployment\nand FA position adjustment. To address the complex problem, we propose\nbeamfomring for RIS/UAV and FA-UAV deployment (BRAUD) scheme by employing\nalternative optimization, successive convex approximation (SCA) and sequential\nrank-one constraint relaxation (SROCR) method for the decomposed subproblems.\nSimulation results demonstrate the effectiveness of RIS-FA-UAV, achieving the\nhighest rate among existing architectures without FA/UAV/RIS deployment and\nwithout proper beamforming. Moreover, BRAUD achieves the highest rate among\nbenchmarks of drop-rank method, heuristic optimizations and conventional\nzero-forcing beamforming as well as random method."
                },
                "authors": [
                    {
                        "name": "Li-Hsiang Shen"
                    },
                    {
                        "name": "Yi-Hsuan Chiu"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Hsuan Chiu"
                },
                "author": "Yi-Hsuan Chiu",
                "arxiv_comment": "accepted by IEEE WCL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09345v1",
                "updated": "2025-01-16T07:58:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    58,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T07:58:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    58,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "Rational Tuning of LLM Cascades via Probabilistic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rational Tuning of LLM Cascades via Probabilistic Modeling"
                },
                "summary": "Understanding the reliability of large language models (LLMs) has recently\ngarnered significant attention. Given LLMs' propensity to hallucinate, as well\nas their high sensitivity to prompt design, it is already challenging to\npredict the performance of an individual LLM. However, the problem becomes more\ncomplex for compound LLM systems such as cascades, where in addition to each\nmodel's standalone performance, we must understand how the error rates of\ndifferent models interact. In this paper, we present a probabilistic model for\nthe joint performance distribution of a sequence of LLMs, which enables a\nframework for rationally tuning the confidence thresholds of a LLM cascade\nusing continuous optimization. Compared to selecting confidence thresholds\nusing grid search, our parametric Markov-copula model significantly improves\nruntime scaling with respect to the length of the cascade and the desired\nresolution of the cost-error curve, turning them from intractable into\nlow-order polynomial. In addition, the optimal thresholds computed using our\ncontinuous optimization-based algorithm increasingly outperform those found via\ngrid search as cascade length grows, improving the area under the cost-error\ncurve by 1.9% on average for cascades consisting of at least three models.\nOverall, our Markov-copula model provides a rational basis for tuning LLM\ncascade performance and points to the potential of probabilistic methods in\nanalyzing LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the reliability of large language models (LLMs) has recently\ngarnered significant attention. Given LLMs' propensity to hallucinate, as well\nas their high sensitivity to prompt design, it is already challenging to\npredict the performance of an individual LLM. However, the problem becomes more\ncomplex for compound LLM systems such as cascades, where in addition to each\nmodel's standalone performance, we must understand how the error rates of\ndifferent models interact. In this paper, we present a probabilistic model for\nthe joint performance distribution of a sequence of LLMs, which enables a\nframework for rationally tuning the confidence thresholds of a LLM cascade\nusing continuous optimization. Compared to selecting confidence thresholds\nusing grid search, our parametric Markov-copula model significantly improves\nruntime scaling with respect to the length of the cascade and the desired\nresolution of the cost-error curve, turning them from intractable into\nlow-order polynomial. In addition, the optimal thresholds computed using our\ncontinuous optimization-based algorithm increasingly outperform those found via\ngrid search as cascade length grows, improving the area under the cost-error\ncurve by 1.9% on average for cascades consisting of at least three models.\nOverall, our Markov-copula model provides a rational basis for tuning LLM\ncascade performance and points to the potential of probabilistic methods in\nanalyzing LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael J. Zellinger"
                    },
                    {
                        "name": "Matt Thomson"
                    }
                ],
                "author_detail": {
                    "name": "Matt Thomson"
                },
                "author": "Matt Thomson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09164v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09164v4",
                "updated": "2025-01-16T07:50:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    50,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-12T10:59:32Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    10,
                    59,
                    32,
                    4,
                    194,
                    0
                ],
                "title": "TPIA: Towards Target-specific Prompt Injection Attack against\n  Code-oriented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPIA: Towards Target-specific Prompt Injection Attack against\n  Code-oriented Large Language Models"
                },
                "summary": "Recently, code-oriented large language models (Code LLMs) have been widely\nexploited to simplify and facilitate programming. With these tools, developers\ncan easily generate the desired complete functional code based on incomplete\ncode snippets and natural language prompts. Unfortunately, a few pioneering\nworks revealed that these Code LLMs are vulnerable to backdoor and adversarial\nattacks. The former poisons the training data or model parameters, hijacking\nthe LLMs to generate malicious code snippets when encountering the trigger. The\nlatter crafts malicious adversarial input codes to reduce the quality of the\ngenerated codes. However, both attacks have some inherent limitations: backdoor\nattacks rely on the adversary's capability of controlling the model training\nprocess; adversarial attacks struggle with fulfilling specific malicious\npurposes. This paper presents a novel attack paradigm against Code LLMs, namely\ntarget-specific prompt injection attack (TPIA). TPIA generates non-functional\nperturbations containing the information of malicious instructions and inserts\nthem into the victim's code context by spreading them into potentially used\ndependencies (e.g., packages or RAG's knowledge base). It induces the Code LLMs\nto generate attacker-specified malicious code snippets at the target location.\nIn general, we compress the attacker-specified malicious objective into the\nperturbation by adversarial optimization based on greedy token search. We\ncollect 13 representative malicious objectives to design 31 threat cases for\nthree popular programming languages. We show that our TPIA can successfully\nattack three representative open-source Code LLMs (with an ASR of up to 97.9%)\nand two mainstream commercial Code LLM-integrated applications (with an ASR of\nover 90%) in all threat cases, using only a 12-token perturbation. Our work\nalerts a new practical threat of using Code LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, code-oriented large language models (Code LLMs) have been widely\nexploited to simplify and facilitate programming. With these tools, developers\ncan easily generate the desired complete functional code based on incomplete\ncode snippets and natural language prompts. Unfortunately, a few pioneering\nworks revealed that these Code LLMs are vulnerable to backdoor and adversarial\nattacks. The former poisons the training data or model parameters, hijacking\nthe LLMs to generate malicious code snippets when encountering the trigger. The\nlatter crafts malicious adversarial input codes to reduce the quality of the\ngenerated codes. However, both attacks have some inherent limitations: backdoor\nattacks rely on the adversary's capability of controlling the model training\nprocess; adversarial attacks struggle with fulfilling specific malicious\npurposes. This paper presents a novel attack paradigm against Code LLMs, namely\ntarget-specific prompt injection attack (TPIA). TPIA generates non-functional\nperturbations containing the information of malicious instructions and inserts\nthem into the victim's code context by spreading them into potentially used\ndependencies (e.g., packages or RAG's knowledge base). It induces the Code LLMs\nto generate attacker-specified malicious code snippets at the target location.\nIn general, we compress the attacker-specified malicious objective into the\nperturbation by adversarial optimization based on greedy token search. We\ncollect 13 representative malicious objectives to design 31 threat cases for\nthree popular programming languages. We show that our TPIA can successfully\nattack three representative open-source Code LLMs (with an ASR of up to 97.9%)\nand two mainstream commercial Code LLM-integrated applications (with an ASR of\nover 90%) in all threat cases, using only a 12-token perturbation. Our work\nalerts a new practical threat of using Code LLMs."
                },
                "authors": [
                    {
                        "name": "Yuchen Yang"
                    },
                    {
                        "name": "Hongwei Yao"
                    },
                    {
                        "name": "Bingrun Yang"
                    },
                    {
                        "name": "Yiling He"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    },
                    {
                        "name": "Chun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chun Chen"
                },
                "author": "Chun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09164v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09164v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03337v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03337v4",
                "updated": "2025-01-16T07:40:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    7,
                    40,
                    27,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-22T07:19:12Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    19,
                    12,
                    0,
                    204,
                    0
                ],
                "title": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements"
                },
                "summary": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI"
                },
                "authors": [
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Yazhe Niu"
                    },
                    {
                        "name": "Shuai Hu"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "29 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03337v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03337v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09316v1",
                "updated": "2025-01-16T06:14:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    14,
                    58,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T06:14:58Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    14,
                    58,
                    3,
                    16,
                    0
                ],
                "title": "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs"
                },
                "summary": "Despite significant advancements in general-purpose AI agents, several\nchallenges still hinder their practical application in real-world scenarios.\nFirst, the limited planning capabilities of Large Language Models (LLM)\nrestrict AI agents from effectively solving complex tasks that require\nlong-horizon planning. Second, general-purpose AI agents struggle to\nefficiently utilize domain-specific knowledge and human expertise. In this\npaper, we introduce the Standard Operational Procedure-guided Agent\n(SOP-agent), a novel framework for constructing domain-specific agents through\npseudocode-style Standard Operational Procedures (SOPs) written in natural\nlanguage. Formally, we represent a SOP as a decision graph, which is traversed\nto guide the agent in completing tasks specified by the SOP. We conduct\nextensive experiments across tasks in multiple domains, including\ndecision-making, search and reasoning, code generation, data cleaning, and\ngrounded customer service. The SOP-agent demonstrates excellent versatility,\nachieving performance superior to general-purpose agent frameworks and\ncomparable to domain-specific agent systems. Additionally, we introduce the\nGrounded Customer Service Benchmark, the first benchmark designed to evaluate\nthe grounded decision-making capabilities of AI agents in customer service\nscenarios based on SOPs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in general-purpose AI agents, several\nchallenges still hinder their practical application in real-world scenarios.\nFirst, the limited planning capabilities of Large Language Models (LLM)\nrestrict AI agents from effectively solving complex tasks that require\nlong-horizon planning. Second, general-purpose AI agents struggle to\nefficiently utilize domain-specific knowledge and human expertise. In this\npaper, we introduce the Standard Operational Procedure-guided Agent\n(SOP-agent), a novel framework for constructing domain-specific agents through\npseudocode-style Standard Operational Procedures (SOPs) written in natural\nlanguage. Formally, we represent a SOP as a decision graph, which is traversed\nto guide the agent in completing tasks specified by the SOP. We conduct\nextensive experiments across tasks in multiple domains, including\ndecision-making, search and reasoning, code generation, data cleaning, and\ngrounded customer service. The SOP-agent demonstrates excellent versatility,\nachieving performance superior to general-purpose agent frameworks and\ncomparable to domain-specific agent systems. Additionally, we introduce the\nGrounded Customer Service Benchmark, the first benchmark designed to evaluate\nthe grounded decision-making capabilities of AI agents in customer service\nscenarios based on SOPs."
                },
                "authors": [
                    {
                        "name": "Anbang Ye"
                    },
                    {
                        "name": "Qianran Ma"
                    },
                    {
                        "name": "Jia Chen"
                    },
                    {
                        "name": "Muqi Li"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Fujiao Liu"
                    },
                    {
                        "name": "Siqi Mai"
                    },
                    {
                        "name": "Meichen Lu"
                    },
                    {
                        "name": "Haitao Bao"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "arxiv_comment": "35 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15862v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15862v4",
                "updated": "2025-01-16T06:07:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    7,
                    12,
                    3,
                    16,
                    0
                ],
                "published": "2024-11-24T14:38:59Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    14,
                    38,
                    59,
                    6,
                    329,
                    0
                ],
                "title": "Do LLMs Really Think Step-by-step In Implicit Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Really Think Step-by-step In Implicit Reasoning?"
                },
                "summary": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yijiong Yu"
                },
                "author": "Yijiong Yu",
                "arxiv_comment": "The code is in\n  https://github.com/yuyijiong/if_step_by_step_implicit_CoT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15862v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15862v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09310v1",
                "updated": "2025-01-16T05:54:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    54,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T05:54:59Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    54,
                    59,
                    3,
                    16,
                    0
                ],
                "title": "A Study of In-Context-Learning-Based Text-to-SQL Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of In-Context-Learning-Based Text-to-SQL Errors"
                },
                "summary": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead."
                },
                "authors": [
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Ruoyi Qiao"
                    },
                    {
                        "name": "Jiazhen Zou"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Yuchen Shao"
                    },
                    {
                        "name": "Yueling Zhang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    }
                ],
                "author_detail": {
                    "name": "Geguang Pu"
                },
                "author": "Geguang Pu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09307v1",
                "updated": "2025-01-16T05:40:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    40,
                    37,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T05:40:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    40,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "RoboReflect: Robotic Reflective Reasoning for Grasping\n  Ambiguous-Condition Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboReflect: Robotic Reflective Reasoning for Grasping\n  Ambiguous-Condition Objects"
                },
                "summary": "As robotic technology rapidly develops, robots are being employed in an\nincreasing number of fields. However, due to the complexity of deployment\nenvironments or the prevalence of ambiguous-condition objects, the practical\napplication of robotics still faces many challenges, leading to frequent\nerrors. Traditional methods and some LLM-based approaches, although improved,\nstill require substantial human intervention and struggle with autonomous error\ncorrection in complex scenarios.In this work, we propose RoboReflect, a novel\nframework leveraging large vision-language models (LVLMs) to enable\nself-reflection and autonomous error correction in robotic grasping tasks.\nRoboReflect allows robots to automatically adjust their strategies based on\nunsuccessful attempts until successful execution is achieved.The corrected\nstrategies are saved in a memory for future task reference.We evaluate\nRoboReflect through extensive testing on eight common objects prone to\nambiguous conditions of three categories.Our results demonstrate that\nRoboReflect not only outperforms existing grasp pose estimation methods like\nAnyGrasp and high-level action planning techniques using GPT-4V but also\nsignificantly enhances the robot's ability to adapt and correct errors\nindependently. These findings underscore the critical importance of autonomous\nselfreflection in robotic systems while effectively addressing the challenges\nposed by ambiguous environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robotic technology rapidly develops, robots are being employed in an\nincreasing number of fields. However, due to the complexity of deployment\nenvironments or the prevalence of ambiguous-condition objects, the practical\napplication of robotics still faces many challenges, leading to frequent\nerrors. Traditional methods and some LLM-based approaches, although improved,\nstill require substantial human intervention and struggle with autonomous error\ncorrection in complex scenarios.In this work, we propose RoboReflect, a novel\nframework leveraging large vision-language models (LVLMs) to enable\nself-reflection and autonomous error correction in robotic grasping tasks.\nRoboReflect allows robots to automatically adjust their strategies based on\nunsuccessful attempts until successful execution is achieved.The corrected\nstrategies are saved in a memory for future task reference.We evaluate\nRoboReflect through extensive testing on eight common objects prone to\nambiguous conditions of three categories.Our results demonstrate that\nRoboReflect not only outperforms existing grasp pose estimation methods like\nAnyGrasp and high-level action planning techniques using GPT-4V but also\nsignificantly enhances the robot's ability to adapt and correct errors\nindependently. These findings underscore the critical importance of autonomous\nselfreflection in robotic systems while effectively addressing the challenges\nposed by ambiguous environments."
                },
                "authors": [
                    {
                        "name": "Zhen Luo"
                    },
                    {
                        "name": "Yixuan Yang"
                    },
                    {
                        "name": "Chang Cai"
                    },
                    {
                        "name": "Yanfu Zhang"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09292v1",
                "updated": "2025-01-16T04:56:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    56,
                    33,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:56:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    56,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy."
                },
                "authors": [
                    {
                        "name": "Kaustubh D. Dhole"
                    }
                ],
                "author_detail": {
                    "name": "Kaustubh D. Dhole"
                },
                "author": "Kaustubh D. Dhole",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09291v1",
                "updated": "2025-01-16T04:53:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    53,
                    29,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:53:29Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    53,
                    29,
                    3,
                    16,
                    0
                ],
                "title": "LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport"
                },
                "summary": "Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap."
                },
                "authors": [
                    {
                        "name": "Kyeongha Rho"
                    },
                    {
                        "name": "Hyeongkeun Lee"
                    },
                    {
                        "name": "Valentio Iverson"
                    },
                    {
                        "name": "Joon Son Chung"
                    }
                ],
                "author_detail": {
                    "name": "Joon Son Chung"
                },
                "author": "Joon Son Chung",
                "arxiv_comment": "5 pages, 2 figures; Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v3",
                "updated": "2025-01-16T04:08:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    8,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 11,632\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 11,632\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09274v1",
                "updated": "2025-01-16T03:44:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    44,
                    16,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T03:44:16Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    44,
                    16,
                    3,
                    16,
                    0
                ],
                "title": "Large Language Model is Secretly a Protein Sequence Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model is Secretly a Protein Sequence Optimizer"
                },
                "summary": "We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes."
                },
                "authors": [
                    {
                        "name": "Yinkai Wang"
                    },
                    {
                        "name": "Jiaxing He"
                    },
                    {
                        "name": "Yuanqi Du"
                    },
                    {
                        "name": "Xiaohui Chen"
                    },
                    {
                        "name": "Jianan Canal Li"
                    },
                    {
                        "name": "Li-Ping Liu"
                    },
                    {
                        "name": "Xiaolin Xu"
                    },
                    {
                        "name": "Soha Hassoun"
                    }
                ],
                "author_detail": {
                    "name": "Soha Hassoun"
                },
                "author": "Soha Hassoun",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09267v1",
                "updated": "2025-01-16T03:34:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    34,
                    36,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T03:34:36Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    34,
                    36,
                    3,
                    16,
                    0
                ],
                "title": "Are Open-Vocabulary Models Ready for Detection of MEP Elements on\n  Construction Sites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Open-Vocabulary Models Ready for Detection of MEP Elements on\n  Construction Sites"
                },
                "summary": "The construction industry has long explored robotics and computer vision, yet\ntheir deployment on construction sites remains very limited. These technologies\nhave the potential to revolutionize traditional workflows by enhancing\naccuracy, efficiency, and safety in construction management. Ground robots\nequipped with advanced vision systems could automate tasks such as monitoring\nmechanical, electrical, and plumbing (MEP) systems. The present research\nevaluates the applicability of open-vocabulary vision-language models compared\nto fine-tuned, lightweight, closed-set object detectors for detecting MEP\ncomponents using a mobile ground robotic platform. A dataset collected with\ncameras mounted on a ground robot was manually annotated and analyzed to\ncompare model performance. The results demonstrate that, despite the\nversatility of vision-language models, fine-tuned lightweight models still\nlargely outperform them in specialized environments and for domain-specific\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The construction industry has long explored robotics and computer vision, yet\ntheir deployment on construction sites remains very limited. These technologies\nhave the potential to revolutionize traditional workflows by enhancing\naccuracy, efficiency, and safety in construction management. Ground robots\nequipped with advanced vision systems could automate tasks such as monitoring\nmechanical, electrical, and plumbing (MEP) systems. The present research\nevaluates the applicability of open-vocabulary vision-language models compared\nto fine-tuned, lightweight, closed-set object detectors for detecting MEP\ncomponents using a mobile ground robotic platform. A dataset collected with\ncameras mounted on a ground robot was manually annotated and analyzed to\ncompare model performance. The results demonstrate that, despite the\nversatility of vision-language models, fine-tuned lightweight models still\nlargely outperform them in specialized environments and for domain-specific\ntasks."
                },
                "authors": [
                    {
                        "name": "Abdalwhab Abdalwhab"
                    },
                    {
                        "name": "Ali Imran"
                    },
                    {
                        "name": "Sina Heydarian"
                    },
                    {
                        "name": "Ivanka Iordanova"
                    },
                    {
                        "name": "David St-Onge"
                    }
                ],
                "author_detail": {
                    "name": "David St-Onge"
                },
                "author": "David St-Onge",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09265v1",
                "updated": "2025-01-16T03:30:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    30,
                    47,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T03:30:47Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    30,
                    47,
                    3,
                    16,
                    0
                ],
                "title": "Perspective Transition of Large Language Models for Solving Subjective\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perspective Transition of Large Language Models for Solving Subjective\n  Tasks"
                },
                "summary": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems."
                },
                "authors": [
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Yuanchi Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Fuwen Luo"
                    },
                    {
                        "name": "Yile Wang"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15477v2",
                "updated": "2025-01-16T03:26:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    26,
                    36,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-16T23:01:10Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    23,
                    1,
                    10,
                    6,
                    168,
                    0
                ],
                "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for\n  Multi-label Social Media Text Classification in Disaster Informatics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for\n  Multi-label Social Media Text Classification in Disaster Informatics"
                },
                "summary": "In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios."
                },
                "authors": [
                    {
                        "name": "Kai Yin"
                    },
                    {
                        "name": "Chengkai Liu"
                    },
                    {
                        "name": "Ali Mostafavi"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_comment": "Relevant source code and data is available:\n  https://github.com/KaiYin97/CrsisLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13599v2",
                "updated": "2025-01-16T03:17:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    17,
                    25,
                    3,
                    16,
                    0
                ],
                "published": "2024-11-19T07:45:58Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    7,
                    45,
                    58,
                    1,
                    324,
                    0
                ],
                "title": "Can ChatGPT Overcome Behavioral Biases in the Financial Sector?\n  Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can ChatGPT Overcome Behavioral Biases in the Financial Sector?\n  Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success recently,\ndisplaying exceptional capabilities in creating understandable and organized\ntext. These LLMs have been utilized in diverse fields, such as clinical\nresearch, where domain-specific models like Med-Palm have achieved human-level\nperformance. Recently, researchers have employed advanced prompt engineering to\nenhance the general reasoning ability of LLMs. Despite the remarkable success\nof zero-shot Chain-of-Thoughts (CoT) in solving general reasoning tasks, the\npotential of these methods still remains paid limited attention in the\nfinancial reasoning task.To address this issue, we explore multiple prompt\nstrategies and incorporated semantic news information to improve LLMs'\nperformance on financial reasoning tasks.To the best of our knowledge, we are\nthe first to explore this important issue by applying ChatGPT to the gold\ninvestment.In this work, our aim is to investigate the financial reasoning\ncapabilities of LLMs and their capacity to generate logical and persuasive\ninvestment opinions. We will use ChatGPT, one of the most powerful LLMs\nrecently, and prompt engineering to achieve this goal. Our research will focus\non understanding the ability of LLMs in sophisticated analysis and reasoning\nwithin the context of investment decision-making. Our study finds that ChatGPT\nwith CoT prompt can provide more explainable predictions and overcome\nbehavioral biases, which is crucial in finance-related tasks and can achieve\nhigher investment returns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success recently,\ndisplaying exceptional capabilities in creating understandable and organized\ntext. These LLMs have been utilized in diverse fields, such as clinical\nresearch, where domain-specific models like Med-Palm have achieved human-level\nperformance. Recently, researchers have employed advanced prompt engineering to\nenhance the general reasoning ability of LLMs. Despite the remarkable success\nof zero-shot Chain-of-Thoughts (CoT) in solving general reasoning tasks, the\npotential of these methods still remains paid limited attention in the\nfinancial reasoning task.To address this issue, we explore multiple prompt\nstrategies and incorporated semantic news information to improve LLMs'\nperformance on financial reasoning tasks.To the best of our knowledge, we are\nthe first to explore this important issue by applying ChatGPT to the gold\ninvestment.In this work, our aim is to investigate the financial reasoning\ncapabilities of LLMs and their capacity to generate logical and persuasive\ninvestment opinions. We will use ChatGPT, one of the most powerful LLMs\nrecently, and prompt engineering to achieve this goal. Our research will focus\non understanding the ability of LLMs in sophisticated analysis and reasoning\nwithin the context of investment decision-making. Our study finds that ChatGPT\nwith CoT prompt can provide more explainable predictions and overcome\nbehavioral biases, which is crucial in finance-related tasks and can achieve\nhigher investment returns."
                },
                "authors": [
                    {
                        "name": "Shuoling Liu"
                    },
                    {
                        "name": "Gaoguo Jia"
                    },
                    {
                        "name": "Yuhang Jiang"
                    },
                    {
                        "name": "Liyuan Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09258v1",
                "updated": "2025-01-16T03:01:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    1,
                    50,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T03:01:50Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    3,
                    1,
                    50,
                    3,
                    16,
                    0
                ],
                "title": "Delayed Fusion: Integrating Large Language Models into First-Pass\n  Decoding in End-to-end Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delayed Fusion: Integrating Large Language Models into First-Pass\n  Decoding in End-to-end Speech Recognition"
                },
                "summary": "This paper presents an efficient decoding approach for end-to-end automatic\nspeech recognition (E2E-ASR) with large language models (LLMs). Although\nshallow fusion is the most common approach to incorporate language models into\nE2E-ASR decoding, we face two practical problems with LLMs. (1) LLM inference\nis computationally costly. (2) There may be a vocabulary mismatch between the\nASR model and the LLM. To resolve this mismatch, we need to retrain the ASR\nmodel and/or the LLM, which is at best time-consuming and in many cases not\nfeasible. We propose \"delayed fusion,\" which applies LLM scores to ASR\nhypotheses with a delay during decoding and enables easier use of pre-trained\nLLMs in ASR tasks. This method can reduce not only the number of hypotheses\nscored by the LLM but also the number of LLM inference calls. It also allows\nre-tokenizion of ASR hypotheses during decoding if ASR and LLM employ different\ntokenizations. We demonstrate that delayed fusion provides improved decoding\nspeed and accuracy compared to shallow fusion and N-best rescoring using the\nLibriHeavy ASR corpus and three public LLMs, OpenLLaMA 3B & 7B and Mistral 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an efficient decoding approach for end-to-end automatic\nspeech recognition (E2E-ASR) with large language models (LLMs). Although\nshallow fusion is the most common approach to incorporate language models into\nE2E-ASR decoding, we face two practical problems with LLMs. (1) LLM inference\nis computationally costly. (2) There may be a vocabulary mismatch between the\nASR model and the LLM. To resolve this mismatch, we need to retrain the ASR\nmodel and/or the LLM, which is at best time-consuming and in many cases not\nfeasible. We propose \"delayed fusion,\" which applies LLM scores to ASR\nhypotheses with a delay during decoding and enables easier use of pre-trained\nLLMs in ASR tasks. This method can reduce not only the number of hypotheses\nscored by the LLM but also the number of LLM inference calls. It also allows\nre-tokenizion of ASR hypotheses during decoding if ASR and LLM employ different\ntokenizations. We demonstrate that delayed fusion provides improved decoding\nspeed and accuracy compared to shallow fusion and N-best rescoring using the\nLibriHeavy ASR corpus and three public LLMs, OpenLLaMA 3B & 7B and Mistral 7B."
                },
                "authors": [
                    {
                        "name": "Takaaki Hori"
                    },
                    {
                        "name": "Martin Kocour"
                    },
                    {
                        "name": "Adnan Haider"
                    },
                    {
                        "name": "Erik McDermott"
                    },
                    {
                        "name": "Xiaodan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhuang"
                },
                "author": "Xiaodan Zhuang",
                "arxiv_comment": "Accepted to ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13885v2",
                "updated": "2025-01-16T02:45:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    45,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2024-04-22T05:12:52Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    5,
                    12,
                    52,
                    0,
                    113,
                    0
                ],
                "title": "Surveying Attitudinal Alignment Between Large Language Models Vs. Humans\n  Towards 17 Sustainable Development Goals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveying Attitudinal Alignment Between Large Language Models Vs. Humans\n  Towards 17 Sustainable Development Goals"
                },
                "summary": "Large Language Models (LLMs) have emerged as potent tools for advancing the\nUnited Nations' Sustainable Development Goals (SDGs). However, the attitudinal\ndisparities between LLMs and humans towards these goals can pose significant\nchallenges. This study conducts a comprehensive review and analysis of the\nexisting literature on the attitudes of LLMs towards the 17 SDGs, emphasizing\nthe comparison between their attitudes and support for each goal and those of\nhumans. We examine the potential disparities, primarily focusing on aspects\nsuch as understanding and emotions, cultural and regional differences, task\nobjective variations, and factors considered in the decision-making process.\nThese disparities arise from the underrepresentation and imbalance in LLM\ntraining data, historical biases, quality issues, lack of contextual\nunderstanding, and skewed ethical values reflected. The study also investigates\nthe risks and harms that may arise from neglecting the attitudes of LLMs\ntowards the SDGs, including the exacerbation of social inequalities, racial\ndiscrimination, environmental destruction, and resource wastage. To address\nthese challenges, we propose strategies and recommendations to guide and\nregulate the application of LLMs, ensuring their alignment with the principles\nand goals of the SDGs, and therefore creating a more just, inclusive, and\nsustainable future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as potent tools for advancing the\nUnited Nations' Sustainable Development Goals (SDGs). However, the attitudinal\ndisparities between LLMs and humans towards these goals can pose significant\nchallenges. This study conducts a comprehensive review and analysis of the\nexisting literature on the attitudes of LLMs towards the 17 SDGs, emphasizing\nthe comparison between their attitudes and support for each goal and those of\nhumans. We examine the potential disparities, primarily focusing on aspects\nsuch as understanding and emotions, cultural and regional differences, task\nobjective variations, and factors considered in the decision-making process.\nThese disparities arise from the underrepresentation and imbalance in LLM\ntraining data, historical biases, quality issues, lack of contextual\nunderstanding, and skewed ethical values reflected. The study also investigates\nthe risks and harms that may arise from neglecting the attitudes of LLMs\ntowards the SDGs, including the exacerbation of social inequalities, racial\ndiscrimination, environmental destruction, and resource wastage. To address\nthese challenges, we propose strategies and recommendations to guide and\nregulate the application of LLMs, ensuring their alignment with the principles\nand goals of the SDGs, and therefore creating a more just, inclusive, and\nsustainable future."
                },
                "authors": [
                    {
                        "name": "Qingyang Wu"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Tingsong Xiao"
                    },
                    {
                        "name": "Yunze Xiao"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Shanghai Zhong"
                    },
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Yifan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Yang"
                },
                "author": "Yifan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09254v1",
                "updated": "2025-01-16T02:43:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    43,
                    44,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:43:44Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    43,
                    44,
                    3,
                    16,
                    0
                ],
                "title": "Clone-Robust AI Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clone-Robust AI Alignment"
                },
                "summary": "A key challenge in training Large Language Models (LLMs) is properly aligning\nthem with human preferences. Reinforcement Learning with Human Feedback (RLHF)\nuses pairwise comparisons from human annotators to train reward functions and\nhas emerged as a popular alignment method. However, input datasets in RLHF are\nnot necessarily balanced in the types of questions and answers that are\nincluded. Therefore, we want RLHF algorithms to perform well even when the set\nof alternatives is not uniformly distributed. Drawing on insights from social\nchoice theory, we introduce robustness to approximate clones, a desirable\nproperty of RLHF algorithms which requires that adding near-duplicate\nalternatives does not significantly change the learned reward function. We\nfirst demonstrate that the standard RLHF algorithm based on regularized maximum\nlikelihood estimation (MLE) fails to satisfy this property. We then propose the\nweighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\nby weighting alternatives based on their similarity to other alternatives. This\nnew algorithm guarantees robustness to approximate clones while preserving\ndesirable theoretical properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in training Large Language Models (LLMs) is properly aligning\nthem with human preferences. Reinforcement Learning with Human Feedback (RLHF)\nuses pairwise comparisons from human annotators to train reward functions and\nhas emerged as a popular alignment method. However, input datasets in RLHF are\nnot necessarily balanced in the types of questions and answers that are\nincluded. Therefore, we want RLHF algorithms to perform well even when the set\nof alternatives is not uniformly distributed. Drawing on insights from social\nchoice theory, we introduce robustness to approximate clones, a desirable\nproperty of RLHF algorithms which requires that adding near-duplicate\nalternatives does not significantly change the learned reward function. We\nfirst demonstrate that the standard RLHF algorithm based on regularized maximum\nlikelihood estimation (MLE) fails to satisfy this property. We then propose the\nweighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\nby weighting alternatives based on their similarity to other alternatives. This\nnew algorithm guarantees robustness to approximate clones while preserving\ndesirable theoretical properties."
                },
                "authors": [
                    {
                        "name": "Ariel D. Procaccia"
                    },
                    {
                        "name": "Benjamin Schiffer"
                    },
                    {
                        "name": "Shirley Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shirley Zhang"
                },
                "author": "Shirley Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00824v2",
                "updated": "2025-01-16T02:38:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    38,
                    55,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-01T13:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    0,
                    1,
                    2,
                    1,
                    0
                ],
                "title": "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks"
                },
                "summary": "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality."
                },
                "authors": [
                    {
                        "name": "Rongke Liu"
                    }
                ],
                "author_detail": {
                    "name": "Rongke Liu"
                },
                "author": "Rongke Liu",
                "arxiv_comment": "15 pages, 4 figures, 7 tables. The experiment is still being\n  supplemented. V2 has improvements to writing and typesetting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02914v2",
                "updated": "2025-01-16T02:10:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    10,
                    39,
                    3,
                    16,
                    0
                ],
                "published": "2024-03-05T12:31:24Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    12,
                    31,
                    24,
                    1,
                    65,
                    0
                ],
                "title": "DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal\n  Forecasting"
                },
                "summary": "The ever-increasing sensor service, though opening a precious path and\nproviding a deluge of earth system data for deep-learning-oriented earth\nscience, sadly introduce a daunting obstacle to their industrial level\ndeployment. Concretely, earth science systems rely heavily on the extensive\ndeployment of sensors, however, the data collection from sensors is constrained\nby complex geographical and social factors, making it challenging to achieve\ncomprehensive coverage and uniform deployment. To alleviate the obstacle,\ntraditional approaches to sensor deployment utilize specific algorithms to\ndesign and deploy sensors. These methods \\textit{dynamically adjust the\nactivation times of sensors to optimize the detection process across each\nsub-region}. Regrettably, formulating an activation strategy generally based on\nhistorical observations and geographic characteristics, which make the methods\nand resultant models were neither simple nor practical. Worse still, the\ncomplex technical design may ultimately lead to a model with weak\ngeneralizability. In this paper, we introduce for the first time the concept of\nspatio-temporal data dynamic sparse training and are committed to adaptively,\ndynamically filtering important sensor distributions. To our knowledge, this is\nthe \\textbf{first} proposal (\\textit{termed DynST}) of an\n\\textbf{industry-level} deployment optimization concept at the data level.\nHowever, due to the existence of the temporal dimension, pruning of\nspatio-temporal data may lead to conflicts at different timestamps. To achieve\nthis goal, we employ dynamic merge technology, along with ingenious dimensional\nmapping to mitigate potential impacts caused by the temporal aspect. During the\ntraining process, DynST utilize iterative pruning and sparse training,\nrepeatedly identifying and dynamically removing sensor perception areas that\ncontribute the least to future predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing sensor service, though opening a precious path and\nproviding a deluge of earth system data for deep-learning-oriented earth\nscience, sadly introduce a daunting obstacle to their industrial level\ndeployment. Concretely, earth science systems rely heavily on the extensive\ndeployment of sensors, however, the data collection from sensors is constrained\nby complex geographical and social factors, making it challenging to achieve\ncomprehensive coverage and uniform deployment. To alleviate the obstacle,\ntraditional approaches to sensor deployment utilize specific algorithms to\ndesign and deploy sensors. These methods \\textit{dynamically adjust the\nactivation times of sensors to optimize the detection process across each\nsub-region}. Regrettably, formulating an activation strategy generally based on\nhistorical observations and geographic characteristics, which make the methods\nand resultant models were neither simple nor practical. Worse still, the\ncomplex technical design may ultimately lead to a model with weak\ngeneralizability. In this paper, we introduce for the first time the concept of\nspatio-temporal data dynamic sparse training and are committed to adaptively,\ndynamically filtering important sensor distributions. To our knowledge, this is\nthe \\textbf{first} proposal (\\textit{termed DynST}) of an\n\\textbf{industry-level} deployment optimization concept at the data level.\nHowever, due to the existence of the temporal dimension, pruning of\nspatio-temporal data may lead to conflicts at different timestamps. To achieve\nthis goal, we employ dynamic merge technology, along with ingenious dimensional\nmapping to mitigate potential impacts caused by the temporal aspect. During the\ntraining process, DynST utilize iterative pruning and sparse training,\nrepeatedly identifying and dynamically removing sensor perception areas that\ncontribute the least to future predictions."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haomin Wen"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yutong Xia"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Kun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Wang"
                },
                "author": "Kun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09237v1",
                "updated": "2025-01-16T01:44:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    1,
                    44,
                    5,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T01:44:05Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    1,
                    44,
                    5,
                    3,
                    16,
                    0
                ],
                "title": "Split Fine-Tuning for Large Language Models in Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Fine-Tuning for Large Language Models in Wireless Networks"
                },
                "summary": "Fine-tuning is the process of adapting the pre-trained large language models\n(LLMs) for downstream tasks. Due to substantial parameters, fine-tuning LLMs on\nmobile devices demands considerable memory resources, and suffers from high\ncommunication overhead and long fine-tuning delay. In this paper, we propose an\nefficient LLM fine-tuning scheme in wireless networks, named Split Fine-Tuning\n(SFT), which can accommodate LLM fine-tuning on mobile devices. Specifically,\nan LLM is split into a server-side part on the edge server and a device-side\npart on the mobile device to satisfy the device-side memory constraint. All\ndevices share a server-side model and perform parallel fine-tuning to reduce\nfine-tuning delay. In addition, to reduce significant communication overhead\nincurred by data exchange between devices and the edge server, we propose a\ndata compression scheme by jointly leveraging sparsification, stochastic\nquantization, and lossless encoding methods. Furthermore, we formulate a\nfine-tuning delay minimization problem under accuracy and memory constraints,\ntaking device heterogeneity and channel dynamics into account. To solve the\nproblem, the nonlinear mixed-integer problem is decoupled into two subproblems\nin different timescales. The two-timescale resource management algorithm is\nproposed to jointly optimize the compression rate and transformer block\nallocation in the large timescale using the augmented Lagrangian method, and\ndetermine spectrum resource allocation in the small timescale via sequential\nquadratic programming. Extensive simulation results demonstrate that the\nproposed scheme can reduce the fine-tuning delay by up to 80.2% and\ncommunication overhead by 93.6% compared to state-of-the-art benchmarks, while\nsatisfying device-side memory and model accuracy constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is the process of adapting the pre-trained large language models\n(LLMs) for downstream tasks. Due to substantial parameters, fine-tuning LLMs on\nmobile devices demands considerable memory resources, and suffers from high\ncommunication overhead and long fine-tuning delay. In this paper, we propose an\nefficient LLM fine-tuning scheme in wireless networks, named Split Fine-Tuning\n(SFT), which can accommodate LLM fine-tuning on mobile devices. Specifically,\nan LLM is split into a server-side part on the edge server and a device-side\npart on the mobile device to satisfy the device-side memory constraint. All\ndevices share a server-side model and perform parallel fine-tuning to reduce\nfine-tuning delay. In addition, to reduce significant communication overhead\nincurred by data exchange between devices and the edge server, we propose a\ndata compression scheme by jointly leveraging sparsification, stochastic\nquantization, and lossless encoding methods. Furthermore, we formulate a\nfine-tuning delay minimization problem under accuracy and memory constraints,\ntaking device heterogeneity and channel dynamics into account. To solve the\nproblem, the nonlinear mixed-integer problem is decoupled into two subproblems\nin different timescales. The two-timescale resource management algorithm is\nproposed to jointly optimize the compression rate and transformer block\nallocation in the large timescale using the augmented Lagrangian method, and\ndetermine spectrum resource allocation in the small timescale via sequential\nquadratic programming. Extensive simulation results demonstrate that the\nproposed scheme can reduce the fine-tuning delay by up to 80.2% and\ncommunication overhead by 93.6% compared to state-of-the-art benchmarks, while\nsatisfying device-side memory and model accuracy constraints."
                },
                "authors": [
                    {
                        "name": "Songge Zhang"
                    },
                    {
                        "name": "Guoliang Cheng"
                    },
                    {
                        "name": "Xinyu Huang"
                    },
                    {
                        "name": "Zuguang Li"
                    },
                    {
                        "name": "Wen Wu"
                    },
                    {
                        "name": "Lingyang Song"
                    },
                    {
                        "name": "Xuemin Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Shen"
                },
                "author": "Xuemin Shen",
                "arxiv_comment": "14pages, 10figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09948v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09948v2",
                "updated": "2025-01-16T01:41:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    1,
                    41,
                    48,
                    3,
                    16,
                    0
                ],
                "published": "2024-06-14T11:48:54Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    11,
                    48,
                    54,
                    4,
                    166,
                    0
                ],
                "title": "BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures\n  and Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures\n  and Languages"
                },
                "summary": "Large language models (LLMs) often lack culture-specific knowledge of daily\nlife, especially across diverse regions and non-English languages. Existing\nbenchmarks for evaluating LLMs' cultural sensitivities are limited to a single\nlanguage or collected from online sources such as Wikipedia, which do not\nreflect the mundane everyday lifestyles of diverse regions. That is,\ninformation about the food people eat for their birthday celebrations, spices\nthey typically use, musical instruments youngsters play, or the sports they\npractice in school is common cultural knowledge but uncommon in easily\ncollected online sources, especially for underrepresented cultures. To address\nthis issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate\nLLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises\n52.6k question-answer pairs from 16 countries/regions, in 13 different\nlanguages, including low-resource ones such as Amharic, Assamese, Azerbaijani,\nHausa, and Sundanese. We construct the benchmark to include two formats of\nquestions: short-answer and multiple-choice. We show that LLMs perform better\nfor cultures that are highly represented online, with a maximum 57.34%\ndifference in GPT-4, the best-performing model, in the short-answer format. For\ncultures represented by mid-to-high-resource languages, LLMs perform better in\ntheir local languages, but for cultures represented by low-resource languages,\nLLMs perform better in English than the local languages. We make our dataset\npublicly available at: https://github.com/nlee0212/BLEnD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often lack culture-specific knowledge of daily\nlife, especially across diverse regions and non-English languages. Existing\nbenchmarks for evaluating LLMs' cultural sensitivities are limited to a single\nlanguage or collected from online sources such as Wikipedia, which do not\nreflect the mundane everyday lifestyles of diverse regions. That is,\ninformation about the food people eat for their birthday celebrations, spices\nthey typically use, musical instruments youngsters play, or the sports they\npractice in school is common cultural knowledge but uncommon in easily\ncollected online sources, especially for underrepresented cultures. To address\nthis issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate\nLLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises\n52.6k question-answer pairs from 16 countries/regions, in 13 different\nlanguages, including low-resource ones such as Amharic, Assamese, Azerbaijani,\nHausa, and Sundanese. We construct the benchmark to include two formats of\nquestions: short-answer and multiple-choice. We show that LLMs perform better\nfor cultures that are highly represented online, with a maximum 57.34%\ndifference in GPT-4, the best-performing model, in the short-answer format. For\ncultures represented by mid-to-high-resource languages, LLMs perform better in\ntheir local languages, but for cultures represented by low-resource languages,\nLLMs perform better in English than the local languages. We make our dataset\npublicly available at: https://github.com/nlee0212/BLEnD."
                },
                "authors": [
                    {
                        "name": "Junho Myung"
                    },
                    {
                        "name": "Nayeon Lee"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Jiho Jin"
                    },
                    {
                        "name": "Rifki Afina Putri"
                    },
                    {
                        "name": "Dimosthenis Antypas"
                    },
                    {
                        "name": "Hsuvas Borkakoty"
                    },
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Carla Perez-Almendros"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Yazmín Ibáñez-García"
                    },
                    {
                        "name": "Hwaran Lee"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Kiwoong Park"
                    },
                    {
                        "name": "Anar Sabuhi Rzayev"
                    },
                    {
                        "name": "Nina White"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "arxiv_comment": "Accepted to NeurIPS 2024 Datasets & Benchmark Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09948v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09948v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09225v1",
                "updated": "2025-01-16T01:15:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    1,
                    15,
                    21,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T01:15:21Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    1,
                    15,
                    21,
                    3,
                    16,
                    0
                ],
                "title": "Provenance Guided Rollback Suggestions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provenance Guided Rollback Suggestions"
                },
                "summary": "Advances in incremental Datalog evaluation strategies have made Datalog\npopular among use cases with constantly evolving inputs such as static analysis\nin continuous integration and deployment pipelines. As a result, new logic\nprogramming debugging techniques are needed to support these emerging use\ncases.\n  This paper introduces an incremental debugging technique for Datalog, which\ndetermines the failing changes for a \\emph{rollback} in an incremental setup.\nOur debugging technique leverages a novel incremental provenance method. We\nhave implemented our technique using an incremental version of the Souffl\\'{e}\nDatalog engine and evaluated its effectiveness on the DaCapo Java program\nbenchmarks analyzed by the Doop static analysis library. Compared to\nstate-of-the-art techniques, we can localize faults and suggest rollbacks with\nan overall speedup of over 26.9$\\times$ while providing higher quality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in incremental Datalog evaluation strategies have made Datalog\npopular among use cases with constantly evolving inputs such as static analysis\nin continuous integration and deployment pipelines. As a result, new logic\nprogramming debugging techniques are needed to support these emerging use\ncases.\n  This paper introduces an incremental debugging technique for Datalog, which\ndetermines the failing changes for a \\emph{rollback} in an incremental setup.\nOur debugging technique leverages a novel incremental provenance method. We\nhave implemented our technique using an incremental version of the Souffl\\'{e}\nDatalog engine and evaluated its effectiveness on the DaCapo Java program\nbenchmarks analyzed by the Doop static analysis library. Compared to\nstate-of-the-art techniques, we can localize faults and suggest rollbacks with\nan overall speedup of over 26.9$\\times$ while providing higher quality results."
                },
                "authors": [
                    {
                        "name": "David Zhao"
                    },
                    {
                        "name": "Pavle Subotic"
                    },
                    {
                        "name": "Mukund Raghothaman"
                    },
                    {
                        "name": "Bernhard Scholz"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Scholz"
                },
                "author": "Bernhard Scholz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09213v1",
                "updated": "2025-01-16T00:19:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    0,
                    19,
                    19,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T00:19:19Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    0,
                    19,
                    19,
                    3,
                    16,
                    0
                ],
                "title": "FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the advanced reasoning required for\ncomplex clinical scenarios, such as differential diagnosis or personalized\ntreatment suggestions. We proposed FineMedLM-o1, which leverages high-quality\nsynthetic medical data and long-form reasoning data for Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and\ndeep reasoning capabilities. Additionally, we introduced Test-Time Training\n(TTT) in the medical domain for the first time, facilitating domain adaptation\nand ensuring reliable, accurate reasoning. Experimental results demonstrate\nthat FineMedLM-o1 achieves a 23% average performance improvement over prior\nmodels on key medical benchmarks. Furthermore, the introduction of TTT provides\nan additional 14% performance boost, highlighting its effectiveness in\nenhancing medical reasoning capabilities. To support this process, we also\nproposed a novel method for synthesizing medical dialogue. Compared to other\nopen-source datasets, our dataset stands out as superior in both quality and\ncomplexity. The project and data will be released on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the advanced reasoning required for\ncomplex clinical scenarios, such as differential diagnosis or personalized\ntreatment suggestions. We proposed FineMedLM-o1, which leverages high-quality\nsynthetic medical data and long-form reasoning data for Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and\ndeep reasoning capabilities. Additionally, we introduced Test-Time Training\n(TTT) in the medical domain for the first time, facilitating domain adaptation\nand ensuring reliable, accurate reasoning. Experimental results demonstrate\nthat FineMedLM-o1 achieves a 23% average performance improvement over prior\nmodels on key medical benchmarks. Furthermore, the introduction of TTT provides\nan additional 14% performance boost, highlighting its effectiveness in\nenhancing medical reasoning capabilities. To support this process, we also\nproposed a novel method for synthesizing medical dialogue. Compared to other\nopen-source datasets, our dataset stands out as superior in both quality and\ncomplexity. The project and data will be released on GitHub."
                },
                "authors": [
                    {
                        "name": "Hongzhou Yu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Rui Feng"
                    }
                ],
                "author_detail": {
                    "name": "Rui Feng"
                },
                "author": "Rui Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09210v1",
                "updated": "2025-01-16T00:05:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    0,
                    5,
                    20,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T00:05:20Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    0,
                    5,
                    20,
                    3,
                    16,
                    0
                ],
                "title": "Personalized Parsons Puzzles as Scaffolding Enhance Practice Engagement\n  Over Just Showing LLM-Powered Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Parsons Puzzles as Scaffolding Enhance Practice Engagement\n  Over Just Showing LLM-Powered Solutions"
                },
                "summary": "As generative AI products could generate code and assist students with\nprogramming learning seamlessly, integrating AI into programming education\ncontexts has driven much attention. However, one emerging concern is that\nstudents might get answers without learning from the LLM-generated content. In\nthis work, we deployed the LLM-powered personalized Parsons puzzles as\nscaffolding to write-code practice in a Python learning classroom (PC\ncondition) and conducted an 80-minute randomized between-subjects study. Both\nconditions received the same practice problems. The only difference was that\nwhen requesting help, the control condition showed students a complete solution\n(CC condition), simulating the most traditional LLM output. Results indicated\nthat students who received personalized Parsons puzzles as scaffolding engaged\nin practicing significantly longer than those who received complete solutions\nwhen struggling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative AI products could generate code and assist students with\nprogramming learning seamlessly, integrating AI into programming education\ncontexts has driven much attention. However, one emerging concern is that\nstudents might get answers without learning from the LLM-generated content. In\nthis work, we deployed the LLM-powered personalized Parsons puzzles as\nscaffolding to write-code practice in a Python learning classroom (PC\ncondition) and conducted an 80-minute randomized between-subjects study. Both\nconditions received the same practice problems. The only difference was that\nwhen requesting help, the control condition showed students a complete solution\n(CC condition), simulating the most traditional LLM output. Results indicated\nthat students who received personalized Parsons puzzles as scaffolding engaged\nin practicing significantly longer than those who received complete solutions\nwhen struggling."
                },
                "authors": [
                    {
                        "name": "Xinying Hou"
                    },
                    {
                        "name": "Zihan Wu"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Barbara J. Ericson"
                    }
                ],
                "author_detail": {
                    "name": "Barbara J. Ericson"
                },
                "author": "Barbara J. Ericson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09201v1",
                "updated": "2025-01-15T23:24:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    23,
                    24,
                    32,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T23:24:32Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    23,
                    24,
                    32,
                    2,
                    15,
                    0
                ],
                "title": "Towards Semantics Lifting for Scientific Computing: A Case Study on FFT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Semantics Lifting for Scientific Computing: A Case Study on FFT"
                },
                "summary": "The rise of automated code generation tools, such as large language models\n(LLMs), has introduced new challenges in ensuring the correctness and\nefficiency of scientific software, particularly in complex kernels, where\nnumerical stability, domain-specific optimizations, and precise floating-point\narithmetic are critical. We propose a stepwise semantics lifting approach using\nan extended SPIRAL framework with symbolic execution and theorem proving to\nstatically derive high-level code semantics from LLM-generated kernels. This\nmethod establishes a structured path for verifying the source code's\ncorrectness via a step-by-step lifting procedure to high-level specification.\nWe conducted preliminary tests on the feasibility of this approach by\nsuccessfully lifting GPT-generated fast Fourier transform code to high-level\nspecifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of automated code generation tools, such as large language models\n(LLMs), has introduced new challenges in ensuring the correctness and\nefficiency of scientific software, particularly in complex kernels, where\nnumerical stability, domain-specific optimizations, and precise floating-point\narithmetic are critical. We propose a stepwise semantics lifting approach using\nan extended SPIRAL framework with symbolic execution and theorem proving to\nstatically derive high-level code semantics from LLM-generated kernels. This\nmethod establishes a structured path for verifying the source code's\ncorrectness via a step-by-step lifting procedure to high-level specification.\nWe conducted preliminary tests on the feasibility of this approach by\nsuccessfully lifting GPT-generated fast Fourier transform code to high-level\nspecifications."
                },
                "authors": [
                    {
                        "name": "Naifeng Zhang"
                    },
                    {
                        "name": "Sanil Rao"
                    },
                    {
                        "name": "Mike Franusich"
                    },
                    {
                        "name": "Franz Franchetti"
                    }
                ],
                "author_detail": {
                    "name": "Franz Franchetti"
                },
                "author": "Franz Franchetti",
                "arxiv_comment": "Accepted at the Theory and Practice of Static Analysis Workshop\n  (TPSA), in conjunction with the ACM SIGPLAN Symposium on Principles of\n  Programming Languages (POPL), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09186v1",
                "updated": "2025-01-15T22:23:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    22,
                    23,
                    53,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T22:23:53Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    22,
                    23,
                    53,
                    2,
                    15,
                    0
                ],
                "title": "Guiding Retrieval using LLM-based Listwise Rankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Retrieval using LLM-based Listwise Rankers"
                },
                "summary": "Large Language Models (LLMs) have shown strong promise as rerankers,\nespecially in ``listwise'' settings where an LLM is prompted to rerank several\nsearch results at once. However, this ``cascading'' retrieve-and-rerank\napproach is limited by the bounded recall problem: relevant documents not\nretrieved initially are permanently excluded from the final ranking. Adaptive\nretrieval techniques address this problem, but do not work with listwise\nrerankers because they assume a document's score is computed independently from\nother documents. In this paper, we propose an adaptation of an existing\nadaptive retrieval method that supports the listwise setting and helps guide\nthe retrieval process itself (thereby overcoming the bounded recall problem for\nLLM rerankers). Specifically, our proposed algorithm merges results both from\nthe initial ranking and feedback documents provided by the most relevant\ndocuments seen up to that point. Through extensive experiments across diverse\nLLM rerankers, first stage retrievers, and feedback sources, we demonstrate\nthat our method can improve nDCG@10 by up to 13.23% and recall by 28.02%--all\nwhile keeping the total number of LLM inferences constant and overheads due to\nthe adaptive process minimal. The work opens the door to leveraging LLM-based\nsearch in settings where the initial pool of results is limited, e.g., by\nlegacy systems, or by the cost of deploying a semantic first-stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong promise as rerankers,\nespecially in ``listwise'' settings where an LLM is prompted to rerank several\nsearch results at once. However, this ``cascading'' retrieve-and-rerank\napproach is limited by the bounded recall problem: relevant documents not\nretrieved initially are permanently excluded from the final ranking. Adaptive\nretrieval techniques address this problem, but do not work with listwise\nrerankers because they assume a document's score is computed independently from\nother documents. In this paper, we propose an adaptation of an existing\nadaptive retrieval method that supports the listwise setting and helps guide\nthe retrieval process itself (thereby overcoming the bounded recall problem for\nLLM rerankers). Specifically, our proposed algorithm merges results both from\nthe initial ranking and feedback documents provided by the most relevant\ndocuments seen up to that point. Through extensive experiments across diverse\nLLM rerankers, first stage retrievers, and feedback sources, we demonstrate\nthat our method can improve nDCG@10 by up to 13.23% and recall by 28.02%--all\nwhile keeping the total number of LLM inferences constant and overheads due to\nthe adaptive process minimal. The work opens the door to leveraging LLM-based\nsearch in settings where the initial pool of results is limited, e.g., by\nlegacy systems, or by the cost of deploying a semantic first-stage."
                },
                "authors": [
                    {
                        "name": "Mandeep Rathee"
                    },
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "arxiv_comment": "16 pages, 2 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17967v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17967v3",
                "updated": "2025-01-15T22:20:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    22,
                    20,
                    15,
                    2,
                    15,
                    0
                ],
                "published": "2024-06-25T22:49:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    49,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Unmasking the Imposters: How Censorship and Domain Adaptation Affect the\n  Detection of Machine-Generated Tweets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking the Imposters: How Censorship and Domain Adaptation Affect the\n  Detection of Machine-Generated Tweets"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nimproved the generation of fluent and convincing text, raising concerns about\ntheir potential misuse on social media platforms. We present a comprehensive\nmethodology for creating nine Twitter datasets to examine the generative\ncapabilities of four prominent LLMs: Llama 3, Mistral, Qwen2, and GPT4o. These\ndatasets encompass four censored and five uncensored model configurations,\nincluding 7B and 8B parameter base-instruction models of the three open-source\nLLMs. Additionally, we perform a data quality analysis to assess the\ncharacteristics of textual outputs from human, \"censored,\" and \"uncensored\"\nmodels, employing semantic meaning, lexical richness, structural patterns,\ncontent characteristics, and detector performance metrics to identify\ndifferences and similarities. Our evaluation demonstrates that \"uncensored\"\nmodels significantly undermine the effectiveness of automated detection\nmethods. This study addresses a critical gap by exploring smaller open-source\nmodels and the ramifications of \"uncensoring,\" providing valuable insights into\nhow domain adaptation and content moderation strategies influence both the\ndetectability and structural characteristics of machine-generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nimproved the generation of fluent and convincing text, raising concerns about\ntheir potential misuse on social media platforms. We present a comprehensive\nmethodology for creating nine Twitter datasets to examine the generative\ncapabilities of four prominent LLMs: Llama 3, Mistral, Qwen2, and GPT4o. These\ndatasets encompass four censored and five uncensored model configurations,\nincluding 7B and 8B parameter base-instruction models of the three open-source\nLLMs. Additionally, we perform a data quality analysis to assess the\ncharacteristics of textual outputs from human, \"censored,\" and \"uncensored\"\nmodels, employing semantic meaning, lexical richness, structural patterns,\ncontent characteristics, and detector performance metrics to identify\ndifferences and similarities. Our evaluation demonstrates that \"uncensored\"\nmodels significantly undermine the effectiveness of automated detection\nmethods. This study addresses a critical gap by exploring smaller open-source\nmodels and the ramifications of \"uncensoring,\" providing valuable insights into\nhow domain adaptation and content moderation strategies influence both the\ndetectability and structural characteristics of machine-generated text."
                },
                "authors": [
                    {
                        "name": "Bryan E. Tuck"
                    },
                    {
                        "name": "Rakesh M. Verma"
                    }
                ],
                "author_detail": {
                    "name": "Rakesh M. Verma"
                },
                "author": "Rakesh M. Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17967v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17967v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09182v1",
                "updated": "2025-01-15T22:19:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    22,
                    19,
                    34,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T22:19:34Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    22,
                    19,
                    34,
                    2,
                    15,
                    0
                ],
                "title": "A Blockchain-Enabled Approach to Cross-Border Compliance and Trust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Blockchain-Enabled Approach to Cross-Border Compliance and Trust"
                },
                "summary": "As artificial intelligence (AI) systems become increasingly integral to\ncritical infrastructure and global operations, the need for a unified,\ntrustworthy governance framework is more urgent that ever. This paper proposes\na novel approach to AI governance, utilizing blockchain and distributed ledger\ntechnologies (DLT) to establish a decentralized, globally recognized framework\nthat ensures security, privacy, and trustworthiness of AI systems across\nborders. The paper presents specific implementation scenarios within the\nfinancial sector, outlines a phased deployment timeline over the next decade,\nand addresses potential challenges with solutions grounded in current research.\nBy synthesizing advancements in blockchain, AI ethics, and cybersecurity, this\npaper offers a comprehensive roadmap for a decentralized AI governance\nframework capable of adapting to the complex and evolving landscape of global\nAI regulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence (AI) systems become increasingly integral to\ncritical infrastructure and global operations, the need for a unified,\ntrustworthy governance framework is more urgent that ever. This paper proposes\na novel approach to AI governance, utilizing blockchain and distributed ledger\ntechnologies (DLT) to establish a decentralized, globally recognized framework\nthat ensures security, privacy, and trustworthiness of AI systems across\nborders. The paper presents specific implementation scenarios within the\nfinancial sector, outlines a phased deployment timeline over the next decade,\nand addresses potential challenges with solutions grounded in current research.\nBy synthesizing advancements in blockchain, AI ethics, and cybersecurity, this\npaper offers a comprehensive roadmap for a decentralized AI governance\nframework capable of adapting to the complex and evolving landscape of global\nAI regulation."
                },
                "authors": [
                    {
                        "name": "Vikram Kulothungan"
                    }
                ],
                "author_detail": {
                    "name": "Vikram Kulothungan"
                },
                "author": "Vikram Kulothungan",
                "arxiv_comment": "This is a preprint of paper that has been accepted for Publication at\n  2024 IEEE International Conference on Trust, Privacy and Security in\n  Intelligent Systems, and Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09171v1",
                "updated": "2025-01-15T21:46:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    46,
                    1,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T21:46:01Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    46,
                    1,
                    2,
                    15,
                    0
                ],
                "title": "Generative AI Takes a Statistics Exam: A Comparison of Performance\n  between ChatGPT3.5, ChatGPT4, and ChatGPT4o-mini",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI Takes a Statistics Exam: A Comparison of Performance\n  between ChatGPT3.5, ChatGPT4, and ChatGPT4o-mini"
                },
                "summary": "Many believe that use of generative AI as a private tutor has the potential\nto shrink access and achievement gaps between students and schools with\nabundant resources versus those with fewer resources. Shrinking the gap is\npossible only if paid and free versions of the platforms perform with the same\naccuracy. In this experiment, we investigate the performance of GPT versions\n3.5, 4.0, and 4o-mini on the same 16-question statistics exam given to a class\nof first-year graduate students. While we do not advocate using any generative\nAI platform to complete an exam, the use of exam questions allows us to explore\naspects of ChatGPT's responses to typical questions that students might\nencounter in a statistics course. Results on accuracy indicate that GPT 3.5\nwould fail the exam, GPT4 would perform well, and GPT4o-mini would perform\nsomewhere in between. While we acknowledge the existence of other Generative\nAI/LLMs, our discussion concerns only ChatGPT because it is the most widely\nused platform on college campuses at this time. We further investigate\ndifferences among the AI platforms in the answers for each problem using\nmethods developed for text analytics, such as reading level evaluation and\ntopic modeling. Results indicate that GPT3.5 and 4o-mini have characteristics\nthat are more similar than either of them have with GPT4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many believe that use of generative AI as a private tutor has the potential\nto shrink access and achievement gaps between students and schools with\nabundant resources versus those with fewer resources. Shrinking the gap is\npossible only if paid and free versions of the platforms perform with the same\naccuracy. In this experiment, we investigate the performance of GPT versions\n3.5, 4.0, and 4o-mini on the same 16-question statistics exam given to a class\nof first-year graduate students. While we do not advocate using any generative\nAI platform to complete an exam, the use of exam questions allows us to explore\naspects of ChatGPT's responses to typical questions that students might\nencounter in a statistics course. Results on accuracy indicate that GPT 3.5\nwould fail the exam, GPT4 would perform well, and GPT4o-mini would perform\nsomewhere in between. While we acknowledge the existence of other Generative\nAI/LLMs, our discussion concerns only ChatGPT because it is the most widely\nused platform on college campuses at this time. We further investigate\ndifferences among the AI platforms in the answers for each problem using\nmethods developed for text analytics, such as reading level evaluation and\ntopic modeling. Results indicate that GPT3.5 and 4o-mini have characteristics\nthat are more similar than either of them have with GPT4."
                },
                "authors": [
                    {
                        "name": "Monnie McGee"
                    },
                    {
                        "name": "Bivin Sadler"
                    }
                ],
                "author_detail": {
                    "name": "Bivin Sadler"
                },
                "author": "Bivin Sadler",
                "arxiv_comment": "24 pages, 2 figures, 3 tables. Submitted for publication August,\n  2024; revision submitted January 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09165v1",
                "updated": "2025-01-15T21:32:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    32,
                    4,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T21:32:04Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    32,
                    4,
                    2,
                    15,
                    0
                ],
                "title": "Breaking Barriers or Building Dependency? Exploring Team-LLM\n  Collaboration in AI-infused Classroom Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Barriers or Building Dependency? Exploring Team-LLM\n  Collaboration in AI-infused Classroom Debate"
                },
                "summary": "Classroom debates are a unique form of collaborative learning characterized\nby fast-paced, high-intensity interactions that foster critical thinking and\nteamwork. Despite the recognized importance of debates, the role of AI tools,\nparticularly LLM-based systems, in supporting this dynamic learning environment\nhas been under-explored in HCI. This study addresses this opportunity by\ninvestigating the integration of LLM-based AI into real-time classroom debates.\nOver four weeks, 22 students in a Design History course participated in three\nrounds of debates with support from ChatGPT. The findings reveal how learners\nprompted the AI to offer insights, collaboratively processed its outputs, and\ndivided labor in team-AI interactions. The study also surfaces key advantages\nof AI usage, reducing social anxiety, breaking communication barriers, and\nproviding scaffolding for novices, alongside risks, such as information\noverload and cognitive dependency, which could limit learners' autonomy. We\nthereby discuss a set of nuanced implications for future HCI exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classroom debates are a unique form of collaborative learning characterized\nby fast-paced, high-intensity interactions that foster critical thinking and\nteamwork. Despite the recognized importance of debates, the role of AI tools,\nparticularly LLM-based systems, in supporting this dynamic learning environment\nhas been under-explored in HCI. This study addresses this opportunity by\ninvestigating the integration of LLM-based AI into real-time classroom debates.\nOver four weeks, 22 students in a Design History course participated in three\nrounds of debates with support from ChatGPT. The findings reveal how learners\nprompted the AI to offer insights, collaboratively processed its outputs, and\ndivided labor in team-AI interactions. The study also surfaces key advantages\nof AI usage, reducing social anxiety, breaking communication barriers, and\nproviding scaffolding for novices, alongside risks, such as information\noverload and cognitive dependency, which could limit learners' autonomy. We\nthereby discuss a set of nuanced implications for future HCI exploration."
                },
                "authors": [
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Black Sun"
                    },
                    {
                        "name": "Pengcheng An"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng An"
                },
                "author": "Pengcheng An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09164v1",
                "updated": "2025-01-15T21:30:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    30,
                    3,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T21:30:03Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    30,
                    3,
                    2,
                    15,
                    0
                ],
                "title": "The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and\n  Lithuanian Short Answer Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and\n  Lithuanian Short Answer Matching"
                },
                "summary": "In this work, we address the challenge of evaluating large language models\n(LLMs) on the short answer matching task for Latvian and Lithuanian languages.\nWe introduce novel datasets consisting of 502 Latvian and 690 Lithuanian\nquestion-answer pairs. For each question-answer pair, we generated matched and\nnon-matched answers using a set of alteration rules specifically designed to\nintroduce small but meaningful changes in the text. These generated answers\nserve as test cases to assess the ability of LLMs to detect subtle differences\nin matching of the original answers. A subset of the datasets was manually\nverified for quality and accuracy. Our results show that while larger LLMs,\nsuch as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance in\ndistinguishing matched and non-matched answers, smaller models show more\nvariance. For instance, LLaMa3.1 8b and EuroLLM 9b benefited from few-shot\nexamples, while Mistral Nemo 12b underperformed on detection of subtle text\nalteration, particularly in Lithuanian, even with additional examples. QWEN2.5\n7b and Mistral 7b were able to obtain a strong and comparable performance to\nthe larger 70b models in zero and few shot experiments. Moreover, the\nperformance of Mistral 7b was weaker in few shot experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the challenge of evaluating large language models\n(LLMs) on the short answer matching task for Latvian and Lithuanian languages.\nWe introduce novel datasets consisting of 502 Latvian and 690 Lithuanian\nquestion-answer pairs. For each question-answer pair, we generated matched and\nnon-matched answers using a set of alteration rules specifically designed to\nintroduce small but meaningful changes in the text. These generated answers\nserve as test cases to assess the ability of LLMs to detect subtle differences\nin matching of the original answers. A subset of the datasets was manually\nverified for quality and accuracy. Our results show that while larger LLMs,\nsuch as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance in\ndistinguishing matched and non-matched answers, smaller models show more\nvariance. For instance, LLaMa3.1 8b and EuroLLM 9b benefited from few-shot\nexamples, while Mistral Nemo 12b underperformed on detection of subtle text\nalteration, particularly in Lithuanian, even with additional examples. QWEN2.5\n7b and Mistral 7b were able to obtain a strong and comparable performance to\nthe larger 70b models in zero and few shot experiments. Moreover, the\nperformance of Mistral 7b was weaker in few shot experiments."
                },
                "authors": [
                    {
                        "name": "Yevhen Kostiuk"
                    },
                    {
                        "name": "Oxana Vitman"
                    },
                    {
                        "name": "Łukasz Gagała"
                    },
                    {
                        "name": "Artur Kiulian"
                    }
                ],
                "author_detail": {
                    "name": "Artur Kiulian"
                },
                "author": "Artur Kiulian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09158v1",
                "updated": "2025-01-15T21:19:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    19,
                    1,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T21:19:01Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    19,
                    1,
                    2,
                    15,
                    0
                ],
                "title": "Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy\n  and Consistency for Enhanced Readability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy\n  and Consistency for Enhanced Readability"
                },
                "summary": "Generative artificial intelligence (GenAI) holds great promise as a tool to\nsupport personalized learning. Teachers need tools to efficiently and\neffectively enhance content readability of educational texts so that they are\nmatched to individual students reading levels, while retaining key details.\nLarge Language Models (LLMs) show potential to fill this need, but previous\nresearch notes multiple shortcomings in current approaches. In this study, we\nintroduced a generalized approach and metrics for the systematic evaluation of\nthe accuracy and consistency in which LLMs, prompting techniques, and a novel\nmulti-agent architecture to simplify sixty informational reading passages,\nreducing each from the twelfth grade level down to the eighth, sixth, and\nfourth grade levels. We calculated the degree to which each LLM and prompting\ntechnique accurately achieved the targeted grade level for each passage,\npercentage change in word count, and consistency in maintaining keywords and\nkey phrases (semantic similarity). One-sample t-tests and multiple regression\nmodels revealed significant differences in the best performing LLM and prompt\ntechnique for each of the four metrics. Both LLMs and prompting techniques\ndemonstrated variable utility in grade level accuracy and consistency of\nkeywords and key phrases when attempting to level content down to the fourth\ngrade reading level. These results demonstrate the promise of the application\nof LLMs for efficient and precise automated text simplification, the\nshortcomings of current models and prompting methods in attaining an ideal\nbalance across various evaluation criteria, and a generalizable method to\nevaluate future systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (GenAI) holds great promise as a tool to\nsupport personalized learning. Teachers need tools to efficiently and\neffectively enhance content readability of educational texts so that they are\nmatched to individual students reading levels, while retaining key details.\nLarge Language Models (LLMs) show potential to fill this need, but previous\nresearch notes multiple shortcomings in current approaches. In this study, we\nintroduced a generalized approach and metrics for the systematic evaluation of\nthe accuracy and consistency in which LLMs, prompting techniques, and a novel\nmulti-agent architecture to simplify sixty informational reading passages,\nreducing each from the twelfth grade level down to the eighth, sixth, and\nfourth grade levels. We calculated the degree to which each LLM and prompting\ntechnique accurately achieved the targeted grade level for each passage,\npercentage change in word count, and consistency in maintaining keywords and\nkey phrases (semantic similarity). One-sample t-tests and multiple regression\nmodels revealed significant differences in the best performing LLM and prompt\ntechnique for each of the four metrics. Both LLMs and prompting techniques\ndemonstrated variable utility in grade level accuracy and consistency of\nkeywords and key phrases when attempting to level content down to the fourth\ngrade reading level. These results demonstrate the promise of the application\nof LLMs for efficient and precise automated text simplification, the\nshortcomings of current models and prompting methods in attaining an ideal\nbalance across various evaluation criteria, and a generalizable method to\nevaluate future systems."
                },
                "authors": [
                    {
                        "name": "Stephanie L. Day"
                    },
                    {
                        "name": "Jacapo Cirica"
                    },
                    {
                        "name": "Steven R. Clapp"
                    },
                    {
                        "name": "Veronika Penkova"
                    },
                    {
                        "name": "Amy E. Giroux"
                    },
                    {
                        "name": "Abbey Banta"
                    },
                    {
                        "name": "Catherine Bordeau"
                    },
                    {
                        "name": "Poojitha Mutteneni"
                    },
                    {
                        "name": "Ben D. Sawyer"
                    }
                ],
                "author_detail": {
                    "name": "Ben D. Sawyer"
                },
                "author": "Ben D. Sawyer",
                "arxiv_comment": "64 pages, 9 tables, 6 figures, and supplemental materials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09154v1",
                "updated": "2025-01-15T21:14:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    14,
                    9,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T21:14:09Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    14,
                    9,
                    2,
                    15,
                    0
                ],
                "title": "Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A\n  study on Lithuanian History",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A\n  study on Lithuanian History"
                },
                "summary": "In this work, we evaluated Lithuanian and general history knowledge of\nmultilingual Large Language Models (LLMs) on a multiple-choice\nquestion-answering task. The models were tested on a dataset of Lithuanian\nnational and general history questions translated into Baltic, Nordic, and\nother languages (English, Ukrainian, Arabic) to assess the knowledge sharing\nfrom culturally and historically connected groups. We evaluated GPT-4o,\nLLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral\n7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b).\n  Our results show that GPT-4o consistently outperformed all other models\nacross language groups, with slightly better results for Baltic and Nordic\nlanguages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b\nperformed well but showed weaker alignment with Baltic languages. Smaller\nmodels (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b)\ndemonstrated gaps with LT-related alignment with Baltic languages while\nperforming better on Nordic and other languages. The Nordic fine-tuned models\ndid not surpass multilingual models, indicating that shared cultural or\nhistorical context alone does not guarantee better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we evaluated Lithuanian and general history knowledge of\nmultilingual Large Language Models (LLMs) on a multiple-choice\nquestion-answering task. The models were tested on a dataset of Lithuanian\nnational and general history questions translated into Baltic, Nordic, and\nother languages (English, Ukrainian, Arabic) to assess the knowledge sharing\nfrom culturally and historically connected groups. We evaluated GPT-4o,\nLLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral\n7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b).\n  Our results show that GPT-4o consistently outperformed all other models\nacross language groups, with slightly better results for Baltic and Nordic\nlanguages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b\nperformed well but showed weaker alignment with Baltic languages. Smaller\nmodels (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b)\ndemonstrated gaps with LT-related alignment with Baltic languages while\nperforming better on Nordic and other languages. The Nordic fine-tuned models\ndid not surpass multilingual models, indicating that shared cultural or\nhistorical context alone does not guarantee better performance."
                },
                "authors": [
                    {
                        "name": "Yevhen Kostiuk"
                    },
                    {
                        "name": "Oxana Vitman"
                    },
                    {
                        "name": "Łukasz Gagała"
                    },
                    {
                        "name": "Artur Kiulian"
                    }
                ],
                "author_detail": {
                    "name": "Artur Kiulian"
                },
                "author": "Artur Kiulian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05079v2",
                "updated": "2025-01-15T20:46:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    46,
                    44,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-09T09:01:04Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    1,
                    4,
                    3,
                    9,
                    0
                ],
                "title": "Multimodal-to-Text Prompt Engineering in Large Language Models Using\n  Feature Embeddings for GNSS Interference Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal-to-Text Prompt Engineering in Large Language Models Using\n  Feature Embeddings for GNSS Interference Characterization"
                },
                "summary": "Large language models (LLMs) are advanced AI systems applied across various\ndomains, including NLP, information retrieval, and recommendation systems.\nDespite their adaptability and efficiency, LLMs have not been extensively\nexplored for signal processing tasks, particularly in the domain of global\nnavigation satellite system (GNSS) interference monitoring. GNSS interference\nmonitoring is essential to ensure the reliability of vehicle localization on\nroads, a critical requirement for numerous applications. However, GNSS-based\npositioning is vulnerable to interference from jamming devices, which can\ncompromise its accuracy. The primary objective is to identify, classify, and\nmitigate these interferences. Interpreting GNSS snapshots and the associated\ninterferences presents significant challenges due to the inherent complexity,\nincluding multipath effects, diverse interference types, varying sensor\ncharacteristics, and satellite constellations. In this paper, we extract\nfeatures from a large GNSS dataset and employ LLaVA to retrieve relevant\ninformation from an extensive knowledge base. We employ prompt engineering to\ninterpret the interferences and environmental factors, and utilize t-SNE to\nanalyze the feature embeddings. Our findings demonstrate that the proposed\nmethod is capable of visual and logical reasoning within the GNSS context.\nFurthermore, our pipeline outperforms state-of-the-art machine learning models\nin interference classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are advanced AI systems applied across various\ndomains, including NLP, information retrieval, and recommendation systems.\nDespite their adaptability and efficiency, LLMs have not been extensively\nexplored for signal processing tasks, particularly in the domain of global\nnavigation satellite system (GNSS) interference monitoring. GNSS interference\nmonitoring is essential to ensure the reliability of vehicle localization on\nroads, a critical requirement for numerous applications. However, GNSS-based\npositioning is vulnerable to interference from jamming devices, which can\ncompromise its accuracy. The primary objective is to identify, classify, and\nmitigate these interferences. Interpreting GNSS snapshots and the associated\ninterferences presents significant challenges due to the inherent complexity,\nincluding multipath effects, diverse interference types, varying sensor\ncharacteristics, and satellite constellations. In this paper, we extract\nfeatures from a large GNSS dataset and employ LLaVA to retrieve relevant\ninformation from an extensive knowledge base. We employ prompt engineering to\ninterpret the interferences and environmental factors, and utilize t-SNE to\nanalyze the feature embeddings. Our findings demonstrate that the proposed\nmethod is capable of visual and logical reasoning within the GNSS context.\nFurthermore, our pipeline outperforms state-of-the-art machine learning models\nin interference classification tasks."
                },
                "authors": [
                    {
                        "name": "Harshith Manjunath"
                    },
                    {
                        "name": "Lucas Heublein"
                    },
                    {
                        "name": "Tobias Feigl"
                    },
                    {
                        "name": "Felix Ott"
                    }
                ],
                "author_detail": {
                    "name": "Felix Ott"
                },
                "author": "Felix Ott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T30, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1; H.5; I.4.9; I.4.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06497v2",
                "updated": "2025-01-15T20:43:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    43,
                    44,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-11T10:22:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    10,
                    22,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "PASS: Presentation Automation for Slide Generation and Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASS: Presentation Automation for Slide Generation and Speech"
                },
                "summary": "In today's fast-paced world, effective presentations have become an essential\ntool for communication in both online and offline meetings. The crafting of a\ncompelling presentation requires significant time and effort, from gathering\nkey insights to designing slides that convey information clearly and concisely.\nHowever, despite the wealth of resources available, people often find\nthemselves manually extracting crucial points, analyzing data, and organizing\ncontent in a way that ensures clarity and impact. Furthermore, a successful\npresentation goes beyond just the slides; it demands rehearsal and the ability\nto weave a captivating narrative to fully engage the audience. Although there\nhas been some exploration of automating document-to-slide generation, existing\nresearch is largely centered on converting research papers. In addition,\nautomation of the delivery of these presentations has yet to be addressed. We\nintroduce PASS, a pipeline used to generate slides from general Word documents,\ngoing beyond just research papers, which also automates the oral delivery of\nthe generated slides. PASS analyzes user documents to create a dynamic,\nengaging presentation with an AI-generated voice. Additionally, we developed an\nLLM-based evaluation metric to assess our pipeline across three critical\ndimensions of presentations: relevance, coherence, and redundancy. The data and\ncodes are available at https://github.com/AggarwalTushar/PASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's fast-paced world, effective presentations have become an essential\ntool for communication in both online and offline meetings. The crafting of a\ncompelling presentation requires significant time and effort, from gathering\nkey insights to designing slides that convey information clearly and concisely.\nHowever, despite the wealth of resources available, people often find\nthemselves manually extracting crucial points, analyzing data, and organizing\ncontent in a way that ensures clarity and impact. Furthermore, a successful\npresentation goes beyond just the slides; it demands rehearsal and the ability\nto weave a captivating narrative to fully engage the audience. Although there\nhas been some exploration of automating document-to-slide generation, existing\nresearch is largely centered on converting research papers. In addition,\nautomation of the delivery of these presentations has yet to be addressed. We\nintroduce PASS, a pipeline used to generate slides from general Word documents,\ngoing beyond just research papers, which also automates the oral delivery of\nthe generated slides. PASS analyzes user documents to create a dynamic,\nengaging presentation with an AI-generated voice. Additionally, we developed an\nLLM-based evaluation metric to assess our pipeline across three critical\ndimensions of presentations: relevance, coherence, and redundancy. The data and\ncodes are available at https://github.com/AggarwalTushar/PASS."
                },
                "authors": [
                    {
                        "name": "Tushar Aggarwal"
                    },
                    {
                        "name": "Aarohi Bhand"
                    }
                ],
                "author_detail": {
                    "name": "Aarohi Bhand"
                },
                "author": "Aarohi Bhand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09136v1",
                "updated": "2025-01-15T20:40:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    40,
                    25,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:40:25Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    40,
                    25,
                    2,
                    15,
                    0
                ],
                "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\nby enabling human like text generation and natural language understanding.\nHowever, their reliance on static training data limits their ability to respond\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\nby integrating real time data retrieval to provide contextually relevant and\nup-to-date responses. Despite its promise, traditional RAG systems are\nconstrained by static workflows and lack the adaptability required for\nmultistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\nagents leverage agentic design patterns reflection, planning, tool use, and\nmultiagent collaboration to dynamically manage retrieval strategies,\niteratively refine contextual understanding, and adapt workflows to meet\ncomplex task requirements. This integration enables Agentic RAG systems to\ndeliver unparalleled flexibility, scalability, and context awareness across\ndiverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\nwith its foundational principles and the evolution of RAG paradigms. It\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\napplications in industries such as healthcare, finance, and education, and\nexamines practical implementation strategies. Additionally, it addresses\nchallenges in scaling these systems, ensuring ethical decision making, and\noptimizing performance for real-world applications, while providing detailed\ninsights into frameworks and tools for implementing Agentic RAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\nby enabling human like text generation and natural language understanding.\nHowever, their reliance on static training data limits their ability to respond\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\nby integrating real time data retrieval to provide contextually relevant and\nup-to-date responses. Despite its promise, traditional RAG systems are\nconstrained by static workflows and lack the adaptability required for\nmultistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\nagents leverage agentic design patterns reflection, planning, tool use, and\nmultiagent collaboration to dynamically manage retrieval strategies,\niteratively refine contextual understanding, and adapt workflows to meet\ncomplex task requirements. This integration enables Agentic RAG systems to\ndeliver unparalleled flexibility, scalability, and context awareness across\ndiverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\nwith its foundational principles and the evolution of RAG paradigms. It\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\napplications in industries such as healthcare, finance, and education, and\nexamines practical implementation strategies. Additionally, it addresses\nchallenges in scaling these systems, ensuring ethical decision making, and\noptimizing performance for real-world applications, while providing detailed\ninsights into frameworks and tools for implementing Agentic RAG"
                },
                "authors": [
                    {
                        "name": "Aditi Singh"
                    },
                    {
                        "name": "Abul Ehtesham"
                    },
                    {
                        "name": "Saket Kumar"
                    },
                    {
                        "name": "Tala Talaei Khoei"
                    }
                ],
                "author_detail": {
                    "name": "Tala Talaei Khoei"
                },
                "author": "Tala Talaei Khoei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09135v1",
                "updated": "2025-01-15T20:39:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    39,
                    32,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:39:32Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    39,
                    32,
                    2,
                    15,
                    0
                ],
                "title": "HAFix: History-Augmented Large Language Models for Bug Fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAFix: History-Augmented Large Language Models for Bug Fixing"
                },
                "summary": "Recent studies have explored the performance of Large Language Models (LLMs)\non various Software Engineering (SE) tasks, such as code generation and bug\nfixing. However, these approaches typically rely on the context data from the\ncurrent snapshot of the project, overlooking the potential of rich historical\ndata from real-world software repositories. Additionally, the impact of prompt\nstyles on LLM performance within a historical context remains underexplored. To\naddress these gaps, we propose HAFix, which stands for History-Augmented LLMs\non Bug Fixing, a novel approach that leverages individual historical heuristics\nassociated with bugs and aggregates the results of these heuristics (HAFix-Agg)\nto enhance LLMs' bug-fixing capabilities. To empirically evaluate HAFix, we\nemploy Code Llama on a dataset of 51 single-line bugs, sourced from 11\nopen-source projects, by mining the historical context data of bugs and\noperationalizing this context in the form of seven heuristics. Our evaluation\ndemonstrates that historical heuristics significantly enhance bug-fixing\nperformance. For example, the FLN-all heuristic achieves a 10% improvement in\nperformance compared to a non-historical baseline inspired by GitHub Copilot.\nFurthermore, HAFix-Agg fixes 45% more bugs than the baseline, outperforming\nFLN-all and demonstrating the best performance overall. Moreover, within the\ncontext of historical heuristics, we identify the Instruction style prompt as\nthe most effective template for LLMs in bug fixing. Finally, we provide a\npragmatic trade-off analysis of bug-fixing performance, cost, and time\nefficiency, offering valuable insights for the practical deployment of our\napproach in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored the performance of Large Language Models (LLMs)\non various Software Engineering (SE) tasks, such as code generation and bug\nfixing. However, these approaches typically rely on the context data from the\ncurrent snapshot of the project, overlooking the potential of rich historical\ndata from real-world software repositories. Additionally, the impact of prompt\nstyles on LLM performance within a historical context remains underexplored. To\naddress these gaps, we propose HAFix, which stands for History-Augmented LLMs\non Bug Fixing, a novel approach that leverages individual historical heuristics\nassociated with bugs and aggregates the results of these heuristics (HAFix-Agg)\nto enhance LLMs' bug-fixing capabilities. To empirically evaluate HAFix, we\nemploy Code Llama on a dataset of 51 single-line bugs, sourced from 11\nopen-source projects, by mining the historical context data of bugs and\noperationalizing this context in the form of seven heuristics. Our evaluation\ndemonstrates that historical heuristics significantly enhance bug-fixing\nperformance. For example, the FLN-all heuristic achieves a 10% improvement in\nperformance compared to a non-historical baseline inspired by GitHub Copilot.\nFurthermore, HAFix-Agg fixes 45% more bugs than the baseline, outperforming\nFLN-all and demonstrating the best performance overall. Moreover, within the\ncontext of historical heuristics, we identify the Instruction style prompt as\nthe most effective template for LLMs in bug fixing. Finally, we provide a\npragmatic trade-off analysis of bug-fixing performance, cost, and time\nefficiency, offering valuable insights for the practical deployment of our\napproach in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Yu Shi"
                    },
                    {
                        "name": "Abdul Ali Bangash"
                    },
                    {
                        "name": "Emad Fallahzadeh"
                    },
                    {
                        "name": "Bram Adams"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "arxiv_comment": "55 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09127v1",
                "updated": "2025-01-15T20:22:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    22,
                    35,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    22,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Multilingual LLMs Struggle to Link Orthography and Semantics in\n  Bilingual Word Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual LLMs Struggle to Link Orthography and Semantics in\n  Bilingual Word Processing"
                },
                "summary": "Bilingual lexical processing is shaped by the complex interplay of\nphonological, orthographic, and semantic features of two languages within an\nintegrated mental lexicon. In humans, this is evident in the ease with which\ncognate words - words similar in both orthographic form and meaning (e.g.,\nblind, meaning \"sightless\" in both English and German) - are processed,\ncompared to the challenges posed by interlingual homographs, which share\northographic form but differ in meaning (e.g., gift, meaning \"present\" in\nEnglish but \"poison\" in German). We investigate how multilingual Large Language\nModels (LLMs) handle such phenomena, focusing on English-Spanish,\nEnglish-French, and English-German cognates, non-cognate, and interlingual\nhomographs. Specifically, we evaluate their ability to disambiguate meanings\nand make semantic judgments, both when these word types are presented in\nisolation or within sentence contexts. Our findings reveal that while certain\nLLMs demonstrate strong performance in recognizing cognates and non-cognates in\nisolation, they exhibit significant difficulty in disambiguating interlingual\nhomographs, often performing below random baselines. This suggests LLMs tend to\nrely heavily on orthographic similarities rather than semantic understanding\nwhen interpreting interlingual homographs. Further, we find LLMs exhibit\ndifficulty in retrieving word meanings, with performance in isolative\ndisambiguation tasks having no correlation with semantic understanding.\nFinally, we study how the LLM processes interlingual homographs in incongruent\nsentences. We find models to opt for different strategies in understanding\nEnglish and non-English homographs, highlighting a lack of a unified approach\nto handling cross-lingual ambiguities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bilingual lexical processing is shaped by the complex interplay of\nphonological, orthographic, and semantic features of two languages within an\nintegrated mental lexicon. In humans, this is evident in the ease with which\ncognate words - words similar in both orthographic form and meaning (e.g.,\nblind, meaning \"sightless\" in both English and German) - are processed,\ncompared to the challenges posed by interlingual homographs, which share\northographic form but differ in meaning (e.g., gift, meaning \"present\" in\nEnglish but \"poison\" in German). We investigate how multilingual Large Language\nModels (LLMs) handle such phenomena, focusing on English-Spanish,\nEnglish-French, and English-German cognates, non-cognate, and interlingual\nhomographs. Specifically, we evaluate their ability to disambiguate meanings\nand make semantic judgments, both when these word types are presented in\nisolation or within sentence contexts. Our findings reveal that while certain\nLLMs demonstrate strong performance in recognizing cognates and non-cognates in\nisolation, they exhibit significant difficulty in disambiguating interlingual\nhomographs, often performing below random baselines. This suggests LLMs tend to\nrely heavily on orthographic similarities rather than semantic understanding\nwhen interpreting interlingual homographs. Further, we find LLMs exhibit\ndifficulty in retrieving word meanings, with performance in isolative\ndisambiguation tasks having no correlation with semantic understanding.\nFinally, we study how the LLM processes interlingual homographs in incongruent\nsentences. We find models to opt for different strategies in understanding\nEnglish and non-English homographs, highlighting a lack of a unified approach\nto handling cross-lingual ambiguities."
                },
                "authors": [
                    {
                        "name": "Eshaan Tanwar"
                    },
                    {
                        "name": "Gayatri Oke"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "Code available at:\n  https://github.com/EshaanT/Bilingual_processing_LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]