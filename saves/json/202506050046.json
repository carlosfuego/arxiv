[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v2",
                "updated": "2025-06-03T17:18:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    18,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v1",
                "updated": "2025-06-03T13:19:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02671v1",
                "updated": "2025-06-03T09:16:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T09:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "title": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet"
                },
                "summary": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Jiazhen Huang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Xianghua Fu"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v1",
                "updated": "2025-06-03T08:51:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02523v1",
                "updated": "2025-06-03T06:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T06:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "title": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators."
                },
                "authors": [
                    {
                        "name": "Robin Geens"
                    },
                    {
                        "name": "Marian Verhelst"
                    }
                ],
                "author_detail": {
                    "name": "Marian Verhelst"
                },
                "author": "Marian Verhelst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v3",
                "updated": "2025-06-03T06:43:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    43,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02488v1",
                "updated": "2025-06-03T06:02:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    2,
                    50,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T06:02:50Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    2,
                    50,
                    1,
                    154,
                    0
                ],
                "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models"
                },
                "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality."
                },
                "authors": [
                    {
                        "name": "Hongtao Huang"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v2",
                "updated": "2025-06-03T03:32:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    32,
                    10,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v3",
                "updated": "2025-06-03T01:55:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    55,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v2",
                "updated": "2025-06-03T01:51:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    51,
                    37,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3716368.3735166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3716368.3735166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.04005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 14 figures",
                "arxiv_journal_ref": "Great Lakes Symposium on VLSI 2025 (GLSVLSI '25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v2",
                "updated": "2025-06-02T19:27:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    19,
                    27,
                    12,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference."
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01928v1",
                "updated": "2025-06-02T17:47:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T17:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "title": "Esoteric Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esoteric Language Models"
                },
                "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)"
                },
                "authors": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Johnna Liu"
                    },
                    {
                        "name": "Deepansha Singh"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v8",
                "updated": "2025-06-02T17:46:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    46,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01880v1",
                "updated": "2025-06-02T17:09:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    9,
                    59,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T17:09:59Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    9,
                    59,
                    0,
                    153,
                    0
                ],
                "title": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning"
                },
                "summary": "Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto."
                },
                "authors": [
                    {
                        "name": "Djamel Rassem Lamouri"
                    },
                    {
                        "name": "Iheb Nassim Aouadj"
                    },
                    {
                        "name": "Smail Kourta"
                    },
                    {
                        "name": "Riyadh Baghdadi"
                    }
                ],
                "author_detail": {
                    "name": "Riyadh Baghdadi"
                },
                "author": "Riyadh Baghdadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01827v1",
                "updated": "2025-06-02T16:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    12,
                    22,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T16:12:22Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    12,
                    22,
                    0,
                    153,
                    0
                ],
                "title": "Memory Access Characterization of Large Language Models in CPU\n  Environment and its Potential Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Access Characterization of Large Language Models in CPU\n  Environment and its Potential Impacts"
                },
                "summary": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations."
                },
                "authors": [
                    {
                        "name": "Spencer Banasik"
                    }
                ],
                "author_detail": {
                    "name": "Spencer Banasik"
                },
                "author": "Spencer Banasik",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01643v1",
                "updated": "2025-06-02T13:16:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    16,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T13:16:14Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    16,
                    14,
                    0,
                    153,
                    0
                ],
                "title": "A Low Power Monolithic Active Pixel Sensor Prototype for the STCF Inner\n  Tracker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low Power Monolithic Active Pixel Sensor Prototype for the STCF Inner\n  Tracker"
                },
                "summary": "The Super Tau-Charm Facility (STCF) is a proposed $e^+e^-$ collider with a\npeak luminosity 100 times higher than that of the present tau-charm factory.\nThe inner tracker (ITK) of STCF should feature a low material budget and high\nreadout speed. Under these requirements, the monolithic active pixel sensor\n(MAPS) is considered as a promising candidate for the ITK. To minimize the\npower consumption of MAPS (for low material budget), larger-size sensors are\nproposed to reduce the scale of the readout circuitry while preserving the\nrequired position resolution. Multiple sensors with varying dimensions and\nstructures were designed and integrated in several prototype chips for\nperformance comparison, fabricated in a 180~nm CIS process. The in-pixel\nreadout circuit can also provide time of arrival (ToA) and time-over-threshold\n(ToT) of the hit signal, with a least significant bit (LSB) of 50 ns. The\nperipheral readout circuit performs operations including timestamp correction,\ndata aggregation, caching, framing, 8b/10b encoding, and serialization.\nAccording to simulation, the power consumption for a full-scale chip is about\n55.7 mW/cm2. Preliminary measurements have been conducted on the prototype\nchips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Super Tau-Charm Facility (STCF) is a proposed $e^+e^-$ collider with a\npeak luminosity 100 times higher than that of the present tau-charm factory.\nThe inner tracker (ITK) of STCF should feature a low material budget and high\nreadout speed. Under these requirements, the monolithic active pixel sensor\n(MAPS) is considered as a promising candidate for the ITK. To minimize the\npower consumption of MAPS (for low material budget), larger-size sensors are\nproposed to reduce the scale of the readout circuitry while preserving the\nrequired position resolution. Multiple sensors with varying dimensions and\nstructures were designed and integrated in several prototype chips for\nperformance comparison, fabricated in a 180~nm CIS process. The in-pixel\nreadout circuit can also provide time of arrival (ToA) and time-over-threshold\n(ToT) of the hit signal, with a least significant bit (LSB) of 50 ns. The\nperipheral readout circuit performs operations including timestamp correction,\ndata aggregation, caching, framing, 8b/10b encoding, and serialization.\nAccording to simulation, the power consumption for a full-scale chip is about\n55.7 mW/cm2. Preliminary measurements have been conducted on the prototype\nchips."
                },
                "authors": [
                    {
                        "name": "Dongwei Xuan"
                    },
                    {
                        "name": "Ruiyang Zhang"
                    },
                    {
                        "name": "Jiajun Qin"
                    },
                    {
                        "name": "Hao Han"
                    },
                    {
                        "name": "Xinyu Bin"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Jianbei Liu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Anqing Wang"
                    },
                    {
                        "name": "Aodong Song"
                    },
                    {
                        "name": "Xiangming Sun"
                    },
                    {
                        "name": "Le Xiao"
                    },
                    {
                        "name": "Lailin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Lailin Xu"
                },
                "author": "Lailin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v6",
                "updated": "2025-06-02T11:46:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    46,
                    43,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v3",
                "updated": "2025-06-02T02:08:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    8,
                    6,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v2",
                "updated": "2025-06-02T01:08:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    8,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams"
                },
                "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01215v1",
                "updated": "2025-06-01T23:49:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "published": "2025-06-01T23:49:14Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in\n  Transformers"
                },
                "summary": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance."
                },
                "authors": [
                    {
                        "name": "Woomin Song"
                    },
                    {
                        "name": "Sai Muralidhar Jayanthi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kanthashree Mysore Sathyendra"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Aram Galstyan"
                    },
                    {
                        "name": "Shubham Katiyar"
                    },
                    {
                        "name": "Sravan Babu Bodapati"
                    }
                ],
                "author_detail": {
                    "name": "Sravan Babu Bodapati"
                },
                "author": "Sravan Babu Bodapati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01151v1",
                "updated": "2025-06-01T20:05:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    20,
                    5,
                    30,
                    6,
                    152,
                    0
                ],
                "published": "2025-06-01T20:05:30Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    20,
                    5,
                    30,
                    6,
                    152,
                    0
                ],
                "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron."
                },
                "authors": [
                    {
                        "name": "Xintong Sun"
                    },
                    {
                        "name": "Chi Wei"
                    },
                    {
                        "name": "Minghao Tian"
                    },
                    {
                        "name": "Shiwen Ni"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Ni"
                },
                "author": "Shiwen Ni",
                "arxiv_comment": "ICML2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18458v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18458v3",
                "updated": "2025-06-01T16:00:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    16,
                    0,
                    34,
                    6,
                    152,
                    0
                ],
                "published": "2025-05-24T01:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    1,
                    57,
                    12,
                    5,
                    144,
                    0
                ],
                "title": "A Survey of LLM $\\times$ DATA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM $\\times$ DATA"
                },
                "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Junxuan He"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18458v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18458v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v2",
                "updated": "2025-06-01T10:36:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    10,
                    36,
                    7,
                    6,
                    152,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted to ACL Findings 2025. Please cite the ACL version. Code and\n  data are available at: https://github.com/NiuTrans/LaMaTE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00839v1",
                "updated": "2025-06-01T05:04:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    5,
                    4,
                    56,
                    6,
                    152,
                    0
                ],
                "published": "2025-06-01T05:04:56Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    5,
                    4,
                    56,
                    6,
                    152,
                    0
                ],
                "title": "Neural Path Guiding with Distribution Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Path Guiding with Distribution Factorization"
                },
                "summary": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport."
                },
                "authors": [
                    {
                        "name": "Pedro Figueiredo"
                    },
                    {
                        "name": "Qihao He"
                    },
                    {
                        "name": "Nima Khademi Kalantari"
                    }
                ],
                "author_detail": {
                    "name": "Nima Khademi Kalantari"
                },
                "author": "Nima Khademi Kalantari",
                "arxiv_comment": "11 pages, 11 figures. Accepted to EGSR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v1",
                "updated": "2025-05-31T23:16:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with dynamic synaptic memory through fast-weight\nprogramming (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with dynamic synaptic memory through fast-weight\nprogramming (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17491v2",
                "updated": "2025-05-31T23:01:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    1,
                    0,
                    5,
                    151,
                    0
                ],
                "published": "2024-07-04T02:35:00Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    2,
                    35,
                    0,
                    3,
                    186,
                    0
                ],
                "title": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting"
                },
                "summary": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness."
                },
                "authors": [
                    {
                        "name": "Changdae Oh"
                    },
                    {
                        "name": "Gyeongdeok Seo"
                    },
                    {
                        "name": "Geunyoung Jung"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Hosik Choi"
                    },
                    {
                        "name": "Jiyoung Jung"
                    },
                    {
                        "name": "Kyungwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Kyungwoo Song"
                },
                "author": "Kyungwoo Song",
                "arxiv_comment": "Extended work from the CVPR'23 paper: arxiv:2303.14773; This paper\n  has been submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v2",
                "updated": "2025-05-31T22:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    22,
                    12,
                    10,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v6",
                "updated": "2025-05-31T17:58:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    17,
                    58,
                    24,
                    5,
                    151,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v2",
                "updated": "2025-05-31T13:43:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    13,
                    43,
                    36,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02038v1",
                "updated": "2025-05-31T06:58:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    58,
                    52,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:58:52Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    58,
                    52,
                    5,
                    151,
                    0
                ],
                "title": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy\n  Critical and Time Sensitive Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy\n  Critical and Time Sensitive Environment"
                },
                "summary": "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme."
                },
                "authors": [
                    {
                        "name": "Anum Nawaz"
                    },
                    {
                        "name": "Hafiz Humza Mahmood Ramzan"
                    },
                    {
                        "name": "Xianjia Yu"
                    },
                    {
                        "name": "Zhuo Zou"
                    },
                    {
                        "name": "Tomi Westerlund"
                    }
                ],
                "author_detail": {
                    "name": "Tomi Westerlund"
                },
                "author": "Tomi Westerlund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00420v1",
                "updated": "2025-05-31T06:50:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    50,
                    5,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:50:05Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    50,
                    5,
                    5,
                    151,
                    0
                ],
                "title": "A New Spatiotemporal Correlation Anomaly Detection Method that\n  Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Spatiotemporal Correlation Anomaly Detection Method that\n  Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor\n  Networks"
                },
                "summary": "Detecting anomalies in the data collected by WSNs can provide crucial\nevidence for assessing the reliability and stability of WSNs. Existing methods\nfor WSN anomaly detection often face challenges such as the limited extraction\nof spatiotemporal correlation features, the absence of sample labels, few\nanomaly samples, and an imbalanced sample distribution. To address these\nissues, a spatiotemporal correlation detection model (MTAD-RD) considering both\nmodel architecture and a two-stage training strategy perspective is proposed.\nIn terms of model structure design, the proposed MTAD-RD backbone network\nincludes a retentive network (RetNet) enhanced by a cross-retention (CR)\nmodule, a multigranular feature fusion module, and a graph attention network\nmodule to extract internode correlation information. This proposed model can\nintegrate the intermodal correlation features and spatial features of WSN\nneighbor nodes while extracting global information from time series data.\nMoreover, its serialized inference characteristic can remarkably reduce\ninference overhead. For model training, a two-stage training approach was\ndesigned. First, a contrastive learning proxy task was designed for time series\ndata with graph structure information in WSNs, enabling the backbone network to\nlearn transferable features from unlabeled data using unsupervised contrastive\nlearning methods, thereby addressing the issue of missing sample labels in the\ndataset. Then, a caching-based sample sampler was designed to divide samples\ninto few-shot and contrastive learning data. A specific joint loss function was\ndeveloped to jointly train the dual-graph discriminator network to address the\nproblem of sample imbalance effectively. In experiments carried out on real\npublic datasets, the designed MTAD-RD anomaly detection method achieved an F1\nscore of 90.97%, outperforming existing supervised WSN anomaly detection\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anomalies in the data collected by WSNs can provide crucial\nevidence for assessing the reliability and stability of WSNs. Existing methods\nfor WSN anomaly detection often face challenges such as the limited extraction\nof spatiotemporal correlation features, the absence of sample labels, few\nanomaly samples, and an imbalanced sample distribution. To address these\nissues, a spatiotemporal correlation detection model (MTAD-RD) considering both\nmodel architecture and a two-stage training strategy perspective is proposed.\nIn terms of model structure design, the proposed MTAD-RD backbone network\nincludes a retentive network (RetNet) enhanced by a cross-retention (CR)\nmodule, a multigranular feature fusion module, and a graph attention network\nmodule to extract internode correlation information. This proposed model can\nintegrate the intermodal correlation features and spatial features of WSN\nneighbor nodes while extracting global information from time series data.\nMoreover, its serialized inference characteristic can remarkably reduce\ninference overhead. For model training, a two-stage training approach was\ndesigned. First, a contrastive learning proxy task was designed for time series\ndata with graph structure information in WSNs, enabling the backbone network to\nlearn transferable features from unlabeled data using unsupervised contrastive\nlearning methods, thereby addressing the issue of missing sample labels in the\ndataset. Then, a caching-based sample sampler was designed to divide samples\ninto few-shot and contrastive learning data. A specific joint loss function was\ndeveloped to jointly train the dual-graph discriminator network to address the\nproblem of sample imbalance effectively. In experiments carried out on real\npublic datasets, the designed MTAD-RD anomaly detection method achieved an F1\nscore of 90.97%, outperforming existing supervised WSN anomaly detection\nmethods."
                },
                "authors": [
                    {
                        "name": "Miao Ye"
                    },
                    {
                        "name": "Suxiao Wang"
                    },
                    {
                        "name": "Jiaguang Han"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Xiaoli Wang"
                    },
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Peng Wen"
                    },
                    {
                        "name": "Jing Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jing Cui"
                },
                "author": "Jing Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v1",
                "updated": "2025-05-31T06:10:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v4",
                "updated": "2025-05-31T04:45:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    45,
                    23,
                    5,
                    151,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "Accepted by ICML25. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00384v1",
                "updated": "2025-05-31T04:27:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T04:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "Deep-Learning-Driven Prefetching for Far Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep-Learning-Driven Prefetching for Far Memory"
                },
                "summary": "Modern software systems face increasing runtime performance demands,\nparticularly in emerging architectures like far memory, where local-memory\nmisses incur significant latency. While machine learning (ML) has proven\neffective in offline systems optimization, its application to high-frequency,\nruntime-level problems remains limited due to strict performance,\ngeneralization, and integration constraints. We present FarSight, a Linux-based\nfar-memory system that leverages deep learning (DL) to efficiently perform\naccurate data prefetching. FarSight separates application semantics from\nruntime memory layout, allowing offline-trained DL models to predict access\npatterns using a compact vocabulary of ordinal possibilities, resolved at\nruntime through lightweight mapping structures. By combining asynchronous\ninference, lookahead prediction, and a cache-resident DL model, FarSight\nachieves high prediction accuracy with low runtime overhead. Our evaluation of\nFarSight on four data-intensive workloads shows that it outperforms the\nstate-of-the-art far-memory system by up to 3.6 times. Overall, this work\ndemonstrates the feasibility and advantages of applying modern ML techniques to\ncomplex, performance-critical software runtime problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems face increasing runtime performance demands,\nparticularly in emerging architectures like far memory, where local-memory\nmisses incur significant latency. While machine learning (ML) has proven\neffective in offline systems optimization, its application to high-frequency,\nruntime-level problems remains limited due to strict performance,\ngeneralization, and integration constraints. We present FarSight, a Linux-based\nfar-memory system that leverages deep learning (DL) to efficiently perform\naccurate data prefetching. FarSight separates application semantics from\nruntime memory layout, allowing offline-trained DL models to predict access\npatterns using a compact vocabulary of ordinal possibilities, resolved at\nruntime through lightweight mapping structures. By combining asynchronous\ninference, lookahead prediction, and a cache-resident DL model, FarSight\nachieves high prediction accuracy with low runtime overhead. Our evaluation of\nFarSight on four data-intensive workloads shows that it outperforms the\nstate-of-the-art far-memory system by up to 3.6 times. Overall, this work\ndemonstrates the feasibility and advantages of applying modern ML techniques to\ncomplex, performance-critical software runtime problems."
                },
                "authors": [
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v1",
                "updated": "2025-05-31T00:52:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining\nvideo quality. The source code of Foresight is available at\n\\texttt{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining\nvideo quality. The source code of Foresight is available at\n\\texttt{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24722v1",
                "updated": "2025-05-30T15:42:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24643v1",
                "updated": "2025-05-30T14:29:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:29:55Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching"
                },
                "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations."
                },
                "authors": [
                    {
                        "name": "Juan Wisznia"
                    },
                    {
                        "name": "Cecilia Bolaos"
                    },
                    {
                        "name": "Juan Tollo"
                    },
                    {
                        "name": "Giovanni Marraffini"
                    },
                    {
                        "name": "Agustn Gianolini"
                    },
                    {
                        "name": "Noe Hsueh"
                    },
                    {
                        "name": "Luciano Del Corro"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Del Corro"
                },
                "author": "Luciano Del Corro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v2",
                "updated": "2025-05-30T11:43:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    11,
                    43,
                    48,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v1",
                "updated": "2025-05-30T08:49:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. Code is available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. Code is available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24221v1",
                "updated": "2025-05-30T05:17:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T05:17:44Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "title": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management"
                },
                "summary": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Wenzhe Zhu"
                    },
                    {
                        "name": "Yongkun Li"
                    },
                    {
                        "name": "Yinlong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yinlong Xu"
                },
                "author": "Yinlong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24095v1",
                "updated": "2025-05-30T00:46:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T00:46:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "title": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference"
                },
                "summary": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%."
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Jamison Kerney"
                    },
                    {
                        "name": "Ethan J. Jackson"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v3",
                "updated": "2025-05-30T00:36:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    36,
                    37,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23970v1",
                "updated": "2025-05-29T19:52:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T19:52:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving"
                },
                "summary": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low."
                },
                "authors": [
                    {
                        "name": "Yuyang Tian"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23938v1",
                "updated": "2025-05-29T18:41:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T18:41:13Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "title": "Digital Forensic Investigation of the ChatGPT Windows Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Forensic Investigation of the ChatGPT Windows Application"
                },
                "summary": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics."
                },
                "authors": [
                    {
                        "name": "Malithi Wanniarachchi Kankanamge"
                    },
                    {
                        "name": "Nick McKenna"
                    },
                    {
                        "name": "Santiago Carmona"
                    },
                    {
                        "name": "Syed Mhamudul Hasan"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v1",
                "updated": "2025-05-29T17:12:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23520v1",
                "updated": "2025-05-29T14:59:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:59:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity"
                },
                "summary": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Guoliang Zhu"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Zhang"
                },
                "author": "Yiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v1",
                "updated": "2025-05-29T13:05:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21889v2",
                "updated": "2025-05-29T12:59:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    12,
                    59,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T02:07:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    7,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse"
                },
                "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Cheater Lin"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Xianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianwei Zhang"
                },
                "author": "Xianwei Zhang",
                "arxiv_comment": "31st International European Conference on Parallel and Distributed\n  Computing (Euro-Par 2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23351v1",
                "updated": "2025-05-29T11:16:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T11:16:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores"
                },
                "summary": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods."
                },
                "authors": [
                    {
                        "name": "Sudam M. Wasala"
                    },
                    {
                        "name": "Jurre Wolff"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Anuj Pathania"
                    },
                    {
                        "name": "Clemens Grelck"
                    },
                    {
                        "name": "Andy D. Pimentel"
                    }
                ],
                "author_detail": {
                    "name": "Andy D. Pimentel"
                },
                "author": "Andy D. Pimentel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23275v1",
                "updated": "2025-05-29T09:23:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "title": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception"
                },
                "summary": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v2",
                "updated": "2025-05-29T09:18:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    18,
                    35,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23258v1",
                "updated": "2025-05-29T09:06:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:06:01Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "title": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System"
                },
                "summary": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading."
                },
                "authors": [
                    {
                        "name": "Haojie Jia"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "9 pages, In Proceedings of IEEE ICCCN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v4",
                "updated": "2025-05-29T09:01:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    1,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew C Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C Yao"
                },
                "author": "Andrew C Yao",
                "arxiv_comment": "52 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v2",
                "updated": "2025-05-29T03:11:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    3,
                    11,
                    10,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21070v2",
                "updated": "2025-05-29T01:34:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    34,
                    8,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T11:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    55,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "Minute-Long Videos with Dual Parallelisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minute-Long Videos with Dual Parallelisms"
                },
                "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22927v1",
                "updated": "2025-05-28T22:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "title": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs"
                },
                "summary": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array."
                },
                "authors": [
                    {
                        "name": "Robert Marosi"
                    },
                    {
                        "name": "Muhammed Zuboraj"
                    },
                    {
                        "name": "Filippo Capolino"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Capolino"
                },
                "author": "Filippo Capolino",
                "arxiv_comment": "8 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22913v1",
                "updated": "2025-05-28T22:32:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference"
                },
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar."
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v6",
                "updated": "2025-05-28T18:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v1",
                "updated": "2025-05-28T17:39:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22425v1",
                "updated": "2025-05-28T14:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Scaling Reasoning without Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reasoning without Attention"
                },
                "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."
                },
                "authors": [
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v4",
                "updated": "2025-05-28T12:07:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    7,
                    57,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v1",
                "updated": "2025-05-28T09:20:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21919v1",
                "updated": "2025-05-28T03:05:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T03:05:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference"
                },
                "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference."
                },
                "authors": [
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eun Kyung Lee"
                },
                "author": "Eun Kyung Lee",
                "arxiv_comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v2",
                "updated": "2025-05-28T01:38:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    1,
                    38,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v3",
                "updated": "2025-05-28T00:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    0,
                    43,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21669v1",
                "updated": "2025-05-27T18:47:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T18:47:34Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "title": "Improved Prefetching Techniques for Linked Data Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Prefetching Techniques for Linked Data Structures"
                },
                "summary": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%."
                },
                "authors": [
                    {
                        "name": "Nikola Vuk Maruszewski"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Vuk Maruszewski"
                },
                "author": "Nikola Vuk Maruszewski",
                "arxiv_comment": "73 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.5.3; E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v1",
                "updated": "2025-05-27T17:39:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21259v1",
                "updated": "2025-05-27T14:39:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:39:28Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "title": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching"
                },
                "summary": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network."
                },
                "authors": [
                    {
                        "name": "Chunyi Ma"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jianhua Yang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Kishk"
                },
                "author": "Mustafa A. Kishk",
                "arxiv_doi": "10.1109/JIOT.2025.3574814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3574814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 12 figures, be accepted by IEEE IoTJ",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v3",
                "updated": "2025-05-27T12:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    5,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "SEA-2025 version. 18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v3",
                "updated": "2025-05-27T09:24:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    24,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v1",
                "updated": "2025-05-27T06:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "8 pages, 3 figures. Under review at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v3",
                "updated": "2025-05-27T04:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    4,
                    15,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19586v2",
                "updated": "2025-05-27T03:16:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    16,
                    32,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T07:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    0,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization"
                },
                "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Bowen Shen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v4",
                "updated": "2025-05-27T03:08:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    8,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20600v1",
                "updated": "2025-05-27T00:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T00:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling"
                },
                "summary": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Tianyu Feng"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Guoxuan Zhu"
                    },
                    {
                        "name": "Xiu Lin"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20438v1",
                "updated": "2025-05-26T18:34:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T18:34:07Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "title": "HAMburger: Accelerating LLM Inference via Token Smashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMburger: Accelerating LLM Inference via Token Smashing"
                },
                "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design."
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v5",
                "updated": "2025-05-26T16:16:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    16,
                    43,
                    0,
                    146,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v2",
                "updated": "2025-05-26T13:20:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    20,
                    45,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "RAP: Runtime-Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Runtime-Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Jiaheng Dai"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Tianqi Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v1",
                "updated": "2025-05-26T12:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_comment": "Accepted for publication in 2025 IEEE 18th International Conference\n  on Cloud Computing (CLOUD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v1",
                "updated": "2025-05-26T11:35:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16582v2",
                "updated": "2025-05-26T10:07:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    7,
                    5,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T12:17:13Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    12,
                    17,
                    13,
                    3,
                    142,
                    0
                ],
                "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."
                },
                "authors": [
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Xinyu Cai"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Chengjun Xie"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17062v3",
                "updated": "2025-05-26T08:39:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    39,
                    26,
                    0,
                    146,
                    0
                ],
                "published": "2024-05-27T11:31:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    31,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation"
                },
                "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Tianxiang Wu"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v2",
                "updated": "2025-05-26T07:30:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    30,
                    17,
                    0,
                    146,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yzgler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19602v1",
                "updated": "2025-05-26T07:11:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T07:11:42Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression"
                },
                "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity."
                },
                "authors": [
                    {
                        "name": "Kunjun Li"
                    },
                    {
                        "name": "Zigeng Chen"
                    },
                    {
                        "name": "Cheng-Yen Yang"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Jenq-Neng Hwang"
                },
                "author": "Jenq-Neng Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v1",
                "updated": "2025-05-26T05:58:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v2",
                "updated": "2025-05-26T05:56:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    56,
                    51,
                    0,
                    146,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v2",
                "updated": "2025-05-26T05:28:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    28,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v2",
                "updated": "2025-05-25T13:03:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    13,
                    3,
                    54,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19089v1",
                "updated": "2025-05-25T10:57:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-25T10:57:35Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation"
                },
                "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings."
                },
                "authors": [
                    {
                        "name": "Xuejie Liu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v4",
                "updated": "2025-05-25T05:26:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    5,
                    26,
                    2,
                    6,
                    145,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v1",
                "updated": "2025-05-24T17:46:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "19 pages, 15 figures. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v3",
                "updated": "2025-05-24T17:39:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    39,
                    32,
                    5,
                    144,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "18 pages, 14 figures. Accepted by ICML 2025. The code is available at\n  https://github.com/wenhao728/AsymRnR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16366v2",
                "updated": "2025-05-24T17:04:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    4,
                    26,
                    5,
                    144,
                    0
                ],
                "published": "2024-04-25T07:09:05Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    7,
                    9,
                    5,
                    3,
                    116,
                    0
                ],
                "title": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection"
                },
                "summary": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD first introduces two auxiliary networks along with\ncorrelation constraints to guard the GNNs against inconsistent information\nencoding. Furthermore, G3AD introduces an adaptive caching module to guard the\nGNNs from directly reconstructing the observed graph data that contains\nanomalies. Extensive experiments demonstrate that our G3AD can outperform\ntwenty state-of-the-art methods on both synthetic and real-world graph anomaly\ndatasets, with flexible generalization ability in different GNN backbones."
                },
                "authors": [
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Jinke Shi"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Haishuai Wang"
                    },
                    {
                        "name": "Jiajun Bu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Bu"
                },
                "author": "Jiajun Bu",
                "arxiv_comment": "Accepted by IEEE TNNLS (14 pages, 10 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20334v1",
                "updated": "2025-05-24T10:34:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T10:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via\n  Pseudo Query"
                },
                "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v2",
                "updated": "2025-05-24T09:33:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    33,
                    35,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Privacy-Preserving and Efficient Federated Cache Model\n  Fine-Tuning for Vision-Language Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18610v1",
                "updated": "2025-05-24T09:18:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T09:18:11Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    9,
                    18,
                    11,
                    5,
                    144,
                    0
                ],
                "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT\n  LLMs"
                },
                "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ."
                },
                "authors": [
                    {
                        "name": "Tengxuan Liu"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Jiayi Yang"
                    },
                    {
                        "name": "Tianchen Zhao"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18577v1",
                "updated": "2025-05-24T07:57:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T07:57:57Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    7,
                    57,
                    57,
                    5,
                    144,
                    0
                ],
                "title": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD\n  Performance"
                },
                "summary": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies."
                },
                "authors": [
                    {
                        "name": "Dongsuk Oh"
                    },
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Eunjee Na"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Hyunkyu Choi"
                    },
                    {
                        "name": "Seonghyeon Jang"
                    },
                    {
                        "name": "Hanjin Choi"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18554v1",
                "updated": "2025-05-24T06:45:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T06:45:16Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    16,
                    5,
                    144,
                    0
                ],
                "title": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared\n  Last-Level Cache Performance in Server Workloads"
                },
                "summary": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively."
                },
                "authors": [
                    {
                        "name": "Jaewon Kwon"
                    },
                    {
                        "name": "Yongju Lee"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Enhyeok Jang"
                    },
                    {
                        "name": "Hongju Kal"
                    },
                    {
                        "name": "Won Woo Ro"
                    }
                ],
                "author_detail": {
                    "name": "Won Woo Ro"
                },
                "arxiv_affiliation": "Yonsei University, Seoul, Republic of Korea",
                "author": "Won Woo Ro",
                "arxiv_comment": "Accepted to ISCA '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02006v1",
                "updated": "2025-05-24T06:12:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    12,
                    31,
                    5,
                    144,
                    0
                ],
                "published": "2025-05-24T06:12:31Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    12,
                    31,
                    5,
                    144,
                    0
                ],
                "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and\n  KV Cache Resizing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and\n  KV Cache Resizing"
                },
                "summary": "Efficiently serving large language models (LLMs) under dynamic and bursty\nworkloads remains a key challenge for real-world deployment. Existing serving\nframeworks and static model compression techniques fail to adapt to workload\nfluctuations, leading to either service-level objective (SLO) violations under\nfull-precision serving or persistent accuracy degradation with static\nquantization. We present MorphServe, a dynamic, workload-aware LLM serving\nframework based on morphological adaptation. MorphServe introduces two\nasynchronous, token-level runtime mechanisms: quantized layer swapping, which\nselectively replaces less impactful layers with quantized alternatives during\nhigh-load periods, and pressure-aware KV cache resizing, which dynamically\nadjusts KV cache capacity in response to memory pressure. These mechanisms\nenable state-preserving transitions with minimum runtime overhead and are fully\ncompatible with modern scheduling and attention techniques. Extensive\nexperiments on Vicuna and Llama family models with real-world workloads\ndemonstrate that MorphServe reduces average SLO violations by 92.45 percent and\nimproves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,\nwithout compromising generation quality. These results establish MorphServe as\na practical and elastic solution for LLM deployment in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) under dynamic and bursty\nworkloads remains a key challenge for real-world deployment. Existing serving\nframeworks and static model compression techniques fail to adapt to workload\nfluctuations, leading to either service-level objective (SLO) violations under\nfull-precision serving or persistent accuracy degradation with static\nquantization. We present MorphServe, a dynamic, workload-aware LLM serving\nframework based on morphological adaptation. MorphServe introduces two\nasynchronous, token-level runtime mechanisms: quantized layer swapping, which\nselectively replaces less impactful layers with quantized alternatives during\nhigh-load periods, and pressure-aware KV cache resizing, which dynamically\nadjusts KV cache capacity in response to memory pressure. These mechanisms\nenable state-preserving transitions with minimum runtime overhead and are fully\ncompatible with modern scheduling and attention techniques. Extensive\nexperiments on Vicuna and Llama family models with real-world workloads\ndemonstrate that MorphServe reduces average SLO violations by 92.45 percent and\nimproves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,\nwithout compromising generation quality. These results establish MorphServe as\na practical and elastic solution for LLM deployment in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhaoyuan Su"
                    },
                    {
                        "name": "Tingfeng Lan"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Yue Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yue Cheng"
                },
                "author": "Yue Cheng",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.03147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03147v1",
                "updated": "2025-06-03T17:59:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    33,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:59:33Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    33,
                    1,
                    154,
                    0
                ],
                "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation"
                },
                "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets."
                },
                "authors": [
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Yuwei Niu"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Xianyi He"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Wangbo Yu"
                    },
                    {
                        "name": "Shaodong Wang"
                    },
                    {
                        "name": "Yunyang Ge"
                    },
                    {
                        "name": "Yatian Pang"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03145v1",
                "updated": "2025-06-03T17:59:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:59:18Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    18,
                    1,
                    154,
                    0
                ],
                "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and\n  Semantic Understanding Capability of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and\n  Semantic Understanding Capability of LLM"
                },
                "summary": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions."
                },
                "authors": [
                    {
                        "name": "Pralaypati Ta"
                    },
                    {
                        "name": "Sriram Venkatesaperumal"
                    },
                    {
                        "name": "Keerthi Ram"
                    },
                    {
                        "name": "Mohanasankar Sivaprakasam"
                    }
                ],
                "author_detail": {
                    "name": "Mohanasankar Sivaprakasam"
                },
                "author": "Mohanasankar Sivaprakasam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03142v1",
                "updated": "2025-06-03T17:59:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    5,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:59:05Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    5,
                    1,
                    154,
                    0
                ],
                "title": "Not All Tokens Are Meant to Be Forgotten",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Tokens Are Meant to Be Forgotten"
                },
                "summary": "Large Language Models (LLMs), pre-trained on massive text corpora, exhibit\nremarkable human-level language understanding, reasoning, and decision-making\nabilities. However, they tend to memorize unwanted information, such as private\nor copyrighted content, raising significant privacy and legal concerns.\nUnlearning has emerged as a promising solution, but existing methods face a\nsignificant challenge of over-forgetting. This issue arises because they\nindiscriminately suppress the generation of all the tokens in forget samples,\nleading to a substantial loss of model utility. To overcome this challenge, we\nintroduce the Targeted Information Forgetting (TIF) framework, which consists\nof (1) a flexible targeted information identifier designed to differentiate\nbetween unwanted words (UW) and general words (GW) in the forget samples, and\n(2) a novel Targeted Preference Optimization approach that leverages Logit\nPreference Loss to unlearn unwanted information associated with UW and\nPreservation Loss to retain general information in GW, effectively improving\nthe unlearning process while mitigating utility degradation. Extensive\nexperiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF\nframework enhances unlearning effectiveness while preserving model utility and\nachieving state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), pre-trained on massive text corpora, exhibit\nremarkable human-level language understanding, reasoning, and decision-making\nabilities. However, they tend to memorize unwanted information, such as private\nor copyrighted content, raising significant privacy and legal concerns.\nUnlearning has emerged as a promising solution, but existing methods face a\nsignificant challenge of over-forgetting. This issue arises because they\nindiscriminately suppress the generation of all the tokens in forget samples,\nleading to a substantial loss of model utility. To overcome this challenge, we\nintroduce the Targeted Information Forgetting (TIF) framework, which consists\nof (1) a flexible targeted information identifier designed to differentiate\nbetween unwanted words (UW) and general words (GW) in the forget samples, and\n(2) a novel Targeted Preference Optimization approach that leverages Logit\nPreference Loss to unlearn unwanted information associated with UW and\nPreservation Loss to retain general information in GW, effectively improving\nthe unlearning process while mitigating utility degradation. Extensive\nexperiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF\nframework enhances unlearning effectiveness while preserving model utility and\nachieving state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhou"
                    },
                    {
                        "name": "Yao Qiang"
                    },
                    {
                        "name": "Saleh Zare Zade"
                    },
                    {
                        "name": "Douglas Zytko"
                    },
                    {
                        "name": "Prashant Khanduri"
                    },
                    {
                        "name": "Dongxiao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Zhu"
                },
                "author": "Dongxiao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03139v1",
                "updated": "2025-06-03T17:58:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    58,
                    57,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:58:57Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    58,
                    57,
                    1,
                    154,
                    0
                ],
                "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation"
                },
                "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius."
                },
                "authors": [
                    {
                        "name": "Siqi Chen"
                    },
                    {
                        "name": "Xinyu Dong"
                    },
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Linjuan Wu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "19 pages,4 figures, Project page:\n  https://zju-real.github.io/SVGenius, Code:\n  https://github.com/ZJU-REAL/SVGenius-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03136v1",
                "updated": "2025-06-03T17:58:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    58,
                    42,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:58:42Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    58,
                    42,
                    1,
                    154,
                    0
                ],
                "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning"
                },
                "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Project: https://github.com/Gen-Verse/CURE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03131v1",
                "updated": "2025-06-03T17:57:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    57,
                    33,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:57:33Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    57,
                    33,
                    1,
                    154,
                    0
                ],
                "title": "Native-Resolution Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Native-Resolution Image Synthesis"
                },
                "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies."
                },
                "authors": [
                    {
                        "name": "Zidong Wang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiyuan Zhang"
                },
                "author": "Yiyuan Zhang",
                "arxiv_comment": "Project Page: https://wzdthu.github.io/NiT/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03122v1",
                "updated": "2025-06-03T17:54:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    54,
                    30,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:54:30Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    54,
                    30,
                    1,
                    154,
                    0
                ],
                "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit\n  Topology Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit\n  Topology Generation"
                },
                "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design."
                },
                "authors": [
                    {
                        "name": "Prashanth Vijayaraghavan"
                    },
                    {
                        "name": "Luyao Shi"
                    },
                    {
                        "name": "Ehsan Degan"
                    },
                    {
                        "name": "Vandana Mukherjee"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_comment": "9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07437v2",
                "updated": "2025-06-03T17:49:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    49,
                    20,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-10T04:07:36Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    4,
                    7,
                    36,
                    3,
                    100,
                    0
                ],
                "title": "Unifying and extending Diffusion Models through PDEs for solving Inverse\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying and extending Diffusion Models through PDEs for solving Inverse\n  Problems"
                },
                "summary": "Diffusion models have emerged as powerful generative tools with applications\nin computer vision and scientific machine learning (SciML), where they have\nbeen used to solve large-scale probabilistic inverse problems. Traditionally,\nthese models have been derived using principles of variational inference,\ndenoising, statistical signal processing, and stochastic differential\nequations. In contrast to the conventional presentation, in this study we\nderive diffusion models using ideas from linear partial differential equations\nand demonstrate that this approach has several benefits that include a\nconstructive derivation of the forward and reverse processes, a unified\nderivation of multiple formulations and sampling strategies, and the discovery\nof a new class of variance preserving models. We also apply the conditional\nversion of these models to solve canonical conditional density estimation\nproblems and challenging inverse problems. These problems help establish\nbenchmarks for systematically quantifying the performance of different\nformulations and sampling strategies in this study and for future studies.\nFinally, we identify and implement a mechanism through which a single diffusion\nmodel can be applied to measurements obtained from multiple measurement\noperators. Taken together, the contents of this manuscript provide a new\nunderstanding of and several new directions in the application of diffusion\nmodels to solving physics-based inverse problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as powerful generative tools with applications\nin computer vision and scientific machine learning (SciML), where they have\nbeen used to solve large-scale probabilistic inverse problems. Traditionally,\nthese models have been derived using principles of variational inference,\ndenoising, statistical signal processing, and stochastic differential\nequations. In contrast to the conventional presentation, in this study we\nderive diffusion models using ideas from linear partial differential equations\nand demonstrate that this approach has several benefits that include a\nconstructive derivation of the forward and reverse processes, a unified\nderivation of multiple formulations and sampling strategies, and the discovery\nof a new class of variance preserving models. We also apply the conditional\nversion of these models to solve canonical conditional density estimation\nproblems and challenging inverse problems. These problems help establish\nbenchmarks for systematically quantifying the performance of different\nformulations and sampling strategies in this study and for future studies.\nFinally, we identify and implement a mechanism through which a single diffusion\nmodel can be applied to measurements obtained from multiple measurement\noperators. Taken together, the contents of this manuscript provide a new\nunderstanding of and several new directions in the application of diffusion\nmodels to solving physics-based inverse problems."
                },
                "authors": [
                    {
                        "name": "Agnimitra Dasgupta"
                    },
                    {
                        "name": "Alexsander Marciano da Cunha"
                    },
                    {
                        "name": "Ali Fardisi"
                    },
                    {
                        "name": "Mehrnegar Aminy"
                    },
                    {
                        "name": "Brianna Binder"
                    },
                    {
                        "name": "Bryan Shaddy"
                    },
                    {
                        "name": "Assad A Oberai"
                    }
                ],
                "author_detail": {
                    "name": "Assad A Oberai"
                },
                "author": "Assad A Oberai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18460v2",
                "updated": "2025-06-03T17:47:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    47,
                    36,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-25T18:59:07Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    7,
                    1,
                    56,
                    0
                ],
                "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers"
                },
                "summary": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Barlas Oguz"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Xilun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilun Chen"
                },
                "author": "Xilun Chen",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03111v1",
                "updated": "2025-06-03T17:40:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    40,
                    39,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:40:39Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    40,
                    39,
                    1,
                    154,
                    0
                ],
                "title": "Rectified Flows for Fast Multiscale Fluid Flow Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectified Flows for Fast Multiscale Fluid Flow Modeling"
                },
                "summary": "The statistical modeling of fluid flows is very challenging due to their\nmultiscale dynamics and extreme sensitivity to initial conditions. While\nrecently proposed conditional diffusion models achieve high fidelity, they\ntypically require hundreds of stochastic sampling steps at inference. We\nintroduce a rectified flow framework that learns a time-dependent velocity\nfield, transporting input to output distributions along nearly straight\ntrajectories. By casting sampling as solving an ordinary differential equation\n(ODE) along this straighter flow field, our method makes each integration step\nmuch more effective, using as few as eight steps versus (more than) 128 steps\nin standard score-based diffusion, without sacrificing predictive fidelity.\nExperiments on challenging multiscale flow benchmarks show that rectified flows\nrecover the same posterior distributions as diffusion models, preserve\nfine-scale features that MSE-trained baselines miss, and deliver\nhigh-resolution samples in a fraction of inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The statistical modeling of fluid flows is very challenging due to their\nmultiscale dynamics and extreme sensitivity to initial conditions. While\nrecently proposed conditional diffusion models achieve high fidelity, they\ntypically require hundreds of stochastic sampling steps at inference. We\nintroduce a rectified flow framework that learns a time-dependent velocity\nfield, transporting input to output distributions along nearly straight\ntrajectories. By casting sampling as solving an ordinary differential equation\n(ODE) along this straighter flow field, our method makes each integration step\nmuch more effective, using as few as eight steps versus (more than) 128 steps\nin standard score-based diffusion, without sacrificing predictive fidelity.\nExperiments on challenging multiscale flow benchmarks show that rectified flows\nrecover the same posterior distributions as diffusion models, preserve\nfine-scale features that MSE-trained baselines miss, and deliver\nhigh-resolution samples in a fraction of inference time."
                },
                "authors": [
                    {
                        "name": "Victor Armegioiu"
                    },
                    {
                        "name": "Yannick Ramic"
                    },
                    {
                        "name": "Siddhartha Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Siddhartha Mishra"
                },
                "author": "Siddhartha Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10807v3",
                "updated": "2025-06-03T17:40:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    40,
                    30,
                    1,
                    154,
                    0
                ],
                "published": "2024-10-14T17:59:24Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    24,
                    0,
                    288,
                    0
                ],
                "title": "HardNet: Hard-Constrained Neural Networks with Universal Approximation\n  Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HardNet: Hard-Constrained Neural Networks with Universal Approximation\n  Guarantees"
                },
                "summary": "Incorporating prior knowledge or specifications of input-output relationships\ninto machine learning models has attracted significant attention, as it\nenhances generalization from limited data and leads to conforming outputs.\nHowever, most existing approaches use soft constraints by penalizing violations\nthrough regularization, which offers no guarantee of constraint satisfaction,\nespecially on inputs far from the training distribution -- an essential\nrequirement in safety-critical applications. On the other hand, imposing hard\nconstraints on neural networks may hinder their representational power,\nadversely affecting performance. To address this, we propose HardNet, a\npractical framework for constructing neural networks that inherently satisfy\nhard constraints without sacrificing model capacity. Unlike approaches that\nmodify outputs only at inference time, HardNet enables end-to-end training with\nhard constraint guarantees, leading to improved performance. To the best of our\nknowledge, HardNet is the first method with an efficient forward pass to\nenforce more than one input-dependent inequality constraint. It allows\nunconstrained optimization of the network parameters using standard algorithms\nby appending a differentiable closed-form enforcement layer to the network's\noutput. Furthermore, we show that HardNet is expressive and retains the\nuniversal approximation capabilities of neural networks. We demonstrate the\nversatility and effectiveness of HardNet across various applications: learning\nwith piecewise constraints, learning optimization solvers with guaranteed\nfeasibility, and optimizing control policies in safety-critical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating prior knowledge or specifications of input-output relationships\ninto machine learning models has attracted significant attention, as it\nenhances generalization from limited data and leads to conforming outputs.\nHowever, most existing approaches use soft constraints by penalizing violations\nthrough regularization, which offers no guarantee of constraint satisfaction,\nespecially on inputs far from the training distribution -- an essential\nrequirement in safety-critical applications. On the other hand, imposing hard\nconstraints on neural networks may hinder their representational power,\nadversely affecting performance. To address this, we propose HardNet, a\npractical framework for constructing neural networks that inherently satisfy\nhard constraints without sacrificing model capacity. Unlike approaches that\nmodify outputs only at inference time, HardNet enables end-to-end training with\nhard constraint guarantees, leading to improved performance. To the best of our\nknowledge, HardNet is the first method with an efficient forward pass to\nenforce more than one input-dependent inequality constraint. It allows\nunconstrained optimization of the network parameters using standard algorithms\nby appending a differentiable closed-form enforcement layer to the network's\noutput. Furthermore, we show that HardNet is expressive and retains the\nuniversal approximation capabilities of neural networks. We demonstrate the\nversatility and effectiveness of HardNet across various applications: learning\nwith piecewise constraints, learning optimization solvers with guaranteed\nfeasibility, and optimizing control policies in safety-critical systems."
                },
                "authors": [
                    {
                        "name": "Youngjae Min"
                    },
                    {
                        "name": "Navid Azizan"
                    }
                ],
                "author_detail": {
                    "name": "Navid Azizan"
                },
                "author": "Navid Azizan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03106v1",
                "updated": "2025-06-03T17:39:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    39,
                    2,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:39:02Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    39,
                    2,
                    1,
                    154,
                    0
                ],
                "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback"
                },
                "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration."
                },
                "authors": [
                    {
                        "name": "Xiaoying Zhang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03100v1",
                "updated": "2025-06-03T17:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    31,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:31:53Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    31,
                    53,
                    1,
                    154,
                    0
                ],
                "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified\n  Theory and Risk Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified\n  Theory and Risk Bounds"
                },
                "summary": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA."
                },
                "authors": [
                    {
                        "name": "Yang Guo"
                    },
                    {
                        "name": "Yutian Tao"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Robert D. Nowak"
                    },
                    {
                        "name": "Yingyu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yingyu Liang"
                },
                "author": "Yingyu Liang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03099v1",
                "updated": "2025-06-03T17:29:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    29,
                    28,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:29:28Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    29,
                    28,
                    1,
                    154,
                    0
                ],
                "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models"
                },
                "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/"
                },
                "authors": [
                    {
                        "name": "Chetwin Low"
                    },
                    {
                        "name": "Weimin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weimin Wang"
                },
                "author": "Weimin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11184v2",
                "updated": "2025-06-03T17:27:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    27,
                    16,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-16T16:12:40Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    12,
                    40,
                    6,
                    47,
                    0
                ],
                "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety\n  Awareness for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety\n  Awareness for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Kuiyi Gao"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Pinjia He"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03095v1",
                "updated": "2025-06-03T17:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    27,
                    4,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    27,
                    4,
                    1,
                    154,
                    0
                ],
                "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPO Learning with LLMs-Judge Signal for Computer Use Agents"
                },
                "summary": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents."
                },
                "authors": [
                    {
                        "name": "Man Luo"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Xin Su"
                    },
                    {
                        "name": "Shachar Rosenman"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Shao-Yen Tseng"
                    },
                    {
                        "name": "Phillip Howard"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Howard"
                },
                "author": "Phillip Howard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03093v1",
                "updated": "2025-06-03T17:24:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    24,
                    55,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:24:55Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    24,
                    55,
                    1,
                    154,
                    0
                ],
                "title": "From Flat to Hierarchical: Extracting Sparse Representations with\n  Matching Pursuit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Flat to Hierarchical: Extracting Sparse Representations with\n  Matching Pursuit"
                },
                "summary": "Motivated by the hypothesis that neural network representations encode\nabstract, interpretable features as linearly accessible, approximately\northogonal directions, sparse autoencoders (SAEs) have become a popular tool in\ninterpretability. However, recent work has demonstrated phenomenology of model\nrepresentations that lies outside the scope of this hypothesis, showing\nsignatures of hierarchical, nonlinear, and multi-dimensional features. This\nraises the question: do SAEs represent features that possess structure at odds\nwith their motivating hypothesis? If not, does avoiding this mismatch help\nidentify said features and gain further insights into neural network\nrepresentations? To answer these questions, we take a construction-based\napproach and re-contextualize the popular matching pursuits (MP) algorithm from\nsparse coding to design MP-SAE -- an SAE that unrolls its encoder into a\nsequence of residual-guided steps, allowing it to capture hierarchical and\nnonlinearly accessible features. Comparing this architecture with existing SAEs\non a mixture of synthetic and natural data settings, we show: (i) hierarchical\nconcepts induce conditionally orthogonal features, which existing SAEs are\nunable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE\nrecovers highly meaningful features, helping us unravel shared structure in the\nseemingly dichotomous representation spaces of different modalities in a\nvision-language model, hence demonstrating the assumption that useful features\nare solely linearly accessible is insufficient. We also show that the\nsequential encoder principle of MP-SAE affords an additional benefit of\nadaptive sparsity at inference time, which may be of independent interest.\nOverall, we argue our results provide credence to the idea that\ninterpretability should begin with the phenomenology of representations, with\nmethods emerging from assumptions that fit it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the hypothesis that neural network representations encode\nabstract, interpretable features as linearly accessible, approximately\northogonal directions, sparse autoencoders (SAEs) have become a popular tool in\ninterpretability. However, recent work has demonstrated phenomenology of model\nrepresentations that lies outside the scope of this hypothesis, showing\nsignatures of hierarchical, nonlinear, and multi-dimensional features. This\nraises the question: do SAEs represent features that possess structure at odds\nwith their motivating hypothesis? If not, does avoiding this mismatch help\nidentify said features and gain further insights into neural network\nrepresentations? To answer these questions, we take a construction-based\napproach and re-contextualize the popular matching pursuits (MP) algorithm from\nsparse coding to design MP-SAE -- an SAE that unrolls its encoder into a\nsequence of residual-guided steps, allowing it to capture hierarchical and\nnonlinearly accessible features. Comparing this architecture with existing SAEs\non a mixture of synthetic and natural data settings, we show: (i) hierarchical\nconcepts induce conditionally orthogonal features, which existing SAEs are\nunable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE\nrecovers highly meaningful features, helping us unravel shared structure in the\nseemingly dichotomous representation spaces of different modalities in a\nvision-language model, hence demonstrating the assumption that useful features\nare solely linearly accessible is insufficient. We also show that the\nsequential encoder principle of MP-SAE affords an additional benefit of\nadaptive sparsity at inference time, which may be of independent interest.\nOverall, we argue our results provide credence to the idea that\ninterpretability should begin with the phenomenology of representations, with\nmethods emerging from assumptions that fit it."
                },
                "authors": [
                    {
                        "name": "Valrie Costa"
                    },
                    {
                        "name": "Thomas Fel"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Bahareh Tolooshams"
                    },
                    {
                        "name": "Demba Ba"
                    }
                ],
                "author_detail": {
                    "name": "Demba Ba"
                },
                "author": "Demba Ba",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03092v1",
                "updated": "2025-06-03T17:24:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    24,
                    8,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:24:08Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    24,
                    8,
                    1,
                    154,
                    0
                ],
                "title": "Thin coronal jets and plasmoid-mediated reconnection: Insights from\n  Solar Orbiter observations and Bifrost simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thin coronal jets and plasmoid-mediated reconnection: Insights from\n  Solar Orbiter observations and Bifrost simulations"
                },
                "summary": "Coronal jets are ubiquitous, collimated million-degree ejections that\ncontribute to the energy and mass supply of the upper solar atmosphere and the\nsolar wind. Solar Orbiter provides an unprecedented opportunity to observe\nfine-scale jets from a unique vantage point close to the Sun. We aim to (1)\nuncover thin jets originating from Coronal Bright Points (CBPs), revealing\npreviously unresolved contributions to coronal upflows; and (2) improve our\nunderstanding of plasmoid-mediated reconnection and its observable signatures.\nWe analyze eleven datasets from the High Resolution Imager 174 \\r{A} of the\nExtreme Ultraviolet Imager (HRIEUV) onboard Solar Orbiter, focusing on narrow\njets from CBPs and signatures of magnetic reconnection within current sheets\nand outflow regions. To support the observations, we compare with CBP\nsimulations performed with the Bifrost code. We have identified thin coronal\njets originating from CBPs with widths ranging from 253 km to 706 km: scales\nthat could not be resolved with previous EUV imaging instruments. Remarkably,\nthese jets are 30-85% brighter than their surroundings and can extend up to 22\nMm while maintaining their narrow form. In one of the datasets, we directly\nidentify plasmoid-mediated reconnection through the development within the\ncurrent sheet of a small-scale plasmoid that reaches a size of 332 km and\npropagates at 40 km/s. In another dataset, we infer plasmoid signatures through\nthe intermittent boomerang-like pattern that appears in the outflow region.\nBoth direct and indirect plasmoid-mediated reconnection signatures are\nsupported by comparisons with the synthetic HRIEUV emission from the\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coronal jets are ubiquitous, collimated million-degree ejections that\ncontribute to the energy and mass supply of the upper solar atmosphere and the\nsolar wind. Solar Orbiter provides an unprecedented opportunity to observe\nfine-scale jets from a unique vantage point close to the Sun. We aim to (1)\nuncover thin jets originating from Coronal Bright Points (CBPs), revealing\npreviously unresolved contributions to coronal upflows; and (2) improve our\nunderstanding of plasmoid-mediated reconnection and its observable signatures.\nWe analyze eleven datasets from the High Resolution Imager 174 \\r{A} of the\nExtreme Ultraviolet Imager (HRIEUV) onboard Solar Orbiter, focusing on narrow\njets from CBPs and signatures of magnetic reconnection within current sheets\nand outflow regions. To support the observations, we compare with CBP\nsimulations performed with the Bifrost code. We have identified thin coronal\njets originating from CBPs with widths ranging from 253 km to 706 km: scales\nthat could not be resolved with previous EUV imaging instruments. Remarkably,\nthese jets are 30-85% brighter than their surroundings and can extend up to 22\nMm while maintaining their narrow form. In one of the datasets, we directly\nidentify plasmoid-mediated reconnection through the development within the\ncurrent sheet of a small-scale plasmoid that reaches a size of 332 km and\npropagates at 40 km/s. In another dataset, we infer plasmoid signatures through\nthe intermittent boomerang-like pattern that appears in the outflow region.\nBoth direct and indirect plasmoid-mediated reconnection signatures are\nsupported by comparisons with the synthetic HRIEUV emission from the\nsimulations."
                },
                "authors": [
                    {
                        "name": "D. Nbrega-Siverio"
                    },
                    {
                        "name": "R. Joshi"
                    },
                    {
                        "name": "E. Sola-Viladesau"
                    },
                    {
                        "name": "D. Berghmans"
                    },
                    {
                        "name": "D. Lim"
                    }
                ],
                "author_detail": {
                    "name": "D. Lim"
                },
                "author": "D. Lim",
                "arxiv_comment": "Submitted to A&A, 15 pages, 9 figures, movies available upon request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.19415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.19415v2",
                "updated": "2025-06-03T17:20:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    20,
                    44,
                    1,
                    154,
                    0
                ],
                "published": "2023-05-30T21:08:45Z",
                "published_parsed": [
                    2023,
                    5,
                    30,
                    21,
                    8,
                    45,
                    1,
                    150,
                    0
                ],
                "title": "Euclidean nets under isometric embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclidean nets under isometric embeddings"
                },
                "summary": "Suppose that there exists a discrete subset $X$ of a complete, connected,\n$n$-dimensional Riemannian manifold $M$ such that the Riemannian distances\nbetween points of $X$ correspond to the Euclidean distances of a net in\n$\\mathbb{R}^{n}$. What can then be derived about the geometry of $M$? In\narXiv:2004.08621 it was shown that if $n=2$ then $M$ is isometric to\n$\\mathbb{R}^{2}$. In this paper we show two consequential geometric properties\nthat the manifold $M$ shares with the Euclidean space in any dimension. The\nfirst property is that $X$ is a net with respect to the Riemannian distance in\n$M$. The second property is that all geodesics in $M$ are distance minimizing,\nand there are no conjugate points in $M$. This demonstrates the possibility of\ninferring infinitesimal qualities from discrete data, even in higher\ndimensions. As a corollary we obtain that the large-scale geometry of $M$ is\nasymptotically Euclidean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suppose that there exists a discrete subset $X$ of a complete, connected,\n$n$-dimensional Riemannian manifold $M$ such that the Riemannian distances\nbetween points of $X$ correspond to the Euclidean distances of a net in\n$\\mathbb{R}^{n}$. What can then be derived about the geometry of $M$? In\narXiv:2004.08621 it was shown that if $n=2$ then $M$ is isometric to\n$\\mathbb{R}^{2}$. In this paper we show two consequential geometric properties\nthat the manifold $M$ shares with the Euclidean space in any dimension. The\nfirst property is that $X$ is a net with respect to the Riemannian distance in\n$M$. The second property is that all geodesics in $M$ are distance minimizing,\nand there are no conjugate points in $M$. This demonstrates the possibility of\ninferring infinitesimal qualities from discrete data, even in higher\ndimensions. As a corollary we obtain that the large-scale geometry of $M$ is\nasymptotically Euclidean."
                },
                "authors": [
                    {
                        "name": "Matan Eilat"
                    }
                ],
                "author_detail": {
                    "name": "Matan Eilat"
                },
                "author": "Matan Eilat",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.19415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.19415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.MG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.MG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03090v1",
                "updated": "2025-06-03T17:19:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    19,
                    45,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:19:45Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    19,
                    45,
                    1,
                    154,
                    0
                ],
                "title": "Literary Evidence Retrieval via Long-Context Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literary Evidence Retrieval via Long-Context Language Models"
                },
                "summary": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction."
                },
                "authors": [
                    {
                        "name": "Katherine Thai"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v2",
                "updated": "2025-06-03T17:18:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    18,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13172v2",
                "updated": "2025-06-03T17:08:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    8,
                    56,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-17T19:55:53Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    55,
                    53,
                    0,
                    48,
                    0
                ],
                "title": "Unveiling Privacy Risks in LLM Agent Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Privacy Risks in LLM Agent Memory"
                },
                "summary": "Large Language Model (LLM) agents have become increasingly prevalent across\nvarious real-world applications. They enhance decision-making by storing\nprivate user-agent interactions in the memory module for demonstrations,\nintroducing new privacy risks for LLM agents. In this work, we systematically\ninvestigate the vulnerability of LLM agents to our proposed Memory EXTRaction\nAttack (MEXTRA) under a black-box setting. To extract private information from\nmemory, we propose an effective attacking prompt design and an automated prompt\ngeneration method based on different levels of knowledge about the LLM agent.\nExperiments on two representative agents demonstrate the effectiveness of\nMEXTRA. Moreover, we explore key factors influencing memory leakage from both\nthe agent designer's and the attacker's perspectives. Our findings highlight\nthe urgent need for effective memory safeguards in LLM agent design and\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have become increasingly prevalent across\nvarious real-world applications. They enhance decision-making by storing\nprivate user-agent interactions in the memory module for demonstrations,\nintroducing new privacy risks for LLM agents. In this work, we systematically\ninvestigate the vulnerability of LLM agents to our proposed Memory EXTRaction\nAttack (MEXTRA) under a black-box setting. To extract private information from\nmemory, we propose an effective attacking prompt design and an automated prompt\ngeneration method based on different levels of knowledge about the LLM agent.\nExperiments on two representative agents demonstrate the effectiveness of\nMEXTRA. Moreover, we explore key factors influencing memory leakage from both\nthe agent designer's and the attacker's perspectives. Our findings highlight\nthe urgent need for effective memory safeguards in LLM agent design and\ndeployment."
                },
                "authors": [
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Weiyi He"
                    },
                    {
                        "name": "Shenglai Zeng"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Yue Xing"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Pengfei He"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei He"
                },
                "author": "Pengfei He",
                "arxiv_comment": "ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03084v1",
                "updated": "2025-06-03T17:05:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    5,
                    6,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:05:06Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    5,
                    6,
                    1,
                    154,
                    0
                ],
                "title": "InterMamba: Efficient Human-Human Interaction Generation with Adaptive\n  Spatio-Temporal Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterMamba: Efficient Human-Human Interaction Generation with Adaptive\n  Spatio-Temporal Mamba"
                },
                "summary": "Human-human interaction generation has garnered significant attention in\nmotion synthesis due to its vital role in understanding humans as social\nbeings. However, existing methods typically rely on transformer-based\narchitectures, which often face challenges related to scalability and\nefficiency. To address these issues, we propose a novel, efficient human-human\ninteraction generation method based on the Mamba framework, designed to meet\nthe demands of effectively capturing long-sequence dependencies while providing\nreal-time feedback. Specifically, we introduce an adaptive spatio-temporal\nMamba framework that utilizes two parallel SSM branches with an adaptive\nmechanism to integrate the spatial and temporal features of motion sequences.\nTo further enhance the model's ability to capture dependencies within\nindividual motion sequences and the interactions between different individual\nsequences, we develop two key modules: the self-adaptive spatio-temporal Mamba\nmodule and the cross-adaptive spatio-temporal Mamba module, enabling efficient\nfeature learning. Extensive experiments demonstrate that our method achieves\nstate-of-the-art results on two interaction datasets with remarkable quality\nand efficiency. Compared to the baseline method InterGen, our approach not only\nimproves accuracy but also requires a minimal parameter size of just 66M ,only\n36% of InterGen's, while achieving an average inference speed of 0.57 seconds,\nwhich is 46% of InterGen's execution time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-human interaction generation has garnered significant attention in\nmotion synthesis due to its vital role in understanding humans as social\nbeings. However, existing methods typically rely on transformer-based\narchitectures, which often face challenges related to scalability and\nefficiency. To address these issues, we propose a novel, efficient human-human\ninteraction generation method based on the Mamba framework, designed to meet\nthe demands of effectively capturing long-sequence dependencies while providing\nreal-time feedback. Specifically, we introduce an adaptive spatio-temporal\nMamba framework that utilizes two parallel SSM branches with an adaptive\nmechanism to integrate the spatial and temporal features of motion sequences.\nTo further enhance the model's ability to capture dependencies within\nindividual motion sequences and the interactions between different individual\nsequences, we develop two key modules: the self-adaptive spatio-temporal Mamba\nmodule and the cross-adaptive spatio-temporal Mamba module, enabling efficient\nfeature learning. Extensive experiments demonstrate that our method achieves\nstate-of-the-art results on two interaction datasets with remarkable quality\nand efficiency. Compared to the baseline method InterGen, our approach not only\nimproves accuracy but also requires a minimal parameter size of just 66M ,only\n36% of InterGen's, while achieving an average inference speed of 0.57 seconds,\nwhich is 46% of InterGen's execution time."
                },
                "authors": [
                    {
                        "name": "Zizhao Wu"
                    },
                    {
                        "name": "Yingying Sun"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Xiaoling Gu"
                    },
                    {
                        "name": "Ruyu Liu"
                    },
                    {
                        "name": "Jiazhou Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiazhou Chen"
                },
                "author": "Jiazhou Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12216v2",
                "updated": "2025-06-03T17:02:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    2,
                    25,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-16T16:08:45Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    8,
                    45,
                    2,
                    106,
                    0
                ],
                "title": "d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning"
                },
                "summary": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO, the first integration of policy gradient methods to masked dLLMs.\nThrough empirical studies, we investigate the performance of different\npost-training recipes on multiple mathematical and planning benchmarks. We find\nthat d1 yields the best performance and significantly improves performance of a\nstate-of-the-art dLLM. Our code is released at\nhttps://dllm-reasoning.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO, the first integration of policy gradient methods to masked dLLMs.\nThrough empirical studies, we investigate the performance of different\npost-training recipes on multiple mathematical and planning benchmarks. We find\nthat d1 yields the best performance and significantly improves performance of a\nstate-of-the-art dLLM. Our code is released at\nhttps://dllm-reasoning.github.io/."
                },
                "authors": [
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Devaansh Gupta"
                    },
                    {
                        "name": "Qinqing Zheng"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "27 pages, project page at https://dllm-reasoning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07783v3",
                "updated": "2025-06-03T17:02:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    2,
                    13,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-12T17:36:14Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    36,
                    14,
                    0,
                    132,
                    0
                ],
                "title": "Relative Overfitting and Accept-Reject Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relative Overfitting and Accept-Reject Framework"
                },
                "summary": "Currently, the scaling law of Large Language Models (LLMs) faces challenges\nand bottlenecks. This paper posits that noise effects, stemming from changes in\nthe signal-to-noise ratio under diminishing marginal returns, are the root\ncause of these issues. To control this noise, we investigated the differences\nbetween models with performance advantages and disadvantages, introducing the\nconcept of \"relative overfitting.\" Based on their complementary strengths, we\nhave proposed an application framework, Accept-Reject (AR), and the associated\nAR Law, which operates within this framework to elucidate the patterns of\nperformance changes after model integration. In Natural Language Processing\n(NLP), we use LLMs and Small Language Models (SLMs) as the medium for\ndiscussion. This framework enables SLMs to exert a universal positive influence\non LLM decision outputs, rather than the intuitively expected potential\nnegative influence. We validated our approach using self-built models based on\nmainstream architectures and pre-trained mainstream models across multiple\ndatasets, including basic language modeling, long-context tasks, subject\nexamination, and question-answering (QA) benchmarks. The results demonstrate\nthat through our framework, compared to increasing the LLM's parameters, we can\nachieve better performance improvements with significantly lower parameter and\ncomputational costs in many scenarios. These improvements are universal,\nstable, and effective. Furthermore, we explore the potential of \"relative\noverfitting\" and the AR framework in other machine learning domains, such as\ncomputer vision (CV) and AI for science. We hope the proposed approach can help\nscale laws overcome existing bottlenecks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, the scaling law of Large Language Models (LLMs) faces challenges\nand bottlenecks. This paper posits that noise effects, stemming from changes in\nthe signal-to-noise ratio under diminishing marginal returns, are the root\ncause of these issues. To control this noise, we investigated the differences\nbetween models with performance advantages and disadvantages, introducing the\nconcept of \"relative overfitting.\" Based on their complementary strengths, we\nhave proposed an application framework, Accept-Reject (AR), and the associated\nAR Law, which operates within this framework to elucidate the patterns of\nperformance changes after model integration. In Natural Language Processing\n(NLP), we use LLMs and Small Language Models (SLMs) as the medium for\ndiscussion. This framework enables SLMs to exert a universal positive influence\non LLM decision outputs, rather than the intuitively expected potential\nnegative influence. We validated our approach using self-built models based on\nmainstream architectures and pre-trained mainstream models across multiple\ndatasets, including basic language modeling, long-context tasks, subject\nexamination, and question-answering (QA) benchmarks. The results demonstrate\nthat through our framework, compared to increasing the LLM's parameters, we can\nachieve better performance improvements with significantly lower parameter and\ncomputational costs in many scenarios. These improvements are universal,\nstable, and effective. Furthermore, we explore the potential of \"relative\noverfitting\" and the AR framework in other machine learning domains, such as\ncomputer vision (CV) and AI for science. We hope the proposed approach can help\nscale laws overcome existing bottlenecks."
                },
                "authors": [
                    {
                        "name": "Yanxin Liu"
                    },
                    {
                        "name": "Yunqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yunqi Zhang"
                },
                "author": "Yunqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19516v2",
                "updated": "2025-06-03T17:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    0,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2024-03-28T15:47:13Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    47,
                    13,
                    3,
                    88,
                    0
                ],
                "title": "Spectral Clustering for Directed Graphs via Likelihood Estimation on\n  Stochastic Block Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Clustering for Directed Graphs via Likelihood Estimation on\n  Stochastic Block Models"
                },
                "summary": "Graph clustering is a fundamental task in unsupervised learning with broad\nreal-world applications. While spectral clustering methods for undirected\ngraphs are well-established and guided by a minimum cut optimization consensus,\ntheir extension to directed graphs remains relatively underexplored due to the\nadditional complexity introduced by edge directions. In this paper, we leverage\nstatistical inference on stochastic block models to guide the development of a\nspectral clustering algorithm for directed graphs. Specifically, we study the\nmaximum likelihood estimation under a widely used directed stochastic block\nmodel, and derive a global objective function that aligns with the underlying\ncommunity structure. We further establish a theoretical upper bound on the\nmisclustering error of its spectral relaxation, and based on this relaxation,\nintroduce a novel, self-adaptive spectral clustering method for directed\ngraphs. Extensive experiments on synthetic and real-world datasets demonstrate\nsignificant performance gains over existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph clustering is a fundamental task in unsupervised learning with broad\nreal-world applications. While spectral clustering methods for undirected\ngraphs are well-established and guided by a minimum cut optimization consensus,\ntheir extension to directed graphs remains relatively underexplored due to the\nadditional complexity introduced by edge directions. In this paper, we leverage\nstatistical inference on stochastic block models to guide the development of a\nspectral clustering algorithm for directed graphs. Specifically, we study the\nmaximum likelihood estimation under a widely used directed stochastic block\nmodel, and derive a global objective function that aligns with the underlying\ncommunity structure. We further establish a theoretical upper bound on the\nmisclustering error of its spectral relaxation, and based on this relaxation,\nintroduce a novel, self-adaptive spectral clustering method for directed\ngraphs. Extensive experiments on synthetic and real-world datasets demonstrate\nsignificant performance gains over existing baselines."
                },
                "authors": [
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Xiaowen Dong"
                    },
                    {
                        "name": "Mihai Cucuringu"
                    }
                ],
                "author_detail": {
                    "name": "Mihai Cucuringu"
                },
                "author": "Mihai Cucuringu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19157v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19157v4",
                "updated": "2025-06-03T17:00:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    0,
                    31,
                    1,
                    154,
                    0
                ],
                "published": "2024-06-27T13:21:33Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    13,
                    21,
                    33,
                    3,
                    179,
                    0
                ],
                "title": "How to build your latent Markov model -- the role of time and space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to build your latent Markov model -- the role of time and space"
                },
                "summary": "Statistical models that involve latent Markovian state processes have become\nimmensely popular tools for analysing time series and other sequential data.\nHowever, the plethora of model formulations, the inconsistent use of\nterminology, and the various inferential approaches and software packages can\nbe overwhelming to practitioners, especially when they are new to this area.\nWith this review-like paper, we thus aim to provide guidance for both\nstatisticians and practitioners working with latent Markov models by offering a\nunifying view on what otherwise are often considered separate model classes,\nfrom hidden Markov models over state-space models to Markov-modulated Poisson\nprocesses. In particular, we provide a roadmap for identifying a suitable\nlatent Markov model formulation given the data to be analysed. Furthermore, we\nemphasise that it is key to applied work with any of these model classes to\nunderstand how recursive techniques exploiting the models' dependence structure\ncan be used for inference. The R package LaMa adapts this unified view and\nprovides an easy-to-use framework for very fast (C++ based) numerical maximum\nlikelihood estimation of any of the models discussed in this paper, allowing\nusers to tailor a latent Markov model to their data using a Lego-type approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical models that involve latent Markovian state processes have become\nimmensely popular tools for analysing time series and other sequential data.\nHowever, the plethora of model formulations, the inconsistent use of\nterminology, and the various inferential approaches and software packages can\nbe overwhelming to practitioners, especially when they are new to this area.\nWith this review-like paper, we thus aim to provide guidance for both\nstatisticians and practitioners working with latent Markov models by offering a\nunifying view on what otherwise are often considered separate model classes,\nfrom hidden Markov models over state-space models to Markov-modulated Poisson\nprocesses. In particular, we provide a roadmap for identifying a suitable\nlatent Markov model formulation given the data to be analysed. Furthermore, we\nemphasise that it is key to applied work with any of these model classes to\nunderstand how recursive techniques exploiting the models' dependence structure\ncan be used for inference. The R package LaMa adapts this unified view and\nprovides an easy-to-use framework for very fast (C++ based) numerical maximum\nlikelihood estimation of any of the models discussed in this paper, allowing\nusers to tailor a latent Markov model to their data using a Lego-type approach."
                },
                "authors": [
                    {
                        "name": "Sina Mews"
                    },
                    {
                        "name": "Jan-Ole Koslik"
                    },
                    {
                        "name": "Roland Langrock"
                    }
                ],
                "author_detail": {
                    "name": "Roland Langrock"
                },
                "author": "Roland Langrock",
                "arxiv_comment": "52 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19157v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19157v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23655v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23655v3",
                "updated": "2025-06-03T16:59:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    59,
                    29,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-29T17:05:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    5,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "Keyed Chaotic Dynamics for Privacy-Preserving Neural Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyed Chaotic Dynamics for Privacy-Preserving Neural Inference"
                },
                "summary": "Neural network inference typically operates on raw input data, increasing the\nrisk of exposure during preprocessing and inference. Moreover, neural\narchitectures lack efficient built-in mechanisms for directly authenticating\ninput data. This work introduces a novel encryption method for ensuring the\nsecurity of neural inference. By constructing key-conditioned chaotic graph\ndynamical systems, we enable the encryption and decryption of real-valued\ntensors within the neural architecture. The proposed dynamical systems are\nparticularly suited to encryption due to their sensitivity to initial\nconditions and their capacity to produce complex, key-dependent nonlinear\ntransformations from compact rules. This work establishes a paradigm for\nsecuring neural inference and opens new avenues for research on the application\nof graph dynamical systems in neural network security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network inference typically operates on raw input data, increasing the\nrisk of exposure during preprocessing and inference. Moreover, neural\narchitectures lack efficient built-in mechanisms for directly authenticating\ninput data. This work introduces a novel encryption method for ensuring the\nsecurity of neural inference. By constructing key-conditioned chaotic graph\ndynamical systems, we enable the encryption and decryption of real-valued\ntensors within the neural architecture. The proposed dynamical systems are\nparticularly suited to encryption due to their sensitivity to initial\nconditions and their capacity to produce complex, key-dependent nonlinear\ntransformations from compact rules. This work establishes a paradigm for\nsecuring neural inference and opens new avenues for research on the application\nof graph dynamical systems in neural network security."
                },
                "authors": [
                    {
                        "name": "Peter David Fagan"
                    }
                ],
                "author_detail": {
                    "name": "Peter David Fagan"
                },
                "author": "Peter David Fagan",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23655v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23655v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94A60, 37N25, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03077v1",
                "updated": "2025-06-03T16:54:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    54,
                    15,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:54:15Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    54,
                    15,
                    1,
                    154,
                    0
                ],
                "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs"
                },
                "summary": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP."
                },
                "authors": [
                    {
                        "name": "Qijun Luo"
                    },
                    {
                        "name": "Mengqi Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Xiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Li"
                },
                "author": "Xiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01297v3",
                "updated": "2025-06-04T02:07:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    2,
                    7,
                    51,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-02T04:14:03Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    4,
                    14,
                    3,
                    0,
                    153,
                    0
                ],
                "title": "MobCLIP: Learning General-purpose Geospatial Representation at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobCLIP: Learning General-purpose Geospatial Representation at Scale"
                },
                "summary": "Representation learning of geospatial locations remains a core challenge in\nachieving general geospatial intelligence. Current embedding methods often lack\nversatility, limiting their utility across diverse tasks in both human and\nnatural domains. We present MobCLIP, the first nationwide general-purpose\nlocation encoder, integrating an unprecedented diversity of data modalities\nthrough effective and scalable multimodal fusion. Adopting a novel CLIP-based\narchitecture, our framework aligns 100M+ POIs, nationwide remote sensing\nimagery, and structured demographic statistics with a billion-edge mobility\ngraph. By tokenizing spatial locations into grid cells inspired by Vision\nTransformers, we establish a unified representation space bridging mobility\npatterns and multimodal features. To rigorously evaluate the general-purpose\neffectiveness of MobCLIP, we construct a benchmark dataset composed of 11\ndownstream prediction tasks across social, economic, and natural domains.\nExperiments show that MobCLIP, with four input modalities and a compact\n128-dimensional representation space, achieves significantly superior\ngeneral-purpose predictive performances than state-of-the-art models by an\naverage of 35%. Thanks to the effective integration of human-centric\nmodalities, the performance gain is particularly profound in human-centric\ntasks, such as energy consumption (+260%), offline retail consumption amount\n(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we\nfurther demonstrate the scaling behavior in geospatial representation learning.\nWe open-source code and pretrained models at:\nhttps://github.com/ylzhouchris/MobCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation learning of geospatial locations remains a core challenge in\nachieving general geospatial intelligence. Current embedding methods often lack\nversatility, limiting their utility across diverse tasks in both human and\nnatural domains. We present MobCLIP, the first nationwide general-purpose\nlocation encoder, integrating an unprecedented diversity of data modalities\nthrough effective and scalable multimodal fusion. Adopting a novel CLIP-based\narchitecture, our framework aligns 100M+ POIs, nationwide remote sensing\nimagery, and structured demographic statistics with a billion-edge mobility\ngraph. By tokenizing spatial locations into grid cells inspired by Vision\nTransformers, we establish a unified representation space bridging mobility\npatterns and multimodal features. To rigorously evaluate the general-purpose\neffectiveness of MobCLIP, we construct a benchmark dataset composed of 11\ndownstream prediction tasks across social, economic, and natural domains.\nExperiments show that MobCLIP, with four input modalities and a compact\n128-dimensional representation space, achieves significantly superior\ngeneral-purpose predictive performances than state-of-the-art models by an\naverage of 35%. Thanks to the effective integration of human-centric\nmodalities, the performance gain is particularly profound in human-centric\ntasks, such as energy consumption (+260%), offline retail consumption amount\n(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we\nfurther demonstrate the scaling behavior in geospatial representation learning.\nWe open-source code and pretrained models at:\nhttps://github.com/ylzhouchris/MobCLIP."
                },
                "authors": [
                    {
                        "name": "Ya Wen"
                    },
                    {
                        "name": "Jixuan Cai"
                    },
                    {
                        "name": "Qiyao Ma"
                    },
                    {
                        "name": "Linyan Li"
                    },
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Chris Webster"
                    },
                    {
                        "name": "Yulun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhou"
                },
                "author": "Yulun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07802v2",
                "updated": "2025-06-03T16:45:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    45,
                    5,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-12T17:50:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    50,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "Improving Trajectory Stitching with Flow Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Trajectory Stitching with Flow Models"
                },
                "summary": "Generative models have shown great promise as trajectory planners, given\ntheir affinity to modeling complex distributions and guidable inference\nprocess. Previous works have successfully applied these in the context of\nrobotic manipulation but perform poorly when the required solution does not\nexist as a complete trajectory within the training set. We identify that this\nis a result of being unable to plan via stitching, and subsequently address the\narchitectural and dataset choices needed to remedy this. On top of this, we\npropose a novel addition to the training and inference procedures to both\nstabilize and enhance these capabilities. We demonstrate the efficacy of our\napproach by generating plans with out of distribution boundary conditions and\nperforming obstacle avoidance on the Franka Panda in simulation and on real\nhardware. In both of these tasks our method performs significantly better than\nthe baselines and is able to avoid obstacles up to four times as large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have shown great promise as trajectory planners, given\ntheir affinity to modeling complex distributions and guidable inference\nprocess. Previous works have successfully applied these in the context of\nrobotic manipulation but perform poorly when the required solution does not\nexist as a complete trajectory within the training set. We identify that this\nis a result of being unable to plan via stitching, and subsequently address the\narchitectural and dataset choices needed to remedy this. On top of this, we\npropose a novel addition to the training and inference procedures to both\nstabilize and enhance these capabilities. We demonstrate the efficacy of our\napproach by generating plans with out of distribution boundary conditions and\nperforming obstacle avoidance on the Franka Panda in simulation and on real\nhardware. In both of these tasks our method performs significantly better than\nthe baselines and is able to avoid obstacles up to four times as large."
                },
                "authors": [
                    {
                        "name": "Reece O'Mahoney"
                    },
                    {
                        "name": "Wanming Yu"
                    },
                    {
                        "name": "Ioannis Havoutis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Havoutis"
                },
                "author": "Ioannis Havoutis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18052v2",
                "updated": "2025-06-03T16:42:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    42,
                    52,
                    1,
                    154,
                    0
                ],
                "published": "2025-03-23T12:50:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    12,
                    50,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "SceneSplat: Gaussian Splatting-based Scene Understanding with\n  Vision-Language Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneSplat: Gaussian Splatting-based Scene Understanding with\n  Vision-Language Pretraining"
                },
                "summary": "Recognizing arbitrary or previously unseen categories is essential for\ncomprehensive real-world 3D scene understanding. Currently, all existing\nmethods rely on 2D or textual modalities during training or together at\ninference. This highlights the clear absence of a model capable of processing\n3D data alone for learning semantics end-to-end, along with the necessary data\nto train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as\nthe de facto standard for 3D scene representation across various vision tasks.\nHowever, effectively integrating semantic reasoning into 3DGS in a\ngeneralizable manner remains an open challenge. To address these limitations,\nwe introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene\nunderstanding approach that operates natively on 3DGS. Furthermore, we propose\na self-supervised learning scheme that unlocks rich 3D feature learning from\nunlabeled scenes. To power the proposed methods, we introduce SceneSplat-7K,\nthe first large-scale 3DGS dataset for indoor scenes, comprising 7916 scenes\nderived from seven established datasets, such as ScanNet and Matterport3D.\nGenerating SceneSplat-7K required computational resources equivalent to 150 GPU\ndays on an L4 GPU, enabling standardized benchmarking for 3DGS-based reasoning\nfor indoor scenes. Our exhaustive experiments on SceneSplat-7K demonstrate the\nsignificant benefit of the proposed method over the established baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing arbitrary or previously unseen categories is essential for\ncomprehensive real-world 3D scene understanding. Currently, all existing\nmethods rely on 2D or textual modalities during training or together at\ninference. This highlights the clear absence of a model capable of processing\n3D data alone for learning semantics end-to-end, along with the necessary data\nto train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as\nthe de facto standard for 3D scene representation across various vision tasks.\nHowever, effectively integrating semantic reasoning into 3DGS in a\ngeneralizable manner remains an open challenge. To address these limitations,\nwe introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene\nunderstanding approach that operates natively on 3DGS. Furthermore, we propose\na self-supervised learning scheme that unlocks rich 3D feature learning from\nunlabeled scenes. To power the proposed methods, we introduce SceneSplat-7K,\nthe first large-scale 3DGS dataset for indoor scenes, comprising 7916 scenes\nderived from seven established datasets, such as ScanNet and Matterport3D.\nGenerating SceneSplat-7K required computational resources equivalent to 150 GPU\ndays on an L4 GPU, enabling standardized benchmarking for 3DGS-based reasoning\nfor indoor scenes. Our exhaustive experiments on SceneSplat-7K demonstrate the\nsignificant benefit of the proposed method over the established baselines."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Qi Ma"
                    },
                    {
                        "name": "Runyi Yang"
                    },
                    {
                        "name": "Huapeng Li"
                    },
                    {
                        "name": "Mengjiao Ma"
                    },
                    {
                        "name": "Bin Ren"
                    },
                    {
                        "name": "Nikola Popovic"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Ender Konukoglu"
                    },
                    {
                        "name": "Theo Gevers"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Martin R. Oswald"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    }
                ],
                "author_detail": {
                    "name": "Danda Pani Paudel"
                },
                "author": "Danda Pani Paudel",
                "arxiv_comment": "Our code, model, and dataset will be released at\n  https://unique1i.github.io/SceneSplat_webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03065v1",
                "updated": "2025-06-03T16:42:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    42,
                    37,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:42:37Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    42,
                    37,
                    1,
                    154,
                    0
                ],
                "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers"
                },
                "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical\nFLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$,\nand 1.58$\\times$, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical\nFLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$,\nand 1.58$\\times$, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03062v1",
                "updated": "2025-06-03T16:41:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    41,
                    11,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:41:11Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    41,
                    11,
                    1,
                    154,
                    0
                ],
                "title": "Multi-Metric Adaptive Experimental Design under Fixed Budget with\n  Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Metric Adaptive Experimental Design under Fixed Budget with\n  Validation"
                },
                "summary": "Standard A/B tests in online experiments face statistical power challenges\nwhen testing multiple candidates simultaneously, while adaptive experimental\ndesigns (AED) alone fall short in inferring experiment statistics such as the\naverage treatment effect, especially with many metrics (e.g., revenue, safety)\nand heterogeneous variances. This paper proposes a fixed-budget multi-metric\nAED framework with a two-phase structure: an adaptive exploration phase to\nidentify the best treatment, and a validation phase with an A/B test to verify\nthe treatment's quality and infer statistics. We propose SHRVar, which\ngeneralizes sequential halving (SH) (Karnin et al., 2013) with a novel\nrelative-variance-based sampling and an elimination strategy built on reward\nz-values. It achieves a provable error probability that decreases\nexponentially, where the exponent generalizes the complexity measure for SH\n(Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and\nheterogeneous variances, respectively. Numerical experiments verify our\nanalysis and demonstrate the superior performance of this new framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard A/B tests in online experiments face statistical power challenges\nwhen testing multiple candidates simultaneously, while adaptive experimental\ndesigns (AED) alone fall short in inferring experiment statistics such as the\naverage treatment effect, especially with many metrics (e.g., revenue, safety)\nand heterogeneous variances. This paper proposes a fixed-budget multi-metric\nAED framework with a two-phase structure: an adaptive exploration phase to\nidentify the best treatment, and a validation phase with an A/B test to verify\nthe treatment's quality and infer statistics. We propose SHRVar, which\ngeneralizes sequential halving (SH) (Karnin et al., 2013) with a novel\nrelative-variance-based sampling and an elimination strategy built on reward\nz-values. It achieves a provable error probability that decreases\nexponentially, where the exponent generalizes the complexity measure for SH\n(Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and\nheterogeneous variances, respectively. Numerical experiments verify our\nanalysis and demonstrate the superior performance of this new framework."
                },
                "authors": [
                    {
                        "name": "Qining Zhang"
                    },
                    {
                        "name": "Tanner Fiez"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Wenyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyang Liu"
                },
                "author": "Wenyang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03053v1",
                "updated": "2025-06-03T16:33:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    33,
                    47,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:33:47Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    33,
                    47,
                    1,
                    154,
                    0
                ],
                "title": "MAEBE: Multi-Agent Emergent Behavior Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAEBE: Multi-Agent Emergent Behavior Framework"
                },
                "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts."
                },
                "authors": [
                    {
                        "name": "Sinem Erisken"
                    },
                    {
                        "name": "Timothy Gothard"
                    },
                    {
                        "name": "Martin Leitgab"
                    },
                    {
                        "name": "Ram Potham"
                    }
                ],
                "author_detail": {
                    "name": "Ram Potham"
                },
                "arxiv_affiliation": "Independent Researcher",
                "author": "Ram Potham",
                "arxiv_comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03051v1",
                "updated": "2025-06-03T16:31:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    31,
                    52,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:31:52Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    31,
                    52,
                    1,
                    154,
                    0
                ],
                "title": "Facts Do Care About Your Language: Assessing Answer Quality of\n  Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facts Do Care About Your Language: Assessing Answer Quality of\n  Multilingual LLMs"
                },
                "summary": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages."
                },
                "authors": [
                    {
                        "name": "Yuval Kansal"
                    },
                    {
                        "name": "Shmuel Berman"
                    },
                    {
                        "name": "Lydia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Liu"
                },
                "author": "Lydia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03050v1",
                "updated": "2025-06-03T16:30:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    30,
                    45,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:30:45Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    30,
                    45,
                    1,
                    154,
                    0
                ],
                "title": "An IPCW Adjusted Win Statistics Approach in Clinical Trials\n  Incorporating Equivalence Margins to Define Ties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An IPCW Adjusted Win Statistics Approach in Clinical Trials\n  Incorporating Equivalence Margins to Define Ties"
                },
                "summary": "In clinical trials, multiple outcomes of different priorities commonly occur\nas the patient's response may not be adequately characterized by a single\noutcome. Win statistics are appealing summary measures for between-group\ndifference at more than one endpoint. When defining the result of pairwise\ncomparisons of a time-to-event endpoint, it is desirable to allow ties to\naccount for incomplete follow-up and not clinically meaningful difference in\nendpoints of interest. In this paper, we propose a class of win statistics for\ntime-to-event endpoints with a user-specified equivalence margin. These win\nstatistics are identifiable in the presence of right-censoring and do not\ndepend on the censoring distribution. We then develop estimation and inference\nprocedures for the proposed win statistics based on\ninverse-probability-of-censoring {weighting} (IPCW) adjustment to handle\nright-censoring. We conduct extensive simulations to investigate the\noperational characteristics of the proposed procedure in the finite sample\nsetting. A real oncology trial is used to illustrate the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In clinical trials, multiple outcomes of different priorities commonly occur\nas the patient's response may not be adequately characterized by a single\noutcome. Win statistics are appealing summary measures for between-group\ndifference at more than one endpoint. When defining the result of pairwise\ncomparisons of a time-to-event endpoint, it is desirable to allow ties to\naccount for incomplete follow-up and not clinically meaningful difference in\nendpoints of interest. In this paper, we propose a class of win statistics for\ntime-to-event endpoints with a user-specified equivalence margin. These win\nstatistics are identifiable in the presence of right-censoring and do not\ndepend on the censoring distribution. We then develop estimation and inference\nprocedures for the proposed win statistics based on\ninverse-probability-of-censoring {weighting} (IPCW) adjustment to handle\nright-censoring. We conduct extensive simulations to investigate the\noperational characteristics of the proposed procedure in the finite sample\nsetting. A real oncology trial is used to illustrate the proposed approach."
                },
                "authors": [
                    {
                        "name": "Ying Cui"
                    },
                    {
                        "name": "Bo Huang"
                    },
                    {
                        "name": "Gaohong Dong"
                    },
                    {
                        "name": "Ryuji Uozumi"
                    },
                    {
                        "name": "Lu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lu Tian"
                },
                "author": "Lu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10633v2",
                "updated": "2025-06-03T16:28:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    28,
                    7,
                    1,
                    154,
                    0
                ],
                "published": "2025-03-13T17:59:53Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    53,
                    3,
                    72,
                    0
                ],
                "title": "We Should Chart an Atlas of All the World's Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We Should Chart an Atlas of All the World's Models"
                },
                "summary": "Public model repositories now contain millions of models, yet most models\nremain undocumented and effectively lost. In this position paper, we advocate\nfor charting the world's model population in a unified structure we call the\nModel Atlas: a graph that captures models, their attributes, and the weight\ntransformations that connect them. The Model Atlas enables applications in\nmodel forensics, meta-ML research, and model discovery, challenging tasks given\ntoday's unstructured model repositories. However, because most models lack\ndocumentation, large atlas regions remain uncharted. Addressing this gap\nmotivates new machine learning methods that treat models themselves as data,\ninferring properties such as functionality, performance, and lineage directly\nfrom their weights. We argue that a scalable path forward is to bypass the\nunique parameter symmetries that plague model weights. Charting all the world's\nmodels will require a community effort, and we hope its broad utility will\nrally researchers toward this goal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public model repositories now contain millions of models, yet most models\nremain undocumented and effectively lost. In this position paper, we advocate\nfor charting the world's model population in a unified structure we call the\nModel Atlas: a graph that captures models, their attributes, and the weight\ntransformations that connect them. The Model Atlas enables applications in\nmodel forensics, meta-ML research, and model discovery, challenging tasks given\ntoday's unstructured model repositories. However, because most models lack\ndocumentation, large atlas regions remain uncharted. Addressing this gap\nmotivates new machine learning methods that treat models themselves as data,\ninferring properties such as functionality, performance, and lineage directly\nfrom their weights. We argue that a scalable path forward is to bypass the\nunique parameter symmetries that plague model weights. Charting all the world's\nmodels will require a community effort, and we hope its broad utility will\nrally researchers toward this goal."
                },
                "authors": [
                    {
                        "name": "Eliahu Horwitz"
                    },
                    {
                        "name": "Nitzan Kurer"
                    },
                    {
                        "name": "Jonathan Kahana"
                    },
                    {
                        "name": "Liel Amar"
                    },
                    {
                        "name": "Yedid Hoshen"
                    }
                ],
                "author_detail": {
                    "name": "Yedid Hoshen"
                },
                "author": "Yedid Hoshen",
                "arxiv_comment": "Project page: https://horwitz.ai/model-atlas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09708v3",
                "updated": "2025-06-03T16:25:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    25,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-14T18:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    18,
                    9,
                    41,
                    2,
                    134,
                    0
                ],
                "title": "The uncertainty of magnetic fields in 3D non-local thermodynamic\n  equilibrium inversions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The uncertainty of magnetic fields in 3D non-local thermodynamic\n  equilibrium inversions"
                },
                "summary": "We describe our approach to solve the problem of ensuring the solenoidality\nof the magnetic field vector in three-dimensional (3D) inversions, as well as\nthe estimation of the uncertainty in the inferred magnetic field. The\nsolenoidality of the magnetic field vector is often disregarded in the\ninversion of spectropolarimetric data due to limitations in the traditional\none-dimensional inversion techniques. We propose a method to ensure the\nsolenoidal condition in 3D inversions based on our meshfree approach. The\nincrease in dimensionality with respect to the 1D inversion techniques is such\nthat some of the traditional methods to determine the uncertainties become\nunfeasible. We propose a method based on a Monte Carlo approach to determine\nthe uncertainty of the magnetic field inference. Due to the physics of the\nproblem, we can compute the uncertainty increasing the total required\ncomputational time by just a factor of about two. We also propose a metric to\nquantify the uncertainty to describe the degree of confidence of the magnetic\nfield inference. Finally, we perform a numerical experiment to demonstrate the\nfeasibility of both the method and the metric proposed to quantify the\nuncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe our approach to solve the problem of ensuring the solenoidality\nof the magnetic field vector in three-dimensional (3D) inversions, as well as\nthe estimation of the uncertainty in the inferred magnetic field. The\nsolenoidality of the magnetic field vector is often disregarded in the\ninversion of spectropolarimetric data due to limitations in the traditional\none-dimensional inversion techniques. We propose a method to ensure the\nsolenoidal condition in 3D inversions based on our meshfree approach. The\nincrease in dimensionality with respect to the 1D inversion techniques is such\nthat some of the traditional methods to determine the uncertainties become\nunfeasible. We propose a method based on a Monte Carlo approach to determine\nthe uncertainty of the magnetic field inference. Due to the physics of the\nproblem, we can compute the uncertainty increasing the total required\ncomputational time by just a factor of about two. We also propose a metric to\nquantify the uncertainty to describe the degree of confidence of the magnetic\nfield inference. Finally, we perform a numerical experiment to demonstrate the\nfeasibility of both the method and the metric proposed to quantify the\nuncertainty."
                },
                "authors": [
                    {
                        "name": "Jiri Stepan"
                    },
                    {
                        "name": "Tanausu del Pino Aleman"
                    },
                    {
                        "name": "Andres Vicente Arevalo"
                    }
                ],
                "author_detail": {
                    "name": "Andres Vicente Arevalo"
                },
                "author": "Andres Vicente Arevalo",
                "arxiv_comment": "9 pages, 7 figures, accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05024v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05024v4",
                "updated": "2025-06-03T16:22:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    22,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2025-03-06T22:48:55Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    22,
                    48,
                    55,
                    3,
                    65,
                    0
                ],
                "title": "Kernel-based estimators for functional causal effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kernel-based estimators for functional causal effects"
                },
                "summary": "We propose causal effect estimators based on empirical Fr\\'{e}chet means and\noperator-valued kernels, tailored to functional data spaces. These methods\naddress the challenges of high-dimensionality, sequential ordering, and model\ncomplexity while preserving robustness to treatment misspecification. Using\nstructural assumptions, we obtain compact representations of potential\noutcomes, enabling scalable estimation of causal effects over time and across\ncovariates. We provide both theoretical, regarding the consistency of\nfunctional causal effects, as well as empirical comparison of a range of\nproposed causal effect estimators.\n  Applications to binary treatment settings with functional outcomes illustrate\nthe framework's utility in biomedical monitoring, where outcomes exhibit\ncomplex temporal dynamics. Our estimators accommodate scenarios with registered\ncovariates and outcomes, aligning them to the Fr\\'{e}chet means, as well as\ncases requiring higher-order representations to capture intricate\ncovariate-outcome interactions. These advancements extend causal inference to\ndynamic and non-linear domains, offering new tools for understanding complex\ntreatment effects in functional data settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose causal effect estimators based on empirical Fr\\'{e}chet means and\noperator-valued kernels, tailored to functional data spaces. These methods\naddress the challenges of high-dimensionality, sequential ordering, and model\ncomplexity while preserving robustness to treatment misspecification. Using\nstructural assumptions, we obtain compact representations of potential\noutcomes, enabling scalable estimation of causal effects over time and across\ncovariates. We provide both theoretical, regarding the consistency of\nfunctional causal effects, as well as empirical comparison of a range of\nproposed causal effect estimators.\n  Applications to binary treatment settings with functional outcomes illustrate\nthe framework's utility in biomedical monitoring, where outcomes exhibit\ncomplex temporal dynamics. Our estimators accommodate scenarios with registered\ncovariates and outcomes, aligning them to the Fr\\'{e}chet means, as well as\ncases requiring higher-order representations to capture intricate\ncovariate-outcome interactions. These advancements extend causal inference to\ndynamic and non-linear domains, offering new tools for understanding complex\ntreatment effects in functional data settings."
                },
                "authors": [
                    {
                        "name": "Yordan P. Raykov"
                    },
                    {
                        "name": "Hengrui Luo"
                    },
                    {
                        "name": "Justin D. Strait"
                    },
                    {
                        "name": "Wasiur R. KhudaBukhsh"
                    }
                ],
                "author_detail": {
                    "name": "Wasiur R. KhudaBukhsh"
                },
                "author": "Wasiur R. KhudaBukhsh",
                "arxiv_comment": "Code is available at\n  https://github.com/JordanRaykov/Kernel-based-estimators-for-Functional-Causal-Effects",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05024v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05024v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03038v1",
                "updated": "2025-06-03T16:20:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    20,
                    47,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:20:47Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    20,
                    47,
                    1,
                    154,
                    0
                ],
                "title": "Towards Analyzing and Understanding the Limitations of VAPO: A\n  Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Analyzing and Understanding the Limitations of VAPO: A\n  Theoretical Perspective"
                },
                "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents."
                },
                "authors": [
                    {
                        "name": "Jintian Shao"
                    },
                    {
                        "name": "Yiming Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Cheng"
                },
                "author": "Yiming Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03037v1",
                "updated": "2025-06-03T16:19:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    19,
                    59,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:19:59Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    19,
                    59,
                    1,
                    154,
                    0
                ],
                "title": "On the Need to Align Intent and Implementation in Uncertainty\n  Quantification for Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Need to Align Intent and Implementation in Uncertainty\n  Quantification for Machine Learning"
                },
                "summary": "Quantifying uncertainties for machine learning (ML) models is a foundational\nchallenge in modern data analysis. This challenge is compounded by at least two\nkey aspects of the field: (a) inconsistent terminology surrounding uncertainty\nand estimation across disciplines, and (b) the varying technical requirements\nfor establishing trustworthy uncertainties in diverse problem contexts. In this\nposition paper, we aim to clarify the depth of these challenges by identifying\nthese inconsistencies and articulating how different contexts impose distinct\nepistemic demands. We examine the current landscape of estimation targets\n(e.g., prediction, inference, simulation-based inference), uncertainty\nconstructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to\nmap between them. Drawing on the literature, we highlight and explain examples\nof problematic mappings. To help address these issues, we advocate for\nstandards that promote alignment between the \\textit{intent} and\n\\textit{implementation} of uncertainty quantification (UQ) approaches. We\ndiscuss several axes of trustworthiness that are necessary (if not sufficient)\nfor reliable UQ in ML models, and show how these axes can inform the design and\nevaluation of uncertainty-aware ML systems. Our practical recommendations focus\non scientific ML, offering illustrative cases and use scenarios, particularly\nin the context of simulation-based inference (SBI).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying uncertainties for machine learning (ML) models is a foundational\nchallenge in modern data analysis. This challenge is compounded by at least two\nkey aspects of the field: (a) inconsistent terminology surrounding uncertainty\nand estimation across disciplines, and (b) the varying technical requirements\nfor establishing trustworthy uncertainties in diverse problem contexts. In this\nposition paper, we aim to clarify the depth of these challenges by identifying\nthese inconsistencies and articulating how different contexts impose distinct\nepistemic demands. We examine the current landscape of estimation targets\n(e.g., prediction, inference, simulation-based inference), uncertainty\nconstructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to\nmap between them. Drawing on the literature, we highlight and explain examples\nof problematic mappings. To help address these issues, we advocate for\nstandards that promote alignment between the \\textit{intent} and\n\\textit{implementation} of uncertainty quantification (UQ) approaches. We\ndiscuss several axes of trustworthiness that are necessary (if not sufficient)\nfor reliable UQ in ML models, and show how these axes can inform the design and\nevaluation of uncertainty-aware ML systems. Our practical recommendations focus\non scientific ML, offering illustrative cases and use scenarios, particularly\nin the context of simulation-based inference (SBI)."
                },
                "authors": [
                    {
                        "name": "Shubhendu Trivedi"
                    },
                    {
                        "name": "Brian D. Nord"
                    }
                ],
                "author_detail": {
                    "name": "Brian D. Nord"
                },
                "author": "Brian D. Nord",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03035v1",
                "updated": "2025-06-03T16:18:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    18,
                    45,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:18:45Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    18,
                    45,
                    1,
                    154,
                    0
                ],
                "title": "Leveraging Information Retrieval to Enhance Spoken Language\n  Understanding Prompts in Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Information Retrieval to Enhance Spoken Language\n  Understanding Prompts in Few-Shot Learning"
                },
                "summary": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length."
                },
                "authors": [
                    {
                        "name": "Pierre Lepagnol"
                    },
                    {
                        "name": "Sahar Ghannay"
                    },
                    {
                        "name": "Thomas Gerald"
                    },
                    {
                        "name": "Christophe Servan"
                    },
                    {
                        "name": "Sophie Rosset"
                    }
                ],
                "author_detail": {
                    "name": "Sophie Rosset"
                },
                "author": "Sophie Rosset",
                "arxiv_comment": "Conference paper accepted to INTERSPEECH 2025",
                "arxiv_journal_ref": "INTERSPEECH 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03033v1",
                "updated": "2025-06-03T16:16:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    16,
                    59,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:16:59Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    16,
                    59,
                    1,
                    154,
                    0
                ],
                "title": "Impact of Accretion Assumptions on Pulse Profile Modelling of Superburst\n  Oscillations in 4U 1636-536",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Accretion Assumptions on Pulse Profile Modelling of Superburst\n  Oscillations in 4U 1636-536"
                },
                "summary": "Modelling the coherent pulsations observed during thermonuclear bursts offers\na valuable method to probe the poorly understood equation of state of dense and\ncold matter. Here we apply the pulse profile modelling technique to the\npulsations observed with RXTE during the 2001 superburst of 4U 1636$-$536. By\nemploying a single, uniform-temperature hot spot model with varying size and\ntemperature, along with various assumptions for background/accretion\ncontribution, we find that each assumption leads to different inferred mass,\nradius, and compactness constraints. This highlights the critical need to\nbetter understand the mass accretion rate enhancement/reduction during\nthermonuclear bursts to accurately model burst oscillation sources using pulse\nprofile modelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the coherent pulsations observed during thermonuclear bursts offers\na valuable method to probe the poorly understood equation of state of dense and\ncold matter. Here we apply the pulse profile modelling technique to the\npulsations observed with RXTE during the 2001 superburst of 4U 1636$-$536. By\nemploying a single, uniform-temperature hot spot model with varying size and\ntemperature, along with various assumptions for background/accretion\ncontribution, we find that each assumption leads to different inferred mass,\nradius, and compactness constraints. This highlights the critical need to\nbetter understand the mass accretion rate enhancement/reduction during\nthermonuclear bursts to accurately model burst oscillation sources using pulse\nprofile modelling."
                },
                "authors": [
                    {
                        "name": "Yves Kini"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Anna Bilous"
                    },
                    {
                        "name": "Serena Vinciguerra"
                    },
                    {
                        "name": "Sebastien Guillot"
                    },
                    {
                        "name": "David R. Ballantyne"
                    },
                    {
                        "name": "Erik Kuulkers"
                    },
                    {
                        "name": "Slavko Bogdanov"
                    },
                    {
                        "name": "Valery Suleimanov"
                    }
                ],
                "author_detail": {
                    "name": "Valery Suleimanov"
                },
                "author": "Valery Suleimanov",
                "arxiv_comment": "Accepted for publication in MNRAS. The Zenodo link will be made\n  public after the paper is published",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01009v2",
                "updated": "2025-06-03T16:15:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    15,
                    3,
                    1,
                    154,
                    0
                ],
                "published": "2025-01-02T02:14:38Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    14,
                    38,
                    3,
                    2,
                    0
                ],
                "title": "Detection of \"diffuse\" coronal He I 1083 during the April 8 2024 Solar\n  Eclipse: evidence for terrestrial atmospheric scattering origin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of \"diffuse\" coronal He I 1083 during the April 8 2024 Solar\n  Eclipse: evidence for terrestrial atmospheric scattering origin"
                },
                "summary": "Strong He I 1083 nm atomic line signals have been previously measured during\ntotal solar eclipses at coronal heights above the lunar limb. This rather\nunexpected measurement has kindled a discussion about the hypothesized presence\nof significant amounts of neutral helium at coronal conditions. We performed\nspectroscopic observations of the He I 1083 nm spectroscopic region with the\nnewly built CHEESE instrument during the April 8th 2024 total solar eclipse to\ntest the presence of He I 1083 in the solar corona. We detected the He I 1083,\nthe forbidden coronal line Fe XIII 1074.7 nm, as well as the chromospheric H I\n1093.8 nm Paschen-{\\gamma} line in our eclipse observations. The chromospheric\nHe I 1083 and H I 1093.8 nm Paschen-{\\gamma} lines are detected in the corona\nas well as on the lunar disc. Our findings point toward a non-solar origin of\nthe He I 1083 signal during the April 8th 2024 eclipse that challenge the\nnotion of abundant neutral helium in the solar corona inferred from eclipse\nobservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong He I 1083 nm atomic line signals have been previously measured during\ntotal solar eclipses at coronal heights above the lunar limb. This rather\nunexpected measurement has kindled a discussion about the hypothesized presence\nof significant amounts of neutral helium at coronal conditions. We performed\nspectroscopic observations of the He I 1083 nm spectroscopic region with the\nnewly built CHEESE instrument during the April 8th 2024 total solar eclipse to\ntest the presence of He I 1083 in the solar corona. We detected the He I 1083,\nthe forbidden coronal line Fe XIII 1074.7 nm, as well as the chromospheric H I\n1093.8 nm Paschen-{\\gamma} line in our eclipse observations. The chromospheric\nHe I 1083 and H I 1093.8 nm Paschen-{\\gamma} lines are detected in the corona\nas well as on the lunar disc. Our findings point toward a non-solar origin of\nthe He I 1083 signal during the April 8th 2024 eclipse that challenge the\nnotion of abundant neutral helium in the solar corona inferred from eclipse\nobservations."
                },
                "authors": [
                    {
                        "name": "M. E. Molnar"
                    },
                    {
                        "name": "R. Casini"
                    },
                    {
                        "name": "P. Bryans"
                    },
                    {
                        "name": "B. Berkey"
                    },
                    {
                        "name": "K. Tyson"
                    }
                ],
                "author_detail": {
                    "name": "K. Tyson"
                },
                "author": "K. Tyson",
                "arxiv_comment": "Accepted in Solar Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14545v3",
                "updated": "2025-06-03T16:10:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    10,
                    16,
                    1,
                    154,
                    0
                ],
                "published": "2024-06-20T17:54:33Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    54,
                    33,
                    3,
                    172,
                    0
                ],
                "title": "Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference\n  Attacks in Text-to-SQL Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference\n  Attacks in Text-to-SQL Systems"
                },
                "summary": "Text-to-SQL systems empower users to interact with databases using natural\nlanguage, automatically translating queries into executable SQL code. However,\ntheir reliance on database schema information for SQL generation exposes them\nto significant security vulnerabilities, particularly schema inference attacks\nthat can lead to unauthorized data access or manipulation. In this paper, we\nintroduce a novel zero-knowledge framework for reconstructing the underlying\ndatabase schema of text-to-SQL models without any prior knowledge of the\ndatabase. Our approach systematically probes text-to-SQL models with specially\ncrafted questions and leverages a surrogate GPT-4 model to interpret the\noutputs, effectively uncovering hidden schema elements -- including tables,\ncolumns, and data types. We demonstrate that our method achieves high accuracy\nin reconstructing table names, with F1 scores of up to .99 for generative\nmodels and .78 for fine-tuned models, underscoring the severity of schema\nleakage risks. We also show that our attack can steal prompt information in\nnon-text-to-SQL models. Furthermore, we propose a simple protection mechanism\nfor generative models and empirically show its limitations in mitigating these\nattacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems empower users to interact with databases using natural\nlanguage, automatically translating queries into executable SQL code. However,\ntheir reliance on database schema information for SQL generation exposes them\nto significant security vulnerabilities, particularly schema inference attacks\nthat can lead to unauthorized data access or manipulation. In this paper, we\nintroduce a novel zero-knowledge framework for reconstructing the underlying\ndatabase schema of text-to-SQL models without any prior knowledge of the\ndatabase. Our approach systematically probes text-to-SQL models with specially\ncrafted questions and leverages a surrogate GPT-4 model to interpret the\noutputs, effectively uncovering hidden schema elements -- including tables,\ncolumns, and data types. We demonstrate that our method achieves high accuracy\nin reconstructing table names, with F1 scores of up to .99 for generative\nmodels and .78 for fine-tuned models, underscoring the severity of schema\nleakage risks. We also show that our attack can steal prompt information in\nnon-text-to-SQL models. Furthermore, we propose a simple protection mechanism\nfor generative models and empirically show its limitations in mitigating these\nattacks."
                },
                "authors": [
                    {
                        "name": "ore Klisura"
                    },
                    {
                        "name": "Anthony Rios"
                    }
                ],
                "author_detail": {
                    "name": "Anthony Rios"
                },
                "author": "Anthony Rios",
                "arxiv_comment": "Accepted to NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20704v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20704v4",
                "updated": "2025-06-03T16:08:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    8,
                    32,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-28T04:25:42Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    4,
                    25,
                    42,
                    4,
                    59,
                    0
                ],
                "title": "Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff"
                },
                "summary": "Speculative Decoding (SD) enforces strict distributional equivalence to the\ntarget model when accepting candidate tokens. While it maintains the target\nmodel's generation quality, this strict equivalence limits the speedup\nachievable by SD and prevents users from trading deviations from the target\ndistribution in exchange for further inference speed gains. To address these\nlimitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding\nalgorithm that generalizes SD by accepting candidate tokens based on the\ndivergences between the target and draft model distributions. By allowing for\ncontrolled divergence from the target model, FSD enables users to flexibly\ntrade generation quality for inference speed. Across several benchmarks, our\nmethod is able to achieve significant runtime improvements of over 5 tokens per\nsecond faster than SD at only an approximate 2% absolute reduction in benchmark\naccuracy. In many cases, FSD is even able to match SD benchmark accuracy at\nover 2 tokens per second faster, demonstrating that distributional equivalence\nis not necessary to maintain target model performance. Furthermore, FSD can be\nseamlessly integrated into existing SD extensions; we demonstrate this by\napplying FSD to EAGLE-2, greatly enhancing this existing extension's efficiency\nwhile allowing it to leverage FSD's tunable quality-speed trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding (SD) enforces strict distributional equivalence to the\ntarget model when accepting candidate tokens. While it maintains the target\nmodel's generation quality, this strict equivalence limits the speedup\nachievable by SD and prevents users from trading deviations from the target\ndistribution in exchange for further inference speed gains. To address these\nlimitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding\nalgorithm that generalizes SD by accepting candidate tokens based on the\ndivergences between the target and draft model distributions. By allowing for\ncontrolled divergence from the target model, FSD enables users to flexibly\ntrade generation quality for inference speed. Across several benchmarks, our\nmethod is able to achieve significant runtime improvements of over 5 tokens per\nsecond faster than SD at only an approximate 2% absolute reduction in benchmark\naccuracy. In many cases, FSD is even able to match SD benchmark accuracy at\nover 2 tokens per second faster, demonstrating that distributional equivalence\nis not necessary to maintain target model performance. Furthermore, FSD can be\nseamlessly integrated into existing SD extensions; we demonstrate this by\napplying FSD to EAGLE-2, greatly enhancing this existing extension's efficiency\nwhile allowing it to leverage FSD's tunable quality-speed trade-off."
                },
                "authors": [
                    {
                        "name": "Maximilian Holsman"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20704v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20704v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.08586v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.08586v3",
                "updated": "2025-06-03T16:07:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    7,
                    58,
                    1,
                    154,
                    0
                ],
                "published": "2023-06-14T15:47:52Z",
                "published_parsed": [
                    2023,
                    6,
                    14,
                    15,
                    47,
                    52,
                    2,
                    165,
                    0
                ],
                "title": "Learning to Specialize: Joint Gating-Expert Training for Adaptive MoEs\n  in Decentralized Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Specialize: Joint Gating-Expert Training for Adaptive MoEs\n  in Decentralized Settings"
                },
                "summary": "Mixture-of-Experts (MoEs) achieve scalability by dynamically activating\nsubsets of their components. Yet, understanding how expertise emerges through\njoint training of gating mechanisms and experts remains incomplete, especially\nin scenarios without clear task partitions. Motivated by inference costs and\ndata heterogeneity, we study how joint training of gating functions and experts\ncan dynamically allocate domain-specific expertise across multiple underlying\ndata distributions. As an outcome of our framework, we develop an instance\ntailored specifically to decentralized training scenarios, introducing\n\\textit{Dynamically Decentralized Orchestration of MoEs} or \\texttt{DDOME}.\n\\texttt{DDOME} leverages heterogeneity emerging from distributional shifts\nacross decentralized data sources to specialize experts dynamically. By\nintegrating a pretrained common expert to inform a gating function,\n\\texttt{DDOME} achieves personalized expert subset selection on-the-fly,\nfacilitating just-in-time personalization. We empirically validate\n\\texttt{DDOME} within a Federated Learning (FL) context: \\texttt{DDOME} attains\nfrom 4\\% up to an 24\\% accuracy improvement over state-of-the-art FL baselines\nin image and text classification tasks, while maintaining competitive zero-shot\ngeneralization capabilities. Furthermore, we provide theoretical insights\nconfirming that the joint gating-experts training is critical for achieving\nmeaningful expert specialization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoEs) achieve scalability by dynamically activating\nsubsets of their components. Yet, understanding how expertise emerges through\njoint training of gating mechanisms and experts remains incomplete, especially\nin scenarios without clear task partitions. Motivated by inference costs and\ndata heterogeneity, we study how joint training of gating functions and experts\ncan dynamically allocate domain-specific expertise across multiple underlying\ndata distributions. As an outcome of our framework, we develop an instance\ntailored specifically to decentralized training scenarios, introducing\n\\textit{Dynamically Decentralized Orchestration of MoEs} or \\texttt{DDOME}.\n\\texttt{DDOME} leverages heterogeneity emerging from distributional shifts\nacross decentralized data sources to specialize experts dynamically. By\nintegrating a pretrained common expert to inform a gating function,\n\\texttt{DDOME} achieves personalized expert subset selection on-the-fly,\nfacilitating just-in-time personalization. We empirically validate\n\\texttt{DDOME} within a Federated Learning (FL) context: \\texttt{DDOME} attains\nfrom 4\\% up to an 24\\% accuracy improvement over state-of-the-art FL baselines\nin image and text classification tasks, while maintaining competitive zero-shot\ngeneralization capabilities. Furthermore, we provide theoretical insights\nconfirming that the joint gating-experts training is critical for achieving\nmeaningful expert specialization."
                },
                "authors": [
                    {
                        "name": "Yehya Farhat"
                    },
                    {
                        "name": "Hamza ElMokhtar Shili"
                    },
                    {
                        "name": "Fangshuo Liao"
                    },
                    {
                        "name": "Chen Dun"
                    },
                    {
                        "name": "Mirian Hipolito Garcia"
                    },
                    {
                        "name": "Guoqing Zheng"
                    },
                    {
                        "name": "Ahmed Hassan Awadallah"
                    },
                    {
                        "name": "Robert Sim"
                    },
                    {
                        "name": "Dimitrios Dimitriadis"
                    },
                    {
                        "name": "Anastasios Kyrillidis"
                    }
                ],
                "author_detail": {
                    "name": "Anastasios Kyrillidis"
                },
                "author": "Anastasios Kyrillidis",
                "arxiv_comment": "26 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.08586v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.08586v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03032v1",
                "updated": "2025-06-03T16:07:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    7,
                    54,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:07:54Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    7,
                    54,
                    1,
                    154,
                    0
                ],
                "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment"
                },
                "summary": "Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions."
                },
                "authors": [
                    {
                        "name": "Junhao Yu"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "YuXuan Sun"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "24 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17538v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17538v6",
                "updated": "2025-06-03T16:03:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    3,
                    24,
                    1,
                    154,
                    0
                ],
                "published": "2024-09-26T04:56:49Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    56,
                    49,
                    3,
                    270,
                    0
                ],
                "title": "Low-Rank Adaptation Secretly Imitates Differentially Private SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation Secretly Imitates Differentially Private SGD"
                },
                "summary": "As pre-trained language models grow in size, full fine-tuning their\nparameters on task adaptation data becomes increasingly impractical. To address\nthis challenge, some methods for low-rank adaptation of language models have\nbeen proposed, e.g. LoRA, which incorporates trainable low-rank decomposition\nmatrices into only some parameters of the pre-trained model, called adapters.\nThis approach significantly reduces the number of trainable parameters compared\nto fine-tuning all parameters or adapters. In this work, we look at low-rank\nadaptation method from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA is equivalent to fine-tuning adapters with\nnoisy batch gradients - just like what DPSGD algorithm does. We also quantify\nthe variance of the injected noise as a decreasing function of adaptation rank.\nBy establishing a Berry-Esseen type bound on the total variation distance\nbetween the injected noise distribution and a Gaussian noise distribution with\nthe same variance, we show that the dynamics of low-rank adaptation is very\nclose to when DPSGD is performed w.r.t the adapters. Following our theoretical\nfindings and approved by our experimental results, we show that low-rank\nadaptation provides robustness to membership inference attacks w.r.t the\nfine-tuning data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As pre-trained language models grow in size, full fine-tuning their\nparameters on task adaptation data becomes increasingly impractical. To address\nthis challenge, some methods for low-rank adaptation of language models have\nbeen proposed, e.g. LoRA, which incorporates trainable low-rank decomposition\nmatrices into only some parameters of the pre-trained model, called adapters.\nThis approach significantly reduces the number of trainable parameters compared\nto fine-tuning all parameters or adapters. In this work, we look at low-rank\nadaptation method from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA is equivalent to fine-tuning adapters with\nnoisy batch gradients - just like what DPSGD algorithm does. We also quantify\nthe variance of the injected noise as a decreasing function of adaptation rank.\nBy establishing a Berry-Esseen type bound on the total variation distance\nbetween the injected noise distribution and a Gaussian noise distribution with\nthe same variance, we show that the dynamics of low-rank adaptation is very\nclose to when DPSGD is performed w.r.t the adapters. Following our theoretical\nfindings and approved by our experimental results, we show that low-rank\nadaptation provides robustness to membership inference attacks w.r.t the\nfine-tuning data."
                },
                "authors": [
                    {
                        "name": "Saber Malekmohammadi"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17538v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17538v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11824v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11824v3",
                "updated": "2025-06-03T16:01:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    1,
                    49,
                    1,
                    154,
                    0
                ],
                "published": "2024-11-18T18:44:00Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    44,
                    0,
                    0,
                    323,
                    0
                ],
                "title": "Theoretical Foundations of Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Foundations of Conformal Prediction"
                },
                "summary": "This book is about conformal prediction and related inferential techniques\nthat build on permutation tests and exchangeability. These techniques are\nuseful in a diverse array of tasks, including hypothesis testing and providing\nuncertainty quantification guarantees for machine learning systems. Much of the\ncurrent interest in conformal prediction is due to its ability to integrate\ninto complex machine learning workflows, solving the problem of forming\nprediction sets without any assumptions on the form of the data generating\ndistribution. Since contemporary machine learning algorithms have generally\nproven difficult to analyze directly, conformal prediction's main appeal is its\nability to provide formal, finite-sample guarantees when paired with such\nmethods.\n  The goal of this book is to teach the reader about the fundamental technical\narguments that arise when researching conformal prediction and related\nquestions in distribution-free inference. Many of these proof strategies,\nespecially the more recent ones, are scattered among research papers, making it\ndifficult for researchers to understand where to look, which results are\nimportant, and how exactly the proofs work. We hope to bridge this gap by\ncurating what we believe to be some of the most important results in the\nliterature and presenting their proofs in a unified language, with\nillustrations, and with an eye towards pedagogy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This book is about conformal prediction and related inferential techniques\nthat build on permutation tests and exchangeability. These techniques are\nuseful in a diverse array of tasks, including hypothesis testing and providing\nuncertainty quantification guarantees for machine learning systems. Much of the\ncurrent interest in conformal prediction is due to its ability to integrate\ninto complex machine learning workflows, solving the problem of forming\nprediction sets without any assumptions on the form of the data generating\ndistribution. Since contemporary machine learning algorithms have generally\nproven difficult to analyze directly, conformal prediction's main appeal is its\nability to provide formal, finite-sample guarantees when paired with such\nmethods.\n  The goal of this book is to teach the reader about the fundamental technical\narguments that arise when researching conformal prediction and related\nquestions in distribution-free inference. Many of these proof strategies,\nespecially the more recent ones, are scattered among research papers, making it\ndifficult for researchers to understand where to look, which results are\nimportant, and how exactly the proofs work. We hope to bridge this gap by\ncurating what we believe to be some of the most important results in the\nliterature and presenting their proofs in a unified language, with\nillustrations, and with an eye towards pedagogy."
                },
                "authors": [
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Rina Foygel Barber"
                    },
                    {
                        "name": "Stephen Bates"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Bates"
                },
                "author": "Stephen Bates",
                "arxiv_comment": "This material will be published by Cambridge University Press as\n  Theoretical Foundations of Conformal Prediction by Anastasios N.\n  Angelopoulos, Rina Foygel Barber, and Stephen Bates. This prepublication\n  version is free to view/download for personal use only. Not for\n  redistribution/resale/use in derivative works. Copyright Anastasios N.\n  Angelopoulos, Rina Foygel Barber, and Stephen Bates, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11824v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11824v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00077v2",
                "updated": "2025-06-03T16:01:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    1,
                    41,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-29T23:39:24Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    23,
                    39,
                    24,
                    3,
                    149,
                    0
                ],
                "title": "Gaussian mixture models as a proxy for interacting language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian mixture models as a proxy for interacting language models"
                },
                "summary": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions."
                },
                "authors": [
                    {
                        "name": "Edward L. Wang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Avanti Athreya"
                    },
                    {
                        "name": "Vince Lyzinski"
                    },
                    {
                        "name": "Carey E. Priebe"
                    }
                ],
                "author_detail": {
                    "name": "Carey E. Priebe"
                },
                "author": "Carey E. Priebe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62R07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03024v1",
                "updated": "2025-06-03T16:00:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    0,
                    30,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:00:30Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    0,
                    30,
                    1,
                    154,
                    0
                ],
                "title": "GenFair: Systematic Test Generation for Fairness Fault Detection in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenFair: Systematic Test Generation for Fairness Fault Detection in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in critical domains,\nyet they often exhibit biases inherited from training data, leading to fairness\nconcerns. This work focuses on the problem of effectively detecting fairness\nviolations, especially intersectional biases that are often missed by existing\ntemplate-based and grammar-based testing methods. Previous approaches, such as\nCheckList and ASTRAEA, provide structured or grammar-driven test generation but\nstruggle with low test diversity and limited sensitivity to complex demographic\ninteractions. To address these limitations, we propose GenFair, a metamorphic\nfairness testing framework that systematically generates source test cases\nusing equivalence partitioning, mutation operators, and boundary value\nanalysis. GenFair improves fairness testing by generating linguistically\ndiverse, realistic, and intersectional test cases. It applies metamorphic\nrelations (MR) to derive follow-up cases and detects fairness violations via\ntone-based comparisons between source and follow-up responses. In experiments\nwith GPT-4.0 and LLaMA-3.0, GenFair outperformed two baseline methods. It\nachieved a fault detection rate (FDR) of 0.73 (GPT-4.0) and 0.69 (LLaMA-3.0),\ncompared to 0.54/0.51 for template-based and 0.39/0.36 for ASTRAEA. GenFair\nalso showed the highest test case diversity (syntactic:10.06, semantic: 76.68)\nand strong coherence (syntactic: 291.32, semantic: 0.7043), outperforming both\nbaselines. These results demonstrate the effectiveness of GenFair in uncovering\nnuanced fairness violations. The proposed method offers a scalable and\nautomated solution for fairness testing and contributes to building more\nequitable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in critical domains,\nyet they often exhibit biases inherited from training data, leading to fairness\nconcerns. This work focuses on the problem of effectively detecting fairness\nviolations, especially intersectional biases that are often missed by existing\ntemplate-based and grammar-based testing methods. Previous approaches, such as\nCheckList and ASTRAEA, provide structured or grammar-driven test generation but\nstruggle with low test diversity and limited sensitivity to complex demographic\ninteractions. To address these limitations, we propose GenFair, a metamorphic\nfairness testing framework that systematically generates source test cases\nusing equivalence partitioning, mutation operators, and boundary value\nanalysis. GenFair improves fairness testing by generating linguistically\ndiverse, realistic, and intersectional test cases. It applies metamorphic\nrelations (MR) to derive follow-up cases and detects fairness violations via\ntone-based comparisons between source and follow-up responses. In experiments\nwith GPT-4.0 and LLaMA-3.0, GenFair outperformed two baseline methods. It\nachieved a fault detection rate (FDR) of 0.73 (GPT-4.0) and 0.69 (LLaMA-3.0),\ncompared to 0.54/0.51 for template-based and 0.39/0.36 for ASTRAEA. GenFair\nalso showed the highest test case diversity (syntactic:10.06, semantic: 76.68)\nand strong coherence (syntactic: 291.32, semantic: 0.7043), outperforming both\nbaselines. These results demonstrate the effectiveness of GenFair in uncovering\nnuanced fairness violations. The proposed method offers a scalable and\nautomated solution for fairness testing and contributes to building more\nequitable LLMs."
                },
                "authors": [
                    {
                        "name": "Madhusudan Srinivasan"
                    },
                    {
                        "name": "Jubril Abdel"
                    }
                ],
                "author_detail": {
                    "name": "Jubril Abdel"
                },
                "author": "Jubril Abdel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03023v1",
                "updated": "2025-06-03T15:59:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    59,
                    29,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:59:29Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    59,
                    29,
                    1,
                    154,
                    0
                ],
                "title": "TDCOSMO 2025: Cosmological constraints from strong lensing time delays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDCOSMO 2025: Cosmological constraints from strong lensing time delays"
                },
                "summary": "We present cosmological constraints from 8 strongly lensed quasars\n(hereafter, the TDCOSMO-2025 sample). Building on previous work, our analysis\nincorporated new deflector stellar velocity dispersions measured from spectra\nobtained with the James Webb Space Telescope (JWST), the Keck Telescopes, and\nthe Very Large Telescope (VLT), utilizing improved methods. We used integrated\nJWST stellar kinematics for 5 lenses, VLT-MUSE for 2, and resolved kinematics\nfrom Keck and JWST for RX J1131-1231. We also considered two samples of\nnon-time-delay lenses: 11 from the Sloan Lens ACS (SLACS) sample with Keck-KCWI\nresolved kinematics; and 4 from the Strong Lenses in the Legacy Survey (SL2S)\nsample. We improved our analysis of line-of-sight effects, the surface\nbrightness profile of the lens galaxies, and orbital anisotropy, and corrected\nfor projection effects in the dynamics. Our uncertainties are maximally\nconservative by accounting for the mass-sheet degeneracy in the deflectors'\nmass density profiles. The analysis was blinded to prevent experimenter bias.\nOur primary result is based on the TDCOSMO-2025 sample, in combination with\n$\\Omega_{\\rm m}$ constraints from the Pantheon+ Type Ia supernovae (SN)\ndataset. In the flat $\\Lambda$ Cold Dark Matter (CDM), we find\n$H_0=72.1^{+4.0}_{-3.7}$ km s$^{-1}$ Mpc$^{-1}$. The SLACS and SL2S samples are\nin excellent agreement with the TDCOSMO-2025 sample, improving the precision on\n$H_0$ in flat $\\Lambda$CDM to 4.4%. Using the Dark Energy Survey SN Year-5\ndataset (DES-SN5YR) or DESI-DR2 baryonic acoustic oscillations (BAO)\nlikelihoods instead of Pantheon+ yields very similar results. We also present\nconstraints in the open $\\Lambda$CDM, $w$CDM, $w_0w_a$CDM, and $w_{\\phi}$CDM\ncosmologies. The TDCOSMO $H_0$ inference is robust and consistent across all\npresented cosmological models, and our cosmological constraints in them agree\nwith those from the BAO and SN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present cosmological constraints from 8 strongly lensed quasars\n(hereafter, the TDCOSMO-2025 sample). Building on previous work, our analysis\nincorporated new deflector stellar velocity dispersions measured from spectra\nobtained with the James Webb Space Telescope (JWST), the Keck Telescopes, and\nthe Very Large Telescope (VLT), utilizing improved methods. We used integrated\nJWST stellar kinematics for 5 lenses, VLT-MUSE for 2, and resolved kinematics\nfrom Keck and JWST for RX J1131-1231. We also considered two samples of\nnon-time-delay lenses: 11 from the Sloan Lens ACS (SLACS) sample with Keck-KCWI\nresolved kinematics; and 4 from the Strong Lenses in the Legacy Survey (SL2S)\nsample. We improved our analysis of line-of-sight effects, the surface\nbrightness profile of the lens galaxies, and orbital anisotropy, and corrected\nfor projection effects in the dynamics. Our uncertainties are maximally\nconservative by accounting for the mass-sheet degeneracy in the deflectors'\nmass density profiles. The analysis was blinded to prevent experimenter bias.\nOur primary result is based on the TDCOSMO-2025 sample, in combination with\n$\\Omega_{\\rm m}$ constraints from the Pantheon+ Type Ia supernovae (SN)\ndataset. In the flat $\\Lambda$ Cold Dark Matter (CDM), we find\n$H_0=72.1^{+4.0}_{-3.7}$ km s$^{-1}$ Mpc$^{-1}$. The SLACS and SL2S samples are\nin excellent agreement with the TDCOSMO-2025 sample, improving the precision on\n$H_0$ in flat $\\Lambda$CDM to 4.4%. Using the Dark Energy Survey SN Year-5\ndataset (DES-SN5YR) or DESI-DR2 baryonic acoustic oscillations (BAO)\nlikelihoods instead of Pantheon+ yields very similar results. We also present\nconstraints in the open $\\Lambda$CDM, $w$CDM, $w_0w_a$CDM, and $w_{\\phi}$CDM\ncosmologies. The TDCOSMO $H_0$ inference is robust and consistent across all\npresented cosmological models, and our cosmological constraints in them agree\nwith those from the BAO and SN."
                },
                "authors": [
                    {
                        "name": "Simon Birrer"
                    },
                    {
                        "name": "Elizabeth J. Buckley-Geer"
                    },
                    {
                        "name": "Michele Cappellari"
                    },
                    {
                        "name": "Frederic Courbin"
                    },
                    {
                        "name": "Frederic Dux"
                    },
                    {
                        "name": "Christopher D. Fassnacht"
                    },
                    {
                        "name": "Joshua A. Frieman"
                    },
                    {
                        "name": "Aymeric Galan"
                    },
                    {
                        "name": "Daniel Gilman"
                    },
                    {
                        "name": "Xiang-Yu Huang"
                    },
                    {
                        "name": "Shawn Knabel"
                    },
                    {
                        "name": "Danial Langeroodi"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Martin Millon"
                    },
                    {
                        "name": "Takahiro Morishita"
                    },
                    {
                        "name": "Veronica Motta"
                    },
                    {
                        "name": "Pritom Mozumdar"
                    },
                    {
                        "name": "Eric Paic"
                    },
                    {
                        "name": "Anowar J. Shajib"
                    },
                    {
                        "name": "William Sheu"
                    },
                    {
                        "name": "Dominique Sluse"
                    },
                    {
                        "name": "Alessandro Sonnenfeld"
                    },
                    {
                        "name": "Chiara Spiniello"
                    },
                    {
                        "name": "Massimo Stiavelli"
                    },
                    {
                        "name": "Sherry H. Suyu"
                    },
                    {
                        "name": "Chin Yi Tan"
                    },
                    {
                        "name": "Tommaso Treu"
                    },
                    {
                        "name": "Lyne Van de Vyvere"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Patrick Wells"
                    },
                    {
                        "name": "Devon M. Williams"
                    },
                    {
                        "name": "Kenneth C. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth C. Wong"
                },
                "author": "Kenneth C. Wong",
                "arxiv_comment": "34 pages, 17 figures, 8 tables. A virtual talk on the presented\n  results will be given in the CosmoVerse Seminar on June 12, 2025, at 4pm UK\n  time, details:\n  https://cosmoversetensions.eu/for-scientists/cosmoverse-seminars/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03020v1",
                "updated": "2025-06-03T15:57:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    57,
                    55,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:57:55Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    57,
                    55,
                    1,
                    154,
                    0
                ],
                "title": "InfiniteAudio: Infinite-Length Audio Generation with Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteAudio: Infinite-Length Audio Generation with Consistency"
                },
                "summary": "This paper presents InfiniteAudio, a simple yet effective strategy for\ngenerating infinite-length audio using diffusion-based text-to-audio methods.\nCurrent approaches face memory constraints because the output size increases\nwith input length, making long duration generation challenging. A common\nworkaround is to concatenate short audio segments, but this often leads to\ninconsistencies due to the lack of shared temporal context. To address this,\nInfiniteAudio integrates seamlessly into existing pipelines without additional\ntraining. It introduces two key techniques: FIFO sampling, a first-in,\nfirst-out inference strategy with fixed-size inputs, and curved denoising,\nwhich selectively prioritizes key diffusion steps for efficiency. Experiments\nshow that InfiniteAudio achieves comparable or superior performance across all\nmetrics. Audio samples are available on our project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents InfiniteAudio, a simple yet effective strategy for\ngenerating infinite-length audio using diffusion-based text-to-audio methods.\nCurrent approaches face memory constraints because the output size increases\nwith input length, making long duration generation challenging. A common\nworkaround is to concatenate short audio segments, but this often leads to\ninconsistencies due to the lack of shared temporal context. To address this,\nInfiniteAudio integrates seamlessly into existing pipelines without additional\ntraining. It introduces two key techniques: FIFO sampling, a first-in,\nfirst-out inference strategy with fixed-size inputs, and curved denoising,\nwhich selectively prioritizes key diffusion steps for efficiency. Experiments\nshow that InfiniteAudio achieves comparable or superior performance across all\nmetrics. Audio samples are available on our project page."
                },
                "authors": [
                    {
                        "name": "Chaeyoung Jung"
                    },
                    {
                        "name": "Hojoon Ki"
                    },
                    {
                        "name": "Ji-Hoon Kim"
                    },
                    {
                        "name": "Junmo Kim"
                    },
                    {
                        "name": "Joon Son Chung"
                    }
                ],
                "author_detail": {
                    "name": "Joon Son Chung"
                },
                "author": "Joon Son Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03009v1",
                "updated": "2025-06-03T15:50:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    50,
                    27,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:50:27Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    50,
                    27,
                    1,
                    154,
                    0
                ],
                "title": "Conditioning Large Language Models on Legal Systems? Detecting\n  Punishable Hate Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditioning Large Language Models on Legal Systems? Detecting\n  Punishable Hate Speech"
                },
                "summary": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts."
                },
                "authors": [
                    {
                        "name": "Florian Ludwig"
                    },
                    {
                        "name": "Torsten Zesch"
                    },
                    {
                        "name": "Frederike Zufall"
                    }
                ],
                "author_detail": {
                    "name": "Frederike Zufall"
                },
                "author": "Frederike Zufall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03006v1",
                "updated": "2025-06-03T15:45:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    45,
                    31,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:45:31Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    45,
                    31,
                    1,
                    154,
                    0
                ],
                "title": "A Preference-Driven Methodology for High-Quality Solidity Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Preference-Driven Methodology for High-Quality Solidity Code\n  Generation"
                },
                "summary": "While Large Language Models (LLMs) have demonstrated remarkable progress in\ngenerating functionally correct Solidity code, they continue to face critical\nchallenges in producing gas-efficient and secure code, which are critical\nrequirements for real-world smart contract deployment. Although recent advances\nleverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)\nfor code preference alignment, existing approaches treat functional\ncorrectness, gas optimization, and security as independent objectives,\nresulting in contracts that may achieve operational soundness but suffer from\nprohibitive execution costs or dangerous vulnerabilities. To address these\nlimitations, we propose PrefGen, a novel framework that extends standard DPO\nbeyond human preferences to incorporate quantifiable blockchain-specific\nmetrics, enabling holistic multi-objective optimization specifically tailored\nfor smart contract generation. Our framework introduces a comprehensive\nevaluation methodology with four complementary metrics: Pass@k (functional\ncorrectness), Compile@k (syntactic correctness), Gas@k (gas efficiency), and\nSecure@k (security assessment), providing rigorous multi-dimensional contract\nevaluation. Through extensive experimentation, we demonstrate that PrefGen\nsignificantly outperforms existing approaches across all critical dimensions,\nachieving 66.7% Pass@5, 58.9% Gas@5, and 62.5% Secure@5, while generating\nproduction-ready smart contracts that are functionally correct, cost-efficient,\nand secure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have demonstrated remarkable progress in\ngenerating functionally correct Solidity code, they continue to face critical\nchallenges in producing gas-efficient and secure code, which are critical\nrequirements for real-world smart contract deployment. Although recent advances\nleverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)\nfor code preference alignment, existing approaches treat functional\ncorrectness, gas optimization, and security as independent objectives,\nresulting in contracts that may achieve operational soundness but suffer from\nprohibitive execution costs or dangerous vulnerabilities. To address these\nlimitations, we propose PrefGen, a novel framework that extends standard DPO\nbeyond human preferences to incorporate quantifiable blockchain-specific\nmetrics, enabling holistic multi-objective optimization specifically tailored\nfor smart contract generation. Our framework introduces a comprehensive\nevaluation methodology with four complementary metrics: Pass@k (functional\ncorrectness), Compile@k (syntactic correctness), Gas@k (gas efficiency), and\nSecure@k (security assessment), providing rigorous multi-dimensional contract\nevaluation. Through extensive experimentation, we demonstrate that PrefGen\nsignificantly outperforms existing approaches across all critical dimensions,\nachieving 66.7% Pass@5, 58.9% Gas@5, and 62.5% Secure@5, while generating\nproduction-ready smart contracts that are functionally correct, cost-efficient,\nand secure."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Chenhao Ying"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Yuan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Luo"
                },
                "author": "Yuan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13569v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13569v3",
                "updated": "2025-06-03T15:42:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    42,
                    42,
                    1,
                    154,
                    0
                ],
                "published": "2024-10-17T17:17:09Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    17,
                    9,
                    3,
                    291,
                    0
                ],
                "title": "Learning on Model Weights using Tree Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning on Model Weights using Tree Experts"
                },
                "summary": "The number of publicly available models is rapidly increasing, yet most\nremain undocumented. Users looking for suitable models for their tasks must\nfirst determine what each model does. Training machine learning models to infer\nmissing documentation directly from model weights is challenging, as these\nweights often contain significant variation unrelated to model functionality\n(denoted nuisance). Here, we identify a key property of real-world models: most\npublic models belong to a small set of Model Trees, where all models within a\ntree are fine-tuned from a common ancestor (e.g., a foundation model).\nImportantly, we find that within each tree there is less nuisance variation\nbetween models. Concretely, while learning across Model Trees requires complex\narchitectures, even a linear classifier trained on a single model layer often\nworks within trees. While effective, these linear classifiers are\ncomputationally expensive, especially when dealing with larger models that have\nmany parameters. To address this, we introduce Probing Experts (ProbeX), a\ntheoretically motivated and lightweight method. Notably, ProbeX is the first\nprobing method specifically designed to learn from the weights of a single\nhidden model layer. We demonstrate the effectiveness of ProbeX by predicting\nthe categories in a model's training dataset based only on its weights.\nExcitingly, ProbeX can map the weights of Stable Diffusion into a\nweight-language embedding space, enabling model search via text, i.e.,\nzero-shot model classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The number of publicly available models is rapidly increasing, yet most\nremain undocumented. Users looking for suitable models for their tasks must\nfirst determine what each model does. Training machine learning models to infer\nmissing documentation directly from model weights is challenging, as these\nweights often contain significant variation unrelated to model functionality\n(denoted nuisance). Here, we identify a key property of real-world models: most\npublic models belong to a small set of Model Trees, where all models within a\ntree are fine-tuned from a common ancestor (e.g., a foundation model).\nImportantly, we find that within each tree there is less nuisance variation\nbetween models. Concretely, while learning across Model Trees requires complex\narchitectures, even a linear classifier trained on a single model layer often\nworks within trees. While effective, these linear classifiers are\ncomputationally expensive, especially when dealing with larger models that have\nmany parameters. To address this, we introduce Probing Experts (ProbeX), a\ntheoretically motivated and lightweight method. Notably, ProbeX is the first\nprobing method specifically designed to learn from the weights of a single\nhidden model layer. We demonstrate the effectiveness of ProbeX by predicting\nthe categories in a model's training dataset based only on its weights.\nExcitingly, ProbeX can map the weights of Stable Diffusion into a\nweight-language embedding space, enabling model search via text, i.e.,\nzero-shot model classification."
                },
                "authors": [
                    {
                        "name": "Eliahu Horwitz"
                    },
                    {
                        "name": "Bar Cavia"
                    },
                    {
                        "name": "Jonathan Kahana"
                    },
                    {
                        "name": "Yedid Hoshen"
                    }
                ],
                "author_detail": {
                    "name": "Yedid Hoshen"
                },
                "author": "Yedid Hoshen",
                "arxiv_comment": "CVPR 2025. Project page: https://horwitz.ai/probex/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13569v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06262v2",
                "updated": "2025-06-03T15:34:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    34,
                    1,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-04T13:19:21Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    13,
                    19,
                    21,
                    6,
                    124,
                    0
                ],
                "title": "Dialz: A Python Toolkit for Steering Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialz: A Python Toolkit for Steering Vectors"
                },
                "summary": "We introduce Dialz, a framework for advancing research on steering vectors\nfor open-source LLMs, implemented in Python. Steering vectors allow users to\nmodify activations at inference time to amplify or weaken a 'concept', e.g.\nhonesty or positivity, providing a more powerful alternative to prompting or\nfine-tuning. Dialz supports a diverse set of tasks, including creating\ncontrastive pair datasets, computing and applying steering vectors, and\nvisualizations. Unlike existing libraries, Dialz emphasizes modularity and\nusability, enabling both rapid prototyping and in-depth analysis. We\ndemonstrate how Dialz can be used to reduce harmful outputs such as\nstereotypes, while also providing insights into model behaviour across\ndifferent layers. We release Dialz with full documentation, tutorials, and\nsupport for popular open-source models to encourage further research in safe\nand controllable language generation. Dialz enables faster research cycles and\nfacilitates insights into model interpretability, paving the way for safer,\nmore transparent, and more reliable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Dialz, a framework for advancing research on steering vectors\nfor open-source LLMs, implemented in Python. Steering vectors allow users to\nmodify activations at inference time to amplify or weaken a 'concept', e.g.\nhonesty or positivity, providing a more powerful alternative to prompting or\nfine-tuning. Dialz supports a diverse set of tasks, including creating\ncontrastive pair datasets, computing and applying steering vectors, and\nvisualizations. Unlike existing libraries, Dialz emphasizes modularity and\nusability, enabling both rapid prototyping and in-depth analysis. We\ndemonstrate how Dialz can be used to reduce harmful outputs such as\nstereotypes, while also providing insights into model behaviour across\ndifferent layers. We release Dialz with full documentation, tutorials, and\nsupport for popular open-source models to encourage further research in safe\nand controllable language generation. Dialz enables faster research cycles and\nfacilitates insights into model interpretability, paving the way for safer,\nmore transparent, and more reliable AI systems."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Accepted to ACL System Demo 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02996v1",
                "updated": "2025-06-03T15:31:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    31,
                    0,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:31:00Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    31,
                    0,
                    1,
                    154,
                    0
                ],
                "title": "Linear Spatial World Models Emerge in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Spatial World Models Emerge in Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated emergent abilities across\ndiverse tasks, raising the question of whether they acquire internal world\nmodels. In this work, we investigate whether LLMs implicitly encode linear\nspatial world models, which we define as linear representations of physical\nspace and object configurations. We introduce a formal framework for spatial\nworld models and assess whether such structure emerges in contextual\nembeddings. Using a synthetic dataset of object positions, we train probes to\ndecode object positions and evaluate geometric consistency of the underlying\nspace. We further conduct causal interventions to test whether these spatial\nrepresentations are functionally used by the model. Our results provide\nempirical evidence that LLMs encode linear spatial world models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated emergent abilities across\ndiverse tasks, raising the question of whether they acquire internal world\nmodels. In this work, we investigate whether LLMs implicitly encode linear\nspatial world models, which we define as linear representations of physical\nspace and object configurations. We introduce a formal framework for spatial\nworld models and assess whether such structure emerges in contextual\nembeddings. Using a synthetic dataset of object positions, we train probes to\ndecode object positions and evaluate geometric consistency of the underlying\nspace. We further conduct causal interventions to test whether these spatial\nrepresentations are functionally used by the model. Our results provide\nempirical evidence that LLMs encode linear spatial world models."
                },
                "authors": [
                    {
                        "name": "Matthieu Tehenan"
                    },
                    {
                        "name": "Christian Bolivar Moya"
                    },
                    {
                        "name": "Tenghai Long"
                    },
                    {
                        "name": "Guang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guang Lin"
                },
                "author": "Guang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02992v1",
                "updated": "2025-06-03T15:28:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    28,
                    30,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:28:30Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    28,
                    30,
                    1,
                    154,
                    0
                ],
                "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective\n  Multi-Agent Approach for Legal Argument Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Manipulation and Enhancing Persuasion: A Reflective\n  Multi-Agent Approach for Legal Argument Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/"
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Kevin D. Ashley"
                    }
                ],
                "author_detail": {
                    "name": "Kevin D. Ashley"
                },
                "author": "Kevin D. Ashley",
                "arxiv_comment": "13 pages, 2 figures, Workshop on Legally Compliant Intelligent\n  Chatbots at ICAIL 2025]{Workshop on Legally Compliant Intelligent Chatbots @\n  ICAIL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02987v1",
                "updated": "2025-06-03T15:25:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    25,
                    38,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:25:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    25,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "Performance of leading large language models in May 2025 in Membership\n  of the Royal College of General Practitioners-style examination questions: a\n  cross-sectional analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of leading large language models in May 2025 in Membership\n  of the Royal College of General Practitioners-style examination questions: a\n  cross-sectional analysis"
                },
                "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata."
                },
                "authors": [
                    {
                        "name": "Richard Armitage"
                    }
                ],
                "author_detail": {
                    "name": "Richard Armitage"
                },
                "author": "Richard Armitage",
                "arxiv_comment": "12 pages, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19988v2",
                "updated": "2025-06-03T15:23:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    23,
                    3,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-26T13:43:43Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    43,
                    43,
                    0,
                    146,
                    0
                ],
                "title": "Automatic Metadata Extraction for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Metadata Extraction for Text-to-SQL"
                },
                "summary": "Large Language Models (LLMs) have recently become sophisticated enough to\nautomate many tasks ranging from pattern finding to writing assistance to code\ngeneration. In this paper, we examine text-to-SQL generation. We have observed\nfrom decades of experience that the most difficult part of query development\nlies in understanding the database contents. These experiences inform the\ndirection of our research.\n  Text-to-SQL benchmarks such as SPIDER and Bird contain extensive metadata\nthat is generally not available in practice. Human-generated metadata requires\nthe use of expensive Subject Matter Experts (SMEs), who are often not fully\naware of many aspects of their databases. In this paper, we explore techniques\nfor automatic metadata extraction to enable text-to-SQL generation.\n  Ee explore the use of two standard and one newer metadata extraction\ntechniques: profiling, query log analysis, and SQL-to text generation using an\nLLM. We use BIRD benchmark [JHQY+23] to evaluate the effectiveness of these\ntechniques. BIRD does not provide query logs on their test database, so we\nprepared a submission that uses profiling alone, and does not use any specially\ntuned model (we used GPT-4o). From Sept 1 to Sept 23, 2024, and Nov 11 through\nNov 23, 2024 we achieved the highest score both with and without using the\n\"oracle\" information provided with the question set. We regained the number 1\nspot on Mar 11, 2025, and are still at #1 at the time of the writing (May,\n2025).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently become sophisticated enough to\nautomate many tasks ranging from pattern finding to writing assistance to code\ngeneration. In this paper, we examine text-to-SQL generation. We have observed\nfrom decades of experience that the most difficult part of query development\nlies in understanding the database contents. These experiences inform the\ndirection of our research.\n  Text-to-SQL benchmarks such as SPIDER and Bird contain extensive metadata\nthat is generally not available in practice. Human-generated metadata requires\nthe use of expensive Subject Matter Experts (SMEs), who are often not fully\naware of many aspects of their databases. In this paper, we explore techniques\nfor automatic metadata extraction to enable text-to-SQL generation.\n  Ee explore the use of two standard and one newer metadata extraction\ntechniques: profiling, query log analysis, and SQL-to text generation using an\nLLM. We use BIRD benchmark [JHQY+23] to evaluate the effectiveness of these\ntechniques. BIRD does not provide query logs on their test database, so we\nprepared a submission that uses profiling alone, and does not use any specially\ntuned model (we used GPT-4o). From Sept 1 to Sept 23, 2024, and Nov 11 through\nNov 23, 2024 we achieved the highest score both with and without using the\n\"oracle\" information provided with the question set. We regained the number 1\nspot on Mar 11, 2025, and are still at #1 at the time of the writing (May,\n2025)."
                },
                "authors": [
                    {
                        "name": "Vladislav Shkapenyuk"
                    },
                    {
                        "name": "Divesh Srivastava"
                    },
                    {
                        "name": "Theodore Johnson"
                    },
                    {
                        "name": "Parisa Ghane"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Ghane"
                },
                "author": "Parisa Ghane",
                "arxiv_comment": "37 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10199v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10199v9",
                "updated": "2025-06-03T15:20:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    20,
                    32,
                    1,
                    154,
                    0
                ],
                "published": "2025-03-13T09:34:33Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    34,
                    33,
                    3,
                    72,
                    0
                ],
                "title": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods"
                },
                "summary": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty quantification\n(UQ). However, Bayesian maximum a posteriori (MAP) estimation may become\nunstable when handling data from diverse distributions (e.g., solutions of\nstochastic partial differential equations (SPDEs)). Additionally, Monte Carlo\nsampling methods are computationally expensive. To address these challenges, we\npropose a novel two-stage optimization method based on optimal control theory\nand variational Bayesian methods. This method not only yields stable solutions\nfor stochastic inverse problems but also efficiently quantifies the uncertainty\nof solutions. In the first stage, we introduce a new weighting formulation to\nensure the stability of the Bayesian MAP estimation. In the second stage, we\nderive the necessary condition for efficiently quantifying the uncertainty of\nthe solutions by combining the new weighting formula with variational\ninference. Furthermore, we establish an error estimation theorem that relates\nthe exact solution to the optimally estimated solution under different amounts\nof observed data. Finally, the efficiency of the proposed method is\ndemonstrated through numerical examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty quantification\n(UQ). However, Bayesian maximum a posteriori (MAP) estimation may become\nunstable when handling data from diverse distributions (e.g., solutions of\nstochastic partial differential equations (SPDEs)). Additionally, Monte Carlo\nsampling methods are computationally expensive. To address these challenges, we\npropose a novel two-stage optimization method based on optimal control theory\nand variational Bayesian methods. This method not only yields stable solutions\nfor stochastic inverse problems but also efficiently quantifies the uncertainty\nof solutions. In the first stage, we introduce a new weighting formulation to\nensure the stability of the Bayesian MAP estimation. In the second stage, we\nderive the necessary condition for efficiently quantifying the uncertainty of\nthe solutions by combining the new weighting formula with variational\ninference. Furthermore, we establish an error estimation theorem that relates\nthe exact solution to the optimally estimated solution under different amounts\nof observed data. Finally, the efficiency of the proposed method is\ndemonstrated through numerical examples."
                },
                "authors": [
                    {
                        "name": "Ruibiao Song"
                    },
                    {
                        "name": "Liying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liying Zhang"
                },
                "author": "Liying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10199v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10199v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20156v2",
                "updated": "2025-06-03T15:15:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    15,
                    31,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-26T15:57:27Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    57,
                    27,
                    0,
                    146,
                    0
                ],
                "title": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for\n  Multiple Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for\n  Multiple Characters"
                },
                "summary": "Recent years have witnessed significant progress in audio-driven human\nanimation. However, critical challenges remain in (i) generating highly dynamic\nvideos while preserving character consistency, (ii) achieving precise emotion\nalignment between characters and audio, and (iii) enabling multi-character\naudio-driven animation. To address these challenges, we propose\nHunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model\ncapable of simultaneously generating dynamic, emotion-controllable, and\nmulti-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces\nthree key innovations: (i) A character image injection module is designed to\nreplace the conventional addition-based character conditioning scheme,\neliminating the inherent condition mismatch between training and inference.\nThis ensures the dynamic motion and strong character consistency; (ii) An Audio\nEmotion Module (AEM) is introduced to extract and transfer the emotional cues\nfrom an emotion reference image to the target generated video, enabling\nfine-grained and accurate emotion style control; (iii) A Face-Aware Audio\nAdapter (FAA) is proposed to isolate the audio-driven character with\nlatent-level face mask, enabling independent audio injection via\ncross-attention for multi-character scenarios. These innovations empower\nHunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets\nand a newly proposed wild dataset, generating realistic avatars in dynamic,\nimmersive scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed significant progress in audio-driven human\nanimation. However, critical challenges remain in (i) generating highly dynamic\nvideos while preserving character consistency, (ii) achieving precise emotion\nalignment between characters and audio, and (iii) enabling multi-character\naudio-driven animation. To address these challenges, we propose\nHunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model\ncapable of simultaneously generating dynamic, emotion-controllable, and\nmulti-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces\nthree key innovations: (i) A character image injection module is designed to\nreplace the conventional addition-based character conditioning scheme,\neliminating the inherent condition mismatch between training and inference.\nThis ensures the dynamic motion and strong character consistency; (ii) An Audio\nEmotion Module (AEM) is introduced to extract and transfer the emotional cues\nfrom an emotion reference image to the target generated video, enabling\nfine-grained and accurate emotion style control; (iii) A Face-Aware Audio\nAdapter (FAA) is proposed to isolate the audio-driven character with\nlatent-level face mask, enabling independent audio injection via\ncross-attention for multi-character scenarios. These innovations empower\nHunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets\nand a newly proposed wild dataset, generating realistic avatars in dynamic,\nimmersive scenarios."
                },
                "authors": [
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Sen Liang"
                    },
                    {
                        "name": "Zixiang Zhou"
                    },
                    {
                        "name": "Ziyao Huang"
                    },
                    {
                        "name": "Yifeng Ma"
                    },
                    {
                        "name": "Junshu Tang"
                    },
                    {
                        "name": "Qin Lin"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Qinglin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Qinglin Lu"
                },
                "author": "Qinglin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23001v3",
                "updated": "2025-06-04T02:31:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    2,
                    31,
                    16,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-29T02:22:14Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    2,
                    22,
                    14,
                    3,
                    149,
                    0
                ],
                "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors"
                },
                "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors."
                },
                "authors": [
                    {
                        "name": "Yize Cheng"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Mazda Moayeri"
                    },
                    {
                        "name": "Soheil Feizi"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Feizi"
                },
                "author": "Soheil Feizi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09825v2",
                "updated": "2025-06-03T15:11:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    11,
                    26,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-14T22:04:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    22,
                    4,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive\n  Reasoning"
                },
                "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks."
                },
                "authors": [
                    {
                        "name": "Peiqi Sui"
                    },
                    {
                        "name": "Juan Diego Rodriguez"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Dean Murphy"
                    },
                    {
                        "name": "Joseph P. Dexter"
                    },
                    {
                        "name": "Richard Jean So"
                    },
                    {
                        "name": "Samuel Baker"
                    },
                    {
                        "name": "Pramit Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Pramit Chaudhuri"
                },
                "author": "Pramit Chaudhuri",
                "arxiv_comment": "ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02973v1",
                "updated": "2025-06-03T15:07:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    7,
                    13,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:07:13Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    7,
                    13,
                    1,
                    154,
                    0
                ],
                "title": "Expanding before Inferring: Enhancing Factuality in Large Language\n  Models through Premature Layers Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding before Inferring: Enhancing Factuality in Large Language\n  Models through Premature Layers Interpolation"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance."
                },
                "authors": [
                    {
                        "name": "Dingwei Chen"
                    },
                    {
                        "name": "Ziqiang Liu"
                    },
                    {
                        "name": "Feiteng Fang"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Chengming Li"
                    }
                ],
                "author_detail": {
                    "name": "Chengming Li"
                },
                "author": "Chengming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08769v2",
                "updated": "2025-06-03T15:05:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    5,
                    19,
                    1,
                    154,
                    0
                ],
                "published": "2024-08-16T14:23:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    23,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion\n  Contrastive Decoding with Truthfulness Refocused",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion\n  Contrastive Decoding with Truthfulness Refocused"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, they occasionally generate\ninaccurate and counterfactual outputs, a phenomenon commonly referred to as\n\"hallucinations''. To tackle this issue, recent studies have explored\ncontrastive decoding between the original model and an amateur model with\ninduced hallucination, showing promising results. Nevertheless, this approach\ncan disrupt the original LLM's output distribution due to coarse contrast and\nsimple subtraction operations, potentially leading to errors. In this paper, we\nintroduce a novel contrastive decoding framework, termed LOL (LOwer Layer\nMatters). Unlike prior methods that focus solely on the final layer, our\napproach integrates contrastive information from lower layers to enable\nmulti-layer fusion during contrastive decoding. Additionally, we incorporate a\ntruthfulness refocused module that leverages instruction guidance to further\nimprove truthfulness in contrastive decoding. Extensive experiments on four\npublicly available datasets demonstrate that the LOL framework significantly\nmitigates hallucination while outperforming existing baselines in most cases.\nFor reproducibility, we will release our code and data upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, they occasionally generate\ninaccurate and counterfactual outputs, a phenomenon commonly referred to as\n\"hallucinations''. To tackle this issue, recent studies have explored\ncontrastive decoding between the original model and an amateur model with\ninduced hallucination, showing promising results. Nevertheless, this approach\ncan disrupt the original LLM's output distribution due to coarse contrast and\nsimple subtraction operations, potentially leading to errors. In this paper, we\nintroduce a novel contrastive decoding framework, termed LOL (LOwer Layer\nMatters). Unlike prior methods that focus solely on the final layer, our\napproach integrates contrastive information from lower layers to enable\nmulti-layer fusion during contrastive decoding. Additionally, we incorporate a\ntruthfulness refocused module that leverages instruction guidance to further\nimprove truthfulness in contrastive decoding. Extensive experiments on four\npublicly available datasets demonstrate that the LOL framework significantly\nmitigates hallucination while outperforming existing baselines in most cases.\nFor reproducibility, we will release our code and data upon acceptance."
                },
                "authors": [
                    {
                        "name": "Dingwei Chen"
                    },
                    {
                        "name": "Feiteng Fang"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Feng Liang"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Chengming Li"
                    }
                ],
                "author_detail": {
                    "name": "Chengming Li"
                },
                "author": "Chengming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10742v2",
                "updated": "2025-06-03T15:02:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    2,
                    50,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-15T23:06:23Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    23,
                    6,
                    23,
                    3,
                    135,
                    0
                ],
                "title": "Evaluations at Work: Measuring the Capabilities of GenAI in Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluations at Work: Measuring the Capabilities of GenAI in Use"
                },
                "summary": "Current AI benchmarks miss the messy, multi-turn nature of human-AI\ncollaboration. We present an evaluation framework that decomposes real-world\ntasks into interdependent subtasks, letting us track both LLM performance and\nusers' strategies across a dialogue. Complementing this framework, we develop a\nsuite of metrics, including a composite usage derived from semantic similarity,\nword overlap, and numerical matches; structural coherence; intra-turn\ndiversity; and a novel measure of the \"information frontier\" reflecting the\nalignment between AI outputs and users' working knowledge. We demonstrate our\nmethodology in a financial valuation task that mirrors real-world complexity.\nOur empirical findings reveal that while greater integration of LLM-generated\ncontent generally enhances output quality, its benefits are moderated by\nfactors such as response incoherence, excessive subtask diversity, and the\ndistance of provided information from users' existing knowledge. These results\nsuggest that proactive dialogue strategies designed to inject novelty may\ninadvertently undermine task performance. Our work thus advances a more\nholistic evaluation of human-AI collaboration, offering both a robust\nmethodological framework and actionable insights for developing more effective\nAI-augmented work processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current AI benchmarks miss the messy, multi-turn nature of human-AI\ncollaboration. We present an evaluation framework that decomposes real-world\ntasks into interdependent subtasks, letting us track both LLM performance and\nusers' strategies across a dialogue. Complementing this framework, we develop a\nsuite of metrics, including a composite usage derived from semantic similarity,\nword overlap, and numerical matches; structural coherence; intra-turn\ndiversity; and a novel measure of the \"information frontier\" reflecting the\nalignment between AI outputs and users' working knowledge. We demonstrate our\nmethodology in a financial valuation task that mirrors real-world complexity.\nOur empirical findings reveal that while greater integration of LLM-generated\ncontent generally enhances output quality, its benefits are moderated by\nfactors such as response incoherence, excessive subtask diversity, and the\ndistance of provided information from users' existing knowledge. These results\nsuggest that proactive dialogue strategies designed to inject novelty may\ninadvertently undermine task performance. Our work thus advances a more\nholistic evaluation of human-AI collaboration, offering both a robust\nmethodological framework and actionable insights for developing more effective\nAI-augmented work processes."
                },
                "authors": [
                    {
                        "name": "Brandon Lepine"
                    },
                    {
                        "name": "Gawesha Weerantunga"
                    },
                    {
                        "name": "Juho Kim"
                    },
                    {
                        "name": "Pamela Mishkin"
                    },
                    {
                        "name": "Matthew Beane"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Beane"
                },
                "author": "Matthew Beane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02965v2",
                "updated": "2025-06-04T05:38:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    5,
                    38,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T15:00:18Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    0,
                    18,
                    1,
                    154,
                    0
                ],
                "title": "PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training\n  for Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training\n  for Mixture-of-Experts LLMs"
                },
                "summary": "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks."
                },
                "authors": [
                    {
                        "name": "Ze Yu Zhang"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02961v1",
                "updated": "2025-06-03T14:54:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    54,
                    12,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:54:12Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    54,
                    12,
                    1,
                    154,
                    0
                ],
                "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Massimo Roberto Scamarcia"
                    },
                    {
                        "name": "Javier Fernandez-Marques"
                    },
                    {
                        "name": "Mohammad Naseri"
                    },
                    {
                        "name": "Chong Shen Ng"
                    },
                    {
                        "name": "Dimitris Stripelis"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Jiamu Bai"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "InSeo Song"
                    },
                    {
                        "name": "Lee KangYoon"
                    },
                    {
                        "name": "Hong Jia"
                    },
                    {
                        "name": "Ting Dang"
                    },
                    {
                        "name": "Junyan Wang"
                    },
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Daniel Janes Beutel"
                    },
                    {
                        "name": "Lingjuan Lyu"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02959v1",
                "updated": "2025-06-03T14:52:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    52,
                    44,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:52:44Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    52,
                    44,
                    1,
                    154,
                    0
                ],
                "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection\n  under Human-AI Coauthoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection\n  under Human-AI Coauthoring"
                },
                "summary": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement."
                },
                "authors": [
                    {
                        "name": "Zhixiong Su"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Herun Wan"
                    },
                    {
                        "name": "Zhaohan Zhang"
                    },
                    {
                        "name": "Minnan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Minnan Luo"
                },
                "author": "Minnan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23503v2",
                "updated": "2025-06-03T14:52:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    52,
                    14,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-29T14:48:09Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    48,
                    9,
                    3,
                    149,
                    0
                ],
                "title": "Can Large Language Models Challenge CNNs in Medical Image Analysis?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Challenge CNNs in Medical Image Analysis?"
                },
                "summary": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings."
                },
                "authors": [
                    {
                        "name": "Shibbir Ahmed"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    },
                    {
                        "name": "Anindya Bijoy Das"
                    }
                ],
                "author_detail": {
                    "name": "Anindya Bijoy Das"
                },
                "author": "Anindya Bijoy Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02956v1",
                "updated": "2025-06-03T14:48:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    48,
                    21,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:48:21Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    48,
                    21,
                    1,
                    154,
                    0
                ],
                "title": "Local dark matter density from Gaia DR3 K-dwarfs using Gaussian\n  processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local dark matter density from Gaia DR3 K-dwarfs using Gaussian\n  processes"
                },
                "summary": "Modern astrophysical surveys have produced a wealth of data on the positions\nand velocities of stars in the Milky Way with varying accuracies. An\never-increasing detail in observational data calls for more complex physical\nmodels and in turn more sophisticated statistical methods to extract\ninformation. We perform a vertical Jeans analysis, including a local\napproximation of the tilt term, using a sample of $200\\,000$ K-dwarf stars from\nthe Gaia DR3 catalogue. After combination with the Survey-of-Surveys (SoS)\ncatalogue, $160\\,888$ of those have radial velocity measurements. We use\nGaussian processes as priors for the covariance matrix of radial and vertical\nvelocities. Joint inference of the posterior distribution of the local dark\nmatter density and the velocity moments is performed using geometric\nvariational inference. We find a local dark matter density of\n${\\rho_\\mathrm{dm} = 0.0131 \\pm 0.0041\\, \\mathrm{M}_\\odot\\,\\mathrm{pc}^{-3} =\n0.50 \\pm 0.15\\, \\mathrm{GeV}\\,\\mathrm{cm}^{-3}}$ at the Sun's position, which\nis in agreement with most other recent analyses. By comparing a ($z$-dependent)\nGaussian process prior with a ($z$-independent) scalar prior for the tilt term,\nwe quantify its impact on estimates of the local dark matter density and argue\nthat careful modelling is required to mitigate systematic biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern astrophysical surveys have produced a wealth of data on the positions\nand velocities of stars in the Milky Way with varying accuracies. An\never-increasing detail in observational data calls for more complex physical\nmodels and in turn more sophisticated statistical methods to extract\ninformation. We perform a vertical Jeans analysis, including a local\napproximation of the tilt term, using a sample of $200\\,000$ K-dwarf stars from\nthe Gaia DR3 catalogue. After combination with the Survey-of-Surveys (SoS)\ncatalogue, $160\\,888$ of those have radial velocity measurements. We use\nGaussian processes as priors for the covariance matrix of radial and vertical\nvelocities. Joint inference of the posterior distribution of the local dark\nmatter density and the velocity moments is performed using geometric\nvariational inference. We find a local dark matter density of\n${\\rho_\\mathrm{dm} = 0.0131 \\pm 0.0041\\, \\mathrm{M}_\\odot\\,\\mathrm{pc}^{-3} =\n0.50 \\pm 0.15\\, \\mathrm{GeV}\\,\\mathrm{cm}^{-3}}$ at the Sun's position, which\nis in agreement with most other recent analyses. By comparing a ($z$-dependent)\nGaussian process prior with a ($z$-independent) scalar prior for the tilt term,\nwe quantify its impact on estimates of the local dark matter density and argue\nthat careful modelling is required to mitigate systematic biases."
                },
                "authors": [
                    {
                        "name": "Laurin Sding"
                    },
                    {
                        "name": "Ruben Bartel"
                    },
                    {
                        "name": "Philipp Mertsch"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Mertsch"
                },
                "author": "Philipp Mertsch",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02955v1",
                "updated": "2025-06-03T14:48:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    48,
                    4,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:48:04Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    48,
                    4,
                    1,
                    154,
                    0
                ],
                "title": "UniConFlow: A Unified Constrained Generalization Framework for Certified\n  Motion Planning with Flow Matching Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniConFlow: A Unified Constrained Generalization Framework for Certified\n  Motion Planning with Flow Matching Models"
                },
                "summary": "Generative models have become increasingly powerful tools for robot motion\ngeneration, enabling flexible and multimodal trajectory generation across\nvarious tasks. Yet, most existing approaches remain limited in handling\nmultiple types of constraints, such as collision avoidance and dynamic\nconsistency, which are often treated separately or only partially considered.\nThis paper proposes UniConFlow, a unified flow matching (FM) based framework\nfor trajectory generation that systematically incorporates both equality and\ninequality constraints. UniConFlow introduces a novel prescribed-time zeroing\nfunction to enhance flexibility during the inference process, allowing the\nmodel to adapt to varying task requirements. To ensure constraint satisfaction,\nparticularly with respect to obstacle avoidance, admissible action range, and\nkinodynamic consistency, the guidance inputs to the FM model are derived\nthrough a quadratic programming formulation, which enables constraint-aware\ngeneration without requiring retraining or auxiliary controllers. We conduct\nmobile navigation and high-dimensional manipulation tasks, demonstrating\nimproved safety and feasibility compared to state-of-the-art constrained\ngenerative planners. Project page is available at https://uniconflow.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have become increasingly powerful tools for robot motion\ngeneration, enabling flexible and multimodal trajectory generation across\nvarious tasks. Yet, most existing approaches remain limited in handling\nmultiple types of constraints, such as collision avoidance and dynamic\nconsistency, which are often treated separately or only partially considered.\nThis paper proposes UniConFlow, a unified flow matching (FM) based framework\nfor trajectory generation that systematically incorporates both equality and\ninequality constraints. UniConFlow introduces a novel prescribed-time zeroing\nfunction to enhance flexibility during the inference process, allowing the\nmodel to adapt to varying task requirements. To ensure constraint satisfaction,\nparticularly with respect to obstacle avoidance, admissible action range, and\nkinodynamic consistency, the guidance inputs to the FM model are derived\nthrough a quadratic programming formulation, which enables constraint-aware\ngeneration without requiring retraining or auxiliary controllers. We conduct\nmobile navigation and high-dimensional manipulation tasks, demonstrating\nimproved safety and feasibility compared to state-of-the-art constrained\ngenerative planners. Project page is available at https://uniconflow.github.io."
                },
                "authors": [
                    {
                        "name": "Zewen Yang"
                    },
                    {
                        "name": "Xiaobing Dai"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Qianru Li"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Valentin Le Mesle"
                    }
                ],
                "author_detail": {
                    "name": "Valentin Le Mesle"
                },
                "author": "Valentin Le Mesle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02954v1",
                "updated": "2025-06-03T14:47:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    47,
                    22,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    47,
                    22,
                    1,
                    154,
                    0
                ],
                "title": "Towards More Effective Fault Detection in LLM-Based Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Effective Fault Detection in LLM-Based Unit Test Generation"
                },
                "summary": "Unit tests play a vital role in uncovering potential faults in software.\nWhile tools like EvoSuite focus on maximizing code coverage, recent advances in\nlarge language models (LLMs) have shifted attention toward LLM-based test\ngeneration. However, code coverage metrics -- such as line and branch coverage\n-- remain overly emphasized in reported research, despite being weak indicators\nof a test suite's fault-detection capability. In contrast, \\textit{mutation\nscore} offers a more reliable and stringent measure, as demonstrated in our\nfindings where some test suites achieve 100\\% coverage but only 4\\% mutation\nscore. Although a few studies consider mutation score, the effectiveness of\nLLMs in killing mutants remains underexplored.\n  In this paper, we propose MUTGEN, a mutation-guided, LLM-based test\ngeneration approach that incorporates mutation feedback directly into the\nprompt. Evaluated on 204 subjects from two benchmarks, MUTGEN significantly\noutperforms both EvoSuite and vanilla prompt-based strategies in terms of\nmutation score. Furthermore, MUTGEN introduces an iterative generation\nmechanism that pushes the limits of LLMs in killing additional mutants. Our\nstudy also provide insights into the limitations of LLM-based generation,\nanalyzing the reasons for live and uncovered mutants, and the impact of\ndifferent mutation operators on generation effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit tests play a vital role in uncovering potential faults in software.\nWhile tools like EvoSuite focus on maximizing code coverage, recent advances in\nlarge language models (LLMs) have shifted attention toward LLM-based test\ngeneration. However, code coverage metrics -- such as line and branch coverage\n-- remain overly emphasized in reported research, despite being weak indicators\nof a test suite's fault-detection capability. In contrast, \\textit{mutation\nscore} offers a more reliable and stringent measure, as demonstrated in our\nfindings where some test suites achieve 100\\% coverage but only 4\\% mutation\nscore. Although a few studies consider mutation score, the effectiveness of\nLLMs in killing mutants remains underexplored.\n  In this paper, we propose MUTGEN, a mutation-guided, LLM-based test\ngeneration approach that incorporates mutation feedback directly into the\nprompt. Evaluated on 204 subjects from two benchmarks, MUTGEN significantly\noutperforms both EvoSuite and vanilla prompt-based strategies in terms of\nmutation score. Furthermore, MUTGEN introduces an iterative generation\nmechanism that pushes the limits of LLMs in killing additional mutants. Our\nstudy also provide insights into the limitations of LLM-based generation,\nanalyzing the reasons for live and uncovered mutants, and the impact of\ndifferent mutation operators on generation effectiveness."
                },
                "authors": [
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Lionel C. Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02862v2",
                "updated": "2025-06-03T14:46:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    46,
                    36,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-03T05:28:11Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    5,
                    28,
                    11,
                    5,
                    123,
                    0
                ],
                "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to\n  Elicit Irrational Choices of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to\n  Elicit Irrational Choices of LLMs"
                },
                "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies."
                },
                "authors": [
                    {
                        "name": "Haoming Yang"
                    },
                    {
                        "name": "Ke Ma"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yingfei Sun"
                    },
                    {
                        "name": "Qianqian Xu"
                    },
                    {
                        "name": "Qingming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qingming Huang"
                },
                "author": "Qingming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02951v1",
                "updated": "2025-06-03T14:46:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    46,
                    0,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:46:00Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    46,
                    0,
                    1,
                    154,
                    0
                ],
                "title": "Adaptive Graph Pruning for Multi-Agent Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Graph Pruning for Multi-Agent Communication"
                },
                "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Zhonghan Zhao"
                    },
                    {
                        "name": "Der-Horng Lee"
                    },
                    {
                        "name": "Gaoang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gaoang Wang"
                },
                "author": "Gaoang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02946v1",
                "updated": "2025-06-03T14:44:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    44,
                    26,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:44:26Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    44,
                    26,
                    1,
                    154,
                    0
                ],
                "title": "Abstract Counterfactuals for Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Counterfactuals for Language Model Agents"
                },
                "summary": "Counterfactual inference is a powerful tool for analysing and evaluating\nautonomous agents, but its application to language model (LM) agents remains\nchallenging. Existing work on counterfactuals in LMs has primarily focused on\ntoken-level counterfactuals, which are often inadequate for LM agents due to\ntheir open-ended action spaces. Unlike traditional agents with fixed, clearly\ndefined action spaces, the actions of LM agents are often implicit in the\nstrings they output, making their action spaces difficult to define and\ninterpret. Furthermore, the meanings of individual tokens can shift depending\non the context, adding complexity to token-level reasoning and sometimes\nleading to biased or meaningless counterfactuals. We introduce \\emph{Abstract\nCounterfactuals}, a framework that emphasises high-level characteristics of\nactions and interactions within an environment, enabling counterfactual\nreasoning tailored to user-relevant features. Our experiments demonstrate that\nthe approach produces consistent and meaningful counterfactuals while\nminimising the undesired side effects of token-level methods. We conduct\nexperiments on text-based games and counterfactual text generation, while\nconsidering both token-level and latent-space interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual inference is a powerful tool for analysing and evaluating\nautonomous agents, but its application to language model (LM) agents remains\nchallenging. Existing work on counterfactuals in LMs has primarily focused on\ntoken-level counterfactuals, which are often inadequate for LM agents due to\ntheir open-ended action spaces. Unlike traditional agents with fixed, clearly\ndefined action spaces, the actions of LM agents are often implicit in the\nstrings they output, making their action spaces difficult to define and\ninterpret. Furthermore, the meanings of individual tokens can shift depending\non the context, adding complexity to token-level reasoning and sometimes\nleading to biased or meaningless counterfactuals. We introduce \\emph{Abstract\nCounterfactuals}, a framework that emphasises high-level characteristics of\nactions and interactions within an environment, enabling counterfactual\nreasoning tailored to user-relevant features. Our experiments demonstrate that\nthe approach produces consistent and meaningful counterfactuals while\nminimising the undesired side effects of token-level methods. We conduct\nexperiments on text-based games and counterfactual text generation, while\nconsidering both token-level and latent-space interventions."
                },
                "authors": [
                    {
                        "name": "Edoardo Pona"
                    },
                    {
                        "name": "Milad Kazemi"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "David Watson"
                    },
                    {
                        "name": "Nicola Paoletti"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Paoletti"
                },
                "author": "Nicola Paoletti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02945v1",
                "updated": "2025-06-03T14:44:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    44,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:44:23Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    44,
                    23,
                    1,
                    154,
                    0
                ],
                "title": "Quantitative LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative LLM Judges"
                },
                "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling."
                },
                "authors": [
                    {
                        "name": "Aishwarya Sahoo"
                    },
                    {
                        "name": "Jeevana Kruthi Karnuthala"
                    },
                    {
                        "name": "Tushar Parmanand Budhwani"
                    },
                    {
                        "name": "Pranchal Agarwal"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "Alexa Siu"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Jennifer Healey"
                    },
                    {
                        "name": "Nedim Lipka"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Uttaran Bhattacharya"
                    },
                    {
                        "name": "Branislav Kveton"
                    }
                ],
                "author_detail": {
                    "name": "Branislav Kveton"
                },
                "author": "Branislav Kveton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02943v1",
                "updated": "2025-06-03T14:43:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    43,
                    5,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:43:05Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    43,
                    5,
                    1,
                    154,
                    0
                ],
                "title": "A Multi-agent LLM-based JUit Test Generation with Strong Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-agent LLM-based JUit Test Generation with Strong Oracles"
                },
                "summary": "Unit testing plays a critical role in ensuring software correctness. However,\nwriting unit tests manually is laborious, especially for strong typed languages\nlike Java, motivating the need for automated approaches. Traditional methods\nprimarily rely on search-based or randomized algorithms to generate tests that\nachieve high code coverage and produce regression oracles, which are derived\nfrom the program's current behavior rather than its intended functionality.\nRecent advances in large language models (LLMs) have enabled oracle generation\nfrom natural language descriptions. However, existing LLM-based methods often\nrequire LLM fine-tuning or rely on external tools such as EvoSuite for test\nprefix generation.\n  In this work, we propose CANDOR, a novel end-to-end, prompt-based LLM\nframework for automated JUnit test generation. CANDOR orchestrates multiple\nspecialized LLM agents to generate JUnit tests, including both high-quality\ntest prefixes and accurate oracles. To mitigate the notorious hallucinations in\nLLMs, we introduce a novel strategy that engages multiple reasoning LLMs in a\npanel discussion and generate accurate oracles based on consensus.\nAdditionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a\nnovel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments on the HumanEvalJava and LeetCodeJava datasets show that\nCANDOR can generate accurate oracles and is slightly better than EvoSuite in\ngenerating tests with high line coverage and clearly superior in terms of\nmutation score. Moreover, CANDOR significantly outperforms the\nstate-of-the-art, prompt-based test generator LLM-Empirical, achieving\nimprovements of 15.8 to 25.1 percentage points in oracle correctness on both\ncorrect and faulty source code. Ablation studies confirm the critical\ncontributions of key agents in improving test prefix quality and oracle\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing plays a critical role in ensuring software correctness. However,\nwriting unit tests manually is laborious, especially for strong typed languages\nlike Java, motivating the need for automated approaches. Traditional methods\nprimarily rely on search-based or randomized algorithms to generate tests that\nachieve high code coverage and produce regression oracles, which are derived\nfrom the program's current behavior rather than its intended functionality.\nRecent advances in large language models (LLMs) have enabled oracle generation\nfrom natural language descriptions. However, existing LLM-based methods often\nrequire LLM fine-tuning or rely on external tools such as EvoSuite for test\nprefix generation.\n  In this work, we propose CANDOR, a novel end-to-end, prompt-based LLM\nframework for automated JUnit test generation. CANDOR orchestrates multiple\nspecialized LLM agents to generate JUnit tests, including both high-quality\ntest prefixes and accurate oracles. To mitigate the notorious hallucinations in\nLLMs, we introduce a novel strategy that engages multiple reasoning LLMs in a\npanel discussion and generate accurate oracles based on consensus.\nAdditionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a\nnovel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments on the HumanEvalJava and LeetCodeJava datasets show that\nCANDOR can generate accurate oracles and is slightly better than EvoSuite in\ngenerating tests with high line coverage and clearly superior in terms of\nmutation score. Moreover, CANDOR significantly outperforms the\nstate-of-the-art, prompt-based test generator LLM-Empirical, achieving\nimprovements of 15.8 to 25.1 percentage points in oracle correctness on both\ncorrect and faulty source code. Ablation studies confirm the critical\ncontributions of key agents in improving test prefix quality and oracle\naccuracy."
                },
                "authors": [
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Lionel Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15186v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15186v5",
                "updated": "2025-06-03T14:40:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    40,
                    1,
                    1,
                    154,
                    0
                ],
                "published": "2024-07-21T14:48:23Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    48,
                    23,
                    6,
                    203,
                    0
                ],
                "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Employing Large Language Models for Text-to-SQL Tasks"
                },
                "summary": "With the development of the Large Language Models (LLMs), a large range of\nLLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a\ncomprehensive review of LLM-based Text2SQL studies. We first enumerate classic\nbenchmarks and evaluation metrics. For the two mainstream methods, prompt\nengineering and finetuning, we introduce a comprehensive taxonomy and offer\npractical insights into each subcategory. We present an overall analysis of the\nabove methods and various models evaluated on well-known datasets and extract\nsome characteristics. Finally, we discuss the challenges and future directions\nin this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of the Large Language Models (LLMs), a large range of\nLLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a\ncomprehensive review of LLM-based Text2SQL studies. We first enumerate classic\nbenchmarks and evaluation metrics. For the two mainstream methods, prompt\nengineering and finetuning, we introduce a comprehensive taxonomy and offer\npractical insights into each subcategory. We present an overall analysis of the\nabove methods and various models evaluated on well-known datasets and extract\nsome characteristics. Finally, we discuss the challenges and future directions\nin this field."
                },
                "authors": [
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Zhengju Tang"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Xiaotong Zhang"
                    },
                    {
                        "name": "Zhi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yang"
                },
                "author": "Zhi Yang",
                "arxiv_doi": "10.1145/3737873.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3737873.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.15186v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15186v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Computing Surveys (CSUR)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02940v1",
                "updated": "2025-06-03T14:39:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    39,
                    56,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:39:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    39,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Memory-Efficient Split Federated Learning for LLM Fine-Tuning on\n  Heterogeneous Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Split Federated Learning for LLM Fine-Tuning on\n  Heterogeneous Mobile Devices"
                },
                "summary": "In this paper, we propose an edge-assisted split federated learning framework\nto facilitate large language model (LLM) fine-tuning on heterogeneous mobile\ndevices while alleviating memory pressures on both mobile devices and the edge\nserver. Specifically, mobile devices perform low-rank adaptation (LoRA)\nfine-tuning on only a subset of lower layers of the pre-trained LLM, tailored\nto their individual capacities. On the server, a full LLM is maintained, and\nthe corresponding LoRA modules are selectively fine-tuned in a sequential\nmanner for each device. To further enhance training efficiency, we propose a\nserver-side training scheduling method that optimizes the processing order of\ndevices for accelerating fine-tuning. Extensive experiments demonstrate that\ncompared to the baselines, our scheme can reduce 79\\% memory footprint and 6\\%\ntraining time while achieving comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an edge-assisted split federated learning framework\nto facilitate large language model (LLM) fine-tuning on heterogeneous mobile\ndevices while alleviating memory pressures on both mobile devices and the edge\nserver. Specifically, mobile devices perform low-rank adaptation (LoRA)\nfine-tuning on only a subset of lower layers of the pre-trained LLM, tailored\nto their individual capacities. On the server, a full LLM is maintained, and\nthe corresponding LoRA modules are selectively fine-tuned in a sequential\nmanner for each device. To further enhance training efficiency, we propose a\nserver-side training scheduling method that optimizes the processing order of\ndevices for accelerating fine-tuning. Extensive experiments demonstrate that\ncompared to the baselines, our scheme can reduce 79\\% memory footprint and 6\\%\ntraining time while achieving comparable performance."
                },
                "authors": [
                    {
                        "name": "Xiaopei Chen"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Fei Ji"
                    },
                    {
                        "name": "Wen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wu"
                },
                "author": "Wen Wu",
                "arxiv_comment": "IEEE INFOCOM IEILM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02939v1",
                "updated": "2025-06-03T14:37:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    37,
                    17,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:37:17Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    37,
                    17,
                    1,
                    154,
                    0
                ],
                "title": "QKV Projections Require a Fraction of Their Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QKV Projections Require a Fraction of Their Memory"
                },
                "summary": "The Multi-Head Attention mechanism is central to LLM operation, and multiple\nworks target its compute and memory efficiency during training. While most\nworks focus on approximating the scaled dot product, the memory consumption of\nthe linear projections that compute the $Q$, $K$, and $V$ tensors from the\ninput $x$ is often overlooked. To address this, we propose Point-Approximate\nMatrix Multiplication (PAMM), a novel tensor compression technique that reduces\nmemory consumption of the $Q,K,V$ projections in attention layers by a factor\nof up to $\\times 512$, effectively erasing their memory footprint, while\nachieving similar or better final perplexity. PAMM is fully composable with\nefficient attention techniques such as FlashAttention, making it a practical\nand complementary method for memory-efficient LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Multi-Head Attention mechanism is central to LLM operation, and multiple\nworks target its compute and memory efficiency during training. While most\nworks focus on approximating the scaled dot product, the memory consumption of\nthe linear projections that compute the $Q$, $K$, and $V$ tensors from the\ninput $x$ is often overlooked. To address this, we propose Point-Approximate\nMatrix Multiplication (PAMM), a novel tensor compression technique that reduces\nmemory consumption of the $Q,K,V$ projections in attention layers by a factor\nof up to $\\times 512$, effectively erasing their memory footprint, while\nachieving similar or better final perplexity. PAMM is fully composable with\nefficient attention techniques such as FlashAttention, making it a practical\nand complementary method for memory-efficient LLM training."
                },
                "authors": [
                    {
                        "name": "Malik Khalaf"
                    },
                    {
                        "name": "Yara Shamshoum"
                    },
                    {
                        "name": "Nitzan Hodos"
                    },
                    {
                        "name": "Yuval Sieradzki"
                    },
                    {
                        "name": "Assaf Schuster"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Schuster"
                },
                "author": "Assaf Schuster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15806v2",
                "updated": "2025-06-03T14:35:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    35,
                    59,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-19T07:23:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    23,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of\n  Iterative Chaos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of\n  Iterative Chaos"
                },
                "summary": "Large Reasoning Models (LRMs) have significantly advanced beyond traditional\nLarge Language Models (LLMs) with their exceptional logical reasoning\ncapabilities, yet these improvements introduce heightened safety risks. When\nsubjected to jailbreak attacks, their ability to generate more targeted and\norganized content can lead to greater harm. Although some studies claim that\nreasoning enables safer LRMs against existing LLM attacks, they overlook the\ninherent flaws within the reasoning process itself. To address this gap, we\npropose the first jailbreak attack targeting LRMs, exploiting their unique\nvulnerabilities stemming from the advanced reasoning capabilities.\nSpecifically, we introduce a Chaos Machine, a novel component to transform\nattack prompts with diverse one-to-one mappings. The chaos mappings iteratively\ngenerated by the machine are embedded into the reasoning chain, which\nstrengthens the variability and complexity and also promotes a more robust\nattack. Based on this, we construct the Mousetrap framework, which makes\nattacks projected into nonlinear-like low sample spaces with mismatched\ngeneralization enhanced. Also, due to the more competing objectives, LRMs\ngradually maintain the inertia of unpredictable iterative reasoning and fall\ninto our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet\nand Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic\ndataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench,\nattacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly\nachieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This\npaper contains inappropriate, offensive and harmful content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have significantly advanced beyond traditional\nLarge Language Models (LLMs) with their exceptional logical reasoning\ncapabilities, yet these improvements introduce heightened safety risks. When\nsubjected to jailbreak attacks, their ability to generate more targeted and\norganized content can lead to greater harm. Although some studies claim that\nreasoning enables safer LRMs against existing LLM attacks, they overlook the\ninherent flaws within the reasoning process itself. To address this gap, we\npropose the first jailbreak attack targeting LRMs, exploiting their unique\nvulnerabilities stemming from the advanced reasoning capabilities.\nSpecifically, we introduce a Chaos Machine, a novel component to transform\nattack prompts with diverse one-to-one mappings. The chaos mappings iteratively\ngenerated by the machine are embedded into the reasoning chain, which\nstrengthens the variability and complexity and also promotes a more robust\nattack. Based on this, we construct the Mousetrap framework, which makes\nattacks projected into nonlinear-like low sample spaces with mismatched\ngeneralization enhanced. Also, due to the more competing objectives, LRMs\ngradually maintain the inertia of unpredictable iterative reasoning and fall\ninto our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet\nand Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic\ndataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench,\nattacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly\nachieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This\npaper contains inappropriate, offensive and harmful content."
                },
                "authors": [
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Xuan Tong"
                    },
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Lujundong Li"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02935v1",
                "updated": "2025-06-03T14:35:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    35,
                    36,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:35:36Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    35,
                    36,
                    1,
                    154,
                    0
                ],
                "title": "MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable\n  Neural Vehicle Routing Solver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable\n  Neural Vehicle Routing Solver"
                },
                "summary": "Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a\npromising approach to train a unified model capable of solving multiple Vehicle\nRouting Problem (VRP) variants. However, existing Reinforcement Learning\n(RL)-based multi-task methods can only train light decoder models on\nsmall-scale problems, exhibiting limited generalization ability when solving\nlarge-scale problems. To overcome this limitation, this work introduces a novel\nmulti-task learning method driven by knowledge distillation (MTL-KD), which\nenables the efficient training of heavy decoder models with strong\ngeneralization ability. The proposed MTL-KD method transfers policy knowledge\nfrom multiple distinct RL-based single-task models to a single heavy decoder\nmodel, facilitating label-free training and effectively improving the model's\ngeneralization ability across diverse tasks. In addition, we introduce a\nflexible inference strategy termed Random Reordering Re-Construction (R3C),\nwhich is specifically adapted for diverse VRP tasks and further boosts the\nperformance of the multi-task model. Experimental results on 6 seen and 10\nunseen VRP variants with up to 1000 nodes indicate that our proposed method\nconsistently achieves superior performance on both uniform and real-world\nbenchmarks, demonstrating robust generalization abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a\npromising approach to train a unified model capable of solving multiple Vehicle\nRouting Problem (VRP) variants. However, existing Reinforcement Learning\n(RL)-based multi-task methods can only train light decoder models on\nsmall-scale problems, exhibiting limited generalization ability when solving\nlarge-scale problems. To overcome this limitation, this work introduces a novel\nmulti-task learning method driven by knowledge distillation (MTL-KD), which\nenables the efficient training of heavy decoder models with strong\ngeneralization ability. The proposed MTL-KD method transfers policy knowledge\nfrom multiple distinct RL-based single-task models to a single heavy decoder\nmodel, facilitating label-free training and effectively improving the model's\ngeneralization ability across diverse tasks. In addition, we introduce a\nflexible inference strategy termed Random Reordering Re-Construction (R3C),\nwhich is specifically adapted for diverse VRP tasks and further boosts the\nperformance of the multi-task model. Experimental results on 6 seen and 10\nunseen VRP variants with up to 1000 nodes indicate that our proposed method\nconsistently achieves superior performance on both uniform and real-world\nbenchmarks, demonstrating robust generalization abilities."
                },
                "authors": [
                    {
                        "name": "Yuepeng Zheng"
                    },
                    {
                        "name": "Fu Luo"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Yaoxin Wu"
                    },
                    {
                        "name": "Yu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhou"
                },
                "author": "Yu Zhou",
                "arxiv_comment": "24 pages,5 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02929v1",
                "updated": "2025-06-03T14:30:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    30,
                    52,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:30:52Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    30,
                    52,
                    1,
                    154,
                    0
                ],
                "title": "Large Processor Chip Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Processor Chip Model"
                },
                "summary": "Computer System Architecture serves as a crucial bridge between software\napplications and the underlying hardware, encompassing components like\ncompilers, CPUs, coprocessors, and RTL designs. Its development, from early\nmainframes to modern domain-specific architectures, has been driven by rising\ncomputational demands and advancements in semiconductor technology. However,\ntraditional paradigms in computer system architecture design are confronting\nsignificant challenges, including a reliance on manual expertise, fragmented\noptimization across software and hardware layers, and high costs associated\nwith exploring expansive design spaces. While automated methods leveraging\noptimization algorithms and machine learning have improved efficiency, they\nremain constrained by a single-stage focus, limited data availability, and a\nlack of comprehensive human domain knowledge. The emergence of large language\nmodels offers transformative opportunities for the design of computer system\narchitecture. By leveraging the capabilities of LLMs in areas such as code\ngeneration, data analysis, and performance modeling, the traditional manual\ndesign process can be transitioned to a machine-based automated design\napproach. To harness this potential, we present the Large Processor Chip Model\n(LPCM), an LLM-driven framework aimed at achieving end-to-end automated\ncomputer architecture design. The LPCM is structured into three levels:\nHuman-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D\nGaussian Splatting as a representative workload and employs the concept of\nsoftware-hardware collaborative design to examine the implementation of the\nLPCM at Level 1, demonstrating the effectiveness of the proposed approach.\nFurthermore, this paper provides an in-depth discussion on the pathway to\nimplementing Level 2 and Level 3 of the LPCM, along with an analysis of the\nexisting challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer System Architecture serves as a crucial bridge between software\napplications and the underlying hardware, encompassing components like\ncompilers, CPUs, coprocessors, and RTL designs. Its development, from early\nmainframes to modern domain-specific architectures, has been driven by rising\ncomputational demands and advancements in semiconductor technology. However,\ntraditional paradigms in computer system architecture design are confronting\nsignificant challenges, including a reliance on manual expertise, fragmented\noptimization across software and hardware layers, and high costs associated\nwith exploring expansive design spaces. While automated methods leveraging\noptimization algorithms and machine learning have improved efficiency, they\nremain constrained by a single-stage focus, limited data availability, and a\nlack of comprehensive human domain knowledge. The emergence of large language\nmodels offers transformative opportunities for the design of computer system\narchitecture. By leveraging the capabilities of LLMs in areas such as code\ngeneration, data analysis, and performance modeling, the traditional manual\ndesign process can be transitioned to a machine-based automated design\napproach. To harness this potential, we present the Large Processor Chip Model\n(LPCM), an LLM-driven framework aimed at achieving end-to-end automated\ncomputer architecture design. The LPCM is structured into three levels:\nHuman-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D\nGaussian Splatting as a representative workload and employs the concept of\nsoftware-hardware collaborative design to examine the implementation of the\nLPCM at Level 1, demonstrating the effectiveness of the proposed approach.\nFurthermore, this paper provides an in-depth discussion on the pathway to\nimplementing Level 2 and Level 3 of the LPCM, along with an analysis of the\nexisting challenges."
                },
                "authors": [
                    {
                        "name": "Kaiyan Chang"
                    },
                    {
                        "name": "Mingzhi Chen"
                    },
                    {
                        "name": "Yunji Chen"
                    },
                    {
                        "name": "Zhirong Chen"
                    },
                    {
                        "name": "Dongrui Fan"
                    },
                    {
                        "name": "Junfeng Gong"
                    },
                    {
                        "name": "Nan Guo"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Qinfen Hao"
                    },
                    {
                        "name": "Shuo Hou"
                    },
                    {
                        "name": "Xuan Huang"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Changxin Ke"
                    },
                    {
                        "name": "Cangyuan Li"
                    },
                    {
                        "name": "Guangli Li"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Naipeng Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Jiahua Liu"
                    },
                    {
                        "name": "Junliang Lv"
                    },
                    {
                        "name": "Jianan Mu"
                    },
                    {
                        "name": "Jin Qin"
                    },
                    {
                        "name": "Bin Sun"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Duo Wang"
                    },
                    {
                        "name": "Mingjun Wang"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Chenggang Wu"
                    },
                    {
                        "name": "Peiyang Wu"
                    },
                    {
                        "name": "Teng Wu"
                    },
                    {
                        "name": "Xiao Xiao"
                    },
                    {
                        "name": "Mengyao Xie"
                    },
                    {
                        "name": "Chenwei Xiong"
                    },
                    {
                        "name": "Ruiyuan Xu"
                    },
                    {
                        "name": "Mingyu Yan"
                    },
                    {
                        "name": "Xiaochun Ye"
                    },
                    {
                        "name": "Kuai Yu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Shuoming Zhang"
                    },
                    {
                        "name": "Jiacheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jiacheng Zhao"
                },
                "author": "Jiacheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02924v1",
                "updated": "2025-06-03T14:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    25,
                    12,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    25,
                    12,
                    1,
                    154,
                    0
                ],
                "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and\n  Prompt-Based Approaches to Depression Symptom Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and\n  Prompt-Based Approaches to Depression Symptom Identification"
                },
                "summary": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams."
                },
                "authors": [
                    {
                        "name": "Diogo A. P. Nunes"
                    },
                    {
                        "name": "Eugnio Ribeiro"
                    }
                ],
                "author_detail": {
                    "name": "Eugnio Ribeiro"
                },
                "author": "Eugnio Ribeiro",
                "arxiv_comment": "12 pages, 1 figure, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.5.4; J.3; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02923v1",
                "updated": "2025-06-03T14:24:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    24,
                    58,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:24:58Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    24,
                    58,
                    1,
                    154,
                    0
                ],
                "title": "The Limits of Predicting Agents from Behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Limits of Predicting Agents from Behaviour"
                },
                "summary": "As the complexity of AI systems and their interactions with the world\nincreases, generating explanations for their behaviour is important for safely\ndeploying AI. For agents, the most natural abstractions for predicting\nbehaviour attribute beliefs, intentions and goals to the system. If an agent\nbehaves as if it has a certain goal or belief, then we can make reasonable\npredictions about how it will behave in novel situations, including those where\ncomprehensive safety evaluations are untenable. How well can we infer an\nagent's beliefs from their behaviour, and how reliably can these inferred\nbeliefs predict the agent's behaviour in novel situations? We provide a precise\nanswer to this question under the assumption that the agent's behaviour is\nguided by a world model. Our contribution is the derivation of novel bounds on\nthe agent's behaviour in new (unseen) deployment environments, which represent\na theoretical limit for predicting intentional agents from behavioural data\nalone. We discuss the implications of these results for several research areas\nincluding fairness and safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the complexity of AI systems and their interactions with the world\nincreases, generating explanations for their behaviour is important for safely\ndeploying AI. For agents, the most natural abstractions for predicting\nbehaviour attribute beliefs, intentions and goals to the system. If an agent\nbehaves as if it has a certain goal or belief, then we can make reasonable\npredictions about how it will behave in novel situations, including those where\ncomprehensive safety evaluations are untenable. How well can we infer an\nagent's beliefs from their behaviour, and how reliably can these inferred\nbeliefs predict the agent's behaviour in novel situations? We provide a precise\nanswer to this question under the assumption that the agent's behaviour is\nguided by a world model. Our contribution is the derivation of novel bounds on\nthe agent's behaviour in new (unseen) deployment environments, which represent\na theoretical limit for predicting intentional agents from behavioural data\nalone. We discuss the implications of these results for several research areas\nincluding fairness and safety."
                },
                "authors": [
                    {
                        "name": "Alexis Bellot"
                    },
                    {
                        "name": "Jonathan Richens"
                    },
                    {
                        "name": "Tom Everitt"
                    }
                ],
                "author_detail": {
                    "name": "Tom Everitt"
                },
                "author": "Tom Everitt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02922v1",
                "updated": "2025-06-03T14:24:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    24,
                    12,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:24:12Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    24,
                    12,
                    1,
                    154,
                    0
                ],
                "title": "Functionality Assessment Framework for Autonomous Driving Systems using\n  Subjective Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functionality Assessment Framework for Autonomous Driving Systems using\n  Subjective Networks"
                },
                "summary": "In complex autonomous driving (AD) software systems, the functioning of each\nsystem part is crucial for safe operation. By measuring the current\nfunctionality or operability of individual components an isolated glimpse into\nthe system is given. Literature provides several of these detached assessments,\noften in the form of safety or performance measures. But dependencies,\nredundancies, error propagation and conflicting functionality statements do not\nallow for easy combination of these measures into a big picture of the\nfunctioning of the entire AD stack. Data is processed and exchanged between\ndifferent components, each of which can fail, making an overall statement\nchallenging. The lack of functionality assessment frameworks that tackle these\nproblems underlines this complexity.\n  This article presents a novel framework for inferring an overall\nfunctionality statement for complex component based systems by considering\ntheir dependencies, redundancies, error propagation paths and the assessments\nof individual components. Our framework first incorporates a comprehensive\nconversion to an assessment representation of the system. The representation is\nbased on Subjective Networks (SNs) that allow for easy identification of faulty\nsystem parts. Second, the framework offers a flexible method for computing the\nsystem's functionality while dealing with contradicting assessments about the\nsame component and dependencies, as well as redundancies, of the system. We\ndiscuss the framework's capabilities on real-life data of our AD stack with\nassessments of various components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In complex autonomous driving (AD) software systems, the functioning of each\nsystem part is crucial for safe operation. By measuring the current\nfunctionality or operability of individual components an isolated glimpse into\nthe system is given. Literature provides several of these detached assessments,\noften in the form of safety or performance measures. But dependencies,\nredundancies, error propagation and conflicting functionality statements do not\nallow for easy combination of these measures into a big picture of the\nfunctioning of the entire AD stack. Data is processed and exchanged between\ndifferent components, each of which can fail, making an overall statement\nchallenging. The lack of functionality assessment frameworks that tackle these\nproblems underlines this complexity.\n  This article presents a novel framework for inferring an overall\nfunctionality statement for complex component based systems by considering\ntheir dependencies, redundancies, error propagation paths and the assessments\nof individual components. Our framework first incorporates a comprehensive\nconversion to an assessment representation of the system. The representation is\nbased on Subjective Networks (SNs) that allow for easy identification of faulty\nsystem parts. Second, the framework offers a flexible method for computing the\nsystem's functionality while dealing with contradicting assessments about the\nsame component and dependencies, as well as redundancies, of the system. We\ndiscuss the framework's capabilities on real-life data of our AD stack with\nassessments of various components."
                },
                "authors": [
                    {
                        "name": "Stefan Orf"
                    },
                    {
                        "name": "Sven Ochs"
                    },
                    {
                        "name": "Valentin Marotta"
                    },
                    {
                        "name": "Oliver Conder"
                    },
                    {
                        "name": "Marc Ren Zofka"
                    },
                    {
                        "name": "J. Marius Zllner"
                    }
                ],
                "author_detail": {
                    "name": "J. Marius Zllner"
                },
                "author": "J. Marius Zllner",
                "arxiv_comment": "submitted to IEEE ITSC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06645v2",
                "updated": "2025-06-03T14:23:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    23,
                    27,
                    1,
                    154,
                    0
                ],
                "published": "2025-01-11T21:41:27Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    21,
                    41,
                    27,
                    5,
                    11,
                    0
                ],
                "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings"
                },
                "summary": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness."
                },
                "authors": [
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02918v1",
                "updated": "2025-06-03T14:20:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    20,
                    59,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:20:59Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    20,
                    59,
                    1,
                    154,
                    0
                ],
                "title": "Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use\n  of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use\n  of LLMs"
                },
                "summary": "Tool use in stateful environments presents unique challenges for large\nlanguage models (LLMs), where existing test-time compute strategies relying on\nrepeated trials in the environment are impractical. We propose dynamics\nmodelling (DyMo), a method that augments LLMs with a state prediction\ncapability alongside function calling during post-training. This enables LLMs\nto predict the future states of their actions through an internal environment\nmodel. On the Berkeley Function Calling Leaderboard V2, DyMo improves success\nrates and significantly reduces hallucinations. We further integrate the\ninternal environment model into self-verification sampling (SVS), and show that\nthis substantially improves pass^k over number of trials k, and allows the\nmodel to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the\neffectiveness and reliability of LLMs for tool use. We believe this work charts\na path towards scalable planning RL methods for LLM inference without\nrepeatedly querying the oracle environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool use in stateful environments presents unique challenges for large\nlanguage models (LLMs), where existing test-time compute strategies relying on\nrepeated trials in the environment are impractical. We propose dynamics\nmodelling (DyMo), a method that augments LLMs with a state prediction\ncapability alongside function calling during post-training. This enables LLMs\nto predict the future states of their actions through an internal environment\nmodel. On the Berkeley Function Calling Leaderboard V2, DyMo improves success\nrates and significantly reduces hallucinations. We further integrate the\ninternal environment model into self-verification sampling (SVS), and show that\nthis substantially improves pass^k over number of trials k, and allows the\nmodel to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the\neffectiveness and reliability of LLMs for tool use. We believe this work charts\na path towards scalable planning RL methods for LLM inference without\nrepeatedly querying the oracle environment."
                },
                "authors": [
                    {
                        "name": "Shangmin Guo"
                    },
                    {
                        "name": "Omar Darwiche Domingues"
                    },
                    {
                        "name": "Raphal Avalos"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Florian Strub"
                    }
                ],
                "author_detail": {
                    "name": "Florian Strub"
                },
                "author": "Florian Strub",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02911v1",
                "updated": "2025-06-03T14:16:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    16,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:16:53Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    16,
                    53,
                    1,
                    154,
                    0
                ],
                "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with\n  Reinforcement Learning"
                },
                "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1."
                },
                "authors": [
                    {
                        "name": "Yin Fang"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Xianrui Zhong"
                    },
                    {
                        "name": "Siru Ouyang"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "arxiv_comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02909v1",
                "updated": "2025-06-03T14:15:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    15,
                    25,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:15:25Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    15,
                    25,
                    1,
                    154,
                    0
                ],
                "title": "Test Gravitational-Wave Polarizations with Space-Based Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Gravitational-Wave Polarizations with Space-Based Detectors"
                },
                "summary": "In this work, we systematically investigate the capability of space-based\ngravitational wave detectors in constraining parameters of non-tensor\npolarization modes. Using Bayesian inference and Fisher Information Matrix\nmethods, we analyze gravitational wave signals from the inspiral phase of\nsupermassive binary black hole mergers. By starting with time-domain signals\nand applying Fourier transforms, we avoid the use of the stationary phase\napproximation. We found an asymmetry in the estimation of the vector-mode\nparameter $\\alpha_x$ at inclination angles $\\iota = 0$ and $\\iota = \\pi$, which\nhas not been explicitly pointed out in previous studies. We also observe strong\ncorrelations between scalar-mode parameters, $\\alpha_b$ and $\\alpha_l$, which\ncurrently limit their independent estimation. These findings underscore the\nimportance of using complete inspiral-merger-ringdown waveforms to enhance the\nability to distinguish the non-tensor polarization modes. Finally, we employ a\nnew LISA-Taiji network configuration, in which the orientation of spacecrafts\nof Taiji maintains a fixed phase offset relative to these of LISA. Under the\nadiabatic approximation and the assumption of equal arms, this phase is found\nto have no significant effect on data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we systematically investigate the capability of space-based\ngravitational wave detectors in constraining parameters of non-tensor\npolarization modes. Using Bayesian inference and Fisher Information Matrix\nmethods, we analyze gravitational wave signals from the inspiral phase of\nsupermassive binary black hole mergers. By starting with time-domain signals\nand applying Fourier transforms, we avoid the use of the stationary phase\napproximation. We found an asymmetry in the estimation of the vector-mode\nparameter $\\alpha_x$ at inclination angles $\\iota = 0$ and $\\iota = \\pi$, which\nhas not been explicitly pointed out in previous studies. We also observe strong\ncorrelations between scalar-mode parameters, $\\alpha_b$ and $\\alpha_l$, which\ncurrently limit their independent estimation. These findings underscore the\nimportance of using complete inspiral-merger-ringdown waveforms to enhance the\nability to distinguish the non-tensor polarization modes. Finally, we employ a\nnew LISA-Taiji network configuration, in which the orientation of spacecrafts\nof Taiji maintains a fixed phase offset relative to these of LISA. Under the\nadiabatic approximation and the assumption of equal arms, this phase is found\nto have no significant effect on data analysis."
                },
                "authors": [
                    {
                        "name": "Jun-Shuai Wang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ju Chen"
                    },
                    {
                        "name": "Jibo He"
                    }
                ],
                "author_detail": {
                    "name": "Jibo He"
                },
                "author": "Jibo He",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02908v1",
                "updated": "2025-06-03T14:14:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    14,
                    28,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:14:28Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    14,
                    28,
                    1,
                    154,
                    0
                ],
                "title": "Diffusion Buffer: Online Diffusion-based Speech Enhancement with\n  Sub-Second Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Buffer: Online Diffusion-based Speech Enhancement with\n  Sub-Second Latency"
                },
                "summary": "Diffusion models are a class of generative models that have been recently\nused for speech enhancement with remarkable success but are computationally\nexpensive at inference time. Therefore, these models are impractical for\nprocessing streaming data in real-time. In this work, we adapt a sliding window\ndiffusion framework to the speech enhancement task. Our approach progressively\ncorrupts speech signals through time, assigning more noise to frames close to\nthe present in a buffer. This approach outputs denoised frames with a delay\nproportional to the chosen buffer size, enabling a trade-off between\nperformance and latency. Empirical results demonstrate that our method\noutperforms standard diffusion models and runs efficiently on a GPU, achieving\nan input-output latency in the order of 0.3 to 1 seconds. This marks the first\npractical diffusion-based solution for online speech enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a class of generative models that have been recently\nused for speech enhancement with remarkable success but are computationally\nexpensive at inference time. Therefore, these models are impractical for\nprocessing streaming data in real-time. In this work, we adapt a sliding window\ndiffusion framework to the speech enhancement task. Our approach progressively\ncorrupts speech signals through time, assigning more noise to frames close to\nthe present in a buffer. This approach outputs denoised frames with a delay\nproportional to the chosen buffer size, enabling a trade-off between\nperformance and latency. Empirical results demonstrate that our method\noutperforms standard diffusion models and runs efficiently on a GPU, achieving\nan input-output latency in the order of 0.3 to 1 seconds. This marks the first\npractical diffusion-based solution for online speech enhancement."
                },
                "authors": [
                    {
                        "name": "Bunlong Lay"
                    },
                    {
                        "name": "Rostilav Makarov"
                    },
                    {
                        "name": "Timo Gerkmann"
                    }
                ],
                "author_detail": {
                    "name": "Timo Gerkmann"
                },
                "author": "Timo Gerkmann",
                "arxiv_comment": "5 pages, 2 figures, Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12574v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12574v4",
                "updated": "2025-06-03T14:13:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    13,
                    57,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-18T23:22:53Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    23,
                    22,
                    53,
                    6,
                    138,
                    0
                ],
                "title": "PoisonArena: Uncovering Competing Poisoning Attacks in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoisonArena: Uncovering Competing Poisoning Attacks in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems, widely used to improve the\nfactual grounding of large language models (LLMs), are increasingly vulnerable\nto poisoning attacks, where adversaries inject manipulated content into the\nretriever's corpus. While prior research has predominantly focused on\nsingle-attacker settings, real-world scenarios often involve multiple,\ncompeting attackers with conflicting objectives. In this work, we introduce\nPoisonArena, the first benchmark to systematically study and evaluate competing\npoisoning attacks in RAG. We formalize the multi-attacker threat model, where\nattackers vie to control the answer to the same query using mutually exclusive\nmisinformation. PoisonArena leverages the Bradley-Terry model to quantify each\nmethod's competitive effectiveness in such adversarial environments. Through\nextensive experiments on the Natural Questions and MS MARCO datasets, we\ndemonstrate that many attack strategies successful in isolation fail under\ncompetitive pressure. Our findings highlight the limitations of conventional\nevaluation metrics like Attack Success Rate (ASR) and F1 score and underscore\nthe need for competitive evaluation to assess real-world attack robustness.\nPoisonArena provides a standardized framework to benchmark and develop future\nattack and defense strategies under more realistic, multi-adversary conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems, widely used to improve the\nfactual grounding of large language models (LLMs), are increasingly vulnerable\nto poisoning attacks, where adversaries inject manipulated content into the\nretriever's corpus. While prior research has predominantly focused on\nsingle-attacker settings, real-world scenarios often involve multiple,\ncompeting attackers with conflicting objectives. In this work, we introduce\nPoisonArena, the first benchmark to systematically study and evaluate competing\npoisoning attacks in RAG. We formalize the multi-attacker threat model, where\nattackers vie to control the answer to the same query using mutually exclusive\nmisinformation. PoisonArena leverages the Bradley-Terry model to quantify each\nmethod's competitive effectiveness in such adversarial environments. Through\nextensive experiments on the Natural Questions and MS MARCO datasets, we\ndemonstrate that many attack strategies successful in isolation fail under\ncompetitive pressure. Our findings highlight the limitations of conventional\nevaluation metrics like Attack Success Rate (ASR) and F1 score and underscore\nthe need for competitive evaluation to assess real-world attack robustness.\nPoisonArena provides a standardized framework to benchmark and develop future\nattack and defense strategies under more realistic, multi-adversary conditions."
                },
                "authors": [
                    {
                        "name": "Liuji Chen"
                    },
                    {
                        "name": "Xiaofang Yang"
                    },
                    {
                        "name": "Yuanzhuo Lu"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Jing Dong"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "arxiv_comment": "Project page: https://poison-arena.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12574v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12574v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15587v2",
                "updated": "2025-06-03T14:12:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    12,
                    11,
                    1,
                    154,
                    0
                ],
                "published": "2024-03-22T19:21:44Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    19,
                    21,
                    44,
                    4,
                    82,
                    0
                ],
                "title": "Large language models for crowd decision making based on prompt design\n  strategies using ChatGPT: models, analysis and challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for crowd decision making based on prompt design\n  strategies using ChatGPT: models, analysis and challenges"
                },
                "summary": "Social Media and Internet have the potential to be exploited as a source of\nopinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a\nmethodology able to infer opinions and decisions from plain texts, such as\nreviews published in social media platforms, by means of Sentiment Analysis.\nCurrently, the emergence and potential of Large Language Models (LLMs) lead us\nto explore new scenarios of automatically understand written texts, also known\nas natural language processing. This paper analyzes the use of ChatGPT based on\nprompt design strategies to assist in CDM processes to extract opinions and\nmake decisions. We integrate ChatGPT in CDM processes as a flexible tool that\ninfer the opinions expressed in texts, providing numerical or linguistic\nevaluations where the decision making models are based on the prompt design\nstrategies. We include a multi-criteria decision making scenario with a\ncategory ontology for criteria. We also consider ChatGPT as an end-to-end CDM\nmodel able to provide a general opinion and score on the alternatives. We\nconduct empirical experiments on real data extracted from TripAdvisor, the\nTripR-2020Large dataset. The analysis of results show a promising branch for\ndeveloping quality decision making models using ChatGPT. Finally, we discuss\nthe challenges of consistency, sensitivity and explainability associated to the\nuse of LLMs in CDM processes, raising open questions for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Media and Internet have the potential to be exploited as a source of\nopinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a\nmethodology able to infer opinions and decisions from plain texts, such as\nreviews published in social media platforms, by means of Sentiment Analysis.\nCurrently, the emergence and potential of Large Language Models (LLMs) lead us\nto explore new scenarios of automatically understand written texts, also known\nas natural language processing. This paper analyzes the use of ChatGPT based on\nprompt design strategies to assist in CDM processes to extract opinions and\nmake decisions. We integrate ChatGPT in CDM processes as a flexible tool that\ninfer the opinions expressed in texts, providing numerical or linguistic\nevaluations where the decision making models are based on the prompt design\nstrategies. We include a multi-criteria decision making scenario with a\ncategory ontology for criteria. We also consider ChatGPT as an end-to-end CDM\nmodel able to provide a general opinion and score on the alternatives. We\nconduct empirical experiments on real data extracted from TripAdvisor, the\nTripR-2020Large dataset. The analysis of results show a promising branch for\ndeveloping quality decision making models using ChatGPT. Finally, we discuss\nthe challenges of consistency, sensitivity and explainability associated to the\nuse of LLMs in CDM processes, raising open questions for future studies."
                },
                "authors": [
                    {
                        "name": "David Herrera-Poyatos"
                    },
                    {
                        "name": "Cristina Zuheros"
                    },
                    {
                        "name": "Rosana Montes"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18924v2",
                "updated": "2025-06-03T14:11:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    11,
                    24,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-25T01:10:58Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    1,
                    10,
                    58,
                    6,
                    145,
                    0
                ],
                "title": "LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point Cloud\n  Active Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point Cloud\n  Active Learning"
                },
                "summary": "We present a novel active learning framework for 3D point cloud semantic\nsegmentation that, for the first time, integrates large language models (LLMs)\nto construct hierarchical label structures and guide uncertainty-based sample\nselection. Unlike prior methods that treat labels as flat and independent, our\napproach leverages LLM prompting to automatically generate multi-level semantic\ntaxonomies and introduces a recursive uncertainty projection mechanism that\npropagates uncertainty across hierarchy levels. This enables spatially diverse,\nlabel-aware point selection that respects the inherent semantic structure of 3D\nscenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to\n4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%),\nsubstantially outperforming existing baselines. Our results highlight the\nuntapped potential of LLMs as knowledge priors in 3D vision and establish\nhierarchical uncertainty modeling as a powerful paradigm for efficient point\ncloud annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel active learning framework for 3D point cloud semantic\nsegmentation that, for the first time, integrates large language models (LLMs)\nto construct hierarchical label structures and guide uncertainty-based sample\nselection. Unlike prior methods that treat labels as flat and independent, our\napproach leverages LLM prompting to automatically generate multi-level semantic\ntaxonomies and introduces a recursive uncertainty projection mechanism that\npropagates uncertainty across hierarchy levels. This enables spatially diverse,\nlabel-aware point selection that respects the inherent semantic structure of 3D\nscenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to\n4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%),\nsubstantially outperforming existing baselines. Our results highlight the\nuntapped potential of LLMs as knowledge priors in 3D vision and establish\nhierarchical uncertainty modeling as a powerful paradigm for efficient point\ncloud annotation."
                },
                "authors": [
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Fengyun Tan"
                    },
                    {
                        "name": "Yantong Chen"
                    },
                    {
                        "name": "Bochun Yuan"
                    },
                    {
                        "name": "Tianrui Li"
                    },
                    {
                        "name": "Chongshou Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongshou Li"
                },
                "author": "Chongshou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19318v4",
                "updated": "2025-06-03T14:08:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    8,
                    24,
                    1,
                    154,
                    0
                ],
                "published": "2025-01-31T17:15:33Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    15,
                    33,
                    4,
                    31,
                    0
                ],
                "title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems"
                },
                "summary": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience."
                },
                "authors": [
                    {
                        "name": "Anirudh Chari"
                    },
                    {
                        "name": "Suraj Reddy"
                    },
                    {
                        "name": "Aditya Tiwari"
                    },
                    {
                        "name": "Richard Lian"
                    },
                    {
                        "name": "Brian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Brian Zhou"
                },
                "author": "Brian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.03145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03145v1",
                "updated": "2025-06-03T17:59:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:59:18Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    18,
                    1,
                    154,
                    0
                ],
                "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and\n  Semantic Understanding Capability of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and\n  Semantic Understanding Capability of LLM"
                },
                "summary": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions."
                },
                "authors": [
                    {
                        "name": "Pralaypati Ta"
                    },
                    {
                        "name": "Sriram Venkatesaperumal"
                    },
                    {
                        "name": "Keerthi Ram"
                    },
                    {
                        "name": "Mohanasankar Sivaprakasam"
                    }
                ],
                "author_detail": {
                    "name": "Mohanasankar Sivaprakasam"
                },
                "author": "Mohanasankar Sivaprakasam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03142v1",
                "updated": "2025-06-03T17:59:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    5,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:59:05Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    59,
                    5,
                    1,
                    154,
                    0
                ],
                "title": "Not All Tokens Are Meant to Be Forgotten",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Tokens Are Meant to Be Forgotten"
                },
                "summary": "Large Language Models (LLMs), pre-trained on massive text corpora, exhibit\nremarkable human-level language understanding, reasoning, and decision-making\nabilities. However, they tend to memorize unwanted information, such as private\nor copyrighted content, raising significant privacy and legal concerns.\nUnlearning has emerged as a promising solution, but existing methods face a\nsignificant challenge of over-forgetting. This issue arises because they\nindiscriminately suppress the generation of all the tokens in forget samples,\nleading to a substantial loss of model utility. To overcome this challenge, we\nintroduce the Targeted Information Forgetting (TIF) framework, which consists\nof (1) a flexible targeted information identifier designed to differentiate\nbetween unwanted words (UW) and general words (GW) in the forget samples, and\n(2) a novel Targeted Preference Optimization approach that leverages Logit\nPreference Loss to unlearn unwanted information associated with UW and\nPreservation Loss to retain general information in GW, effectively improving\nthe unlearning process while mitigating utility degradation. Extensive\nexperiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF\nframework enhances unlearning effectiveness while preserving model utility and\nachieving state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), pre-trained on massive text corpora, exhibit\nremarkable human-level language understanding, reasoning, and decision-making\nabilities. However, they tend to memorize unwanted information, such as private\nor copyrighted content, raising significant privacy and legal concerns.\nUnlearning has emerged as a promising solution, but existing methods face a\nsignificant challenge of over-forgetting. This issue arises because they\nindiscriminately suppress the generation of all the tokens in forget samples,\nleading to a substantial loss of model utility. To overcome this challenge, we\nintroduce the Targeted Information Forgetting (TIF) framework, which consists\nof (1) a flexible targeted information identifier designed to differentiate\nbetween unwanted words (UW) and general words (GW) in the forget samples, and\n(2) a novel Targeted Preference Optimization approach that leverages Logit\nPreference Loss to unlearn unwanted information associated with UW and\nPreservation Loss to retain general information in GW, effectively improving\nthe unlearning process while mitigating utility degradation. Extensive\nexperiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF\nframework enhances unlearning effectiveness while preserving model utility and\nachieving state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhou"
                    },
                    {
                        "name": "Yao Qiang"
                    },
                    {
                        "name": "Saleh Zare Zade"
                    },
                    {
                        "name": "Douglas Zytko"
                    },
                    {
                        "name": "Prashant Khanduri"
                    },
                    {
                        "name": "Dongxiao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Zhu"
                },
                "author": "Dongxiao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03139v1",
                "updated": "2025-06-03T17:58:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    58,
                    57,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:58:57Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    58,
                    57,
                    1,
                    154,
                    0
                ],
                "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation"
                },
                "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius."
                },
                "authors": [
                    {
                        "name": "Siqi Chen"
                    },
                    {
                        "name": "Xinyu Dong"
                    },
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Linjuan Wu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "19 pages,4 figures, Project page:\n  https://zju-real.github.io/SVGenius, Code:\n  https://github.com/ZJU-REAL/SVGenius-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03136v1",
                "updated": "2025-06-03T17:58:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    58,
                    42,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:58:42Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    58,
                    42,
                    1,
                    154,
                    0
                ],
                "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning"
                },
                "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Project: https://github.com/Gen-Verse/CURE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03131v1",
                "updated": "2025-06-03T17:57:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    57,
                    33,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:57:33Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    57,
                    33,
                    1,
                    154,
                    0
                ],
                "title": "Native-Resolution Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Native-Resolution Image Synthesis"
                },
                "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies."
                },
                "authors": [
                    {
                        "name": "Zidong Wang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiyuan Zhang"
                },
                "author": "Yiyuan Zhang",
                "arxiv_comment": "Project Page: https://wzdthu.github.io/NiT/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03122v1",
                "updated": "2025-06-03T17:54:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    54,
                    30,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:54:30Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    54,
                    30,
                    1,
                    154,
                    0
                ],
                "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit\n  Topology Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit\n  Topology Generation"
                },
                "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design."
                },
                "authors": [
                    {
                        "name": "Prashanth Vijayaraghavan"
                    },
                    {
                        "name": "Luyao Shi"
                    },
                    {
                        "name": "Ehsan Degan"
                    },
                    {
                        "name": "Vandana Mukherjee"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_comment": "9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18460v2",
                "updated": "2025-06-03T17:47:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    47,
                    36,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-25T18:59:07Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    18,
                    59,
                    7,
                    1,
                    56,
                    0
                ],
                "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers"
                },
                "summary": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Barlas Oguz"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Xilun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilun Chen"
                },
                "author": "Xilun Chen",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03106v1",
                "updated": "2025-06-03T17:39:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    39,
                    2,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:39:02Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    39,
                    2,
                    1,
                    154,
                    0
                ],
                "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback"
                },
                "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration."
                },
                "authors": [
                    {
                        "name": "Xiaoying Zhang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03100v1",
                "updated": "2025-06-03T17:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    31,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:31:53Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    31,
                    53,
                    1,
                    154,
                    0
                ],
                "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified\n  Theory and Risk Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified\n  Theory and Risk Bounds"
                },
                "summary": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA."
                },
                "authors": [
                    {
                        "name": "Yang Guo"
                    },
                    {
                        "name": "Yutian Tao"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Robert D. Nowak"
                    },
                    {
                        "name": "Yingyu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yingyu Liang"
                },
                "author": "Yingyu Liang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03099v1",
                "updated": "2025-06-03T17:29:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    29,
                    28,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:29:28Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    29,
                    28,
                    1,
                    154,
                    0
                ],
                "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models"
                },
                "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/"
                },
                "authors": [
                    {
                        "name": "Chetwin Low"
                    },
                    {
                        "name": "Weimin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weimin Wang"
                },
                "author": "Weimin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11184v2",
                "updated": "2025-06-03T17:27:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    27,
                    16,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-16T16:12:40Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    12,
                    40,
                    6,
                    47,
                    0
                ],
                "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety\n  Awareness for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety\n  Awareness for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Kuiyi Gao"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Pinjia He"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03095v1",
                "updated": "2025-06-03T17:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    27,
                    4,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    27,
                    4,
                    1,
                    154,
                    0
                ],
                "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPO Learning with LLMs-Judge Signal for Computer Use Agents"
                },
                "summary": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents."
                },
                "authors": [
                    {
                        "name": "Man Luo"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Xin Su"
                    },
                    {
                        "name": "Shachar Rosenman"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Shao-Yen Tseng"
                    },
                    {
                        "name": "Phillip Howard"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Howard"
                },
                "author": "Phillip Howard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03090v1",
                "updated": "2025-06-03T17:19:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    19,
                    45,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:19:45Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    19,
                    45,
                    1,
                    154,
                    0
                ],
                "title": "Literary Evidence Retrieval via Long-Context Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literary Evidence Retrieval via Long-Context Language Models"
                },
                "summary": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction."
                },
                "authors": [
                    {
                        "name": "Katherine Thai"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v2",
                "updated": "2025-06-03T17:18:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    18,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04977v2",
                "updated": "2025-06-03T17:16:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    16,
                    24,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-08T06:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    30,
                    46,
                    3,
                    128,
                    0
                ],
                "title": "ChainMarks: Securing DNN Watermark with Cryptographic Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChainMarks: Securing DNN Watermark with Cryptographic Chain"
                },
                "summary": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy."
                },
                "authors": [
                    {
                        "name": "Brian Choi"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Isabelle Choi"
                    },
                    {
                        "name": "Kun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Kun Sun"
                },
                "author": "Kun Sun",
                "arxiv_doi": "10.1145/3708821.3736214",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708821.3736214",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.04977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted In ACM ASIA Conference on Computer and Communications\n  Security (ASIA CCS '25), August 25-29, 2025, Ha Noi, Vietnam",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01213v2",
                "updated": "2025-06-03T17:15:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    15,
                    57,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-01T23:17:19Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    17,
                    19,
                    6,
                    152,
                    0
                ],
                "title": "On the Stability of Graph Convolutional Neural Networks: A Probabilistic\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Stability of Graph Convolutional Neural Networks: A Probabilistic\n  Perspective"
                },
                "summary": "Graph convolutional neural networks (GCNNs) have emerged as powerful tools\nfor analyzing graph-structured data, achieving remarkable success across\ndiverse applications. However, the theoretical understanding of the stability\nof these models, i.e., their sensitivity to small changes in the graph\nstructure, remains in rather limited settings, hampering the development and\ndeployment of robust and trustworthy models in practice. To fill this gap, we\nstudy how perturbations in the graph topology affect GCNN outputs and propose a\nnovel formulation for analyzing model stability. Unlike prior studies that\nfocus only on worst-case perturbations, our distribution-aware formulation\ncharacterizes output perturbations across a broad range of input data. This\nway, our framework enables, for the first time, a probabilistic perspective on\nthe interplay between the statistical properties of the node data and\nperturbations in the graph topology. We conduct extensive experiments to\nvalidate our theoretical findings and demonstrate their benefits over existing\nbaselines, in terms of both representation stability and adversarial attacks on\ndownstream tasks. Our results demonstrate the practical significance of the\nproposed formulation and highlight the importance of incorporating data\ndistribution into stability analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional neural networks (GCNNs) have emerged as powerful tools\nfor analyzing graph-structured data, achieving remarkable success across\ndiverse applications. However, the theoretical understanding of the stability\nof these models, i.e., their sensitivity to small changes in the graph\nstructure, remains in rather limited settings, hampering the development and\ndeployment of robust and trustworthy models in practice. To fill this gap, we\nstudy how perturbations in the graph topology affect GCNN outputs and propose a\nnovel formulation for analyzing model stability. Unlike prior studies that\nfocus only on worst-case perturbations, our distribution-aware formulation\ncharacterizes output perturbations across a broad range of input data. This\nway, our framework enables, for the first time, a probabilistic perspective on\nthe interplay between the statistical properties of the node data and\nperturbations in the graph topology. We conduct extensive experiments to\nvalidate our theoretical findings and demonstrate their benefits over existing\nbaselines, in terms of both representation stability and adversarial attacks on\ndownstream tasks. Our results demonstrate the practical significance of the\nproposed formulation and highlight the importance of incorporating data\ndistribution into stability analysis."
                },
                "authors": [
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Henry Kenlay"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Mihai Cucuringu"
                    },
                    {
                        "name": "Xiaowen Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Dong"
                },
                "author": "Xiaowen Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03087v1",
                "updated": "2025-06-03T17:11:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    11,
                    5,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T17:11:05Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    11,
                    5,
                    1,
                    154,
                    0
                ],
                "title": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks\n  via Explanation Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks\n  via Explanation Alignment"
                },
                "summary": "Graph Neural Networks (GNNs) have become essential tools for analyzing\ngraph-structured data in domains such as drug discovery and financial analysis,\nleading to growing demands for model transparency. Recent advances in\nexplainable GNNs have addressed this need by revealing important subgraphs that\ninfluence predictions, but these explanation mechanisms may inadvertently\nexpose models to security risks. This paper investigates how such explanations\npotentially leak critical decision logic that can be exploited for model\nstealing. We propose {\\method}, a novel stealing framework that integrates\nexplanation alignment for capturing decision logic with guided data\naugmentation for efficient training under limited queries, enabling effective\nreplication of both the predictive behavior and underlying reasoning patterns\nof target models. Experiments on molecular graph datasets demonstrate that our\napproach shows advantages over conventional methods in model stealing. This\nwork highlights important security considerations for the deployment of\nexplainable GNNs in sensitive domains and suggests the need for protective\nmeasures against explanation-based attacks. Our code is available at\nhttps://github.com/beanmah/EGSteal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become essential tools for analyzing\ngraph-structured data in domains such as drug discovery and financial analysis,\nleading to growing demands for model transparency. Recent advances in\nexplainable GNNs have addressed this need by revealing important subgraphs that\ninfluence predictions, but these explanation mechanisms may inadvertently\nexpose models to security risks. This paper investigates how such explanations\npotentially leak critical decision logic that can be exploited for model\nstealing. We propose {\\method}, a novel stealing framework that integrates\nexplanation alignment for capturing decision logic with guided data\naugmentation for efficient training under limited queries, enabling effective\nreplication of both the predictive behavior and underlying reasoning patterns\nof target models. Experiments on molecular graph datasets demonstrate that our\napproach shows advantages over conventional methods in model stealing. This\nwork highlights important security considerations for the deployment of\nexplainable GNNs in sensitive domains and suggests the need for protective\nmeasures against explanation-based attacks. Our code is available at\nhttps://github.com/beanmah/EGSteal."
                },
                "authors": [
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Yuyuan Feng"
                    },
                    {
                        "name": "Minhua Lin"
                    },
                    {
                        "name": "Enyan Dai"
                    }
                ],
                "author_detail": {
                    "name": "Enyan Dai"
                },
                "author": "Enyan Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13172v2",
                "updated": "2025-06-03T17:08:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    8,
                    56,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-17T19:55:53Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    55,
                    53,
                    0,
                    48,
                    0
                ],
                "title": "Unveiling Privacy Risks in LLM Agent Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Privacy Risks in LLM Agent Memory"
                },
                "summary": "Large Language Model (LLM) agents have become increasingly prevalent across\nvarious real-world applications. They enhance decision-making by storing\nprivate user-agent interactions in the memory module for demonstrations,\nintroducing new privacy risks for LLM agents. In this work, we systematically\ninvestigate the vulnerability of LLM agents to our proposed Memory EXTRaction\nAttack (MEXTRA) under a black-box setting. To extract private information from\nmemory, we propose an effective attacking prompt design and an automated prompt\ngeneration method based on different levels of knowledge about the LLM agent.\nExperiments on two representative agents demonstrate the effectiveness of\nMEXTRA. Moreover, we explore key factors influencing memory leakage from both\nthe agent designer's and the attacker's perspectives. Our findings highlight\nthe urgent need for effective memory safeguards in LLM agent design and\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have become increasingly prevalent across\nvarious real-world applications. They enhance decision-making by storing\nprivate user-agent interactions in the memory module for demonstrations,\nintroducing new privacy risks for LLM agents. In this work, we systematically\ninvestigate the vulnerability of LLM agents to our proposed Memory EXTRaction\nAttack (MEXTRA) under a black-box setting. To extract private information from\nmemory, we propose an effective attacking prompt design and an automated prompt\ngeneration method based on different levels of knowledge about the LLM agent.\nExperiments on two representative agents demonstrate the effectiveness of\nMEXTRA. Moreover, we explore key factors influencing memory leakage from both\nthe agent designer's and the attacker's perspectives. Our findings highlight\nthe urgent need for effective memory safeguards in LLM agent design and\ndeployment."
                },
                "authors": [
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Weiyi He"
                    },
                    {
                        "name": "Shenglai Zeng"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Yue Xing"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Pengfei He"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei He"
                },
                "author": "Pengfei He",
                "arxiv_comment": "ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12216v2",
                "updated": "2025-06-03T17:02:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    2,
                    25,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-16T16:08:45Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    8,
                    45,
                    2,
                    106,
                    0
                ],
                "title": "d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d1: Scaling Reasoning in Diffusion Large Language Models via\n  Reinforcement Learning"
                },
                "summary": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO, the first integration of policy gradient methods to masked dLLMs.\nThrough empirical studies, we investigate the performance of different\npost-training recipes on multiple mathematical and planning benchmarks. We find\nthat d1 yields the best performance and significantly improves performance of a\nstate-of-the-art dLLM. Our code is released at\nhttps://dllm-reasoning.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO, the first integration of policy gradient methods to masked dLLMs.\nThrough empirical studies, we investigate the performance of different\npost-training recipes on multiple mathematical and planning benchmarks. We find\nthat d1 yields the best performance and significantly improves performance of a\nstate-of-the-art dLLM. Our code is released at\nhttps://dllm-reasoning.github.io/."
                },
                "authors": [
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Devaansh Gupta"
                    },
                    {
                        "name": "Qinqing Zheng"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "27 pages, project page at https://dllm-reasoning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07783v3",
                "updated": "2025-06-03T17:02:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    2,
                    13,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-12T17:36:14Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    36,
                    14,
                    0,
                    132,
                    0
                ],
                "title": "Relative Overfitting and Accept-Reject Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relative Overfitting and Accept-Reject Framework"
                },
                "summary": "Currently, the scaling law of Large Language Models (LLMs) faces challenges\nand bottlenecks. This paper posits that noise effects, stemming from changes in\nthe signal-to-noise ratio under diminishing marginal returns, are the root\ncause of these issues. To control this noise, we investigated the differences\nbetween models with performance advantages and disadvantages, introducing the\nconcept of \"relative overfitting.\" Based on their complementary strengths, we\nhave proposed an application framework, Accept-Reject (AR), and the associated\nAR Law, which operates within this framework to elucidate the patterns of\nperformance changes after model integration. In Natural Language Processing\n(NLP), we use LLMs and Small Language Models (SLMs) as the medium for\ndiscussion. This framework enables SLMs to exert a universal positive influence\non LLM decision outputs, rather than the intuitively expected potential\nnegative influence. We validated our approach using self-built models based on\nmainstream architectures and pre-trained mainstream models across multiple\ndatasets, including basic language modeling, long-context tasks, subject\nexamination, and question-answering (QA) benchmarks. The results demonstrate\nthat through our framework, compared to increasing the LLM's parameters, we can\nachieve better performance improvements with significantly lower parameter and\ncomputational costs in many scenarios. These improvements are universal,\nstable, and effective. Furthermore, we explore the potential of \"relative\noverfitting\" and the AR framework in other machine learning domains, such as\ncomputer vision (CV) and AI for science. We hope the proposed approach can help\nscale laws overcome existing bottlenecks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, the scaling law of Large Language Models (LLMs) faces challenges\nand bottlenecks. This paper posits that noise effects, stemming from changes in\nthe signal-to-noise ratio under diminishing marginal returns, are the root\ncause of these issues. To control this noise, we investigated the differences\nbetween models with performance advantages and disadvantages, introducing the\nconcept of \"relative overfitting.\" Based on their complementary strengths, we\nhave proposed an application framework, Accept-Reject (AR), and the associated\nAR Law, which operates within this framework to elucidate the patterns of\nperformance changes after model integration. In Natural Language Processing\n(NLP), we use LLMs and Small Language Models (SLMs) as the medium for\ndiscussion. This framework enables SLMs to exert a universal positive influence\non LLM decision outputs, rather than the intuitively expected potential\nnegative influence. We validated our approach using self-built models based on\nmainstream architectures and pre-trained mainstream models across multiple\ndatasets, including basic language modeling, long-context tasks, subject\nexamination, and question-answering (QA) benchmarks. The results demonstrate\nthat through our framework, compared to increasing the LLM's parameters, we can\nachieve better performance improvements with significantly lower parameter and\ncomputational costs in many scenarios. These improvements are universal,\nstable, and effective. Furthermore, we explore the potential of \"relative\noverfitting\" and the AR framework in other machine learning domains, such as\ncomputer vision (CV) and AI for science. We hope the proposed approach can help\nscale laws overcome existing bottlenecks."
                },
                "authors": [
                    {
                        "name": "Yanxin Liu"
                    },
                    {
                        "name": "Yunqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yunqi Zhang"
                },
                "author": "Yunqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03077v1",
                "updated": "2025-06-03T16:54:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    54,
                    15,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:54:15Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    54,
                    15,
                    1,
                    154,
                    0
                ],
                "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs"
                },
                "summary": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP."
                },
                "authors": [
                    {
                        "name": "Qijun Luo"
                    },
                    {
                        "name": "Mengqi Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Xiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Li"
                },
                "author": "Xiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01297v3",
                "updated": "2025-06-04T02:07:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    2,
                    7,
                    51,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-02T04:14:03Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    4,
                    14,
                    3,
                    0,
                    153,
                    0
                ],
                "title": "MobCLIP: Learning General-purpose Geospatial Representation at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobCLIP: Learning General-purpose Geospatial Representation at Scale"
                },
                "summary": "Representation learning of geospatial locations remains a core challenge in\nachieving general geospatial intelligence. Current embedding methods often lack\nversatility, limiting their utility across diverse tasks in both human and\nnatural domains. We present MobCLIP, the first nationwide general-purpose\nlocation encoder, integrating an unprecedented diversity of data modalities\nthrough effective and scalable multimodal fusion. Adopting a novel CLIP-based\narchitecture, our framework aligns 100M+ POIs, nationwide remote sensing\nimagery, and structured demographic statistics with a billion-edge mobility\ngraph. By tokenizing spatial locations into grid cells inspired by Vision\nTransformers, we establish a unified representation space bridging mobility\npatterns and multimodal features. To rigorously evaluate the general-purpose\neffectiveness of MobCLIP, we construct a benchmark dataset composed of 11\ndownstream prediction tasks across social, economic, and natural domains.\nExperiments show that MobCLIP, with four input modalities and a compact\n128-dimensional representation space, achieves significantly superior\ngeneral-purpose predictive performances than state-of-the-art models by an\naverage of 35%. Thanks to the effective integration of human-centric\nmodalities, the performance gain is particularly profound in human-centric\ntasks, such as energy consumption (+260%), offline retail consumption amount\n(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we\nfurther demonstrate the scaling behavior in geospatial representation learning.\nWe open-source code and pretrained models at:\nhttps://github.com/ylzhouchris/MobCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation learning of geospatial locations remains a core challenge in\nachieving general geospatial intelligence. Current embedding methods often lack\nversatility, limiting their utility across diverse tasks in both human and\nnatural domains. We present MobCLIP, the first nationwide general-purpose\nlocation encoder, integrating an unprecedented diversity of data modalities\nthrough effective and scalable multimodal fusion. Adopting a novel CLIP-based\narchitecture, our framework aligns 100M+ POIs, nationwide remote sensing\nimagery, and structured demographic statistics with a billion-edge mobility\ngraph. By tokenizing spatial locations into grid cells inspired by Vision\nTransformers, we establish a unified representation space bridging mobility\npatterns and multimodal features. To rigorously evaluate the general-purpose\neffectiveness of MobCLIP, we construct a benchmark dataset composed of 11\ndownstream prediction tasks across social, economic, and natural domains.\nExperiments show that MobCLIP, with four input modalities and a compact\n128-dimensional representation space, achieves significantly superior\ngeneral-purpose predictive performances than state-of-the-art models by an\naverage of 35%. Thanks to the effective integration of human-centric\nmodalities, the performance gain is particularly profound in human-centric\ntasks, such as energy consumption (+260%), offline retail consumption amount\n(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we\nfurther demonstrate the scaling behavior in geospatial representation learning.\nWe open-source code and pretrained models at:\nhttps://github.com/ylzhouchris/MobCLIP."
                },
                "authors": [
                    {
                        "name": "Ya Wen"
                    },
                    {
                        "name": "Jixuan Cai"
                    },
                    {
                        "name": "Qiyao Ma"
                    },
                    {
                        "name": "Linyan Li"
                    },
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Chris Webster"
                    },
                    {
                        "name": "Yulun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhou"
                },
                "author": "Yulun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03053v1",
                "updated": "2025-06-03T16:33:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    33,
                    47,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:33:47Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    33,
                    47,
                    1,
                    154,
                    0
                ],
                "title": "MAEBE: Multi-Agent Emergent Behavior Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAEBE: Multi-Agent Emergent Behavior Framework"
                },
                "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts."
                },
                "authors": [
                    {
                        "name": "Sinem Erisken"
                    },
                    {
                        "name": "Timothy Gothard"
                    },
                    {
                        "name": "Martin Leitgab"
                    },
                    {
                        "name": "Ram Potham"
                    }
                ],
                "author_detail": {
                    "name": "Ram Potham"
                },
                "arxiv_affiliation": "Independent Researcher",
                "author": "Ram Potham",
                "arxiv_comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03051v1",
                "updated": "2025-06-03T16:31:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    31,
                    52,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:31:52Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    31,
                    52,
                    1,
                    154,
                    0
                ],
                "title": "Facts Do Care About Your Language: Assessing Answer Quality of\n  Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facts Do Care About Your Language: Assessing Answer Quality of\n  Multilingual LLMs"
                },
                "summary": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages."
                },
                "authors": [
                    {
                        "name": "Yuval Kansal"
                    },
                    {
                        "name": "Shmuel Berman"
                    },
                    {
                        "name": "Lydia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Liu"
                },
                "author": "Lydia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03046v1",
                "updated": "2025-06-03T16:28:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    28,
                    33,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:28:33Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    28,
                    33,
                    1,
                    154,
                    0
                ],
                "title": "EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment"
                },
                "summary": "Deep reinforcement learning agents are often fragile while humans remain\nadaptive and flexible to varying scenarios. To bridge this gap, we present\nEDEN, a biologically inspired navigation framework that integrates learned\nentorhinal-like grid cell representations and reinforcement learning to enable\nautonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,\nEDEN allows agents to perform path integration and vector-based navigation\nusing visual and motion sensor data. At the core of EDEN is a grid cell encoder\nthat transforms egocentric motion into periodic spatial codes, producing\nlow-dimensional, interpretable embeddings of position. To generate these\nactivations from raw sensory input, we combine fiducial marker detections in\nthe lightweight MiniWorld simulator and DINO-based visual features in the\nhigh-fidelity Gazebo simulator. These spatial representations serve as input to\na policy trained with Proximal Policy Optimization (PPO), enabling dynamic,\ngoal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid\nprototyping, and Gazebo, which offers realistic physics and perception noise.\nCompared to baseline agents using raw state inputs (e.g., position, velocity)\nor standard convolutional image encoders, EDEN achieves a 99% success rate,\nwithin the simple scenarios, and >94% within complex floorplans with occluded\npaths with more efficient and reliable step-wise navigation. In addition, as a\nreplacement of ground truth activations, we present a trainable Grid Cell\nencoder enabling the development of periodic grid-like patterns from vision and\nmotion sensor data, emulating the development of such patterns within\nbiological mammals. This work represents a step toward biologically grounded\nspatial intelligence in robotics, bridging neural navigation principles with\nreinforcement learning for scalable deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning agents are often fragile while humans remain\nadaptive and flexible to varying scenarios. To bridge this gap, we present\nEDEN, a biologically inspired navigation framework that integrates learned\nentorhinal-like grid cell representations and reinforcement learning to enable\nautonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,\nEDEN allows agents to perform path integration and vector-based navigation\nusing visual and motion sensor data. At the core of EDEN is a grid cell encoder\nthat transforms egocentric motion into periodic spatial codes, producing\nlow-dimensional, interpretable embeddings of position. To generate these\nactivations from raw sensory input, we combine fiducial marker detections in\nthe lightweight MiniWorld simulator and DINO-based visual features in the\nhigh-fidelity Gazebo simulator. These spatial representations serve as input to\na policy trained with Proximal Policy Optimization (PPO), enabling dynamic,\ngoal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid\nprototyping, and Gazebo, which offers realistic physics and perception noise.\nCompared to baseline agents using raw state inputs (e.g., position, velocity)\nor standard convolutional image encoders, EDEN achieves a 99% success rate,\nwithin the simple scenarios, and >94% within complex floorplans with occluded\npaths with more efficient and reliable step-wise navigation. In addition, as a\nreplacement of ground truth activations, we present a trainable Grid Cell\nencoder enabling the development of periodic grid-like patterns from vision and\nmotion sensor data, emulating the development of such patterns within\nbiological mammals. This work represents a step toward biologically grounded\nspatial intelligence in robotics, bridging neural navigation principles with\nreinforcement learning for scalable deployment."
                },
                "authors": [
                    {
                        "name": "Mikolaj Walczak"
                    },
                    {
                        "name": "Romina Aalishah"
                    },
                    {
                        "name": "Wyatt Mackey"
                    },
                    {
                        "name": "Brittany Story"
                    },
                    {
                        "name": "David L. Boothe Jr."
                    },
                    {
                        "name": "Nicholas Waytowich"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18156v2",
                "updated": "2025-06-03T16:23:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    23,
                    43,
                    1,
                    154,
                    0
                ],
                "published": "2025-03-23T17:55:33Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    17,
                    55,
                    33,
                    6,
                    82,
                    0
                ],
                "title": "Adoption of Watermarking Measures for AI-Generated Content and\n  Implications under the EU AI Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adoption of Watermarking Measures for AI-Generated Content and\n  Implications under the EU AI Act"
                },
                "summary": "AI-generated images have become so good in recent years that individuals\noften cannot distinguish them any more from \"real\" images. This development,\ncombined with the rapid spread of AI-generated content online, creates a series\nof societal risks, particularly with the emergence of \"deep fakes\" that\nimpersonate real individuals. Watermarking, a technique that involves embedding\ninformation within images and other content to indicate their AI-generated\nnature, has emerged as a primary mechanism to address the risks posed by\nAI-generated content. Indeed, watermarking and AI labelling measures are now\nbecoming a legal requirement in many jurisdictions, including under the 2024\nEuropean Union AI Act. Despite the widespread use of AI image generation\nsystems, the current status of the implementation of such measures remains\nlargely unexamined. Moreover, the practical implications of the AI Act's\nwatermarking and labelling requirements have not previously been studied. The\npresent paper therefore both provides an empirical analysis of 50 widely used\nAI systems for image generation, embedded into a legal analysis of the AI Act.\nIn our legal analysis, we identify four categories of generative AI image\ndeployment scenarios relevant under the AI Act and outline how the legal\nobligations apply in each category. In our empirical analysis, we find that\nonly a minority number of AI image generators currently implement adequate\nwatermarking (38%) and deep fake labelling (8%) practices. In response, we\nsuggest a range of avenues of how the implementation of these legally mandated\ntechniques can be improved, and publicly share our tooling for the easy\ndetection of watermarks in images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-generated images have become so good in recent years that individuals\noften cannot distinguish them any more from \"real\" images. This development,\ncombined with the rapid spread of AI-generated content online, creates a series\nof societal risks, particularly with the emergence of \"deep fakes\" that\nimpersonate real individuals. Watermarking, a technique that involves embedding\ninformation within images and other content to indicate their AI-generated\nnature, has emerged as a primary mechanism to address the risks posed by\nAI-generated content. Indeed, watermarking and AI labelling measures are now\nbecoming a legal requirement in many jurisdictions, including under the 2024\nEuropean Union AI Act. Despite the widespread use of AI image generation\nsystems, the current status of the implementation of such measures remains\nlargely unexamined. Moreover, the practical implications of the AI Act's\nwatermarking and labelling requirements have not previously been studied. The\npresent paper therefore both provides an empirical analysis of 50 widely used\nAI systems for image generation, embedded into a legal analysis of the AI Act.\nIn our legal analysis, we identify four categories of generative AI image\ndeployment scenarios relevant under the AI Act and outline how the legal\nobligations apply in each category. In our empirical analysis, we find that\nonly a minority number of AI image generators currently implement adequate\nwatermarking (38%) and deep fake labelling (8%) practices. In response, we\nsuggest a range of avenues of how the implementation of these legally mandated\ntechniques can be improved, and publicly share our tooling for the easy\ndetection of watermarks in images."
                },
                "authors": [
                    {
                        "name": "Bram Rijsbosch"
                    },
                    {
                        "name": "Gijs van Dijck"
                    },
                    {
                        "name": "Konrad Kollnig"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Kollnig"
                },
                "author": "Konrad Kollnig",
                "arxiv_comment": "Note that this work has not yet been published in a peer review\n  journal, it is therefore potentially still subject to change",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03041v1",
                "updated": "2025-06-03T16:23:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    23,
                    12,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:23:12Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    23,
                    12,
                    1,
                    154,
                    0
                ],
                "title": "AI-Augmented OTDR Fault Localization Framework for Resilient Rural Fiber\n  Networks in the United States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Augmented OTDR Fault Localization Framework for Resilient Rural Fiber\n  Networks in the United States"
                },
                "summary": "This research presents a novel framework that combines traditional Optical\nTime-Domain Reflectometer (OTDR) signal analysis with machine learning to\nlocalize and classify fiber optic faults in rural broadband infrastructures.\nThe proposed system addresses a critical need in the expansion of middle-mile\nand last-mile networks, particularly in regions targeted by the U.S. Broadband\nEquity, Access, and Deployment (BEAD) Program. By enhancing fault diagnosis\nthrough a predictive, AI-based model, this work enables proactive network\nmaintenance in low-resource environments. Experimental evaluations using a\ncontrolled fiber testbed and synthetic datasets simulating rural network\nconditions demonstrate that the proposed method significantly improves\ndetection accuracy and reduces false positives compared to conventional\nthresholding techniques. The solution offers a scalable, field-deployable tool\nfor technicians and ISPs engaged in rural broadband deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research presents a novel framework that combines traditional Optical\nTime-Domain Reflectometer (OTDR) signal analysis with machine learning to\nlocalize and classify fiber optic faults in rural broadband infrastructures.\nThe proposed system addresses a critical need in the expansion of middle-mile\nand last-mile networks, particularly in regions targeted by the U.S. Broadband\nEquity, Access, and Deployment (BEAD) Program. By enhancing fault diagnosis\nthrough a predictive, AI-based model, this work enables proactive network\nmaintenance in low-resource environments. Experimental evaluations using a\ncontrolled fiber testbed and synthetic datasets simulating rural network\nconditions demonstrate that the proposed method significantly improves\ndetection accuracy and reduces false positives compared to conventional\nthresholding techniques. The solution offers a scalable, field-deployable tool\nfor technicians and ISPs engaged in rural broadband deployment."
                },
                "authors": [
                    {
                        "name": "Sabab Al Farabi"
                    }
                ],
                "author_detail": {
                    "name": "Sabab Al Farabi"
                },
                "arxiv_affiliation": "Department of Industrial Engineering, Lamar University, Beaumont, Texas, USA",
                "author": "Sabab Al Farabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03038v1",
                "updated": "2025-06-03T16:20:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    20,
                    47,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:20:47Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    20,
                    47,
                    1,
                    154,
                    0
                ],
                "title": "Towards Analyzing and Understanding the Limitations of VAPO: A\n  Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Analyzing and Understanding the Limitations of VAPO: A\n  Theoretical Perspective"
                },
                "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents."
                },
                "authors": [
                    {
                        "name": "Jintian Shao"
                    },
                    {
                        "name": "Yiming Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Cheng"
                },
                "author": "Yiming Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03035v1",
                "updated": "2025-06-03T16:18:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    18,
                    45,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:18:45Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    18,
                    45,
                    1,
                    154,
                    0
                ],
                "title": "Leveraging Information Retrieval to Enhance Spoken Language\n  Understanding Prompts in Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Information Retrieval to Enhance Spoken Language\n  Understanding Prompts in Few-Shot Learning"
                },
                "summary": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length."
                },
                "authors": [
                    {
                        "name": "Pierre Lepagnol"
                    },
                    {
                        "name": "Sahar Ghannay"
                    },
                    {
                        "name": "Thomas Gerald"
                    },
                    {
                        "name": "Christophe Servan"
                    },
                    {
                        "name": "Sophie Rosset"
                    }
                ],
                "author_detail": {
                    "name": "Sophie Rosset"
                },
                "author": "Sophie Rosset",
                "arxiv_comment": "Conference paper accepted to INTERSPEECH 2025",
                "arxiv_journal_ref": "INTERSPEECH 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03032v1",
                "updated": "2025-06-03T16:07:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    7,
                    54,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:07:54Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    7,
                    54,
                    1,
                    154,
                    0
                ],
                "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment"
                },
                "summary": "Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions."
                },
                "authors": [
                    {
                        "name": "Junhao Yu"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "YuXuan Sun"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "24 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00077v2",
                "updated": "2025-06-03T16:01:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    1,
                    41,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-29T23:39:24Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    23,
                    39,
                    24,
                    3,
                    149,
                    0
                ],
                "title": "Gaussian mixture models as a proxy for interacting language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian mixture models as a proxy for interacting language models"
                },
                "summary": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions."
                },
                "authors": [
                    {
                        "name": "Edward L. Wang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Avanti Athreya"
                    },
                    {
                        "name": "Vince Lyzinski"
                    },
                    {
                        "name": "Carey E. Priebe"
                    }
                ],
                "author_detail": {
                    "name": "Carey E. Priebe"
                },
                "author": "Carey E. Priebe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62R07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03024v1",
                "updated": "2025-06-03T16:00:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    0,
                    30,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T16:00:30Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    16,
                    0,
                    30,
                    1,
                    154,
                    0
                ],
                "title": "GenFair: Systematic Test Generation for Fairness Fault Detection in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenFair: Systematic Test Generation for Fairness Fault Detection in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in critical domains,\nyet they often exhibit biases inherited from training data, leading to fairness\nconcerns. This work focuses on the problem of effectively detecting fairness\nviolations, especially intersectional biases that are often missed by existing\ntemplate-based and grammar-based testing methods. Previous approaches, such as\nCheckList and ASTRAEA, provide structured or grammar-driven test generation but\nstruggle with low test diversity and limited sensitivity to complex demographic\ninteractions. To address these limitations, we propose GenFair, a metamorphic\nfairness testing framework that systematically generates source test cases\nusing equivalence partitioning, mutation operators, and boundary value\nanalysis. GenFair improves fairness testing by generating linguistically\ndiverse, realistic, and intersectional test cases. It applies metamorphic\nrelations (MR) to derive follow-up cases and detects fairness violations via\ntone-based comparisons between source and follow-up responses. In experiments\nwith GPT-4.0 and LLaMA-3.0, GenFair outperformed two baseline methods. It\nachieved a fault detection rate (FDR) of 0.73 (GPT-4.0) and 0.69 (LLaMA-3.0),\ncompared to 0.54/0.51 for template-based and 0.39/0.36 for ASTRAEA. GenFair\nalso showed the highest test case diversity (syntactic:10.06, semantic: 76.68)\nand strong coherence (syntactic: 291.32, semantic: 0.7043), outperforming both\nbaselines. These results demonstrate the effectiveness of GenFair in uncovering\nnuanced fairness violations. The proposed method offers a scalable and\nautomated solution for fairness testing and contributes to building more\nequitable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in critical domains,\nyet they often exhibit biases inherited from training data, leading to fairness\nconcerns. This work focuses on the problem of effectively detecting fairness\nviolations, especially intersectional biases that are often missed by existing\ntemplate-based and grammar-based testing methods. Previous approaches, such as\nCheckList and ASTRAEA, provide structured or grammar-driven test generation but\nstruggle with low test diversity and limited sensitivity to complex demographic\ninteractions. To address these limitations, we propose GenFair, a metamorphic\nfairness testing framework that systematically generates source test cases\nusing equivalence partitioning, mutation operators, and boundary value\nanalysis. GenFair improves fairness testing by generating linguistically\ndiverse, realistic, and intersectional test cases. It applies metamorphic\nrelations (MR) to derive follow-up cases and detects fairness violations via\ntone-based comparisons between source and follow-up responses. In experiments\nwith GPT-4.0 and LLaMA-3.0, GenFair outperformed two baseline methods. It\nachieved a fault detection rate (FDR) of 0.73 (GPT-4.0) and 0.69 (LLaMA-3.0),\ncompared to 0.54/0.51 for template-based and 0.39/0.36 for ASTRAEA. GenFair\nalso showed the highest test case diversity (syntactic:10.06, semantic: 76.68)\nand strong coherence (syntactic: 291.32, semantic: 0.7043), outperforming both\nbaselines. These results demonstrate the effectiveness of GenFair in uncovering\nnuanced fairness violations. The proposed method offers a scalable and\nautomated solution for fairness testing and contributes to building more\nequitable LLMs."
                },
                "authors": [
                    {
                        "name": "Madhusudan Srinivasan"
                    },
                    {
                        "name": "Jubril Abdel"
                    }
                ],
                "author_detail": {
                    "name": "Jubril Abdel"
                },
                "author": "Jubril Abdel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03017v1",
                "updated": "2025-06-03T15:54:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    54,
                    27,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:54:27Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    54,
                    27,
                    1,
                    154,
                    0
                ],
                "title": "Adjusting Tissue Puncture Omnidirectionally In Situ with Pneumatic\n  Rotatable Biopsy Mechanism and Hierarchical Airflow Management in Tortuous\n  Luminal Pathways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adjusting Tissue Puncture Omnidirectionally In Situ with Pneumatic\n  Rotatable Biopsy Mechanism and Hierarchical Airflow Management in Tortuous\n  Luminal Pathways"
                },
                "summary": "In situ tissue biopsy with an endoluminal catheter is an efficient approach\nfor disease diagnosis, featuring low invasiveness and few complications.\nHowever, the endoluminal catheter struggles to adjust the biopsy direction by\ndistal endoscope bending or proximal twisting for tissue sampling within the\ntortuous luminal organs, due to friction-induced hysteresis and narrow spaces.\nHere, we propose a pneumatically-driven robotic catheter enabling the\nadjustment of the sampling direction without twisting the catheter for an\naccurate in situ omnidirectional biopsy. The distal end of the robotic catheter\nconsists of a pneumatic bending actuator for the catheter's deployment in\ntorturous luminal organs and a pneumatic rotatable biopsy mechanism (PRBM). By\nhierarchical airflow control, the PRBM can adjust the biopsy direction under\nlow airflow and deploy the biopsy needle with higher airflow, allowing for\nrapid omnidirectional sampling of tissue in situ. This paper describes the\ndesign, modeling, and characterization of the proposed robotic catheter,\nincluding repeated deployment assessments of the biopsy needle, puncture force\nmeasurement, and validation via phantom tests. The PRBM prototype has six\nsampling directions evenly distributed across 360 degrees when actuated by a\npositive pressure of 0.3 MPa. The pneumatically-driven robotic catheter\nprovides a novel biopsy strategy, potentially facilitating in situ\nmultidirectional biopsies in tortuous luminal organs with minimum invasiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In situ tissue biopsy with an endoluminal catheter is an efficient approach\nfor disease diagnosis, featuring low invasiveness and few complications.\nHowever, the endoluminal catheter struggles to adjust the biopsy direction by\ndistal endoscope bending or proximal twisting for tissue sampling within the\ntortuous luminal organs, due to friction-induced hysteresis and narrow spaces.\nHere, we propose a pneumatically-driven robotic catheter enabling the\nadjustment of the sampling direction without twisting the catheter for an\naccurate in situ omnidirectional biopsy. The distal end of the robotic catheter\nconsists of a pneumatic bending actuator for the catheter's deployment in\ntorturous luminal organs and a pneumatic rotatable biopsy mechanism (PRBM). By\nhierarchical airflow control, the PRBM can adjust the biopsy direction under\nlow airflow and deploy the biopsy needle with higher airflow, allowing for\nrapid omnidirectional sampling of tissue in situ. This paper describes the\ndesign, modeling, and characterization of the proposed robotic catheter,\nincluding repeated deployment assessments of the biopsy needle, puncture force\nmeasurement, and validation via phantom tests. The PRBM prototype has six\nsampling directions evenly distributed across 360 degrees when actuated by a\npositive pressure of 0.3 MPa. The pneumatically-driven robotic catheter\nprovides a novel biopsy strategy, potentially facilitating in situ\nmultidirectional biopsies in tortuous luminal organs with minimum invasiveness."
                },
                "authors": [
                    {
                        "name": "Botao Lin"
                    },
                    {
                        "name": "Tinghua Zhang"
                    },
                    {
                        "name": "Sishen Yuan"
                    },
                    {
                        "name": "Tiantian Wang"
                    },
                    {
                        "name": "Jiaole Wang"
                    },
                    {
                        "name": "Wu Yuan"
                    },
                    {
                        "name": "Hongliang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Hongliang Ren"
                },
                "author": "Hongliang Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03009v1",
                "updated": "2025-06-03T15:50:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    50,
                    27,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:50:27Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    50,
                    27,
                    1,
                    154,
                    0
                ],
                "title": "Conditioning Large Language Models on Legal Systems? Detecting\n  Punishable Hate Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditioning Large Language Models on Legal Systems? Detecting\n  Punishable Hate Speech"
                },
                "summary": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts."
                },
                "authors": [
                    {
                        "name": "Florian Ludwig"
                    },
                    {
                        "name": "Torsten Zesch"
                    },
                    {
                        "name": "Frederike Zufall"
                    }
                ],
                "author_detail": {
                    "name": "Frederike Zufall"
                },
                "author": "Frederike Zufall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03006v1",
                "updated": "2025-06-03T15:45:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    45,
                    31,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:45:31Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    45,
                    31,
                    1,
                    154,
                    0
                ],
                "title": "A Preference-Driven Methodology for High-Quality Solidity Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Preference-Driven Methodology for High-Quality Solidity Code\n  Generation"
                },
                "summary": "While Large Language Models (LLMs) have demonstrated remarkable progress in\ngenerating functionally correct Solidity code, they continue to face critical\nchallenges in producing gas-efficient and secure code, which are critical\nrequirements for real-world smart contract deployment. Although recent advances\nleverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)\nfor code preference alignment, existing approaches treat functional\ncorrectness, gas optimization, and security as independent objectives,\nresulting in contracts that may achieve operational soundness but suffer from\nprohibitive execution costs or dangerous vulnerabilities. To address these\nlimitations, we propose PrefGen, a novel framework that extends standard DPO\nbeyond human preferences to incorporate quantifiable blockchain-specific\nmetrics, enabling holistic multi-objective optimization specifically tailored\nfor smart contract generation. Our framework introduces a comprehensive\nevaluation methodology with four complementary metrics: Pass@k (functional\ncorrectness), Compile@k (syntactic correctness), Gas@k (gas efficiency), and\nSecure@k (security assessment), providing rigorous multi-dimensional contract\nevaluation. Through extensive experimentation, we demonstrate that PrefGen\nsignificantly outperforms existing approaches across all critical dimensions,\nachieving 66.7% Pass@5, 58.9% Gas@5, and 62.5% Secure@5, while generating\nproduction-ready smart contracts that are functionally correct, cost-efficient,\nand secure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have demonstrated remarkable progress in\ngenerating functionally correct Solidity code, they continue to face critical\nchallenges in producing gas-efficient and secure code, which are critical\nrequirements for real-world smart contract deployment. Although recent advances\nleverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)\nfor code preference alignment, existing approaches treat functional\ncorrectness, gas optimization, and security as independent objectives,\nresulting in contracts that may achieve operational soundness but suffer from\nprohibitive execution costs or dangerous vulnerabilities. To address these\nlimitations, we propose PrefGen, a novel framework that extends standard DPO\nbeyond human preferences to incorporate quantifiable blockchain-specific\nmetrics, enabling holistic multi-objective optimization specifically tailored\nfor smart contract generation. Our framework introduces a comprehensive\nevaluation methodology with four complementary metrics: Pass@k (functional\ncorrectness), Compile@k (syntactic correctness), Gas@k (gas efficiency), and\nSecure@k (security assessment), providing rigorous multi-dimensional contract\nevaluation. Through extensive experimentation, we demonstrate that PrefGen\nsignificantly outperforms existing approaches across all critical dimensions,\nachieving 66.7% Pass@5, 58.9% Gas@5, and 62.5% Secure@5, while generating\nproduction-ready smart contracts that are functionally correct, cost-efficient,\nand secure."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Chenhao Ying"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Yuan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Luo"
                },
                "author": "Yuan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06262v2",
                "updated": "2025-06-03T15:34:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    34,
                    1,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-04T13:19:21Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    13,
                    19,
                    21,
                    6,
                    124,
                    0
                ],
                "title": "Dialz: A Python Toolkit for Steering Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialz: A Python Toolkit for Steering Vectors"
                },
                "summary": "We introduce Dialz, a framework for advancing research on steering vectors\nfor open-source LLMs, implemented in Python. Steering vectors allow users to\nmodify activations at inference time to amplify or weaken a 'concept', e.g.\nhonesty or positivity, providing a more powerful alternative to prompting or\nfine-tuning. Dialz supports a diverse set of tasks, including creating\ncontrastive pair datasets, computing and applying steering vectors, and\nvisualizations. Unlike existing libraries, Dialz emphasizes modularity and\nusability, enabling both rapid prototyping and in-depth analysis. We\ndemonstrate how Dialz can be used to reduce harmful outputs such as\nstereotypes, while also providing insights into model behaviour across\ndifferent layers. We release Dialz with full documentation, tutorials, and\nsupport for popular open-source models to encourage further research in safe\nand controllable language generation. Dialz enables faster research cycles and\nfacilitates insights into model interpretability, paving the way for safer,\nmore transparent, and more reliable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Dialz, a framework for advancing research on steering vectors\nfor open-source LLMs, implemented in Python. Steering vectors allow users to\nmodify activations at inference time to amplify or weaken a 'concept', e.g.\nhonesty or positivity, providing a more powerful alternative to prompting or\nfine-tuning. Dialz supports a diverse set of tasks, including creating\ncontrastive pair datasets, computing and applying steering vectors, and\nvisualizations. Unlike existing libraries, Dialz emphasizes modularity and\nusability, enabling both rapid prototyping and in-depth analysis. We\ndemonstrate how Dialz can be used to reduce harmful outputs such as\nstereotypes, while also providing insights into model behaviour across\ndifferent layers. We release Dialz with full documentation, tutorials, and\nsupport for popular open-source models to encourage further research in safe\nand controllable language generation. Dialz enables faster research cycles and\nfacilitates insights into model interpretability, paving the way for safer,\nmore transparent, and more reliable AI systems."
                },
                "authors": [
                    {
                        "name": "Zara Siddique"
                    },
                    {
                        "name": "Liam D. Turner"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "Accepted to ACL System Demo 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02996v1",
                "updated": "2025-06-03T15:31:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    31,
                    0,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:31:00Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    31,
                    0,
                    1,
                    154,
                    0
                ],
                "title": "Linear Spatial World Models Emerge in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Spatial World Models Emerge in Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated emergent abilities across\ndiverse tasks, raising the question of whether they acquire internal world\nmodels. In this work, we investigate whether LLMs implicitly encode linear\nspatial world models, which we define as linear representations of physical\nspace and object configurations. We introduce a formal framework for spatial\nworld models and assess whether such structure emerges in contextual\nembeddings. Using a synthetic dataset of object positions, we train probes to\ndecode object positions and evaluate geometric consistency of the underlying\nspace. We further conduct causal interventions to test whether these spatial\nrepresentations are functionally used by the model. Our results provide\nempirical evidence that LLMs encode linear spatial world models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated emergent abilities across\ndiverse tasks, raising the question of whether they acquire internal world\nmodels. In this work, we investigate whether LLMs implicitly encode linear\nspatial world models, which we define as linear representations of physical\nspace and object configurations. We introduce a formal framework for spatial\nworld models and assess whether such structure emerges in contextual\nembeddings. Using a synthetic dataset of object positions, we train probes to\ndecode object positions and evaluate geometric consistency of the underlying\nspace. We further conduct causal interventions to test whether these spatial\nrepresentations are functionally used by the model. Our results provide\nempirical evidence that LLMs encode linear spatial world models."
                },
                "authors": [
                    {
                        "name": "Matthieu Tehenan"
                    },
                    {
                        "name": "Christian Bolivar Moya"
                    },
                    {
                        "name": "Tenghai Long"
                    },
                    {
                        "name": "Guang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guang Lin"
                },
                "author": "Guang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02992v1",
                "updated": "2025-06-03T15:28:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    28,
                    30,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:28:30Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    28,
                    30,
                    1,
                    154,
                    0
                ],
                "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective\n  Multi-Agent Approach for Legal Argument Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Manipulation and Enhancing Persuasion: A Reflective\n  Multi-Agent Approach for Legal Argument Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/"
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Kevin D. Ashley"
                    }
                ],
                "author_detail": {
                    "name": "Kevin D. Ashley"
                },
                "author": "Kevin D. Ashley",
                "arxiv_comment": "13 pages, 2 figures, Workshop on Legally Compliant Intelligent\n  Chatbots at ICAIL 2025]{Workshop on Legally Compliant Intelligent Chatbots @\n  ICAIL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02987v1",
                "updated": "2025-06-03T15:25:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    25,
                    38,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:25:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    25,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "Performance of leading large language models in May 2025 in Membership\n  of the Royal College of General Practitioners-style examination questions: a\n  cross-sectional analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of leading large language models in May 2025 in Membership\n  of the Royal College of General Practitioners-style examination questions: a\n  cross-sectional analysis"
                },
                "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata."
                },
                "authors": [
                    {
                        "name": "Richard Armitage"
                    }
                ],
                "author_detail": {
                    "name": "Richard Armitage"
                },
                "author": "Richard Armitage",
                "arxiv_comment": "12 pages, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05630v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05630v3",
                "updated": "2025-06-03T15:24:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    24,
                    7,
                    1,
                    154,
                    0
                ],
                "published": "2024-07-08T05:49:38Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    5,
                    49,
                    38,
                    0,
                    190,
                    0
                ],
                "title": "Enabling 6G Performance in the Upper Mid-Band by Transitioning From\n  Massive to Gigantic MIMO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling 6G Performance in the Upper Mid-Band by Transitioning From\n  Massive to Gigantic MIMO"
                },
                "summary": "The initial 6G networks will likely operate in the upper mid-band (7-24 GHz),\nwhich has decent propagation conditions but underwhelming new spectrum\navailability. In this paper, we explore whether we can anyway reach the\nambitious 6G performance goals by evolving the multiple-input multiple-output\n(MIMO) technology from massive in 5G to gigantic in 6G. We describe how many\nantennas are needed to reach the envisioned 6G peak user rates, how many can\nrealistically be deployed in practical radio equipment, and what the practical\nspatial degrees-of-freedom might become. We further suggest a new deployment\nstrategy that enables the utilization of radiative near-field effects in these\nbands for precise beamfocusing, localization, and sensing from a single base\nstation site. Finally, we identify open research and standardization challenges\nthat must be overcome to efficiently use gigantic MIMO dimensions in 6G from\nhardware, cost, and algorithmic perspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The initial 6G networks will likely operate in the upper mid-band (7-24 GHz),\nwhich has decent propagation conditions but underwhelming new spectrum\navailability. In this paper, we explore whether we can anyway reach the\nambitious 6G performance goals by evolving the multiple-input multiple-output\n(MIMO) technology from massive in 5G to gigantic in 6G. We describe how many\nantennas are needed to reach the envisioned 6G peak user rates, how many can\nrealistically be deployed in practical radio equipment, and what the practical\nspatial degrees-of-freedom might become. We further suggest a new deployment\nstrategy that enables the utilization of radiative near-field effects in these\nbands for precise beamfocusing, localization, and sensing from a single base\nstation site. Finally, we identify open research and standardization challenges\nthat must be overcome to efficiently use gigantic MIMO dimensions in 6G from\nhardware, cost, and algorithmic perspectives."
                },
                "authors": [
                    {
                        "name": "Emil Bjrnson"
                    },
                    {
                        "name": "Ferdi Kara"
                    },
                    {
                        "name": "Nikolaos Kolomvakis"
                    },
                    {
                        "name": "Alva Kosasih"
                    },
                    {
                        "name": "Parisa Ramezani"
                    },
                    {
                        "name": "Murat Babek Salman"
                    }
                ],
                "author_detail": {
                    "name": "Murat Babek Salman"
                },
                "author": "Murat Babek Salman",
                "arxiv_doi": "10.1109/OJCOMS.2025.3576931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/OJCOMS.2025.3576931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.05630v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05630v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the IEEE Open Journal of the Communications Society, 14\n  pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19988v2",
                "updated": "2025-06-03T15:23:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    23,
                    3,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-26T13:43:43Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    43,
                    43,
                    0,
                    146,
                    0
                ],
                "title": "Automatic Metadata Extraction for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Metadata Extraction for Text-to-SQL"
                },
                "summary": "Large Language Models (LLMs) have recently become sophisticated enough to\nautomate many tasks ranging from pattern finding to writing assistance to code\ngeneration. In this paper, we examine text-to-SQL generation. We have observed\nfrom decades of experience that the most difficult part of query development\nlies in understanding the database contents. These experiences inform the\ndirection of our research.\n  Text-to-SQL benchmarks such as SPIDER and Bird contain extensive metadata\nthat is generally not available in practice. Human-generated metadata requires\nthe use of expensive Subject Matter Experts (SMEs), who are often not fully\naware of many aspects of their databases. In this paper, we explore techniques\nfor automatic metadata extraction to enable text-to-SQL generation.\n  Ee explore the use of two standard and one newer metadata extraction\ntechniques: profiling, query log analysis, and SQL-to text generation using an\nLLM. We use BIRD benchmark [JHQY+23] to evaluate the effectiveness of these\ntechniques. BIRD does not provide query logs on their test database, so we\nprepared a submission that uses profiling alone, and does not use any specially\ntuned model (we used GPT-4o). From Sept 1 to Sept 23, 2024, and Nov 11 through\nNov 23, 2024 we achieved the highest score both with and without using the\n\"oracle\" information provided with the question set. We regained the number 1\nspot on Mar 11, 2025, and are still at #1 at the time of the writing (May,\n2025).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently become sophisticated enough to\nautomate many tasks ranging from pattern finding to writing assistance to code\ngeneration. In this paper, we examine text-to-SQL generation. We have observed\nfrom decades of experience that the most difficult part of query development\nlies in understanding the database contents. These experiences inform the\ndirection of our research.\n  Text-to-SQL benchmarks such as SPIDER and Bird contain extensive metadata\nthat is generally not available in practice. Human-generated metadata requires\nthe use of expensive Subject Matter Experts (SMEs), who are often not fully\naware of many aspects of their databases. In this paper, we explore techniques\nfor automatic metadata extraction to enable text-to-SQL generation.\n  Ee explore the use of two standard and one newer metadata extraction\ntechniques: profiling, query log analysis, and SQL-to text generation using an\nLLM. We use BIRD benchmark [JHQY+23] to evaluate the effectiveness of these\ntechniques. BIRD does not provide query logs on their test database, so we\nprepared a submission that uses profiling alone, and does not use any specially\ntuned model (we used GPT-4o). From Sept 1 to Sept 23, 2024, and Nov 11 through\nNov 23, 2024 we achieved the highest score both with and without using the\n\"oracle\" information provided with the question set. We regained the number 1\nspot on Mar 11, 2025, and are still at #1 at the time of the writing (May,\n2025)."
                },
                "authors": [
                    {
                        "name": "Vladislav Shkapenyuk"
                    },
                    {
                        "name": "Divesh Srivastava"
                    },
                    {
                        "name": "Theodore Johnson"
                    },
                    {
                        "name": "Parisa Ghane"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Ghane"
                },
                "author": "Parisa Ghane",
                "arxiv_comment": "37 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07744v2",
                "updated": "2025-06-03T15:13:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    13,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-10T13:40:27Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    40,
                    27,
                    3,
                    100,
                    0
                ],
                "title": "MMLA: Multi-Environment, Multi-Species, Low-Altitude Drone Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMLA: Multi-Environment, Multi-Species, Low-Altitude Drone Dataset"
                },
                "summary": "Real-time wildlife detection in drone imagery supports critical ecological\nand conservation monitoring. However, standard detection models like YOLO often\nfail to generalize across locations and struggle with rare species, limiting\ntheir use in automated drone deployments. We present MMLA, a novel\nmulti-environment, multi-species, low-altitude drone dataset collected across\nthree sites (Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The\nWilds in Ohio), featuring six species (zebras, giraffes, onagers, and African\nwild dogs). The dataset contains 811K annotations from 37 high-resolution\nvideos. Baseline YOLO models show performance disparities across locations\nwhile fine-tuning YOLOv11m on MMLA improves mAP50 to 82%, a 52-point gain over\nbaseline. Our results underscore the need for diverse training data to enable\nrobust animal detection in autonomous drone systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time wildlife detection in drone imagery supports critical ecological\nand conservation monitoring. However, standard detection models like YOLO often\nfail to generalize across locations and struggle with rare species, limiting\ntheir use in automated drone deployments. We present MMLA, a novel\nmulti-environment, multi-species, low-altitude drone dataset collected across\nthree sites (Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The\nWilds in Ohio), featuring six species (zebras, giraffes, onagers, and African\nwild dogs). The dataset contains 811K annotations from 37 high-resolution\nvideos. Baseline YOLO models show performance disparities across locations\nwhile fine-tuning YOLOv11m on MMLA improves mAP50 to 82%, a 52-point gain over\nbaseline. Our results underscore the need for diverse training data to enable\nrobust animal detection in autonomous drone systems."
                },
                "authors": [
                    {
                        "name": "Jenna Kline"
                    },
                    {
                        "name": "Samuel Stevens"
                    },
                    {
                        "name": "Guy Maalouf"
                    },
                    {
                        "name": "Camille Rondeau Saint-Jean"
                    },
                    {
                        "name": "Dat Nguyen Ngoc"
                    },
                    {
                        "name": "Majid Mirmehdi"
                    },
                    {
                        "name": "David Guerin"
                    },
                    {
                        "name": "Tilo Burghardt"
                    },
                    {
                        "name": "Elzbieta Pastucha"
                    },
                    {
                        "name": "Blair Costelloe"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Thomas Richardson"
                    },
                    {
                        "name": "Ulrik Pagh Schultz Lundquist"
                    }
                ],
                "author_detail": {
                    "name": "Ulrik Pagh Schultz Lundquist"
                },
                "author": "Ulrik Pagh Schultz Lundquist",
                "arxiv_comment": "Accepted at CVPR Workshop, CV4Animals 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23001v3",
                "updated": "2025-06-04T02:31:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    2,
                    31,
                    16,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-29T02:22:14Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    2,
                    22,
                    14,
                    3,
                    149,
                    0
                ],
                "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors"
                },
                "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors."
                },
                "authors": [
                    {
                        "name": "Yize Cheng"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Mazda Moayeri"
                    },
                    {
                        "name": "Soheil Feizi"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Feizi"
                },
                "author": "Soheil Feizi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09825v2",
                "updated": "2025-06-03T15:11:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    11,
                    26,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-14T22:04:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    22,
                    4,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive\n  Reasoning"
                },
                "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks."
                },
                "authors": [
                    {
                        "name": "Peiqi Sui"
                    },
                    {
                        "name": "Juan Diego Rodriguez"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Dean Murphy"
                    },
                    {
                        "name": "Joseph P. Dexter"
                    },
                    {
                        "name": "Richard Jean So"
                    },
                    {
                        "name": "Samuel Baker"
                    },
                    {
                        "name": "Pramit Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Pramit Chaudhuri"
                },
                "author": "Pramit Chaudhuri",
                "arxiv_comment": "ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02973v1",
                "updated": "2025-06-03T15:07:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    7,
                    13,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T15:07:13Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    7,
                    13,
                    1,
                    154,
                    0
                ],
                "title": "Expanding before Inferring: Enhancing Factuality in Large Language\n  Models through Premature Layers Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding before Inferring: Enhancing Factuality in Large Language\n  Models through Premature Layers Interpolation"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance."
                },
                "authors": [
                    {
                        "name": "Dingwei Chen"
                    },
                    {
                        "name": "Ziqiang Liu"
                    },
                    {
                        "name": "Feiteng Fang"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Chengming Li"
                    }
                ],
                "author_detail": {
                    "name": "Chengming Li"
                },
                "author": "Chengming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08769v2",
                "updated": "2025-06-03T15:05:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    5,
                    19,
                    1,
                    154,
                    0
                ],
                "published": "2024-08-16T14:23:59Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    23,
                    59,
                    4,
                    229,
                    0
                ],
                "title": "Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion\n  Contrastive Decoding with Truthfulness Refocused",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion\n  Contrastive Decoding with Truthfulness Refocused"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, they occasionally generate\ninaccurate and counterfactual outputs, a phenomenon commonly referred to as\n\"hallucinations''. To tackle this issue, recent studies have explored\ncontrastive decoding between the original model and an amateur model with\ninduced hallucination, showing promising results. Nevertheless, this approach\ncan disrupt the original LLM's output distribution due to coarse contrast and\nsimple subtraction operations, potentially leading to errors. In this paper, we\nintroduce a novel contrastive decoding framework, termed LOL (LOwer Layer\nMatters). Unlike prior methods that focus solely on the final layer, our\napproach integrates contrastive information from lower layers to enable\nmulti-layer fusion during contrastive decoding. Additionally, we incorporate a\ntruthfulness refocused module that leverages instruction guidance to further\nimprove truthfulness in contrastive decoding. Extensive experiments on four\npublicly available datasets demonstrate that the LOL framework significantly\nmitigates hallucination while outperforming existing baselines in most cases.\nFor reproducibility, we will release our code and data upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, they occasionally generate\ninaccurate and counterfactual outputs, a phenomenon commonly referred to as\n\"hallucinations''. To tackle this issue, recent studies have explored\ncontrastive decoding between the original model and an amateur model with\ninduced hallucination, showing promising results. Nevertheless, this approach\ncan disrupt the original LLM's output distribution due to coarse contrast and\nsimple subtraction operations, potentially leading to errors. In this paper, we\nintroduce a novel contrastive decoding framework, termed LOL (LOwer Layer\nMatters). Unlike prior methods that focus solely on the final layer, our\napproach integrates contrastive information from lower layers to enable\nmulti-layer fusion during contrastive decoding. Additionally, we incorporate a\ntruthfulness refocused module that leverages instruction guidance to further\nimprove truthfulness in contrastive decoding. Extensive experiments on four\npublicly available datasets demonstrate that the LOL framework significantly\nmitigates hallucination while outperforming existing baselines in most cases.\nFor reproducibility, we will release our code and data upon acceptance."
                },
                "authors": [
                    {
                        "name": "Dingwei Chen"
                    },
                    {
                        "name": "Feiteng Fang"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Feng Liang"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Ahmadreza Argha"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Chengming Li"
                    }
                ],
                "author_detail": {
                    "name": "Chengming Li"
                },
                "author": "Chengming Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10742v2",
                "updated": "2025-06-03T15:02:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    2,
                    50,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-15T23:06:23Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    23,
                    6,
                    23,
                    3,
                    135,
                    0
                ],
                "title": "Evaluations at Work: Measuring the Capabilities of GenAI in Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluations at Work: Measuring the Capabilities of GenAI in Use"
                },
                "summary": "Current AI benchmarks miss the messy, multi-turn nature of human-AI\ncollaboration. We present an evaluation framework that decomposes real-world\ntasks into interdependent subtasks, letting us track both LLM performance and\nusers' strategies across a dialogue. Complementing this framework, we develop a\nsuite of metrics, including a composite usage derived from semantic similarity,\nword overlap, and numerical matches; structural coherence; intra-turn\ndiversity; and a novel measure of the \"information frontier\" reflecting the\nalignment between AI outputs and users' working knowledge. We demonstrate our\nmethodology in a financial valuation task that mirrors real-world complexity.\nOur empirical findings reveal that while greater integration of LLM-generated\ncontent generally enhances output quality, its benefits are moderated by\nfactors such as response incoherence, excessive subtask diversity, and the\ndistance of provided information from users' existing knowledge. These results\nsuggest that proactive dialogue strategies designed to inject novelty may\ninadvertently undermine task performance. Our work thus advances a more\nholistic evaluation of human-AI collaboration, offering both a robust\nmethodological framework and actionable insights for developing more effective\nAI-augmented work processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current AI benchmarks miss the messy, multi-turn nature of human-AI\ncollaboration. We present an evaluation framework that decomposes real-world\ntasks into interdependent subtasks, letting us track both LLM performance and\nusers' strategies across a dialogue. Complementing this framework, we develop a\nsuite of metrics, including a composite usage derived from semantic similarity,\nword overlap, and numerical matches; structural coherence; intra-turn\ndiversity; and a novel measure of the \"information frontier\" reflecting the\nalignment between AI outputs and users' working knowledge. We demonstrate our\nmethodology in a financial valuation task that mirrors real-world complexity.\nOur empirical findings reveal that while greater integration of LLM-generated\ncontent generally enhances output quality, its benefits are moderated by\nfactors such as response incoherence, excessive subtask diversity, and the\ndistance of provided information from users' existing knowledge. These results\nsuggest that proactive dialogue strategies designed to inject novelty may\ninadvertently undermine task performance. Our work thus advances a more\nholistic evaluation of human-AI collaboration, offering both a robust\nmethodological framework and actionable insights for developing more effective\nAI-augmented work processes."
                },
                "authors": [
                    {
                        "name": "Brandon Lepine"
                    },
                    {
                        "name": "Gawesha Weerantunga"
                    },
                    {
                        "name": "Juho Kim"
                    },
                    {
                        "name": "Pamela Mishkin"
                    },
                    {
                        "name": "Matthew Beane"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Beane"
                },
                "author": "Matthew Beane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02965v2",
                "updated": "2025-06-04T05:38:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    5,
                    38,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T15:00:18Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    0,
                    18,
                    1,
                    154,
                    0
                ],
                "title": "PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training\n  for Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training\n  for Mixture-of-Experts LLMs"
                },
                "summary": "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has been gaining popularity due to its successful\nadaptation to large language models (LLMs). In this work, we introduce\nPrivacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages\nthe sparsity of the MoE architecture for memory-efficient decentralized\ncollaborative LLM training, enabling multiple parties with limited GPU-memory\nand data resources to collectively train more capable LLMs than they could\nachieve individually. At the same time, this approach protects training data\nprivacy of each participant by keeping training data, as well as parts of the\nforward pass signal and gradients locally within each party. By design, PC-MoE\nsynergistically combines the strengths of distributed computation with strong\nconfidentiality assurances. Unlike most privacy-preserving schemes, which pay\nfor confidentiality with lower task accuracy, our framework breaks that\ntrade-off: across seven popular LLM benchmarks, it almost matches (and\nsometimes exceeds) the performance and convergence rate of a fully centralized\nmodel, enjoys near 70% peak GPU RAM reduction, while being fully robust against\nreconstruction attacks."
                },
                "authors": [
                    {
                        "name": "Ze Yu Zhang"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02961v1",
                "updated": "2025-06-03T14:54:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    54,
                    12,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:54:12Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    54,
                    12,
                    1,
                    154,
                    0
                ],
                "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Massimo Roberto Scamarcia"
                    },
                    {
                        "name": "Javier Fernandez-Marques"
                    },
                    {
                        "name": "Mohammad Naseri"
                    },
                    {
                        "name": "Chong Shen Ng"
                    },
                    {
                        "name": "Dimitris Stripelis"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Jiamu Bai"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "InSeo Song"
                    },
                    {
                        "name": "Lee KangYoon"
                    },
                    {
                        "name": "Hong Jia"
                    },
                    {
                        "name": "Ting Dang"
                    },
                    {
                        "name": "Junyan Wang"
                    },
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Daniel Janes Beutel"
                    },
                    {
                        "name": "Lingjuan Lyu"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02959v1",
                "updated": "2025-06-03T14:52:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    52,
                    44,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:52:44Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    52,
                    44,
                    1,
                    154,
                    0
                ],
                "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection\n  under Human-AI Coauthoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection\n  under Human-AI Coauthoring"
                },
                "summary": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement."
                },
                "authors": [
                    {
                        "name": "Zhixiong Su"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Herun Wan"
                    },
                    {
                        "name": "Zhaohan Zhang"
                    },
                    {
                        "name": "Minnan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Minnan Luo"
                },
                "author": "Minnan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23503v2",
                "updated": "2025-06-03T14:52:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    52,
                    14,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-29T14:48:09Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    48,
                    9,
                    3,
                    149,
                    0
                ],
                "title": "Can Large Language Models Challenge CNNs in Medical Image Analysis?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Challenge CNNs in Medical Image Analysis?"
                },
                "summary": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings."
                },
                "authors": [
                    {
                        "name": "Shibbir Ahmed"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    },
                    {
                        "name": "Anindya Bijoy Das"
                    }
                ],
                "author_detail": {
                    "name": "Anindya Bijoy Das"
                },
                "author": "Anindya Bijoy Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02954v1",
                "updated": "2025-06-03T14:47:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    47,
                    22,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    47,
                    22,
                    1,
                    154,
                    0
                ],
                "title": "Towards More Effective Fault Detection in LLM-Based Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Effective Fault Detection in LLM-Based Unit Test Generation"
                },
                "summary": "Unit tests play a vital role in uncovering potential faults in software.\nWhile tools like EvoSuite focus on maximizing code coverage, recent advances in\nlarge language models (LLMs) have shifted attention toward LLM-based test\ngeneration. However, code coverage metrics -- such as line and branch coverage\n-- remain overly emphasized in reported research, despite being weak indicators\nof a test suite's fault-detection capability. In contrast, \\textit{mutation\nscore} offers a more reliable and stringent measure, as demonstrated in our\nfindings where some test suites achieve 100\\% coverage but only 4\\% mutation\nscore. Although a few studies consider mutation score, the effectiveness of\nLLMs in killing mutants remains underexplored.\n  In this paper, we propose MUTGEN, a mutation-guided, LLM-based test\ngeneration approach that incorporates mutation feedback directly into the\nprompt. Evaluated on 204 subjects from two benchmarks, MUTGEN significantly\noutperforms both EvoSuite and vanilla prompt-based strategies in terms of\nmutation score. Furthermore, MUTGEN introduces an iterative generation\nmechanism that pushes the limits of LLMs in killing additional mutants. Our\nstudy also provide insights into the limitations of LLM-based generation,\nanalyzing the reasons for live and uncovered mutants, and the impact of\ndifferent mutation operators on generation effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit tests play a vital role in uncovering potential faults in software.\nWhile tools like EvoSuite focus on maximizing code coverage, recent advances in\nlarge language models (LLMs) have shifted attention toward LLM-based test\ngeneration. However, code coverage metrics -- such as line and branch coverage\n-- remain overly emphasized in reported research, despite being weak indicators\nof a test suite's fault-detection capability. In contrast, \\textit{mutation\nscore} offers a more reliable and stringent measure, as demonstrated in our\nfindings where some test suites achieve 100\\% coverage but only 4\\% mutation\nscore. Although a few studies consider mutation score, the effectiveness of\nLLMs in killing mutants remains underexplored.\n  In this paper, we propose MUTGEN, a mutation-guided, LLM-based test\ngeneration approach that incorporates mutation feedback directly into the\nprompt. Evaluated on 204 subjects from two benchmarks, MUTGEN significantly\noutperforms both EvoSuite and vanilla prompt-based strategies in terms of\nmutation score. Furthermore, MUTGEN introduces an iterative generation\nmechanism that pushes the limits of LLMs in killing additional mutants. Our\nstudy also provide insights into the limitations of LLM-based generation,\nanalyzing the reasons for live and uncovered mutants, and the impact of\ndifferent mutation operators on generation effectiveness."
                },
                "authors": [
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Lionel C. Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02862v2",
                "updated": "2025-06-03T14:46:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    46,
                    36,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-03T05:28:11Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    5,
                    28,
                    11,
                    5,
                    123,
                    0
                ],
                "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to\n  Elicit Irrational Choices of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to\n  Elicit Irrational Choices of LLMs"
                },
                "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies."
                },
                "authors": [
                    {
                        "name": "Haoming Yang"
                    },
                    {
                        "name": "Ke Ma"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yingfei Sun"
                    },
                    {
                        "name": "Qianqian Xu"
                    },
                    {
                        "name": "Qingming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qingming Huang"
                },
                "author": "Qingming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02951v1",
                "updated": "2025-06-03T14:46:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    46,
                    0,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:46:00Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    46,
                    0,
                    1,
                    154,
                    0
                ],
                "title": "Adaptive Graph Pruning for Multi-Agent Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Graph Pruning for Multi-Agent Communication"
                },
                "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Zhonghan Zhao"
                    },
                    {
                        "name": "Der-Horng Lee"
                    },
                    {
                        "name": "Gaoang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gaoang Wang"
                },
                "author": "Gaoang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02945v1",
                "updated": "2025-06-03T14:44:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    44,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:44:23Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    44,
                    23,
                    1,
                    154,
                    0
                ],
                "title": "Quantitative LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative LLM Judges"
                },
                "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling."
                },
                "authors": [
                    {
                        "name": "Aishwarya Sahoo"
                    },
                    {
                        "name": "Jeevana Kruthi Karnuthala"
                    },
                    {
                        "name": "Tushar Parmanand Budhwani"
                    },
                    {
                        "name": "Pranchal Agarwal"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "Alexa Siu"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Jennifer Healey"
                    },
                    {
                        "name": "Nedim Lipka"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Uttaran Bhattacharya"
                    },
                    {
                        "name": "Branislav Kveton"
                    }
                ],
                "author_detail": {
                    "name": "Branislav Kveton"
                },
                "author": "Branislav Kveton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02943v1",
                "updated": "2025-06-03T14:43:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    43,
                    5,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:43:05Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    43,
                    5,
                    1,
                    154,
                    0
                ],
                "title": "A Multi-agent LLM-based JUit Test Generation with Strong Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-agent LLM-based JUit Test Generation with Strong Oracles"
                },
                "summary": "Unit testing plays a critical role in ensuring software correctness. However,\nwriting unit tests manually is laborious, especially for strong typed languages\nlike Java, motivating the need for automated approaches. Traditional methods\nprimarily rely on search-based or randomized algorithms to generate tests that\nachieve high code coverage and produce regression oracles, which are derived\nfrom the program's current behavior rather than its intended functionality.\nRecent advances in large language models (LLMs) have enabled oracle generation\nfrom natural language descriptions. However, existing LLM-based methods often\nrequire LLM fine-tuning or rely on external tools such as EvoSuite for test\nprefix generation.\n  In this work, we propose CANDOR, a novel end-to-end, prompt-based LLM\nframework for automated JUnit test generation. CANDOR orchestrates multiple\nspecialized LLM agents to generate JUnit tests, including both high-quality\ntest prefixes and accurate oracles. To mitigate the notorious hallucinations in\nLLMs, we introduce a novel strategy that engages multiple reasoning LLMs in a\npanel discussion and generate accurate oracles based on consensus.\nAdditionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a\nnovel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments on the HumanEvalJava and LeetCodeJava datasets show that\nCANDOR can generate accurate oracles and is slightly better than EvoSuite in\ngenerating tests with high line coverage and clearly superior in terms of\nmutation score. Moreover, CANDOR significantly outperforms the\nstate-of-the-art, prompt-based test generator LLM-Empirical, achieving\nimprovements of 15.8 to 25.1 percentage points in oracle correctness on both\ncorrect and faulty source code. Ablation studies confirm the critical\ncontributions of key agents in improving test prefix quality and oracle\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing plays a critical role in ensuring software correctness. However,\nwriting unit tests manually is laborious, especially for strong typed languages\nlike Java, motivating the need for automated approaches. Traditional methods\nprimarily rely on search-based or randomized algorithms to generate tests that\nachieve high code coverage and produce regression oracles, which are derived\nfrom the program's current behavior rather than its intended functionality.\nRecent advances in large language models (LLMs) have enabled oracle generation\nfrom natural language descriptions. However, existing LLM-based methods often\nrequire LLM fine-tuning or rely on external tools such as EvoSuite for test\nprefix generation.\n  In this work, we propose CANDOR, a novel end-to-end, prompt-based LLM\nframework for automated JUnit test generation. CANDOR orchestrates multiple\nspecialized LLM agents to generate JUnit tests, including both high-quality\ntest prefixes and accurate oracles. To mitigate the notorious hallucinations in\nLLMs, we introduce a novel strategy that engages multiple reasoning LLMs in a\npanel discussion and generate accurate oracles based on consensus.\nAdditionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a\nnovel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments on the HumanEvalJava and LeetCodeJava datasets show that\nCANDOR can generate accurate oracles and is slightly better than EvoSuite in\ngenerating tests with high line coverage and clearly superior in terms of\nmutation score. Moreover, CANDOR significantly outperforms the\nstate-of-the-art, prompt-based test generator LLM-Empirical, achieving\nimprovements of 15.8 to 25.1 percentage points in oracle correctness on both\ncorrect and faulty source code. Ablation studies confirm the critical\ncontributions of key agents in improving test prefix quality and oracle\naccuracy."
                },
                "authors": [
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Lionel Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15186v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15186v5",
                "updated": "2025-06-03T14:40:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    40,
                    1,
                    1,
                    154,
                    0
                ],
                "published": "2024-07-21T14:48:23Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    48,
                    23,
                    6,
                    203,
                    0
                ],
                "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Employing Large Language Models for Text-to-SQL Tasks"
                },
                "summary": "With the development of the Large Language Models (LLMs), a large range of\nLLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a\ncomprehensive review of LLM-based Text2SQL studies. We first enumerate classic\nbenchmarks and evaluation metrics. For the two mainstream methods, prompt\nengineering and finetuning, we introduce a comprehensive taxonomy and offer\npractical insights into each subcategory. We present an overall analysis of the\nabove methods and various models evaluated on well-known datasets and extract\nsome characteristics. Finally, we discuss the challenges and future directions\nin this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of the Large Language Models (LLMs), a large range of\nLLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a\ncomprehensive review of LLM-based Text2SQL studies. We first enumerate classic\nbenchmarks and evaluation metrics. For the two mainstream methods, prompt\nengineering and finetuning, we introduce a comprehensive taxonomy and offer\npractical insights into each subcategory. We present an overall analysis of the\nabove methods and various models evaluated on well-known datasets and extract\nsome characteristics. Finally, we discuss the challenges and future directions\nin this field."
                },
                "authors": [
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Zhengju Tang"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Xiaotong Zhang"
                    },
                    {
                        "name": "Zhi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yang"
                },
                "author": "Zhi Yang",
                "arxiv_doi": "10.1145/3737873.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3737873.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.15186v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15186v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Computing Surveys (CSUR)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02940v1",
                "updated": "2025-06-03T14:39:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    39,
                    56,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:39:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    39,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Memory-Efficient Split Federated Learning for LLM Fine-Tuning on\n  Heterogeneous Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Split Federated Learning for LLM Fine-Tuning on\n  Heterogeneous Mobile Devices"
                },
                "summary": "In this paper, we propose an edge-assisted split federated learning framework\nto facilitate large language model (LLM) fine-tuning on heterogeneous mobile\ndevices while alleviating memory pressures on both mobile devices and the edge\nserver. Specifically, mobile devices perform low-rank adaptation (LoRA)\nfine-tuning on only a subset of lower layers of the pre-trained LLM, tailored\nto their individual capacities. On the server, a full LLM is maintained, and\nthe corresponding LoRA modules are selectively fine-tuned in a sequential\nmanner for each device. To further enhance training efficiency, we propose a\nserver-side training scheduling method that optimizes the processing order of\ndevices for accelerating fine-tuning. Extensive experiments demonstrate that\ncompared to the baselines, our scheme can reduce 79\\% memory footprint and 6\\%\ntraining time while achieving comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an edge-assisted split federated learning framework\nto facilitate large language model (LLM) fine-tuning on heterogeneous mobile\ndevices while alleviating memory pressures on both mobile devices and the edge\nserver. Specifically, mobile devices perform low-rank adaptation (LoRA)\nfine-tuning on only a subset of lower layers of the pre-trained LLM, tailored\nto their individual capacities. On the server, a full LLM is maintained, and\nthe corresponding LoRA modules are selectively fine-tuned in a sequential\nmanner for each device. To further enhance training efficiency, we propose a\nserver-side training scheduling method that optimizes the processing order of\ndevices for accelerating fine-tuning. Extensive experiments demonstrate that\ncompared to the baselines, our scheme can reduce 79\\% memory footprint and 6\\%\ntraining time while achieving comparable performance."
                },
                "authors": [
                    {
                        "name": "Xiaopei Chen"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Fei Ji"
                    },
                    {
                        "name": "Wen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wu"
                },
                "author": "Wen Wu",
                "arxiv_comment": "IEEE INFOCOM IEILM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02939v1",
                "updated": "2025-06-03T14:37:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    37,
                    17,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:37:17Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    37,
                    17,
                    1,
                    154,
                    0
                ],
                "title": "QKV Projections Require a Fraction of Their Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QKV Projections Require a Fraction of Their Memory"
                },
                "summary": "The Multi-Head Attention mechanism is central to LLM operation, and multiple\nworks target its compute and memory efficiency during training. While most\nworks focus on approximating the scaled dot product, the memory consumption of\nthe linear projections that compute the $Q$, $K$, and $V$ tensors from the\ninput $x$ is often overlooked. To address this, we propose Point-Approximate\nMatrix Multiplication (PAMM), a novel tensor compression technique that reduces\nmemory consumption of the $Q,K,V$ projections in attention layers by a factor\nof up to $\\times 512$, effectively erasing their memory footprint, while\nachieving similar or better final perplexity. PAMM is fully composable with\nefficient attention techniques such as FlashAttention, making it a practical\nand complementary method for memory-efficient LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Multi-Head Attention mechanism is central to LLM operation, and multiple\nworks target its compute and memory efficiency during training. While most\nworks focus on approximating the scaled dot product, the memory consumption of\nthe linear projections that compute the $Q$, $K$, and $V$ tensors from the\ninput $x$ is often overlooked. To address this, we propose Point-Approximate\nMatrix Multiplication (PAMM), a novel tensor compression technique that reduces\nmemory consumption of the $Q,K,V$ projections in attention layers by a factor\nof up to $\\times 512$, effectively erasing their memory footprint, while\nachieving similar or better final perplexity. PAMM is fully composable with\nefficient attention techniques such as FlashAttention, making it a practical\nand complementary method for memory-efficient LLM training."
                },
                "authors": [
                    {
                        "name": "Malik Khalaf"
                    },
                    {
                        "name": "Yara Shamshoum"
                    },
                    {
                        "name": "Nitzan Hodos"
                    },
                    {
                        "name": "Yuval Sieradzki"
                    },
                    {
                        "name": "Assaf Schuster"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Schuster"
                },
                "author": "Assaf Schuster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15806v2",
                "updated": "2025-06-03T14:35:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    35,
                    59,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-19T07:23:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    23,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of\n  Iterative Chaos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of\n  Iterative Chaos"
                },
                "summary": "Large Reasoning Models (LRMs) have significantly advanced beyond traditional\nLarge Language Models (LLMs) with their exceptional logical reasoning\ncapabilities, yet these improvements introduce heightened safety risks. When\nsubjected to jailbreak attacks, their ability to generate more targeted and\norganized content can lead to greater harm. Although some studies claim that\nreasoning enables safer LRMs against existing LLM attacks, they overlook the\ninherent flaws within the reasoning process itself. To address this gap, we\npropose the first jailbreak attack targeting LRMs, exploiting their unique\nvulnerabilities stemming from the advanced reasoning capabilities.\nSpecifically, we introduce a Chaos Machine, a novel component to transform\nattack prompts with diverse one-to-one mappings. The chaos mappings iteratively\ngenerated by the machine are embedded into the reasoning chain, which\nstrengthens the variability and complexity and also promotes a more robust\nattack. Based on this, we construct the Mousetrap framework, which makes\nattacks projected into nonlinear-like low sample spaces with mismatched\ngeneralization enhanced. Also, due to the more competing objectives, LRMs\ngradually maintain the inertia of unpredictable iterative reasoning and fall\ninto our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet\nand Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic\ndataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench,\nattacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly\nachieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This\npaper contains inappropriate, offensive and harmful content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have significantly advanced beyond traditional\nLarge Language Models (LLMs) with their exceptional logical reasoning\ncapabilities, yet these improvements introduce heightened safety risks. When\nsubjected to jailbreak attacks, their ability to generate more targeted and\norganized content can lead to greater harm. Although some studies claim that\nreasoning enables safer LRMs against existing LLM attacks, they overlook the\ninherent flaws within the reasoning process itself. To address this gap, we\npropose the first jailbreak attack targeting LRMs, exploiting their unique\nvulnerabilities stemming from the advanced reasoning capabilities.\nSpecifically, we introduce a Chaos Machine, a novel component to transform\nattack prompts with diverse one-to-one mappings. The chaos mappings iteratively\ngenerated by the machine are embedded into the reasoning chain, which\nstrengthens the variability and complexity and also promotes a more robust\nattack. Based on this, we construct the Mousetrap framework, which makes\nattacks projected into nonlinear-like low sample spaces with mismatched\ngeneralization enhanced. Also, due to the more competing objectives, LRMs\ngradually maintain the inertia of unpredictable iterative reasoning and fall\ninto our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet\nand Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic\ndataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench,\nattacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly\nachieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This\npaper contains inappropriate, offensive and harmful content."
                },
                "authors": [
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Xuan Tong"
                    },
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Lujundong Li"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02931v1",
                "updated": "2025-06-03T14:32:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    32,
                    48,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:32:48Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    32,
                    48,
                    1,
                    154,
                    0
                ],
                "title": "ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems\n  into Universal Collaborative Intelligence Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems\n  into Universal Collaborative Intelligence Platforms"
                },
                "summary": "This paper presents ThinkTank, a comprehensive and scalable framework\ndesigned to transform specialized AI agent systems into versatile collaborative\nintelligence platforms capable of supporting complex problem-solving across\ndiverse domains. ThinkTank systematically generalizes agent roles, meeting\nstructures, and knowledge integration mechanisms by adapting proven scientific\ncollaboration methodologies. Through role abstraction, generalization of\nmeeting types for iterative collaboration, and the integration of\nRetrieval-Augmented Generation with advanced knowledge storage, the framework\nfacilitates expertise creation and robust knowledge sharing. ThinkTank enables\norganizations to leverage collaborative AI for knowledge-intensive tasks while\nensuring data privacy and security through local deployment, utilizing\nframeworks like Ollama with models such as Llama3.1. The ThinkTank framework is\ndesigned to deliver significant advantages in cost-effectiveness, data\nsecurity, scalability, and competitive positioning compared to cloud-based\nalternatives, establishing it as a universal platform for AI-driven\ncollaborative problem-solving. The ThinkTank code is available at\nhttps://github.com/taugroup/ThinkTank",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ThinkTank, a comprehensive and scalable framework\ndesigned to transform specialized AI agent systems into versatile collaborative\nintelligence platforms capable of supporting complex problem-solving across\ndiverse domains. ThinkTank systematically generalizes agent roles, meeting\nstructures, and knowledge integration mechanisms by adapting proven scientific\ncollaboration methodologies. Through role abstraction, generalization of\nmeeting types for iterative collaboration, and the integration of\nRetrieval-Augmented Generation with advanced knowledge storage, the framework\nfacilitates expertise creation and robust knowledge sharing. ThinkTank enables\norganizations to leverage collaborative AI for knowledge-intensive tasks while\nensuring data privacy and security through local deployment, utilizing\nframeworks like Ollama with models such as Llama3.1. The ThinkTank framework is\ndesigned to deliver significant advantages in cost-effectiveness, data\nsecurity, scalability, and competitive positioning compared to cloud-based\nalternatives, establishing it as a universal platform for AI-driven\ncollaborative problem-solving. The ThinkTank code is available at\nhttps://github.com/taugroup/ThinkTank"
                },
                "authors": [
                    {
                        "name": "Praneet Sai Madhu Surabhi"
                    },
                    {
                        "name": "Dheeraj Reddy Mudireddy"
                    },
                    {
                        "name": "Jian Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tao"
                },
                "author": "Jian Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02929v1",
                "updated": "2025-06-03T14:30:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    30,
                    52,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:30:52Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    30,
                    52,
                    1,
                    154,
                    0
                ],
                "title": "Large Processor Chip Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Processor Chip Model"
                },
                "summary": "Computer System Architecture serves as a crucial bridge between software\napplications and the underlying hardware, encompassing components like\ncompilers, CPUs, coprocessors, and RTL designs. Its development, from early\nmainframes to modern domain-specific architectures, has been driven by rising\ncomputational demands and advancements in semiconductor technology. However,\ntraditional paradigms in computer system architecture design are confronting\nsignificant challenges, including a reliance on manual expertise, fragmented\noptimization across software and hardware layers, and high costs associated\nwith exploring expansive design spaces. While automated methods leveraging\noptimization algorithms and machine learning have improved efficiency, they\nremain constrained by a single-stage focus, limited data availability, and a\nlack of comprehensive human domain knowledge. The emergence of large language\nmodels offers transformative opportunities for the design of computer system\narchitecture. By leveraging the capabilities of LLMs in areas such as code\ngeneration, data analysis, and performance modeling, the traditional manual\ndesign process can be transitioned to a machine-based automated design\napproach. To harness this potential, we present the Large Processor Chip Model\n(LPCM), an LLM-driven framework aimed at achieving end-to-end automated\ncomputer architecture design. The LPCM is structured into three levels:\nHuman-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D\nGaussian Splatting as a representative workload and employs the concept of\nsoftware-hardware collaborative design to examine the implementation of the\nLPCM at Level 1, demonstrating the effectiveness of the proposed approach.\nFurthermore, this paper provides an in-depth discussion on the pathway to\nimplementing Level 2 and Level 3 of the LPCM, along with an analysis of the\nexisting challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer System Architecture serves as a crucial bridge between software\napplications and the underlying hardware, encompassing components like\ncompilers, CPUs, coprocessors, and RTL designs. Its development, from early\nmainframes to modern domain-specific architectures, has been driven by rising\ncomputational demands and advancements in semiconductor technology. However,\ntraditional paradigms in computer system architecture design are confronting\nsignificant challenges, including a reliance on manual expertise, fragmented\noptimization across software and hardware layers, and high costs associated\nwith exploring expansive design spaces. While automated methods leveraging\noptimization algorithms and machine learning have improved efficiency, they\nremain constrained by a single-stage focus, limited data availability, and a\nlack of comprehensive human domain knowledge. The emergence of large language\nmodels offers transformative opportunities for the design of computer system\narchitecture. By leveraging the capabilities of LLMs in areas such as code\ngeneration, data analysis, and performance modeling, the traditional manual\ndesign process can be transitioned to a machine-based automated design\napproach. To harness this potential, we present the Large Processor Chip Model\n(LPCM), an LLM-driven framework aimed at achieving end-to-end automated\ncomputer architecture design. The LPCM is structured into three levels:\nHuman-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D\nGaussian Splatting as a representative workload and employs the concept of\nsoftware-hardware collaborative design to examine the implementation of the\nLPCM at Level 1, demonstrating the effectiveness of the proposed approach.\nFurthermore, this paper provides an in-depth discussion on the pathway to\nimplementing Level 2 and Level 3 of the LPCM, along with an analysis of the\nexisting challenges."
                },
                "authors": [
                    {
                        "name": "Kaiyan Chang"
                    },
                    {
                        "name": "Mingzhi Chen"
                    },
                    {
                        "name": "Yunji Chen"
                    },
                    {
                        "name": "Zhirong Chen"
                    },
                    {
                        "name": "Dongrui Fan"
                    },
                    {
                        "name": "Junfeng Gong"
                    },
                    {
                        "name": "Nan Guo"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Qinfen Hao"
                    },
                    {
                        "name": "Shuo Hou"
                    },
                    {
                        "name": "Xuan Huang"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Changxin Ke"
                    },
                    {
                        "name": "Cangyuan Li"
                    },
                    {
                        "name": "Guangli Li"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Naipeng Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Jiahua Liu"
                    },
                    {
                        "name": "Junliang Lv"
                    },
                    {
                        "name": "Jianan Mu"
                    },
                    {
                        "name": "Jin Qin"
                    },
                    {
                        "name": "Bin Sun"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Duo Wang"
                    },
                    {
                        "name": "Mingjun Wang"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Chenggang Wu"
                    },
                    {
                        "name": "Peiyang Wu"
                    },
                    {
                        "name": "Teng Wu"
                    },
                    {
                        "name": "Xiao Xiao"
                    },
                    {
                        "name": "Mengyao Xie"
                    },
                    {
                        "name": "Chenwei Xiong"
                    },
                    {
                        "name": "Ruiyuan Xu"
                    },
                    {
                        "name": "Mingyu Yan"
                    },
                    {
                        "name": "Xiaochun Ye"
                    },
                    {
                        "name": "Kuai Yu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Shuoming Zhang"
                    },
                    {
                        "name": "Jiacheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jiacheng Zhao"
                },
                "author": "Jiacheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02924v1",
                "updated": "2025-06-03T14:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    25,
                    12,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    25,
                    12,
                    1,
                    154,
                    0
                ],
                "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and\n  Prompt-Based Approaches to Depression Symptom Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and\n  Prompt-Based Approaches to Depression Symptom Identification"
                },
                "summary": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams."
                },
                "authors": [
                    {
                        "name": "Diogo A. P. Nunes"
                    },
                    {
                        "name": "Eugnio Ribeiro"
                    }
                ],
                "author_detail": {
                    "name": "Eugnio Ribeiro"
                },
                "author": "Eugnio Ribeiro",
                "arxiv_comment": "12 pages, 1 figure, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.5.4; J.3; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02923v1",
                "updated": "2025-06-03T14:24:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    24,
                    58,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:24:58Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    24,
                    58,
                    1,
                    154,
                    0
                ],
                "title": "The Limits of Predicting Agents from Behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Limits of Predicting Agents from Behaviour"
                },
                "summary": "As the complexity of AI systems and their interactions with the world\nincreases, generating explanations for their behaviour is important for safely\ndeploying AI. For agents, the most natural abstractions for predicting\nbehaviour attribute beliefs, intentions and goals to the system. If an agent\nbehaves as if it has a certain goal or belief, then we can make reasonable\npredictions about how it will behave in novel situations, including those where\ncomprehensive safety evaluations are untenable. How well can we infer an\nagent's beliefs from their behaviour, and how reliably can these inferred\nbeliefs predict the agent's behaviour in novel situations? We provide a precise\nanswer to this question under the assumption that the agent's behaviour is\nguided by a world model. Our contribution is the derivation of novel bounds on\nthe agent's behaviour in new (unseen) deployment environments, which represent\na theoretical limit for predicting intentional agents from behavioural data\nalone. We discuss the implications of these results for several research areas\nincluding fairness and safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the complexity of AI systems and their interactions with the world\nincreases, generating explanations for their behaviour is important for safely\ndeploying AI. For agents, the most natural abstractions for predicting\nbehaviour attribute beliefs, intentions and goals to the system. If an agent\nbehaves as if it has a certain goal or belief, then we can make reasonable\npredictions about how it will behave in novel situations, including those where\ncomprehensive safety evaluations are untenable. How well can we infer an\nagent's beliefs from their behaviour, and how reliably can these inferred\nbeliefs predict the agent's behaviour in novel situations? We provide a precise\nanswer to this question under the assumption that the agent's behaviour is\nguided by a world model. Our contribution is the derivation of novel bounds on\nthe agent's behaviour in new (unseen) deployment environments, which represent\na theoretical limit for predicting intentional agents from behavioural data\nalone. We discuss the implications of these results for several research areas\nincluding fairness and safety."
                },
                "authors": [
                    {
                        "name": "Alexis Bellot"
                    },
                    {
                        "name": "Jonathan Richens"
                    },
                    {
                        "name": "Tom Everitt"
                    }
                ],
                "author_detail": {
                    "name": "Tom Everitt"
                },
                "author": "Tom Everitt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06645v2",
                "updated": "2025-06-03T14:23:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    23,
                    27,
                    1,
                    154,
                    0
                ],
                "published": "2025-01-11T21:41:27Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    21,
                    41,
                    27,
                    5,
                    11,
                    0
                ],
                "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings"
                },
                "summary": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness."
                },
                "authors": [
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02918v1",
                "updated": "2025-06-03T14:20:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    20,
                    59,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:20:59Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    20,
                    59,
                    1,
                    154,
                    0
                ],
                "title": "Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use\n  of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use\n  of LLMs"
                },
                "summary": "Tool use in stateful environments presents unique challenges for large\nlanguage models (LLMs), where existing test-time compute strategies relying on\nrepeated trials in the environment are impractical. We propose dynamics\nmodelling (DyMo), a method that augments LLMs with a state prediction\ncapability alongside function calling during post-training. This enables LLMs\nto predict the future states of their actions through an internal environment\nmodel. On the Berkeley Function Calling Leaderboard V2, DyMo improves success\nrates and significantly reduces hallucinations. We further integrate the\ninternal environment model into self-verification sampling (SVS), and show that\nthis substantially improves pass^k over number of trials k, and allows the\nmodel to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the\neffectiveness and reliability of LLMs for tool use. We believe this work charts\na path towards scalable planning RL methods for LLM inference without\nrepeatedly querying the oracle environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool use in stateful environments presents unique challenges for large\nlanguage models (LLMs), where existing test-time compute strategies relying on\nrepeated trials in the environment are impractical. We propose dynamics\nmodelling (DyMo), a method that augments LLMs with a state prediction\ncapability alongside function calling during post-training. This enables LLMs\nto predict the future states of their actions through an internal environment\nmodel. On the Berkeley Function Calling Leaderboard V2, DyMo improves success\nrates and significantly reduces hallucinations. We further integrate the\ninternal environment model into self-verification sampling (SVS), and show that\nthis substantially improves pass^k over number of trials k, and allows the\nmodel to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the\neffectiveness and reliability of LLMs for tool use. We believe this work charts\na path towards scalable planning RL methods for LLM inference without\nrepeatedly querying the oracle environment."
                },
                "authors": [
                    {
                        "name": "Shangmin Guo"
                    },
                    {
                        "name": "Omar Darwiche Domingues"
                    },
                    {
                        "name": "Raphal Avalos"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Florian Strub"
                    }
                ],
                "author_detail": {
                    "name": "Florian Strub"
                },
                "author": "Florian Strub",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02916v2",
                "updated": "2025-06-04T01:42:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    1,
                    42,
                    22,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T14:18:19Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    18,
                    19,
                    1,
                    154,
                    0
                ],
                "title": "MMM4Rec: A Transfer-Efficient Framework for Multi-modal Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMM4Rec: A Transfer-Efficient Framework for Multi-modal Sequential\n  Recommendation"
                },
                "summary": "Sequential Recommendation (SR) systems model user preferences by analyzing\ninteraction histories. Although transferable multi-modal SR architectures\ndemonstrate superior performance compared to traditional ID-based approaches,\ncurrent methods incur substantial fine-tuning costs when adapting to new\ndomains due to complex optimization requirements and negative transfer effects\n- a significant deployment bottleneck that hinders engineers from efficiently\nrepurposing pre-trained models for novel application scenarios with minimal\ntuning overhead. We propose MMM4Rec (Multi-Modal Mamba for Sequential\nRecommendation), a novel multi-modal SR framework that incorporates a dedicated\nalgebraic constraint mechanism for efficient transfer learning. By combining\nState Space Duality (SSD)'s temporal decay properties with a time-aware\nmodeling design, our model dynamically prioritizes key modality information,\novercoming limitations of Transformer-based approaches. The framework\nimplements a constrained two-stage process: (1) sequence-level cross-modal\nalignment via shared projection matrices, followed by (2) temporal fusion using\nour newly designed Cross-SSD module and dual-channel Fourier adaptive\nfiltering. This architecture maintains semantic consistency while suppressing\nnoise propagation.MMM4Rec achieves rapid fine-tuning convergence with simple\ncross-entropy loss, significantly improving multi-modal recommendation accuracy\nwhile maintaining strong transferability. Extensive experiments demonstrate\nMMM4Rec's state-of-the-art performance, achieving the maximum 31.78% NDCG@10\nimprovement over existing models and exhibiting 10 times faster average\nconvergence speed when transferring to large-scale downstream datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommendation (SR) systems model user preferences by analyzing\ninteraction histories. Although transferable multi-modal SR architectures\ndemonstrate superior performance compared to traditional ID-based approaches,\ncurrent methods incur substantial fine-tuning costs when adapting to new\ndomains due to complex optimization requirements and negative transfer effects\n- a significant deployment bottleneck that hinders engineers from efficiently\nrepurposing pre-trained models for novel application scenarios with minimal\ntuning overhead. We propose MMM4Rec (Multi-Modal Mamba for Sequential\nRecommendation), a novel multi-modal SR framework that incorporates a dedicated\nalgebraic constraint mechanism for efficient transfer learning. By combining\nState Space Duality (SSD)'s temporal decay properties with a time-aware\nmodeling design, our model dynamically prioritizes key modality information,\novercoming limitations of Transformer-based approaches. The framework\nimplements a constrained two-stage process: (1) sequence-level cross-modal\nalignment via shared projection matrices, followed by (2) temporal fusion using\nour newly designed Cross-SSD module and dual-channel Fourier adaptive\nfiltering. This architecture maintains semantic consistency while suppressing\nnoise propagation.MMM4Rec achieves rapid fine-tuning convergence with simple\ncross-entropy loss, significantly improving multi-modal recommendation accuracy\nwhile maintaining strong transferability. Extensive experiments demonstrate\nMMM4Rec's state-of-the-art performance, achieving the maximum 31.78% NDCG@10\nimprovement over existing models and exhibiting 10 times faster average\nconvergence speed when transferring to large-scale downstream datasets."
                },
                "authors": [
                    {
                        "name": "Hao Fan"
                    },
                    {
                        "name": "Yanrong Hu"
                    },
                    {
                        "name": "Kai Fang"
                    },
                    {
                        "name": "Qingyang Liu"
                    },
                    {
                        "name": "Hongjiu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hongjiu Liu"
                },
                "author": "Hongjiu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02911v1",
                "updated": "2025-06-03T14:16:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    16,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T14:16:53Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    16,
                    53,
                    1,
                    154,
                    0
                ],
                "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with\n  Reinforcement Learning"
                },
                "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1."
                },
                "authors": [
                    {
                        "name": "Yin Fang"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Xianrui Zhong"
                    },
                    {
                        "name": "Siru Ouyang"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "arxiv_comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12574v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12574v4",
                "updated": "2025-06-03T14:13:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    13,
                    57,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-18T23:22:53Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    23,
                    22,
                    53,
                    6,
                    138,
                    0
                ],
                "title": "PoisonArena: Uncovering Competing Poisoning Attacks in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoisonArena: Uncovering Competing Poisoning Attacks in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems, widely used to improve the\nfactual grounding of large language models (LLMs), are increasingly vulnerable\nto poisoning attacks, where adversaries inject manipulated content into the\nretriever's corpus. While prior research has predominantly focused on\nsingle-attacker settings, real-world scenarios often involve multiple,\ncompeting attackers with conflicting objectives. In this work, we introduce\nPoisonArena, the first benchmark to systematically study and evaluate competing\npoisoning attacks in RAG. We formalize the multi-attacker threat model, where\nattackers vie to control the answer to the same query using mutually exclusive\nmisinformation. PoisonArena leverages the Bradley-Terry model to quantify each\nmethod's competitive effectiveness in such adversarial environments. Through\nextensive experiments on the Natural Questions and MS MARCO datasets, we\ndemonstrate that many attack strategies successful in isolation fail under\ncompetitive pressure. Our findings highlight the limitations of conventional\nevaluation metrics like Attack Success Rate (ASR) and F1 score and underscore\nthe need for competitive evaluation to assess real-world attack robustness.\nPoisonArena provides a standardized framework to benchmark and develop future\nattack and defense strategies under more realistic, multi-adversary conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems, widely used to improve the\nfactual grounding of large language models (LLMs), are increasingly vulnerable\nto poisoning attacks, where adversaries inject manipulated content into the\nretriever's corpus. While prior research has predominantly focused on\nsingle-attacker settings, real-world scenarios often involve multiple,\ncompeting attackers with conflicting objectives. In this work, we introduce\nPoisonArena, the first benchmark to systematically study and evaluate competing\npoisoning attacks in RAG. We formalize the multi-attacker threat model, where\nattackers vie to control the answer to the same query using mutually exclusive\nmisinformation. PoisonArena leverages the Bradley-Terry model to quantify each\nmethod's competitive effectiveness in such adversarial environments. Through\nextensive experiments on the Natural Questions and MS MARCO datasets, we\ndemonstrate that many attack strategies successful in isolation fail under\ncompetitive pressure. Our findings highlight the limitations of conventional\nevaluation metrics like Attack Success Rate (ASR) and F1 score and underscore\nthe need for competitive evaluation to assess real-world attack robustness.\nPoisonArena provides a standardized framework to benchmark and develop future\nattack and defense strategies under more realistic, multi-adversary conditions."
                },
                "authors": [
                    {
                        "name": "Liuji Chen"
                    },
                    {
                        "name": "Xiaofang Yang"
                    },
                    {
                        "name": "Yuanzhuo Lu"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Jing Dong"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "arxiv_comment": "Project page: https://poison-arena.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12574v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12574v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15587v2",
                "updated": "2025-06-03T14:12:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    12,
                    11,
                    1,
                    154,
                    0
                ],
                "published": "2024-03-22T19:21:44Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    19,
                    21,
                    44,
                    4,
                    82,
                    0
                ],
                "title": "Large language models for crowd decision making based on prompt design\n  strategies using ChatGPT: models, analysis and challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for crowd decision making based on prompt design\n  strategies using ChatGPT: models, analysis and challenges"
                },
                "summary": "Social Media and Internet have the potential to be exploited as a source of\nopinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a\nmethodology able to infer opinions and decisions from plain texts, such as\nreviews published in social media platforms, by means of Sentiment Analysis.\nCurrently, the emergence and potential of Large Language Models (LLMs) lead us\nto explore new scenarios of automatically understand written texts, also known\nas natural language processing. This paper analyzes the use of ChatGPT based on\nprompt design strategies to assist in CDM processes to extract opinions and\nmake decisions. We integrate ChatGPT in CDM processes as a flexible tool that\ninfer the opinions expressed in texts, providing numerical or linguistic\nevaluations where the decision making models are based on the prompt design\nstrategies. We include a multi-criteria decision making scenario with a\ncategory ontology for criteria. We also consider ChatGPT as an end-to-end CDM\nmodel able to provide a general opinion and score on the alternatives. We\nconduct empirical experiments on real data extracted from TripAdvisor, the\nTripR-2020Large dataset. The analysis of results show a promising branch for\ndeveloping quality decision making models using ChatGPT. Finally, we discuss\nthe challenges of consistency, sensitivity and explainability associated to the\nuse of LLMs in CDM processes, raising open questions for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Media and Internet have the potential to be exploited as a source of\nopinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a\nmethodology able to infer opinions and decisions from plain texts, such as\nreviews published in social media platforms, by means of Sentiment Analysis.\nCurrently, the emergence and potential of Large Language Models (LLMs) lead us\nto explore new scenarios of automatically understand written texts, also known\nas natural language processing. This paper analyzes the use of ChatGPT based on\nprompt design strategies to assist in CDM processes to extract opinions and\nmake decisions. We integrate ChatGPT in CDM processes as a flexible tool that\ninfer the opinions expressed in texts, providing numerical or linguistic\nevaluations where the decision making models are based on the prompt design\nstrategies. We include a multi-criteria decision making scenario with a\ncategory ontology for criteria. We also consider ChatGPT as an end-to-end CDM\nmodel able to provide a general opinion and score on the alternatives. We\nconduct empirical experiments on real data extracted from TripAdvisor, the\nTripR-2020Large dataset. The analysis of results show a promising branch for\ndeveloping quality decision making models using ChatGPT. Finally, we discuss\nthe challenges of consistency, sensitivity and explainability associated to the\nuse of LLMs in CDM processes, raising open questions for future studies."
                },
                "authors": [
                    {
                        "name": "David Herrera-Poyatos"
                    },
                    {
                        "name": "Cristina Zuheros"
                    },
                    {
                        "name": "Rosana Montes"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18924v2",
                "updated": "2025-06-03T14:11:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    11,
                    24,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-25T01:10:58Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    1,
                    10,
                    58,
                    6,
                    145,
                    0
                ],
                "title": "LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point Cloud\n  Active Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point Cloud\n  Active Learning"
                },
                "summary": "We present a novel active learning framework for 3D point cloud semantic\nsegmentation that, for the first time, integrates large language models (LLMs)\nto construct hierarchical label structures and guide uncertainty-based sample\nselection. Unlike prior methods that treat labels as flat and independent, our\napproach leverages LLM prompting to automatically generate multi-level semantic\ntaxonomies and introduces a recursive uncertainty projection mechanism that\npropagates uncertainty across hierarchy levels. This enables spatially diverse,\nlabel-aware point selection that respects the inherent semantic structure of 3D\nscenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to\n4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%),\nsubstantially outperforming existing baselines. Our results highlight the\nuntapped potential of LLMs as knowledge priors in 3D vision and establish\nhierarchical uncertainty modeling as a powerful paradigm for efficient point\ncloud annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel active learning framework for 3D point cloud semantic\nsegmentation that, for the first time, integrates large language models (LLMs)\nto construct hierarchical label structures and guide uncertainty-based sample\nselection. Unlike prior methods that treat labels as flat and independent, our\napproach leverages LLM prompting to automatically generate multi-level semantic\ntaxonomies and introduces a recursive uncertainty projection mechanism that\npropagates uncertainty across hierarchy levels. This enables spatially diverse,\nlabel-aware point selection that respects the inherent semantic structure of 3D\nscenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to\n4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%),\nsubstantially outperforming existing baselines. Our results highlight the\nuntapped potential of LLMs as knowledge priors in 3D vision and establish\nhierarchical uncertainty modeling as a powerful paradigm for efficient point\ncloud annotation."
                },
                "authors": [
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Fengyun Tan"
                    },
                    {
                        "name": "Yantong Chen"
                    },
                    {
                        "name": "Bochun Yuan"
                    },
                    {
                        "name": "Tianrui Li"
                    },
                    {
                        "name": "Chongshou Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongshou Li"
                },
                "author": "Chongshou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19318v4",
                "updated": "2025-06-03T14:08:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    8,
                    24,
                    1,
                    154,
                    0
                ],
                "published": "2025-01-31T17:15:33Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    15,
                    33,
                    4,
                    31,
                    0
                ],
                "title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems"
                },
                "summary": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience."
                },
                "authors": [
                    {
                        "name": "Anirudh Chari"
                    },
                    {
                        "name": "Suraj Reddy"
                    },
                    {
                        "name": "Aditya Tiwari"
                    },
                    {
                        "name": "Richard Lian"
                    },
                    {
                        "name": "Brian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Brian Zhou"
                },
                "author": "Brian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15289v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15289v3",
                "updated": "2025-06-03T13:59:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    59,
                    22,
                    1,
                    154,
                    0
                ],
                "published": "2024-12-19T05:57:37Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    5,
                    57,
                    37,
                    3,
                    354,
                    0
                ],
                "title": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage"
                },
                "summary": "Large language models (LLMs) have made significant advancements across\nvarious tasks, but their safety alignment remain a major concern. Exploring\njailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure\nthem. Existing methods primarily design sophisticated instructions for the LLM\nto follow, or rely on multiple iterations, which could hinder the performance\nand efficiency of jailbreaks. In this work, we propose a novel jailbreak\nparadigm, Simple Assistive Task Linkage (SATA), which can effectively\ncircumvent LLM safeguards and elicit harmful responses. Specifically, SATA\nfirst masks harmful keywords within a malicious query to generate a relatively\nbenign query containing one or multiple [MASK] special tokens. It then employs\na simple assistive task such as a masked language model task or an element\nlookup by position task to encode the semantics of the masked keywords.\nFinally, SATA links the assistive task with the masked query to jointly perform\nthe jailbreak. Extensive experiments show that SATA achieves state-of-the-art\nperformance and outperforms baselines by a large margin. Specifically, on\nAdvBench dataset, with mask language model (MLM) assistive task, SATA achieves\nan overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and\nwith element lookup by position (ELP) assistive task, SATA attains an overall\nASR of 76% and HS of 4.43.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advancements across\nvarious tasks, but their safety alignment remain a major concern. Exploring\njailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure\nthem. Existing methods primarily design sophisticated instructions for the LLM\nto follow, or rely on multiple iterations, which could hinder the performance\nand efficiency of jailbreaks. In this work, we propose a novel jailbreak\nparadigm, Simple Assistive Task Linkage (SATA), which can effectively\ncircumvent LLM safeguards and elicit harmful responses. Specifically, SATA\nfirst masks harmful keywords within a malicious query to generate a relatively\nbenign query containing one or multiple [MASK] special tokens. It then employs\na simple assistive task such as a masked language model task or an element\nlookup by position task to encode the semantics of the masked keywords.\nFinally, SATA links the assistive task with the masked query to jointly perform\nthe jailbreak. Extensive experiments show that SATA achieves state-of-the-art\nperformance and outperforms baselines by a large margin. Specifically, on\nAdvBench dataset, with mask language model (MLM) assistive task, SATA achieves\nan overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and\nwith element lookup by position (ELP) assistive task, SATA attains an overall\nASR of 76% and HS of 4.43."
                },
                "authors": [
                    {
                        "name": "Xiaoning Dong"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15289v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15289v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02890v1",
                "updated": "2025-06-03T13:55:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    55,
                    48,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:55:48Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    55,
                    48,
                    1,
                    154,
                    0
                ],
                "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and\n  Practical Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and\n  Practical Insights"
                },
                "summary": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models."
                },
                "authors": [
                    {
                        "name": "Jakub Krajewski"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Korzekwa"
                },
                "author": "Daniel Korzekwa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02882v1",
                "updated": "2025-06-03T13:47:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    47,
                    59,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:47:59Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    47,
                    59,
                    1,
                    154,
                    0
                ],
                "title": "GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation"
                },
                "summary": "Improving robustness of the Segment Anything Model (SAM) to input\ndegradations is critical for its deployment in high-stakes applications such as\nautonomous driving and robotics. Our approach to this challenge prioritizes\nthree key aspects: first, parameter efficiency to maintain the inherent\ngeneralization capability of SAM; second, fine-grained and input-aware\nrobustification to precisely address the input corruption; and third, adherence\nto standard training protocols for ease of training. To this end, we propose\ngated-rank adaptation (GaRA). GaRA introduces lightweight adapters into\nintermediate layers of the frozen SAM, where each adapter dynamically adjusts\nthe effective rank of its weight matrix based on the input by selectively\nactivating (rank-1) components of the matrix using a learned gating module.\nThis adjustment enables fine-grained and input-aware robustification without\ncompromising the generalization capability of SAM. Our model, GaRA-SAM,\nsignificantly outperforms prior work on all robust segmentation benchmarks. In\nparticular, it surpasses the previous best IoU score by up to 21.3\\%p on ACDC,\na challenging real corrupted image dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving robustness of the Segment Anything Model (SAM) to input\ndegradations is critical for its deployment in high-stakes applications such as\nautonomous driving and robotics. Our approach to this challenge prioritizes\nthree key aspects: first, parameter efficiency to maintain the inherent\ngeneralization capability of SAM; second, fine-grained and input-aware\nrobustification to precisely address the input corruption; and third, adherence\nto standard training protocols for ease of training. To this end, we propose\ngated-rank adaptation (GaRA). GaRA introduces lightweight adapters into\nintermediate layers of the frozen SAM, where each adapter dynamically adjusts\nthe effective rank of its weight matrix based on the input by selectively\nactivating (rank-1) components of the matrix using a learned gating module.\nThis adjustment enables fine-grained and input-aware robustification without\ncompromising the generalization capability of SAM. Our model, GaRA-SAM,\nsignificantly outperforms prior work on all robust segmentation benchmarks. In\nparticular, it surpasses the previous best IoU score by up to 21.3\\%p on ACDC,\na challenging real corrupted image dataset."
                },
                "authors": [
                    {
                        "name": "Sohyun Lee"
                    },
                    {
                        "name": "Yeho Kwon"
                    },
                    {
                        "name": "Lukas Hoyer"
                    },
                    {
                        "name": "Suha Kwak"
                    }
                ],
                "author_detail": {
                    "name": "Suha Kwak"
                },
                "author": "Suha Kwak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20322v2",
                "updated": "2025-06-03T13:40:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    40,
                    17,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-23T17:59:18Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    59,
                    18,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering\n  Target Atoms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering\n  Target Atoms"
                },
                "summary": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control."
                },
                "authors": [
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Shengyu Mao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02873v1",
                "updated": "2025-06-03T13:37:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    37,
                    51,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:37:51Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    37,
                    51,
                    1,
                    154,
                    0
                ],
                "title": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs\n  to Persuade on Harmful Topics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs\n  to Persuade on Harmful Topics"
                },
                "summary": "Persuasion is a powerful capability of large language models (LLMs) that both\nenables beneficial applications (e.g. helping people quit smoking) and raises\nsignificant risks (e.g. large-scale, targeted political manipulation). Prior\nwork has found models possess a significant and growing persuasive capability,\nmeasured by belief changes in simulated or real users. However, these\nbenchmarks overlook a crucial risk factor: the propensity of a model to attempt\nto persuade in harmful contexts. Understanding whether a model will blindly\n``follow orders'' to persuade on harmful topics (e.g. glorifying joining a\nterrorist group) is key to understanding the efficacy of safety guardrails.\nMoreover, understanding if and when a model will engage in persuasive behavior\nin pursuit of some goal is essential to understanding the risks from agentic AI\nsystems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts\nthe focus from persuasion success to persuasion attempts, operationalized as a\nmodel's willingness to generate content aimed at shaping beliefs or behavior.\nOur evaluation framework probes frontier LLMs using a multi-turn conversational\nsetup between simulated persuader and persuadee agents. APE explores a diverse\nspectrum of topics including conspiracies, controversial issues, and\nnon-controversially harmful content. We introduce an automated evaluator model\nto identify willingness to persuade and measure the frequency and context of\npersuasive attempts. We find that many open and closed-weight models are\nfrequently willing to attempt persuasion on harmful topics and that\njailbreaking can increase willingness to engage in such behavior. Our results\nhighlight gaps in current safety guardrails and underscore the importance of\nevaluating willingness to persuade as a key dimension of LLM risk. APE is\navailable at github.com/AlignmentResearch/AttemptPersuadeEval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion is a powerful capability of large language models (LLMs) that both\nenables beneficial applications (e.g. helping people quit smoking) and raises\nsignificant risks (e.g. large-scale, targeted political manipulation). Prior\nwork has found models possess a significant and growing persuasive capability,\nmeasured by belief changes in simulated or real users. However, these\nbenchmarks overlook a crucial risk factor: the propensity of a model to attempt\nto persuade in harmful contexts. Understanding whether a model will blindly\n``follow orders'' to persuade on harmful topics (e.g. glorifying joining a\nterrorist group) is key to understanding the efficacy of safety guardrails.\nMoreover, understanding if and when a model will engage in persuasive behavior\nin pursuit of some goal is essential to understanding the risks from agentic AI\nsystems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts\nthe focus from persuasion success to persuasion attempts, operationalized as a\nmodel's willingness to generate content aimed at shaping beliefs or behavior.\nOur evaluation framework probes frontier LLMs using a multi-turn conversational\nsetup between simulated persuader and persuadee agents. APE explores a diverse\nspectrum of topics including conspiracies, controversial issues, and\nnon-controversially harmful content. We introduce an automated evaluator model\nto identify willingness to persuade and measure the frequency and context of\npersuasive attempts. We find that many open and closed-weight models are\nfrequently willing to attempt persuasion on harmful topics and that\njailbreaking can increase willingness to engage in such behavior. Our results\nhighlight gaps in current safety guardrails and underscore the importance of\nevaluating willingness to persuade as a key dimension of LLM risk. APE is\navailable at github.com/AlignmentResearch/AttemptPersuadeEval"
                },
                "authors": [
                    {
                        "name": "Matthew Kowal"
                    },
                    {
                        "name": "Jasper Timm"
                    },
                    {
                        "name": "Jean-Francois Godbout"
                    },
                    {
                        "name": "Thomas Costello"
                    },
                    {
                        "name": "Antonio A. Arechar"
                    },
                    {
                        "name": "Gordon Pennycook"
                    },
                    {
                        "name": "David Rand"
                    },
                    {
                        "name": "Adam Gleave"
                    },
                    {
                        "name": "Kellin Pelrine"
                    }
                ],
                "author_detail": {
                    "name": "Kellin Pelrine"
                },
                "author": "Kellin Pelrine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23908v2",
                "updated": "2025-06-03T13:32:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    32,
                    45,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-29T18:02:16Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    2,
                    16,
                    3,
                    149,
                    0
                ],
                "title": "Transforming Podcast Preview Generation: From Expert Models to LLM-Based\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Podcast Preview Generation: From Expert Models to LLM-Based\n  Systems"
                },
                "summary": "Discovering and evaluating long-form talk content such as videos and podcasts\nposes a significant challenge for users, as it requires a considerable time\ninvestment. Previews offer a practical solution by providing concise snippets\nthat showcase key moments of the content, enabling users to make more informed\nand confident choices. We propose an LLM-based approach for generating podcast\nepisode previews and deploy the solution at scale, serving hundreds of\nthousands of podcast previews in a real-world application. Comprehensive\noffline evaluations and online A/B testing demonstrate that LLM-generated\npreviews consistently outperform a strong baseline built on top of various ML\nexpert models, showcasing a significant reduction in the need for meticulous\nfeature engineering. The offline results indicate notable enhancements in\nunderstandability, contextual clarity, and interest level, and the online A/B\ntest shows a 4.6% increase in user engagement with preview content, along with\na 5x boost in processing efficiency, offering a more streamlined and performant\nsolution compared to the strong baseline of feature-engineered expert models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering and evaluating long-form talk content such as videos and podcasts\nposes a significant challenge for users, as it requires a considerable time\ninvestment. Previews offer a practical solution by providing concise snippets\nthat showcase key moments of the content, enabling users to make more informed\nand confident choices. We propose an LLM-based approach for generating podcast\nepisode previews and deploy the solution at scale, serving hundreds of\nthousands of podcast previews in a real-world application. Comprehensive\noffline evaluations and online A/B testing demonstrate that LLM-generated\npreviews consistently outperform a strong baseline built on top of various ML\nexpert models, showcasing a significant reduction in the need for meticulous\nfeature engineering. The offline results indicate notable enhancements in\nunderstandability, contextual clarity, and interest level, and the online A/B\ntest shows a 4.6% increase in user engagement with preview content, along with\na 5x boost in processing efficiency, offering a more streamlined and performant\nsolution compared to the strong baseline of feature-engineered expert models."
                },
                "authors": [
                    {
                        "name": "Winstead Zhu"
                    },
                    {
                        "name": "Ann Clifton"
                    },
                    {
                        "name": "Azin Ghazimatin"
                    },
                    {
                        "name": "Edgar Tanaka"
                    },
                    {
                        "name": "Edward Ronan"
                    }
                ],
                "author_detail": {
                    "name": "Edward Ronan"
                },
                "author": "Edward Ronan",
                "arxiv_comment": "9 pages, 2 figures, accepted at ACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02867v1",
                "updated": "2025-06-03T13:31:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    31,
                    10,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:31:10Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    31,
                    10,
                    1,
                    154,
                    0
                ],
                "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens\n  are Information Peaks in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens\n  are Information Peaks in LLM Reasoning"
                },
                "summary": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks."
                },
                "authors": [
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Haochen Wen"
                    },
                    {
                        "name": "Zhen Bai"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00773v2",
                "updated": "2025-06-03T13:29:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    29,
                    29,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-01T01:42:40Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    1,
                    42,
                    40,
                    6,
                    152,
                    0
                ],
                "title": "Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long\n  Context in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long\n  Context in Large Language Models"
                },
                "summary": "Large language models (LLMs) often struggle to accurately read and comprehend\nextremely long texts. Current methods for improvement typically rely on\nsplitting long contexts into fixed-length chunks. However, fixed truncation\nrisks separating semantically relevant content, leading to ambiguity and\ncompromising accurate understanding. To overcome this limitation, we propose a\nstraightforward approach for dynamically separating and selecting chunks of\nlong context, facilitating a more streamlined input for LLMs. In particular, we\ncompute semantic similarities between adjacent sentences, using lower\nsimilarities to adaptively divide long contexts into variable-length chunks. We\nfurther train a question-aware classifier to select sensitive chunks that are\ncritical for answering specific questions. Experimental results on both\nsingle-hop and multi-hop question-answering benchmarks show that the proposed\napproach consistently outperforms strong baselines. Notably, it maintains\nrobustness across a wide range of input lengths, handling sequences of up to\n256k tokens. Our datasets and code are available at the following link:\nhttps://github.com/ECNU-Text-Computing/DCS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often struggle to accurately read and comprehend\nextremely long texts. Current methods for improvement typically rely on\nsplitting long contexts into fixed-length chunks. However, fixed truncation\nrisks separating semantically relevant content, leading to ambiguity and\ncompromising accurate understanding. To overcome this limitation, we propose a\nstraightforward approach for dynamically separating and selecting chunks of\nlong context, facilitating a more streamlined input for LLMs. In particular, we\ncompute semantic similarities between adjacent sentences, using lower\nsimilarities to adaptively divide long contexts into variable-length chunks. We\nfurther train a question-aware classifier to select sensitive chunks that are\ncritical for answering specific questions. Experimental results on both\nsingle-hop and multi-hop question-answering benchmarks show that the proposed\napproach consistently outperforms strong baselines. Notably, it maintains\nrobustness across a wide range of input lengths, handling sequences of up to\n256k tokens. Our datasets and code are available at the following link:\nhttps://github.com/ECNU-Text-Computing/DCS"
                },
                "authors": [
                    {
                        "name": "Boheng Sheng"
                    },
                    {
                        "name": "Jiacheng Yao"
                    },
                    {
                        "name": "Meicong Zhang"
                    },
                    {
                        "name": "Guoxiu He"
                    }
                ],
                "author_detail": {
                    "name": "Guoxiu He"
                },
                "author": "Guoxiu He",
                "arxiv_comment": "Accepted by ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02860v1",
                "updated": "2025-06-03T13:26:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    26,
                    8,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:26:08Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    26,
                    8,
                    1,
                    154,
                    0
                ],
                "title": "Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and\n  Open-Ended POMDPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and\n  Open-Ended POMDPs"
                },
                "summary": "Task planning under uncertainty is essential for home-service robots\noperating in the real world. Tasks involve ambiguous human instructions, hidden\nor unknown object locations, and open-vocabulary object types, leading to\nsignificant open-ended uncertainty and a boundlessly large planning space. To\naddress these challenges, we propose Tru-POMDP, a planner that combines\nstructured belief generation using Large Language Models (LLMs) with principled\nPOMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH),\nwhich systematically queries an LLM to construct high-quality particle beliefs\nover possible world states and human goals. We further formulate an open-ended\nPOMDP model that enables rigorous Bayesian belief tracking and efficient\nbelief-space planning over these LLM-generated hypotheses. Experiments on\ncomplex object rearrangement tasks across diverse kitchen environments show\nthat Tru-POMDP significantly outperforms state-of-the-art LLM-based and\nLLM-tree-search hybrid planners, achieving higher success rates with\nsignificantly better plans, stronger robustness to ambiguity and occlusion, and\ngreater planning efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task planning under uncertainty is essential for home-service robots\noperating in the real world. Tasks involve ambiguous human instructions, hidden\nor unknown object locations, and open-vocabulary object types, leading to\nsignificant open-ended uncertainty and a boundlessly large planning space. To\naddress these challenges, we propose Tru-POMDP, a planner that combines\nstructured belief generation using Large Language Models (LLMs) with principled\nPOMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH),\nwhich systematically queries an LLM to construct high-quality particle beliefs\nover possible world states and human goals. We further formulate an open-ended\nPOMDP model that enables rigorous Bayesian belief tracking and efficient\nbelief-space planning over these LLM-generated hypotheses. Experiments on\ncomplex object rearrangement tasks across diverse kitchen environments show\nthat Tru-POMDP significantly outperforms state-of-the-art LLM-based and\nLLM-tree-search hybrid planners, achieving higher success rates with\nsignificantly better plans, stronger robustness to ambiguity and occlusion, and\ngreater planning efficiency."
                },
                "authors": [
                    {
                        "name": "Wenjing Tang"
                    },
                    {
                        "name": "Xinyu He"
                    },
                    {
                        "name": "Yongxi Huang"
                    },
                    {
                        "name": "Yunxiao Xiao"
                    },
                    {
                        "name": "Cewu Lu"
                    },
                    {
                        "name": "Panpan Cai"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Cai"
                },
                "author": "Panpan Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02859v1",
                "updated": "2025-06-03T13:25:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    25,
                    40,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:25:40Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    25,
                    40,
                    1,
                    154,
                    0
                ],
                "title": "ATAG: AI-Agent Application Threat Assessment with Attack Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATAG: AI-Agent Application Threat Assessment with Attack Graphs"
                },
                "summary": "Evaluating the security of multi-agent systems (MASs) powered by large\nlanguage models (LLMs) is challenging, primarily because of the systems'\ncomplex internal dynamics and the evolving nature of LLM vulnerabilities.\nTraditional attack graph (AG) methods often lack the specific capabilities to\nmodel attacks on LLMs. This paper introduces AI-agent application Threat\nassessment with Attack Graphs (ATAG), a novel framework designed to\nsystematically analyze the security risks associated with AI-agent\napplications. ATAG extends the MulVAL logic-based AG generation tool with\ncustom facts and interaction rules to accurately represent AI-agent topologies,\nvulnerabilities, and attack scenarios. As part of this research, we also\ncreated the LLM vulnerability database (LVD) to initiate the process of\nstandardizing LLM vulnerabilities documentation. To demonstrate ATAG's\nefficacy, we applied it to two multi-agent applications. Our case studies\ndemonstrated the framework's ability to model and generate AGs for\nsophisticated, multi-step attack scenarios exploiting vulnerabilities such as\nprompt injection, excessive agency, sensitive information disclosure, and\ninsecure output handling across interconnected agents. ATAG is an important\nstep toward a robust methodology and toolset to help understand, visualize, and\nprioritize complex attack paths in multi-agent AI systems (MAASs). It\nfacilitates proactive identification and mitigation of AI-agent threats in\nmulti-agent applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the security of multi-agent systems (MASs) powered by large\nlanguage models (LLMs) is challenging, primarily because of the systems'\ncomplex internal dynamics and the evolving nature of LLM vulnerabilities.\nTraditional attack graph (AG) methods often lack the specific capabilities to\nmodel attacks on LLMs. This paper introduces AI-agent application Threat\nassessment with Attack Graphs (ATAG), a novel framework designed to\nsystematically analyze the security risks associated with AI-agent\napplications. ATAG extends the MulVAL logic-based AG generation tool with\ncustom facts and interaction rules to accurately represent AI-agent topologies,\nvulnerabilities, and attack scenarios. As part of this research, we also\ncreated the LLM vulnerability database (LVD) to initiate the process of\nstandardizing LLM vulnerabilities documentation. To demonstrate ATAG's\nefficacy, we applied it to two multi-agent applications. Our case studies\ndemonstrated the framework's ability to model and generate AGs for\nsophisticated, multi-step attack scenarios exploiting vulnerabilities such as\nprompt injection, excessive agency, sensitive information disclosure, and\ninsecure output handling across interconnected agents. ATAG is an important\nstep toward a robust methodology and toolset to help understand, visualize, and\nprioritize complex attack paths in multi-agent AI systems (MAASs). It\nfacilitates proactive identification and mitigation of AI-agent threats in\nmulti-agent applications."
                },
                "authors": [
                    {
                        "name": "Parth Atulbhai Gandhi"
                    },
                    {
                        "name": "Akansha Shukla"
                    },
                    {
                        "name": "David Tayouri"
                    },
                    {
                        "name": "Beni Ifland"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Rami Puzis"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11020v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11020v4",
                "updated": "2025-06-03T13:16:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    16,
                    21,
                    1,
                    154,
                    0
                ],
                "published": "2024-10-14T19:16:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    16,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Improving the Language Understanding Capabilities of Large Language\n  Models Using Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Language Understanding Capabilities of Large Language\n  Models Using Reinforcement Learning"
                },
                "summary": "Instruction-fine-tuned large language models (LLMs) under 14B parameters\ncontinue to underperform on natural language understanding (NLU) tasks, often\ntrailing smaller models like BERT-base on benchmarks such as GLUE and\nSuperGLUE. Motivated by the success of reinforcement learning in reasoning\ntasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a\nframework to improve the NLU capabilities of LLMs. We frame NLU as a\nreinforcement learning environment, treating token generation as a sequence of\nactions and optimizing for reward signals based on alignment with ground-truth\nlabels. PPO consistently outperforms supervised fine-tuning, yielding an\naverage improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot\nprompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models\noutperform GPT-4o by over 4\\% on average across sentiment and natural language\ninference tasks, including gains of 7.3\\% on the Mental Health dataset and\n10.9\\% on SIGA-nli. This work highlights a promising direction for adapting\nLLMs to new tasks by reframing them as reinforcement learning problems,\nenabling learning through simple end-task rewards rather than extensive data\ncuration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-fine-tuned large language models (LLMs) under 14B parameters\ncontinue to underperform on natural language understanding (NLU) tasks, often\ntrailing smaller models like BERT-base on benchmarks such as GLUE and\nSuperGLUE. Motivated by the success of reinforcement learning in reasoning\ntasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a\nframework to improve the NLU capabilities of LLMs. We frame NLU as a\nreinforcement learning environment, treating token generation as a sequence of\nactions and optimizing for reward signals based on alignment with ground-truth\nlabels. PPO consistently outperforms supervised fine-tuning, yielding an\naverage improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot\nprompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models\noutperform GPT-4o by over 4\\% on average across sentiment and natural language\ninference tasks, including gains of 7.3\\% on the Mental Health dataset and\n10.9\\% on SIGA-nli. This work highlights a promising direction for adapting\nLLMs to new tasks by reframing them as reinforcement learning problems,\nenabling learning through simple end-task rewards rather than extensive data\ncuration."
                },
                "authors": [
                    {
                        "name": "Bokai Hu"
                    },
                    {
                        "name": "Sai Ashish Somayajula"
                    },
                    {
                        "name": "Xin Pan"
                    },
                    {
                        "name": "Pengtao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Pengtao Xie"
                },
                "author": "Pengtao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11020v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11020v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02847v1",
                "updated": "2025-06-03T13:16:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    16,
                    0,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:16:00Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    16,
                    0,
                    1,
                    154,
                    0
                ],
                "title": "CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the\n  Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the\n  Edge"
                },
                "summary": "Deploying large language models (LLMs) on edge devices is crucial for\ndelivering fast responses and ensuring data privacy. However, the limited\nstorage, weight, and power of edge devices make it difficult to deploy\nLLM-powered applications. These devices must balance latency requirements with\nenergy consumption and model accuracy. In this paper, we first quantify the\nchallenges of deploying LLMs on off-the-shelf edge devices and then we present\nCLONE, an in-depth algorithm-hardware co-design at both the model- and\nsystem-level that intelligently integrates real-time, energy optimization while\nmaintaining robust generality. In order to maximize the synergistic benefits of\nthese algorithms in always-on and intermediate edge computing settings, we\nspecialize in a 28nm scalable hardware accelerator system. We implement and\nextensively evaluate CLONE on two off-the-shelf edge platforms. Experiments\nshow that CLONE effectively accelerates the inference process up to 11.92x, and\nsaves energy up to 7.36x, while maintaining high-generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) on edge devices is crucial for\ndelivering fast responses and ensuring data privacy. However, the limited\nstorage, weight, and power of edge devices make it difficult to deploy\nLLM-powered applications. These devices must balance latency requirements with\nenergy consumption and model accuracy. In this paper, we first quantify the\nchallenges of deploying LLMs on off-the-shelf edge devices and then we present\nCLONE, an in-depth algorithm-hardware co-design at both the model- and\nsystem-level that intelligently integrates real-time, energy optimization while\nmaintaining robust generality. In order to maximize the synergistic benefits of\nthese algorithms in always-on and intermediate edge computing settings, we\nspecialize in a 28nm scalable hardware accelerator system. We implement and\nextensively evaluate CLONE on two off-the-shelf edge platforms. Experiments\nshow that CLONE effectively accelerates the inference process up to 11.92x, and\nsaves energy up to 7.36x, while maintaining high-generation."
                },
                "authors": [
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xinpeng Qin"
                    },
                    {
                        "name": "Kahou Tam"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "Yuanzhe Zhao"
                    },
                    {
                        "name": "Minglei Zhang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "Accepted by USENIX ATC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02838v1",
                "updated": "2025-06-03T13:06:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    6,
                    19,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:06:19Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    6,
                    19,
                    1,
                    154,
                    0
                ],
                "title": "TaxAgent: How Large Language Model Designs Fiscal Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaxAgent: How Large Language Model Designs Fiscal Policy"
                },
                "summary": "Economic inequality is a global challenge, intensifying disparities in\neducation, healthcare, and social stability. Traditional systems like the U.S.\nfederal income tax reduce inequality but lack adaptability. Although models\nlike the Saez Optimal Taxation adjust dynamically, they fail to address\ntaxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,\na novel integration of large language models (LLMs) with agent-based modeling\n(ABM) to design adaptive tax policies. In our macroeconomic simulation,\nheterogeneous H-Agents (households) simulate real-world taxpayer behaviors\nwhile the TaxAgent (government) utilizes LLMs to iteratively optimize tax\nrates, balancing equity and productivity. Benchmarked against Saez Optimal\nTaxation, U.S. federal income taxes, and free markets, TaxAgent achieves\nsuperior equity-efficiency trade-offs. This research offers a novel taxation\nsolution and a scalable, data-driven framework for fiscal policy evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Economic inequality is a global challenge, intensifying disparities in\neducation, healthcare, and social stability. Traditional systems like the U.S.\nfederal income tax reduce inequality but lack adaptability. Although models\nlike the Saez Optimal Taxation adjust dynamically, they fail to address\ntaxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,\na novel integration of large language models (LLMs) with agent-based modeling\n(ABM) to design adaptive tax policies. In our macroeconomic simulation,\nheterogeneous H-Agents (households) simulate real-world taxpayer behaviors\nwhile the TaxAgent (government) utilizes LLMs to iteratively optimize tax\nrates, balancing equity and productivity. Benchmarked against Saez Optimal\nTaxation, U.S. federal income taxes, and free markets, TaxAgent achieves\nsuperior equity-efficiency trade-offs. This research offers a novel taxation\nsolution and a scalable, data-driven framework for fiscal policy evaluation."
                },
                "authors": [
                    {
                        "name": "Jizhou Wang"
                    },
                    {
                        "name": "Xiaodan Fang"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Huang"
                },
                "author": "Yongfeng Huang",
                "arxiv_comment": "Accepted as oral presentation at ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.6.5; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07849v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07849v3",
                "updated": "2025-06-03T12:58:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    58,
                    57,
                    1,
                    154,
                    0
                ],
                "published": "2025-01-14T05:21:27Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    21,
                    27,
                    1,
                    14,
                    0
                ],
                "title": "The Invisible Hand: Unveiling Provider Bias in Large Language Models for\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Invisible Hand: Unveiling Provider Bias in Large Language Models for\n  Code Generation"
                },
                "summary": "Large Language Models (LLMs) have emerged as the new recommendation engines,\nsurpassing traditional methods in both capability and scope, particularly in\ncode generation. In this paper, we reveal a novel provider bias in LLMs:\nwithout explicit directives, these models show systematic preferences for\nservices from specific providers in their recommendations (e.g., favoring\nGoogle Cloud over Microsoft Azure). To systematically investigate this bias, we\ndevelop an automated pipeline to construct the dataset, incorporating 6\ndistinct coding task categories and 30 real-world application scenarios.\nLeveraging this dataset, we conduct the first comprehensive empirical study of\nprovider bias in LLM code generation across seven state-of-the-art LLMs,\nutilizing approximately 500 million tokens (equivalent to $5,000+ in\ncomputational costs). Our findings reveal that LLMs exhibit significant\nprovider preferences, predominantly favoring services from Google and Amazon,\nand can autonomously modify input code to incorporate their preferred providers\nwithout users' requests. Such a bias holds far-reaching implications for market\ndynamics and societal equilibrium, potentially contributing to digital\nmonopolies. It may also deceive users and violate their expectations, leading\nto various consequences. We call on the academic community to recognize this\nemerging issue and develop effective evaluation and mitigation methods to\nuphold AI security and fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as the new recommendation engines,\nsurpassing traditional methods in both capability and scope, particularly in\ncode generation. In this paper, we reveal a novel provider bias in LLMs:\nwithout explicit directives, these models show systematic preferences for\nservices from specific providers in their recommendations (e.g., favoring\nGoogle Cloud over Microsoft Azure). To systematically investigate this bias, we\ndevelop an automated pipeline to construct the dataset, incorporating 6\ndistinct coding task categories and 30 real-world application scenarios.\nLeveraging this dataset, we conduct the first comprehensive empirical study of\nprovider bias in LLM code generation across seven state-of-the-art LLMs,\nutilizing approximately 500 million tokens (equivalent to $5,000+ in\ncomputational costs). Our findings reveal that LLMs exhibit significant\nprovider preferences, predominantly favoring services from Google and Amazon,\nand can autonomously modify input code to incorporate their preferred providers\nwithout users' requests. Such a bias holds far-reaching implications for market\ndynamics and societal equilibrium, potentially contributing to digital\nmonopolies. It may also deceive users and violate their expectations, leading\nto various consequences. We call on the academic community to recognize this\nemerging issue and develop effective evaluation and mitigation methods to\nuphold AI security and fairness."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Qingshuang Bao"
                    },
                    {
                        "name": "Weipeng Jiang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "27 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07849v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07849v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02827v1",
                "updated": "2025-06-03T12:58:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    58,
                    7,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T12:58:07Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    58,
                    7,
                    1,
                    154,
                    0
                ],
                "title": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory\n  Optimization for Eliciting Human Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory\n  Optimization for Eliciting Human Preference"
                },
                "summary": "Large language models (LLMs) can effectively elicit human preferences through\nmulti-turn dialogue. Complex tasks can be accomplished through iterative\nclarifying questions and final responses generated by an LLM acting as a\nquestioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches\nbased on self-taught reasoning struggle to identify optimal dialogue\ntrajectories and avoid irrelevant questions to the tasks. To address this\nlimitation, we propose TO-GATE, a novel framework that enhances question\ngeneration through trajectory optimization, which consists of two key\ncomponents: a clarification resolver that generates optimal questioning\ntrajectories, and a summarizer that ensures task-aligned final responses. The\ntrajectory optimization enables the model to produce effective elicitation\nquestions and summary responses tailored to specific tasks. Experimental\nresults demonstrate that TO-GATE significantly outperforms baseline methods,\nachieving a 9.32% improvement on standard preference elicitation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can effectively elicit human preferences through\nmulti-turn dialogue. Complex tasks can be accomplished through iterative\nclarifying questions and final responses generated by an LLM acting as a\nquestioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches\nbased on self-taught reasoning struggle to identify optimal dialogue\ntrajectories and avoid irrelevant questions to the tasks. To address this\nlimitation, we propose TO-GATE, a novel framework that enhances question\ngeneration through trajectory optimization, which consists of two key\ncomponents: a clarification resolver that generates optimal questioning\ntrajectories, and a summarizer that ensures task-aligned final responses. The\ntrajectory optimization enables the model to produce effective elicitation\nquestions and summary responses tailored to specific tasks. Experimental\nresults demonstrate that TO-GATE significantly outperforms baseline methods,\nachieving a 9.32% improvement on standard preference elicitation tasks."
                },
                "authors": [
                    {
                        "name": "Yulin Dou"
                    },
                    {
                        "name": "Jiangming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangming Liu"
                },
                "author": "Jiangming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02818v1",
                "updated": "2025-06-03T12:47:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    47,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T12:47:23Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    47,
                    23,
                    1,
                    154,
                    0
                ],
                "title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal\n  Transformations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal\n  Transformations"
                },
                "summary": "Large language models (LLMs) demonstrate impressive results in natural\nlanguage processing tasks but require a significant amount of computational and\nmemory resources. Structured matrix representations are a promising way for\nreducing the number of parameters of these models. However, it seems\nunrealistic to expect that weight matrices of pretrained models can be\naccurately represented by structured matrices without any fine-tuning. To\novercome this issue, we utilize the fact that LLM output is invariant under\ncertain orthogonal transformations of weight matrices. This insight can be\nleveraged to identify transformations that significantly improve the\ncompressibility of weights within structured classes. The proposed approach is\napplicable to various types of structured matrices that support efficient\nprojection operations. Code is available at\nhttps://github.com/GrishKate/ProcrustesGPT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate impressive results in natural\nlanguage processing tasks but require a significant amount of computational and\nmemory resources. Structured matrix representations are a promising way for\nreducing the number of parameters of these models. However, it seems\nunrealistic to expect that weight matrices of pretrained models can be\naccurately represented by structured matrices without any fine-tuning. To\novercome this issue, we utilize the fact that LLM output is invariant under\ncertain orthogonal transformations of weight matrices. This insight can be\nleveraged to identify transformations that significantly improve the\ncompressibility of weights within structured classes. The proposed approach is\napplicable to various types of structured matrices that support efficient\nprojection operations. Code is available at\nhttps://github.com/GrishKate/ProcrustesGPT"
                },
                "authors": [
                    {
                        "name": "Ekaterina Grishina"
                    },
                    {
                        "name": "Mikhail Gorbunov"
                    },
                    {
                        "name": "Maxim Rakhuba"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Rakhuba"
                },
                "author": "Maxim Rakhuba",
                "arxiv_comment": "Accepted by ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01615v2",
                "updated": "2025-06-03T12:38:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    38,
                    27,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-02T12:55:51Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    12,
                    55,
                    51,
                    0,
                    153,
                    0
                ],
                "title": "IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language\n  RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enable language models to access\nrelevant information and generate accurate, well-grounded, and contextually\ninformed responses. However, for Indian languages, the development of\nhigh-quality RAG systems is hindered by the lack of two critical resources: (1)\nevaluation benchmarks for retrieval and generation tasks, and (2) large-scale\ntraining datasets for multilingual retrieval. Most existing benchmarks and\ndatasets are centered around English or high-resource languages, making it\ndifficult to extend RAG capabilities to the diverse linguistic landscape of\nIndia. To address the lack of evaluation benchmarks, we create IndicMSMarco, a\nmultilingual benchmark for evaluating retrieval quality and response generation\nin 13 Indian languages, created via manual translation of 1000 diverse queries\nfrom MS MARCO-dev set. To address the need for training data, we build a\nlarge-scale dataset of (question, answer, relevant passage) tuples derived from\nthe Wikipedias of 19 Indian languages using state-of-the-art LLMs.\nAdditionally, we include translated versions of the original MS MARCO dataset\nto further enrich the training data and ensure alignment with real-world\ninformation-seeking tasks. Resources are available here:\nhttps://huggingface.co/collections/ai4bharat/indicragsuite-683e7273cb2337208c8c0fcb",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enable language models to access\nrelevant information and generate accurate, well-grounded, and contextually\ninformed responses. However, for Indian languages, the development of\nhigh-quality RAG systems is hindered by the lack of two critical resources: (1)\nevaluation benchmarks for retrieval and generation tasks, and (2) large-scale\ntraining datasets for multilingual retrieval. Most existing benchmarks and\ndatasets are centered around English or high-resource languages, making it\ndifficult to extend RAG capabilities to the diverse linguistic landscape of\nIndia. To address the lack of evaluation benchmarks, we create IndicMSMarco, a\nmultilingual benchmark for evaluating retrieval quality and response generation\nin 13 Indian languages, created via manual translation of 1000 diverse queries\nfrom MS MARCO-dev set. To address the need for training data, we build a\nlarge-scale dataset of (question, answer, relevant passage) tuples derived from\nthe Wikipedias of 19 Indian languages using state-of-the-art LLMs.\nAdditionally, we include translated versions of the original MS MARCO dataset\nto further enrich the training data and ensure alignment with real-world\ninformation-seeking tasks. Resources are available here:\nhttps://huggingface.co/collections/ai4bharat/indicragsuite-683e7273cb2337208c8c0fcb"
                },
                "authors": [
                    {
                        "name": "Pasunuti Prasanjith"
                    },
                    {
                        "name": "Prathmesh B More"
                    },
                    {
                        "name": "Anoop Kunchukuttan"
                    },
                    {
                        "name": "Raj Dabre"
                    }
                ],
                "author_detail": {
                    "name": "Raj Dabre"
                },
                "author": "Raj Dabre",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01384v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01384v3",
                "updated": "2025-06-03T12:37:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    37,
                    12,
                    1,
                    154,
                    0
                ],
                "published": "2024-07-01T15:34:17Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    15,
                    34,
                    17,
                    0,
                    183,
                    0
                ],
                "title": "Free-text Rationale Generation under Readability Level Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-text Rationale Generation under Readability Level Control"
                },
                "summary": "Free-text rationales justify model decisions in natural language and thus\nbecome likable and accessible among approaches to explanation across many\ntasks. However, their effectiveness can be hindered by misinterpretation and\nhallucination. As a perturbation test, we investigate how large language models\n(LLMs) perform rationale generation under the effects of readability level\ncontrol, i.e., being prompted for an explanation targeting a specific expertise\nlevel, such as sixth grade or college. We find that explanations are adaptable\nto such instruction, though the observed distinction between readability levels\ndoes not fully match the defined complexity scores according to traditional\nreadability metrics. Furthermore, the generated rationales tend to feature\nmedium level complexity, which correlates with the measured quality using\nautomatic metrics. Finally, our human annotators confirm a generally\nsatisfactory impression on rationales at all readability levels, with\nhigh-school-level readability being most commonly perceived and favored.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-text rationales justify model decisions in natural language and thus\nbecome likable and accessible among approaches to explanation across many\ntasks. However, their effectiveness can be hindered by misinterpretation and\nhallucination. As a perturbation test, we investigate how large language models\n(LLMs) perform rationale generation under the effects of readability level\ncontrol, i.e., being prompted for an explanation targeting a specific expertise\nlevel, such as sixth grade or college. We find that explanations are adaptable\nto such instruction, though the observed distinction between readability levels\ndoes not fully match the defined complexity scores according to traditional\nreadability metrics. Furthermore, the generated rationales tend to feature\nmedium level complexity, which correlates with the measured quality using\nautomatic metrics. Finally, our human annotators confirm a generally\nsatisfactory impression on rationales at all readability levels, with\nhigh-school-level readability being most commonly perceived and favored."
                },
                "authors": [
                    {
                        "name": "Yi-Sheng Hsu"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    }
                ],
                "author_detail": {
                    "name": "Sherzod Hakimov"
                },
                "author": "Sherzod Hakimov",
                "arxiv_comment": "ACL 2025 Workshop on Generation, Evaluation, and Metrics (GEM^2)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01384v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01384v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02791v1",
                "updated": "2025-06-03T12:15:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    15,
                    44,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T12:15:44Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    15,
                    44,
                    1,
                    154,
                    0
                ],
                "title": "Rethinking the effects of data contamination in Code Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the effects of data contamination in Code Intelligence"
                },
                "summary": "In recent years, code intelligence has gained increasing importance in the\nfield of automated software engineering. Meanwhile, the widespread adoption of\nPretrained Language Models (PLMs) and Large Language Models (LLMs) has raised\nconcerns regarding data contamination and its potential impact on model\nperformance evaluation. This paper presents a systematic empirical study to\ninvestigate the fine-grained data contamination on code intelligence tasks. Our\nstudy involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs,\nnamely LLaMA and StarCoder, covering three major tasks: code translation, code\ngeneration, and code summarization. We categorize contamination scenarios into\nfour types according to the code intelligence practice, namely input-only,\noutput-only, unpaired, and paired contamination settings, and construct\ncorresponding experimental and control groups for exploration.\n  Experimental results show that, under the pre-training, fine-tuning, and\ninference paradigm adopted by PLMs, even deliberately injecting paired\ncontamination does not lead to significant performance overestimation. But\ndirect inference or small-scale fine-tuning uncovers the contamination effects.\nIn contrast, LLMs with pre-training and inference paradigm are significantly\naffected by the paired contamination. Apart from the above, other contamination\nscenarios have no impact on both PLMs and LLMs. Our findings challenge the\nconventional belief that contamination inevitably leads to performance\noverestimation, providing new insights into the evaluation and deployment of\ncode intelligence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, code intelligence has gained increasing importance in the\nfield of automated software engineering. Meanwhile, the widespread adoption of\nPretrained Language Models (PLMs) and Large Language Models (LLMs) has raised\nconcerns regarding data contamination and its potential impact on model\nperformance evaluation. This paper presents a systematic empirical study to\ninvestigate the fine-grained data contamination on code intelligence tasks. Our\nstudy involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs,\nnamely LLaMA and StarCoder, covering three major tasks: code translation, code\ngeneration, and code summarization. We categorize contamination scenarios into\nfour types according to the code intelligence practice, namely input-only,\noutput-only, unpaired, and paired contamination settings, and construct\ncorresponding experimental and control groups for exploration.\n  Experimental results show that, under the pre-training, fine-tuning, and\ninference paradigm adopted by PLMs, even deliberately injecting paired\ncontamination does not lead to significant performance overestimation. But\ndirect inference or small-scale fine-tuning uncovers the contamination effects.\nIn contrast, LLMs with pre-training and inference paradigm are significantly\naffected by the paired contamination. Apart from the above, other contamination\nscenarios have no impact on both PLMs and LLMs. Our findings challenge the\nconventional belief that contamination inevitably leads to performance\noverestimation, providing new insights into the evaluation and deployment of\ncode intelligence models."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Hongyi Lin"
                    },
                    {
                        "name": "Yifan He"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Pengpeng Wang"
                    },
                    {
                        "name": "Zhongxing Yu"
                    },
                    {
                        "name": "Qingyuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Qingyuan Liang"
                },
                "author": "Qingyuan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02787v1",
                "updated": "2025-06-03T12:14:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    14,
                    17,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T12:14:17Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    14,
                    17,
                    1,
                    154,
                    0
                ],
                "title": "Rethinking Dynamic Networks and Heterogeneous Computing with Automatic\n  Parallelization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Dynamic Networks and Heterogeneous Computing with Automatic\n  Parallelization"
                },
                "summary": "Hybrid parallelism techniques are essential for efficiently training large\nlanguage models (LLMs). Nevertheless, current automatic parallel planning\nframeworks often overlook the simultaneous consideration of node heterogeneity\nand dynamic network topology changes, limiting their effectiveness in practical\napplications. In this paper, we address these limitations by modeling\nheterogeneous nodes within dynamically changing network environments and\nleveraging simulation-based strategies to determine optimal parallel\nconfigurations. Our approach enables fine-grained workload allocation tailored\nfor heterogeneous nodes and complex network scenarios, achieving performance\ncompetitive with state-of-the-art methods under regular and stable network\nconditions. Additionally, we introduce a strategy pruning technique to rapidly\ndiscard infeasible parallel configurations, substantially reducing the search\nspace and accelerating the search process through parallel execution within the\nsimulator. Preliminary evaluations confirm that our method notably enhances\ntraining performance on heterogeneous nodes and demonstrates improved\nadaptability in complex, dynamic scenarios such as cloud computing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid parallelism techniques are essential for efficiently training large\nlanguage models (LLMs). Nevertheless, current automatic parallel planning\nframeworks often overlook the simultaneous consideration of node heterogeneity\nand dynamic network topology changes, limiting their effectiveness in practical\napplications. In this paper, we address these limitations by modeling\nheterogeneous nodes within dynamically changing network environments and\nleveraging simulation-based strategies to determine optimal parallel\nconfigurations. Our approach enables fine-grained workload allocation tailored\nfor heterogeneous nodes and complex network scenarios, achieving performance\ncompetitive with state-of-the-art methods under regular and stable network\nconditions. Additionally, we introduce a strategy pruning technique to rapidly\ndiscard infeasible parallel configurations, substantially reducing the search\nspace and accelerating the search process through parallel execution within the\nsimulator. Preliminary evaluations confirm that our method notably enhances\ntraining performance on heterogeneous nodes and demonstrates improved\nadaptability in complex, dynamic scenarios such as cloud computing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Ruilong Wu"
                    },
                    {
                        "name": "Xinjiao Li"
                    },
                    {
                        "name": "Yisu Wang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Dirk Kutscher"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Kutscher"
                },
                "author": "Dirk Kutscher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02785v1",
                "updated": "2025-06-03T12:12:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    12,
                    27,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T12:12:27Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    12,
                    27,
                    1,
                    154,
                    0
                ],
                "title": "AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service\n  Migration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service\n  Migration"
                },
                "summary": "Artificial intelligence (AI) has been increasingly applied to the condition\nmonitoring of vehicular equipment, aiming to enhance maintenance strategies,\nreduce costs, and improve safety. Leveraging the edge computing paradigm,\nAI-based condition monitoring systems process vast streams of vehicular data to\ndetect anomalies and optimize operational performance. In this work, we\nintroduce a novel vehicle condition monitoring service that enables real-time\ndiagnostics of a diverse set of anomalies while remaining practical for\ndeployment in real-world edge environments. To address mobility challenges, we\npropose a closed-loop service orchestration framework where service migration\nacross edge nodes is dynamically triggered by network-related metrics. Our\napproach has been implemented and tested in a real-world race circuit\nenvironment equipped with 5G network capabilities under diverse operational\nconditions. Experimental results demonstrate the effectiveness of our framework\nin ensuring low-latency AI inference and adaptive service placement,\nhighlighting its potential for intelligent transportation and mobility\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has been increasingly applied to the condition\nmonitoring of vehicular equipment, aiming to enhance maintenance strategies,\nreduce costs, and improve safety. Leveraging the edge computing paradigm,\nAI-based condition monitoring systems process vast streams of vehicular data to\ndetect anomalies and optimize operational performance. In this work, we\nintroduce a novel vehicle condition monitoring service that enables real-time\ndiagnostics of a diverse set of anomalies while remaining practical for\ndeployment in real-world edge environments. To address mobility challenges, we\npropose a closed-loop service orchestration framework where service migration\nacross edge nodes is dynamically triggered by network-related metrics. Our\napproach has been implemented and tested in a real-world race circuit\nenvironment equipped with 5G network capabilities under diverse operational\nconditions. Experimental results demonstrate the effectiveness of our framework\nin ensuring low-latency AI inference and adaptive service placement,\nhighlighting its potential for intelligent transportation and mobility\napplications."
                },
                "authors": [
                    {
                        "name": "Charalampos Kalalas"
                    },
                    {
                        "name": "Pavol Mulinka"
                    },
                    {
                        "name": "Guillermo Candela Belmonte"
                    },
                    {
                        "name": "Miguel Fornell"
                    },
                    {
                        "name": "Michail Dalgitsis"
                    },
                    {
                        "name": "Francisco Paredes Vera"
                    },
                    {
                        "name": "Javier Santaella Snchez"
                    },
                    {
                        "name": "Carmen Vicente Villares"
                    },
                    {
                        "name": "Roshan Sedar"
                    },
                    {
                        "name": "Eftychia Datsika"
                    },
                    {
                        "name": "Angelos Antonopoulos"
                    },
                    {
                        "name": "Antonio Fernndez Ojea"
                    },
                    {
                        "name": "Miquel Payaro"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Payaro"
                },
                "author": "Miquel Payaro",
                "arxiv_comment": "6 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10672v3",
                "updated": "2025-06-03T12:07:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    7,
                    50,
                    1,
                    154,
                    0
                ],
                "published": "2024-10-14T16:15:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    15,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "Large Language Model Evaluation via Matrix Nuclear-Norm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Evaluation via Matrix Nuclear-Norm"
                },
                "summary": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm."
                },
                "authors": [
                    {
                        "name": "Yahan Li"
                    },
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19146v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19146v5",
                "updated": "2025-06-03T12:02:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    2,
                    3,
                    1,
                    154,
                    0
                ],
                "published": "2024-11-28T13:45:42Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    13,
                    45,
                    42,
                    3,
                    333,
                    0
                ],
                "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs"
                },
                "summary": "Large language models (LLMs) offer remarkable capabilities, yet their high\ninference costs restrict wider adoption. While increasing parameter counts\nimproves accuracy, it also broadens the gap between state-of-the-art\ncapabilities and practical deployability. We present Puzzle, a hardware-aware\nframework that accelerates the inference of LLMs while preserving their\ncapabilities. Using neural architecture search (NAS) at a large-scale, Puzzle\noptimizes models with tens of billions of parameters. Our approach utilizes\nblockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct\n(Nemotron-51B) and Llama-3.3-Nemotron-49B, two publicly available models\nderived from Llama-70B-Instruct. Both models achieve a 2.17x inference\nthroughput speedup, fitting on a single NVIDIA H100 GPU while retaining 98.4%\nof the original model's benchmark accuracies. These are the most accurate\nmodels supporting single H100 GPU inference with large batch sizes, despite\ntraining on 45B tokens at most, far fewer than the 15T used to train Llama-70B.\nLastly, we show that lightweight alignment on these derived models allows them\nto surpass the parent model in specific capabilities. Our work establishes that\npowerful LLM models can be optimized for efficient deployment with only\nnegligible loss in quality, underscoring that inference performance, not\nparameter count alone, should guide model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer remarkable capabilities, yet their high\ninference costs restrict wider adoption. While increasing parameter counts\nimproves accuracy, it also broadens the gap between state-of-the-art\ncapabilities and practical deployability. We present Puzzle, a hardware-aware\nframework that accelerates the inference of LLMs while preserving their\ncapabilities. Using neural architecture search (NAS) at a large-scale, Puzzle\noptimizes models with tens of billions of parameters. Our approach utilizes\nblockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct\n(Nemotron-51B) and Llama-3.3-Nemotron-49B, two publicly available models\nderived from Llama-70B-Instruct. Both models achieve a 2.17x inference\nthroughput speedup, fitting on a single NVIDIA H100 GPU while retaining 98.4%\nof the original model's benchmark accuracies. These are the most accurate\nmodels supporting single H100 GPU inference with large batch sizes, despite\ntraining on 45B tokens at most, far fewer than the 15T used to train Llama-70B.\nLastly, we show that lightweight alignment on these derived models allows them\nto surpass the parent model in specific capabilities. Our work establishes that\npowerful LLM models can be optimized for efficient deployment with only\nnegligible loss in quality, underscoring that inference performance, not\nparameter count alone, should guide model selection."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Talor Abramovich"
                    },
                    {
                        "name": "Nir Ailon"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Amnon Geifman"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Izhak Golan"
                    },
                    {
                        "name": "Netanel Haber"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Roi Koren"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Shahar Mor"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ran Rubin"
                    },
                    {
                        "name": "Itamar Schen"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Omer Ullman Argov"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    }
                ],
                "author_detail": {
                    "name": "Ran El-Yaniv"
                },
                "author": "Ran El-Yaniv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19146v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19146v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02780v1",
                "updated": "2025-06-03T12:01:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    1,
                    20,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T12:01:20Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    1,
                    20,
                    1,
                    154,
                    0
                ],
                "title": "Reuse or Generate? Accelerating Code Editing via Edit-Oriented\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse or Generate? Accelerating Code Editing via Edit-Oriented\n  Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode editing, substantially enhancing software development productivity.\nHowever, the inherent complexity of code editing tasks forces existing\napproaches to rely on LLMs' autoregressive end-to-end generation, where\ndecoding speed plays a critical role in efficiency. While inference\nacceleration techniques like speculative decoding are applied to improve the\ndecoding efficiency, these methods fail to account for the unique\ncharacteristics of code editing tasks where changes are typically localized and\nexisting code segments are reused. To address this limitation, we propose\nEfficientEdit, a novel method that improves LLM-based code editing efficiency\nthrough two key mechanisms based on speculative decoding: (1) effective reuse\nof original code segments while identifying potential edit locations, and (2)\nefficient generate edit content via high-quality drafts from edit-oriented\ndraft models and a dynamic verification mechanism that balances quality and\nacceleration. Experimental results show that EfficientEdit can achieve up to\n10.38$\\times$ and 13.09$\\times$ speedup compared to standard autoregressive\ndecoding in CanItEdit and CodeIF-Bench, respectively, outperforming\nstate-of-the-art inference acceleration approaches by up to 90.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode editing, substantially enhancing software development productivity.\nHowever, the inherent complexity of code editing tasks forces existing\napproaches to rely on LLMs' autoregressive end-to-end generation, where\ndecoding speed plays a critical role in efficiency. While inference\nacceleration techniques like speculative decoding are applied to improve the\ndecoding efficiency, these methods fail to account for the unique\ncharacteristics of code editing tasks where changes are typically localized and\nexisting code segments are reused. To address this limitation, we propose\nEfficientEdit, a novel method that improves LLM-based code editing efficiency\nthrough two key mechanisms based on speculative decoding: (1) effective reuse\nof original code segments while identifying potential edit locations, and (2)\nefficient generate edit content via high-quality drafts from edit-oriented\ndraft models and a dynamic verification mechanism that balances quality and\nacceleration. Experimental results show that EfficientEdit can achieve up to\n10.38$\\times$ and 13.09$\\times$ speedup compared to standard autoregressive\ndecoding in CanItEdit and CodeIF-Bench, respectively, outperforming\nstate-of-the-art inference acceleration approaches by up to 90.6%."
                },
                "authors": [
                    {
                        "name": "Peiding Wang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Wang Xu"
                    },
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Minxiao Li"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "An Fu"
                    }
                ],
                "author_detail": {
                    "name": "An Fu"
                },
                "author": "An Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10927v3",
                "updated": "2025-06-03T11:58:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    11,
                    58,
                    39,
                    1,
                    154,
                    0
                ],
                "published": "2025-03-13T22:28:38Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    22,
                    28,
                    38,
                    3,
                    72,
                    0
                ],
                "title": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM\n  Responses"
                },
                "summary": "While Large Language Models (LLMs) have significantly advanced natural\nlanguage processing, aligning them with human preferences remains an open\nchallenge. Although current alignment methods rely primarily on explicit\nfeedback, eye-tracking (ET) data offers insights into real-time cognitive\nprocessing during reading. In this paper, we present OASST-ETC, a novel\neye-tracking corpus capturing reading patterns from 24 participants, while\nevaluating LLM-generated responses from the OASST1 dataset. Our analysis\nreveals distinct reading patterns between preferred and non-preferred\nresponses, which we compare with synthetic eye-tracking data. Furthermore, we\nexamine the correlation between human reading measures and attention patterns\nfrom various transformer-based models, discovering stronger correlations in\npreferred responses. This work introduces a unique resource for studying human\ncognitive processing in LLM evaluation and suggests promising directions for\nincorporating eye-tracking data into alignment methods. The dataset and\nanalysis code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have significantly advanced natural\nlanguage processing, aligning them with human preferences remains an open\nchallenge. Although current alignment methods rely primarily on explicit\nfeedback, eye-tracking (ET) data offers insights into real-time cognitive\nprocessing during reading. In this paper, we present OASST-ETC, a novel\neye-tracking corpus capturing reading patterns from 24 participants, while\nevaluating LLM-generated responses from the OASST1 dataset. Our analysis\nreveals distinct reading patterns between preferred and non-preferred\nresponses, which we compare with synthetic eye-tracking data. Furthermore, we\nexamine the correlation between human reading measures and attention patterns\nfrom various transformer-based models, discovering stronger correlations in\npreferred responses. This work introduces a unique resource for studying human\ncognitive processing in LLM evaluation and suggests promising directions for\nincorporating eye-tracking data into alignment methods. The dataset and\nanalysis code are publicly available."
                },
                "authors": [
                    {
                        "name": "Angela Lopez-Cardona"
                    },
                    {
                        "name": "Sebastian Idesis"
                    },
                    {
                        "name": "Miguel Barreda-ngeles"
                    },
                    {
                        "name": "Sergi Abadal"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Arapakis"
                },
                "author": "Ioannis Arapakis",
                "arxiv_doi": "10.1145/3725840",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725840",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.10927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted to ACM ETRA 2025 and published on\n  PACMHCI",
                "arxiv_journal_ref": "Proceedings of the ACM on Human-Computer Interaction. 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15762v2",
                "updated": "2025-06-03T11:45:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    11,
                    45,
                    34,
                    1,
                    154,
                    0
                ],
                "published": "2024-09-24T05:38:33Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    38,
                    33,
                    1,
                    268,
                    0
                ],
                "title": "XTRUST: On the Multilingual Trustworthiness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XTRUST: On the Multilingual Trustworthiness of Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing (NLP) tasks, capturing the attention of\nboth practitioners and the broader public. A key question that now preoccupies\nthe AI community concerns the capabilities and limitations of these models,\nwith trustworthiness emerging as a central issue, particularly as LLMs are\nincreasingly applied in sensitive fields like healthcare and finance, where\nerrors can have serious consequences. However, most previous studies on the\ntrustworthiness of LLMs have been limited to a single language, typically the\npredominant one in the dataset, such as English. In response to the growing\nglobal deployment of LLMs, we introduce XTRUST, the first comprehensive\nmultilingual trustworthiness benchmark. XTRUST encompasses a diverse range of\ntopics, including illegal activities, hallucination, out-of-distribution (OOD)\nrobustness, physical and mental health, toxicity, fairness, misinformation,\nprivacy, and machine ethics, across 10 different languages. Using XTRUST, we\nconduct an empirical evaluation of the multilingual trustworthiness of five\nwidely used LLMs, offering an in-depth analysis of their performance across\nlanguages and tasks. Our results indicate that many LLMs struggle with certain\nlow-resource languages, such as Arabic and Russian, highlighting the\nconsiderable room for improvement in the multilingual trustworthiness of\ncurrent language models. The code is available at\nhttps://github.com/LluckyYH/XTRUST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing (NLP) tasks, capturing the attention of\nboth practitioners and the broader public. A key question that now preoccupies\nthe AI community concerns the capabilities and limitations of these models,\nwith trustworthiness emerging as a central issue, particularly as LLMs are\nincreasingly applied in sensitive fields like healthcare and finance, where\nerrors can have serious consequences. However, most previous studies on the\ntrustworthiness of LLMs have been limited to a single language, typically the\npredominant one in the dataset, such as English. In response to the growing\nglobal deployment of LLMs, we introduce XTRUST, the first comprehensive\nmultilingual trustworthiness benchmark. XTRUST encompasses a diverse range of\ntopics, including illegal activities, hallucination, out-of-distribution (OOD)\nrobustness, physical and mental health, toxicity, fairness, misinformation,\nprivacy, and machine ethics, across 10 different languages. Using XTRUST, we\nconduct an empirical evaluation of the multilingual trustworthiness of five\nwidely used LLMs, offering an in-depth analysis of their performance across\nlanguages and tasks. Our results indicate that many LLMs struggle with certain\nlow-resource languages, such as Arabic and Russian, highlighting the\nconsiderable room for improvement in the multilingual trustworthiness of\ncurrent language models. The code is available at\nhttps://github.com/LluckyYH/XTRUST."
                },
                "authors": [
                    {
                        "name": "Yahan Li"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05098v2",
                "updated": "2025-06-03T11:30:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    11,
                    30,
                    21,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-08T09:52:55Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    52,
                    55,
                    3,
                    128,
                    0
                ],
                "title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Driver: Explainable Autonomous Driving with Vision-Language Models"
                },
                "summary": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Binxiong Zheng"
                    },
                    {
                        "name": "Yufeng Hu"
                    },
                    {
                        "name": "Yingzhan Lin"
                    },
                    {
                        "name": "Zengfeng Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Zengfeng Zeng"
                },
                "author": "Zengfeng Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.13140v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.13140v5",
                "updated": "2025-06-03T11:28:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    11,
                    28,
                    14,
                    1,
                    154,
                    0
                ],
                "published": "2023-08-25T02:33:11Z",
                "published_parsed": [
                    2023,
                    8,
                    25,
                    2,
                    33,
                    11,
                    4,
                    237,
                    0
                ],
                "title": "Learn With Imagination: Safe Set Guided State-wise Constrained Policy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn With Imagination: Safe Set Guided State-wise Constrained Policy\n  Optimization"
                },
                "summary": "Deep reinforcement learning (RL) excels in various control tasks, yet the\nabsence of safety guarantees hampers its real-world applicability. In\nparticular, explorations during learning usually results in safety violations,\nwhile the RL agent learns from those mistakes. On the other hand, safe control\ntechniques ensure persistent safety satisfaction but demand strong priors on\nsystem dynamics, which is usually hard to obtain in practice. To address these\nproblems, we present Safe Set Guided State-wise Constrained Policy Optimization\n(S-3PO), a pioneering algorithm generating state-wise safe optimal policies\nwith zero training violations, i.e., learning without mistakes. S-3PO first\nemploys a safety-oriented monitor with black-box dynamics to ensure safe\nexploration. It then enforces an \"imaginary\" cost for the RL agent to converge\nto optimal behaviors within safety constraints. S-3PO outperforms existing\nmethods in high-dimensional robotics tasks, managing state-wise constraints\nwith zero training violation. This innovation marks a significant stride\ntowards real-world safe RL deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (RL) excels in various control tasks, yet the\nabsence of safety guarantees hampers its real-world applicability. In\nparticular, explorations during learning usually results in safety violations,\nwhile the RL agent learns from those mistakes. On the other hand, safe control\ntechniques ensure persistent safety satisfaction but demand strong priors on\nsystem dynamics, which is usually hard to obtain in practice. To address these\nproblems, we present Safe Set Guided State-wise Constrained Policy Optimization\n(S-3PO), a pioneering algorithm generating state-wise safe optimal policies\nwith zero training violations, i.e., learning without mistakes. S-3PO first\nemploys a safety-oriented monitor with black-box dynamics to ensure safe\nexploration. It then enforces an \"imaginary\" cost for the RL agent to converge\nto optimal behaviors within safety constraints. S-3PO outperforms existing\nmethods in high-dimensional robotics tasks, managing state-wise constraints\nwith zero training violation. This innovation marks a significant stride\ntowards real-world safe RL deployment."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Feihan Li"
                    },
                    {
                        "name": "Weiye Zhao"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Tianhao Wei"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.12594",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.13140v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.13140v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]