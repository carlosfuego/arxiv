[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v6",
                "updated": "2025-03-01T05:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    5,
                    43,
                    19,
                    5,
                    60,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3706628.3708873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00323v1",
                "updated": "2025-03-01T03:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T03:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLStore: Efficient Federated Learning Storage for non-training workloads"
                },
                "summary": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Samuel Fountain"
                    },
                    {
                        "name": "Ahmed M. Abdelmoniem"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of Côte d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v4",
                "updated": "2025-02-27T03:22:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    3,
                    22,
                    41,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05783v1",
                "updated": "2025-02-26T21:03:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    21,
                    3,
                    2,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T21:03:02Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    21,
                    3,
                    2,
                    2,
                    57,
                    0
                ],
                "title": "Knowledge representation and scalable abstract reasoning for simulated\n  democracy in Unity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge representation and scalable abstract reasoning for simulated\n  democracy in Unity"
                },
                "summary": "We present a novel form of scalable knowledge representation about agents in\na simulated democracy, e-polis, where real users respond to social challenges\nassociated with democratic institutions, structured as Smart Spatial Types, a\nnew type of Smart Building that changes architectural form according to the\nphilosophical doctrine of a visitor. At the end of the game players vote on the\nSmart City that results from their collective choices. Our approach uses\ndeductive systems in an unusual way: by integrating a model of democracy with a\nmodel of a Smart City we are able to prove quality aspects of the simulated\ndemocracy in different urban and social settings, while adding ease and\nflexibility to the development. Second, we can infer and reason with abstract\nknowledge, which is a limitation of the Unity platform; third, our system\nenables real-time decision-making and adaptation of the game flow based on the\nplayer's abstract state, paving the road to explainability. Scalability is\nachieved by maintaining a dual-layer knowledge representation mechanism for\nreasoning about the simulated democracy that functions in a similar way to a\ntwo-level cache. The lower layer knows about the current state of the game by\ncontinually processing a high rate of events produced by the in-built physics\nengine of the Unity platform, e.g., it knows of the position of a player in\nspace, in terms of his coordinates x,y,z as well as their choices for each\nchallenge. The higher layer knows of easily-retrievable, user-defined abstract\nknowledge about current and historical states, e.g., it knows of the political\ndoctrine of a Smart Spatial Type, a player's philosophical doctrine, and the\ncollective philosophical doctrine of a community players with respect to\ncurrent social issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel form of scalable knowledge representation about agents in\na simulated democracy, e-polis, where real users respond to social challenges\nassociated with democratic institutions, structured as Smart Spatial Types, a\nnew type of Smart Building that changes architectural form according to the\nphilosophical doctrine of a visitor. At the end of the game players vote on the\nSmart City that results from their collective choices. Our approach uses\ndeductive systems in an unusual way: by integrating a model of democracy with a\nmodel of a Smart City we are able to prove quality aspects of the simulated\ndemocracy in different urban and social settings, while adding ease and\nflexibility to the development. Second, we can infer and reason with abstract\nknowledge, which is a limitation of the Unity platform; third, our system\nenables real-time decision-making and adaptation of the game flow based on the\nplayer's abstract state, paving the road to explainability. Scalability is\nachieved by maintaining a dual-layer knowledge representation mechanism for\nreasoning about the simulated democracy that functions in a similar way to a\ntwo-level cache. The lower layer knows about the current state of the game by\ncontinually processing a high rate of events produced by the in-built physics\nengine of the Unity platform, e.g., it knows of the position of a player in\nspace, in terms of his coordinates x,y,z as well as their choices for each\nchallenge. The higher layer knows of easily-retrievable, user-defined abstract\nknowledge about current and historical states, e.g., it knows of the political\ndoctrine of a Smart Spatial Type, a player's philosophical doctrine, and the\ncollective philosophical doctrine of a community players with respect to\ncurrent social issues."
                },
                "authors": [
                    {
                        "name": "Eleftheria Katsiri"
                    },
                    {
                        "name": "Alexandros Gazis"
                    },
                    {
                        "name": "Angelos Protopapas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Protopapas"
                },
                "author": "Angelos Protopapas",
                "arxiv_comment": "23 pages, 11 figures, 76 references. This article is under review at\n  WSEAS Transactions on Information Science and Applications from 02.2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.3; C.5.2; C.5.3; C.5.5; C.5.m; C.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v2",
                "updated": "2025-02-25T09:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    42,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00022v1",
                "updated": "2025-02-24T02:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T02:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "title": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour"
                },
                "summary": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization."
                },
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Liubov Talamanova"
                    },
                    {
                        "name": "Alexander Kozlov"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.07493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07493v1",
                "updated": "2025-03-10T16:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    12,
                    50,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    12,
                    50,
                    0,
                    69,
                    0
                ],
                "title": "V2Flow: Unifying Visual Tokenization and Large Language Model\n  Vocabularies for Autoregressive Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V2Flow: Unifying Visual Tokenization and Large Language Model\n  Vocabularies for Autoregressive Image Generation"
                },
                "summary": "We propose V2Flow, a novel tokenizer that produces discrete visual tokens\ncapable of high-fidelity reconstruction, while ensuring structural and latent\ndistribution alignment with the vocabulary space of large language models\n(LLMs). Leveraging this tight visual-vocabulary coupling, V2Flow enables\nautoregressive visual generation on top of existing LLMs. Our approach\nformulates visual tokenization as a flow-matching problem, aiming to learn a\nmapping from a standard normal prior to the continuous image distribution,\nconditioned on token sequences embedded within the LLMs vocabulary space. The\neffectiveness of V2Flow stems from two core designs. First, we propose a Visual\nVocabulary resampler, which compresses visual data into compact token\nsequences, with each represented as a soft categorical distribution over LLM's\nvocabulary. This allows seamless integration of visual tokens into existing\nLLMs for autoregressive visual generation. Second, we present a masked\nautoregressive Rectified-Flow decoder, employing a masked transformer\nencoder-decoder to refine visual tokens into contextually enriched embeddings.\nThese embeddings then condition a dedicated velocity field for precise\nreconstruction. Additionally, an autoregressive rectified-flow sampling\nstrategy is incorporated, ensuring flexible sequence lengths while preserving\ncompetitive reconstruction quality. Extensive experiments show that V2Flow\noutperforms mainstream VQ-based tokenizers and facilitates autoregressive\nvisual generation on top of existing. https://github.com/zhangguiwei610/V2Flow",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose V2Flow, a novel tokenizer that produces discrete visual tokens\ncapable of high-fidelity reconstruction, while ensuring structural and latent\ndistribution alignment with the vocabulary space of large language models\n(LLMs). Leveraging this tight visual-vocabulary coupling, V2Flow enables\nautoregressive visual generation on top of existing LLMs. Our approach\nformulates visual tokenization as a flow-matching problem, aiming to learn a\nmapping from a standard normal prior to the continuous image distribution,\nconditioned on token sequences embedded within the LLMs vocabulary space. The\neffectiveness of V2Flow stems from two core designs. First, we propose a Visual\nVocabulary resampler, which compresses visual data into compact token\nsequences, with each represented as a soft categorical distribution over LLM's\nvocabulary. This allows seamless integration of visual tokens into existing\nLLMs for autoregressive visual generation. Second, we present a masked\nautoregressive Rectified-Flow decoder, employing a masked transformer\nencoder-decoder to refine visual tokens into contextually enriched embeddings.\nThese embeddings then condition a dedicated velocity field for precise\nreconstruction. Additionally, an autoregressive rectified-flow sampling\nstrategy is incorporated, ensuring flexible sequence lengths while preserving\ncompetitive reconstruction quality. Extensive experiments show that V2Flow\noutperforms mainstream VQ-based tokenizers and facilitates autoregressive\nvisual generation on top of existing. https://github.com/zhangguiwei610/V2Flow"
                },
                "authors": [
                    {
                        "name": "Guiwei Zhang"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Mohan Zhou"
                    },
                    {
                        "name": "Yalong Bai"
                    },
                    {
                        "name": "Biye Li"
                    }
                ],
                "author_detail": {
                    "name": "Biye Li"
                },
                "author": "Biye Li",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07482v1",
                "updated": "2025-03-10T15:58:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    58,
                    43,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:58:43Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    58,
                    43,
                    0,
                    69,
                    0
                ],
                "title": "Efficient Membership Inference Attacks by Bayesian Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Membership Inference Attacks by Bayesian Neural Network"
                },
                "summary": "Membership Inference Attacks (MIAs) aim to estimate whether a specific data\npoint was used in the training of a given model. Previous attacks often utilize\nmultiple reference models to approximate the conditional score distribution,\nleading to significant computational overhead. While recent work leverages\nquantile regression to estimate conditional thresholds, it fails to capture\nepistemic uncertainty, resulting in bias in low-density regions. In this work,\nwe propose a novel approach - Bayesian Membership Inference Attack (BMIA),\nwhich performs conditional attack through Bayesian inference. In particular, we\ntransform a trained reference model into Bayesian neural networks by Laplace\napproximation, enabling the direct estimation of the conditional score\ndistribution by probabilistic model parameters. Our method addresses both\nepistemic and aleatoric uncertainty with only a reference model, enabling\nefficient and powerful MIA. Extensive experiments on five datasets demonstrate\nthe effectiveness and efficiency of BMIA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIAs) aim to estimate whether a specific data\npoint was used in the training of a given model. Previous attacks often utilize\nmultiple reference models to approximate the conditional score distribution,\nleading to significant computational overhead. While recent work leverages\nquantile regression to estimate conditional thresholds, it fails to capture\nepistemic uncertainty, resulting in bias in low-density regions. In this work,\nwe propose a novel approach - Bayesian Membership Inference Attack (BMIA),\nwhich performs conditional attack through Bayesian inference. In particular, we\ntransform a trained reference model into Bayesian neural networks by Laplace\napproximation, enabling the direct estimation of the conditional score\ndistribution by probabilistic model parameters. Our method addresses both\nepistemic and aleatoric uncertainty with only a reference model, enabling\nefficient and powerful MIA. Extensive experiments on five datasets demonstrate\nthe effectiveness and efficiency of BMIA."
                },
                "authors": [
                    {
                        "name": "Zhenlong Liu"
                    },
                    {
                        "name": "Wenyu Jiang"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "arxiv_comment": "8 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07465v1",
                "updated": "2025-03-10T15:42:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    42,
                    59,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:42:59Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    42,
                    59,
                    0,
                    69,
                    0
                ],
                "title": "YOLOE: Real-Time Seeing Anything",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLOE: Real-Time Seeing Anything"
                },
                "summary": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3$\\times$\nless training cost and 1.4$\\times$ inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP$^b$ and 0.4 AP$^m$ gains over closed-set YOLOv8-L with nearly 4$\\times$ less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3$\\times$\nless training cost and 1.4$\\times$ inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP$^b$ and 0.4 AP$^m$ gains over closed-set YOLOv8-L with nearly 4$\\times$ less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "15 pages, 9 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07463v1",
                "updated": "2025-03-10T15:42:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    42,
                    7,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:42:07Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    42,
                    7,
                    0,
                    69,
                    0
                ],
                "title": "GenAIReading: Augmenting Human Cognition with Interactive Digital\n  Textbooks Using Large Language Models and Image Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAIReading: Augmenting Human Cognition with Interactive Digital\n  Textbooks Using Large Language Models and Image Generation Models"
                },
                "summary": "Cognitive augmentation is a cornerstone in advancing education, particularly\nthrough personalized learning. However, personalizing extensive textual\nmaterials, such as narratives and academic textbooks, remains challenging due\nto their heavy use, which can hinder learner engagement and understanding.\nBuilding on cognitive theories like Dual Coding Theory -- which posits that\ncombining textual and visual information enhances comprehension and memory --\nthis study explores the potential of Generative AI (GenAI) to enrich\neducational materials. We utilized large language models (LLMs) to generate\nconcise text summaries and image generation models (IGMs) to create visually\naligned content from textual inputs. After recruiting 24 participants, we\nverified that integrating AI-generated supplementary materials significantly\nimproved learning outcomes, increasing post-reading test scores by 7.50%. These\nfindings underscore GenAI's transformative potential in creating adaptive\nlearning environments that enhance cognitive augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive augmentation is a cornerstone in advancing education, particularly\nthrough personalized learning. However, personalizing extensive textual\nmaterials, such as narratives and academic textbooks, remains challenging due\nto their heavy use, which can hinder learner engagement and understanding.\nBuilding on cognitive theories like Dual Coding Theory -- which posits that\ncombining textual and visual information enhances comprehension and memory --\nthis study explores the potential of Generative AI (GenAI) to enrich\neducational materials. We utilized large language models (LLMs) to generate\nconcise text summaries and image generation models (IGMs) to create visually\naligned content from textual inputs. After recruiting 24 participants, we\nverified that integrating AI-generated supplementary materials significantly\nimproved learning outcomes, increasing post-reading test scores by 7.50%. These\nfindings underscore GenAI's transformative potential in creating adaptive\nlearning environments that enhance cognitive augmentation."
                },
                "authors": [
                    {
                        "name": "Ryugo Morita"
                    },
                    {
                        "name": "Ko Watanabe"
                    },
                    {
                        "name": "Jinjia Zhou"
                    },
                    {
                        "name": "Andreas Dengel"
                    },
                    {
                        "name": "Shoya Ishimaru"
                    }
                ],
                "author_detail": {
                    "name": "Shoya Ishimaru"
                },
                "author": "Shoya Ishimaru",
                "arxiv_comment": "Accepted at AHs2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07459v1",
                "updated": "2025-03-10T15:38:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    38,
                    44,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:38:44Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    38,
                    44,
                    0,
                    69,
                    0
                ],
                "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark."
                },
                "authors": [
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Daniel Shao"
                    },
                    {
                        "name": "Jiwoong Sohn"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Chenglin Wu"
                    },
                    {
                        "name": "Wenqi Shi"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Mark Gerstein"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gerstein"
                },
                "author": "Mark Gerstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07457v1",
                "updated": "2025-03-10T15:37:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    37,
                    7,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:37:07Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    37,
                    7,
                    0,
                    69,
                    0
                ],
                "title": "LLMs syntactically adapt their language use to their conversational\n  partner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs syntactically adapt their language use to their conversational\n  partner"
                },
                "summary": "It has been frequently observed that human speakers align their language use\nwith each other during conversations. In this paper, we study empirically\nwhether large language models (LLMs) exhibit the same behavior of\nconversational adaptation. We construct a corpus of conversations between LLMs\nand find that two LLM agents end up making more similar syntactic choices as\nconversations go on, confirming that modern LLMs adapt their language use to\ntheir conversational partners in at least a rudimentary way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been frequently observed that human speakers align their language use\nwith each other during conversations. In this paper, we study empirically\nwhether large language models (LLMs) exhibit the same behavior of\nconversational adaptation. We construct a corpus of conversations between LLMs\nand find that two LLM agents end up making more similar syntactic choices as\nconversations go on, confirming that modern LLMs adapt their language use to\ntheir conversational partners in at least a rudimentary way."
                },
                "authors": [
                    {
                        "name": "Florian Kandra"
                    },
                    {
                        "name": "Vera Demberg"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "4 pages, 1 table, 1 figure, submitted to ACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07456v1",
                "updated": "2025-03-10T15:36:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    36,
                    49,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:36:49Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    36,
                    49,
                    0,
                    69,
                    0
                ],
                "title": "Anatomy-Aware Conditional Image-Text Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anatomy-Aware Conditional Image-Text Retrieval"
                },
                "summary": "Image-Text Retrieval (ITR) finds broad applications in healthcare, aiding\nclinicians and radiologists by automatically retrieving relevant patient cases\nin the database given the query image and/or report, for more efficient\nclinical diagnosis and treatment, especially for rare diseases. However\nconventional ITR systems typically only rely on global image or text\nrepresentations for measuring patient image/report similarities, which overlook\nlocal distinctiveness across patient cases. This often results in suboptimal\nretrieval performance. In this paper, we propose an Anatomical\nLocation-Conditioned Image-Text Retrieval (ALC-ITR) framework, which, given a\nquery image and the associated suspicious anatomical region(s), aims to\nretrieve similar patient cases exhibiting the same disease or symptoms in the\nsame anatomical region. To perform location-conditioned multimodal retrieval,\nwe learn a medical Relevance-Region-Aligned Vision Language (RRA-VL) model with\nsemantic global-level and region-/word-level alignment to produce\ngeneralizable, well-aligned multi-modal representations. Additionally, we\nperform location-conditioned contrastive learning to further utilize cross-pair\nregion-level contrastiveness for improved multi-modal retrieval. We show that\nour proposed RRA-VL achieves state-of-the-art localization performance in\nphase-grounding tasks, and satisfying multi-modal retrieval performance with or\nwithout location conditioning. Finally, we thoroughly investigate the\ngeneralizability and explainability of our proposed ALC-ITR system in providing\nexplanations and preliminary diagnosis reports given retrieved patient cases\n(conditioned on anatomical regions), with proper off-the-shelf LLM prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image-Text Retrieval (ITR) finds broad applications in healthcare, aiding\nclinicians and radiologists by automatically retrieving relevant patient cases\nin the database given the query image and/or report, for more efficient\nclinical diagnosis and treatment, especially for rare diseases. However\nconventional ITR systems typically only rely on global image or text\nrepresentations for measuring patient image/report similarities, which overlook\nlocal distinctiveness across patient cases. This often results in suboptimal\nretrieval performance. In this paper, we propose an Anatomical\nLocation-Conditioned Image-Text Retrieval (ALC-ITR) framework, which, given a\nquery image and the associated suspicious anatomical region(s), aims to\nretrieve similar patient cases exhibiting the same disease or symptoms in the\nsame anatomical region. To perform location-conditioned multimodal retrieval,\nwe learn a medical Relevance-Region-Aligned Vision Language (RRA-VL) model with\nsemantic global-level and region-/word-level alignment to produce\ngeneralizable, well-aligned multi-modal representations. Additionally, we\nperform location-conditioned contrastive learning to further utilize cross-pair\nregion-level contrastiveness for improved multi-modal retrieval. We show that\nour proposed RRA-VL achieves state-of-the-art localization performance in\nphase-grounding tasks, and satisfying multi-modal retrieval performance with or\nwithout location conditioning. Finally, we thoroughly investigate the\ngeneralizability and explainability of our proposed ALC-ITR system in providing\nexplanations and preliminary diagnosis reports given retrieved patient cases\n(conditioned on anatomical regions), with proper off-the-shelf LLM prompts."
                },
                "authors": [
                    {
                        "name": "Meng Zheng"
                    },
                    {
                        "name": "Jiajin Zhang"
                    },
                    {
                        "name": "Benjamin Planche"
                    },
                    {
                        "name": "Zhongpai Gao"
                    },
                    {
                        "name": "Terrence Chen"
                    },
                    {
                        "name": "Ziyan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyan Wu"
                },
                "author": "Ziyan Wu",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07453v1",
                "updated": "2025-03-10T15:31:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    31,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:31:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    31,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "Is a Good Foundation Necessary for Efficient Reinforcement Learning? The\n  Computational Role of the Base Model in Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is a Good Foundation Necessary for Efficient Reinforcement Learning? The\n  Computational Role of the Base Model in Exploration"
                },
                "summary": "Language model alignment (or, reinforcement learning) techniques that\nleverage active exploration -- deliberately encouraging the model to produce\ndiverse, informative responses -- offer the promise of super-human\ncapabilities. However, current understanding of algorithm design primitives for\ncomputationally efficient exploration with language models is limited. To\nbetter understand how to leverage access to powerful pre-trained generative\nmodels to improve the efficiency of exploration, we introduce a new\ncomputational framework for RL with language models, in which the learner\ninteracts with the model through a sampling oracle. Focusing on the linear\nsoftmax model parameterization, we provide new results that reveal the\ncomputational-statistical tradeoffs of efficient exploration:\n  1. Necessity of coverage: Coverage refers to the extent to which the\npre-trained model covers near-optimal responses -- a form of hidden knowledge.\nWe show that coverage, while not necessary for data efficiency, lower bounds\nthe runtime of any algorithm in our framework.\n  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling,\nwhich obtains optimal data efficiency and is computationally efficient whenever\nthe pre-trained model enjoys sufficient coverage, matching our lower bound.\nSpannerSampling leverages inference-time computation with the pre-trained model\nto reduce the effective search space for exploration.\n  3. Insufficiency of training-time interventions: We contrast the result above\nby showing that training-time interventions that produce proper policies cannot\nachieve similar guarantees in polynomial time.\n  4. Computational benefits of multi-turn exploration: Finally, we show that\nunder additional representational assumptions, one can achieve improved runtime\n(replacing sequence-level coverage with token-level coverage) through\nmulti-turn exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model alignment (or, reinforcement learning) techniques that\nleverage active exploration -- deliberately encouraging the model to produce\ndiverse, informative responses -- offer the promise of super-human\ncapabilities. However, current understanding of algorithm design primitives for\ncomputationally efficient exploration with language models is limited. To\nbetter understand how to leverage access to powerful pre-trained generative\nmodels to improve the efficiency of exploration, we introduce a new\ncomputational framework for RL with language models, in which the learner\ninteracts with the model through a sampling oracle. Focusing on the linear\nsoftmax model parameterization, we provide new results that reveal the\ncomputational-statistical tradeoffs of efficient exploration:\n  1. Necessity of coverage: Coverage refers to the extent to which the\npre-trained model covers near-optimal responses -- a form of hidden knowledge.\nWe show that coverage, while not necessary for data efficiency, lower bounds\nthe runtime of any algorithm in our framework.\n  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling,\nwhich obtains optimal data efficiency and is computationally efficient whenever\nthe pre-trained model enjoys sufficient coverage, matching our lower bound.\nSpannerSampling leverages inference-time computation with the pre-trained model\nto reduce the effective search space for exploration.\n  3. Insufficiency of training-time interventions: We contrast the result above\nby showing that training-time interventions that produce proper policies cannot\nachieve similar guarantees in polynomial time.\n  4. Computational benefits of multi-turn exploration: Finally, we show that\nunder additional representational assumptions, one can achieve improved runtime\n(replacing sequence-level coverage with token-level coverage) through\nmulti-turn exploration."
                },
                "authors": [
                    {
                        "name": "Dylan J. Foster"
                    },
                    {
                        "name": "Zakaria Mhammedi"
                    },
                    {
                        "name": "Dhruv Rohatgi"
                    }
                ],
                "author_detail": {
                    "name": "Dhruv Rohatgi"
                },
                "author": "Dhruv Rohatgi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07450v1",
                "updated": "2025-03-10T15:30:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    30,
                    5,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    30,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper"
                },
                "summary": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted."
                },
                "authors": [
                    {
                        "name": "Sargam Yadav"
                    },
                    {
                        "name": "Asifa Mehmood Qureshi"
                    },
                    {
                        "name": "Abhishek Kaushik"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Roisin Loughran"
                    },
                    {
                        "name": "Subramaniam Kazhuparambil"
                    },
                    {
                        "name": "Andrew Shaw"
                    },
                    {
                        "name": "Mohammed Sabry"
                    },
                    {
                        "name": "Niamh St John Lynch"
                    },
                    {
                        "name": ". Nikhil Singh"
                    },
                    {
                        "name": "Padraic O'Hara"
                    },
                    {
                        "name": "Pranay Jaiswal"
                    },
                    {
                        "name": "Roshan Chandru"
                    },
                    {
                        "name": "David Lillis"
                    }
                ],
                "author_detail": {
                    "name": "David Lillis"
                },
                "arxiv_affiliation": "University College Dublin",
                "author": "David Lillis",
                "arxiv_comment": "The project is partially supported by the DkIT Postgraduate\n  Scholarship, Research Ireland under Grant number 13/RC/2094_2, and Grant\n  number 21/FFP-A/925",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07435v1",
                "updated": "2025-03-10T15:18:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    18,
                    10,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:18:10Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    18,
                    10,
                    0,
                    69,
                    0
                ],
                "title": "Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds"
                },
                "summary": "The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods, on average,\nand across multiple openness levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods, on average,\nand across multiple openness levels."
                },
                "authors": [
                    {
                        "name": "Riccardo Mazzieri"
                    },
                    {
                        "name": "Jacopo Pegoraro"
                    },
                    {
                        "name": "Michele Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Rossi"
                },
                "author": "Michele Rossi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07429v1",
                "updated": "2025-03-10T15:13:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    13,
                    38,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:13:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    13,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector\n  Graphics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector\n  Graphics"
                },
                "summary": "Advances in large language models (LLMs) offer new possibilities for\nenhancing math education by automating support for both teachers and students.\nWhile prior work has focused on generating math problems and high-quality\ndistractors, the role of visualization in math learning remains under-explored.\nDiagrams are essential for mathematical thinking and problem-solving, yet\nmanually creating them is time-consuming and requires domain-specific\nexpertise, limiting scalability. Recent research on using LLMs to generate\nScalable Vector Graphics (SVG) presents a promising approach to automating\ndiagram creation. Unlike pixel-based images, SVGs represent geometric figures\nusing XML, allowing seamless scaling and adaptability. Educational platforms\nsuch as Khan Academy and IXL already use SVGs to display math problems and\nhints. In this paper, we explore the use of LLMs to generate math-related\ndiagrams that accompany textual hints via intermediate SVG representations. We\naddress three research questions: (1) how to automatically generate math\ndiagrams in problem-solving hints and evaluate their quality, (2) whether SVG\nis an effective intermediate representation for math diagrams, and (3) what\nprompting strategies and formats are required for LLMs to generate accurate\nSVG-based diagrams. Our contributions include defining the task of\nautomatically generating SVG-based diagrams for math hints, developing an LLM\nprompting-based pipeline, and identifying key strategies for improving diagram\ngeneration. Additionally, we introduce a Visual Question Answering-based\nevaluation setup and conduct ablation studies to assess different pipeline\nvariations. By automating the math diagram creation, we aim to provide students\nand teachers with accurate, conceptually relevant visual aids that enhance\nproblem-solving and learning experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in large language models (LLMs) offer new possibilities for\nenhancing math education by automating support for both teachers and students.\nWhile prior work has focused on generating math problems and high-quality\ndistractors, the role of visualization in math learning remains under-explored.\nDiagrams are essential for mathematical thinking and problem-solving, yet\nmanually creating them is time-consuming and requires domain-specific\nexpertise, limiting scalability. Recent research on using LLMs to generate\nScalable Vector Graphics (SVG) presents a promising approach to automating\ndiagram creation. Unlike pixel-based images, SVGs represent geometric figures\nusing XML, allowing seamless scaling and adaptability. Educational platforms\nsuch as Khan Academy and IXL already use SVGs to display math problems and\nhints. In this paper, we explore the use of LLMs to generate math-related\ndiagrams that accompany textual hints via intermediate SVG representations. We\naddress three research questions: (1) how to automatically generate math\ndiagrams in problem-solving hints and evaluate their quality, (2) whether SVG\nis an effective intermediate representation for math diagrams, and (3) what\nprompting strategies and formats are required for LLMs to generate accurate\nSVG-based diagrams. Our contributions include defining the task of\nautomatically generating SVG-based diagrams for math hints, developing an LLM\nprompting-based pipeline, and identifying key strategies for improving diagram\ngeneration. Additionally, we introduce a Visual Question Answering-based\nevaluation setup and conduct ablation studies to assess different pipeline\nvariations. By automating the math diagram creation, we aim to provide students\nand teachers with accurate, conceptually relevant visual aids that enhance\nproblem-solving and learning experiences."
                },
                "authors": [
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Jeongah Lee"
                    },
                    {
                        "name": "Wanyong Feng"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07426v1",
                "updated": "2025-03-10T15:11:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    11,
                    7,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:11:07Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    11,
                    7,
                    0,
                    69,
                    0
                ],
                "title": "RePO: ReLU-based Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePO: ReLU-based Preference Optimization"
                },
                "summary": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter $\\beta$, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters ($\\beta$, $\\gamma$). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates $\\beta$ via two\nadvances: (1) retaining SimPO's reference-free margins but removing $\\beta$\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case ($\\beta \\to \\infty$), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter $\\beta$, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters ($\\beta$, $\\gamma$). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates $\\beta$ via two\nadvances: (1) retaining SimPO's reference-free margins but removing $\\beta$\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case ($\\beta \\to \\infty$), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune."
                },
                "authors": [
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07425v1",
                "updated": "2025-03-10T15:10:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    10,
                    40,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:10:40Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    10,
                    40,
                    0,
                    69,
                    0
                ],
                "title": "CATPlan: Loss-based Collision Prediction in End-to-End Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CATPlan: Loss-based Collision Prediction in End-to-End Autonomous\n  Driving"
                },
                "summary": "In recent years, there has been increased interest in the design, training,\nand evaluation of end-to-end autonomous driving (AD) systems. One often\noverlooked aspect is the uncertainty of planned trajectories predicted by these\nsystems, despite awareness of their own uncertainty being key to achieve safety\nand robustness. We propose to estimate this uncertainty by adapting loss\nprediction from the uncertainty quantification literature. To this end, we\nintroduce a novel light-weight module, dubbed CATPlan, that is trained to\ndecode motion and planning embeddings into estimates of the collision loss used\nto partially supervise end-to-end AD systems. During inference, these estimates\nare interpreted as collision risk. We evaluate CATPlan on the safety-critical,\nnerf-based, closed-loop benchmark NeuroNCAP and find that it manages to detect\ncollisions with a $54.8\\%$ relative improvement to average precision over a\nGMM-based baseline in which the predicted trajectory is compared to the\nforecasted trajectories of other road users. Our findings indicate that the\naddition of CATPlan can lead to safer end-to-end AD systems and hope that our\nwork will spark increased interest in uncertainty quantification for such\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been increased interest in the design, training,\nand evaluation of end-to-end autonomous driving (AD) systems. One often\noverlooked aspect is the uncertainty of planned trajectories predicted by these\nsystems, despite awareness of their own uncertainty being key to achieve safety\nand robustness. We propose to estimate this uncertainty by adapting loss\nprediction from the uncertainty quantification literature. To this end, we\nintroduce a novel light-weight module, dubbed CATPlan, that is trained to\ndecode motion and planning embeddings into estimates of the collision loss used\nto partially supervise end-to-end AD systems. During inference, these estimates\nare interpreted as collision risk. We evaluate CATPlan on the safety-critical,\nnerf-based, closed-loop benchmark NeuroNCAP and find that it manages to detect\ncollisions with a $54.8\\%$ relative improvement to average precision over a\nGMM-based baseline in which the predicted trajectory is compared to the\nforecasted trajectories of other road users. Our findings indicate that the\naddition of CATPlan can lead to safer end-to-end AD systems and hope that our\nwork will spark increased interest in uncertainty quantification for such\nsystems."
                },
                "authors": [
                    {
                        "name": "Ziliang Xiong"
                    },
                    {
                        "name": "Shipeng Liu"
                    },
                    {
                        "name": "Nathaniel Helgesen"
                    },
                    {
                        "name": "Joakim Johnander"
                    },
                    {
                        "name": "Per-Erik Forssen"
                    }
                ],
                "author_detail": {
                    "name": "Per-Erik Forssen"
                },
                "author": "Per-Erik Forssen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07418v1",
                "updated": "2025-03-10T15:05:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    5,
                    59,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:05:59Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    5,
                    59,
                    0,
                    69,
                    0
                ],
                "title": "AR-Diffusion: Asynchronous Video Generation with Auto-Regressive\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AR-Diffusion: Asynchronous Video Generation with Auto-Regressive\n  Diffusion"
                },
                "summary": "The task of video generation requires synthesizing visually realistic and\ntemporally coherent video frames. Existing methods primarily use asynchronous\nauto-regressive models or synchronous diffusion models to address this\nchallenge. However, asynchronous auto-regressive models often suffer from\ninconsistencies between training and inference, leading to issues such as error\naccumulation, while synchronous diffusion models are limited by their reliance\non rigid sequence length. To address these issues, we introduce Auto-Regressive\nDiffusion (AR-Diffusion), a novel model that combines the strengths of\nauto-regressive and diffusion models for flexible, asynchronous video\ngeneration. Specifically, our approach leverages diffusion to gradually corrupt\nvideo frames in both training and inference, reducing the discrepancy between\nthese phases. Inspired by auto-regressive generation, we incorporate a\nnon-decreasing constraint on the corruption timesteps of individual frames,\nensuring that earlier frames remain clearer than subsequent ones. This setup,\ntogether with temporal causal attention, enables flexible generation of videos\nwith varying lengths while preserving temporal coherence. In addition, we\ndesign two specialized timestep schedulers: the FoPP scheduler for balanced\ntimestep sampling during training, and the AD scheduler for flexible timestep\ndifferences during inference, supporting both synchronous and asynchronous\ngeneration. Extensive experiments demonstrate the superiority of our proposed\nmethod, which achieves competitive and state-of-the-art results across four\nchallenging benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of video generation requires synthesizing visually realistic and\ntemporally coherent video frames. Existing methods primarily use asynchronous\nauto-regressive models or synchronous diffusion models to address this\nchallenge. However, asynchronous auto-regressive models often suffer from\ninconsistencies between training and inference, leading to issues such as error\naccumulation, while synchronous diffusion models are limited by their reliance\non rigid sequence length. To address these issues, we introduce Auto-Regressive\nDiffusion (AR-Diffusion), a novel model that combines the strengths of\nauto-regressive and diffusion models for flexible, asynchronous video\ngeneration. Specifically, our approach leverages diffusion to gradually corrupt\nvideo frames in both training and inference, reducing the discrepancy between\nthese phases. Inspired by auto-regressive generation, we incorporate a\nnon-decreasing constraint on the corruption timesteps of individual frames,\nensuring that earlier frames remain clearer than subsequent ones. This setup,\ntogether with temporal causal attention, enables flexible generation of videos\nwith varying lengths while preserving temporal coherence. In addition, we\ndesign two specialized timestep schedulers: the FoPP scheduler for balanced\ntimestep sampling during training, and the AD scheduler for flexible timestep\ndifferences during inference, supporting both synchronous and asynchronous\ngeneration. Extensive experiments demonstrate the superiority of our proposed\nmethod, which achieves competitive and state-of-the-art results across four\nchallenging benchmarks."
                },
                "authors": [
                    {
                        "name": "Mingzhen Sun"
                    },
                    {
                        "name": "Weining Wang"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiahui Sun"
                    },
                    {
                        "name": "Wanquan Feng"
                    },
                    {
                        "name": "Shanshan Lao"
                    },
                    {
                        "name": "SiYu Zhou"
                    },
                    {
                        "name": "Qian He"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03982v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03982v4",
                "updated": "2025-03-10T14:37:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    37,
                    2,
                    0,
                    69,
                    0
                ],
                "published": "2025-01-07T18:39:28Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    39,
                    28,
                    1,
                    7,
                    0
                ],
                "title": "Free Anytime Validity by Sequentializing a Test and Optional\n  Continuation with Tests as Future Significance Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Anytime Validity by Sequentializing a Test and Optional\n  Continuation with Tests as Future Significance Levels"
                },
                "summary": "Anytime valid sequential tests permit us to stop and continue testing based\non the current data, without invalidating the inference. Given a maximum number\nof observations $N$, one may believe this must come at the cost of power when\ncompared to a conventional test that waits until all $N$ observations have\narrived. Our first contribution is to show that this is false: for any valid\ntest based on $N$ observations, we derive an anytime valid sequential test that\nmatches it after $N$ observations. Our second contribution is that the outcome\nof a continuously-interpreted test can be used as a significance level in\nsubsequent testing, leading to an overall procedure that is valid at the\noriginal significance level. This shows anytime validity and optional\ncontinuation are readily available in traditional testing, without requiring\nexplicit use of e-values. We illustrate this by deriving the anytime valid\nsequentialized $z$-test and $t$-test, which at time $N$ coincide with the\ntraditional $z$-test and $t$-test. Lastly, we show the popular log-optimal\nsequential $z$-test can be interpreted as desiring a rejection by the\ntraditional $z$-test at some tiny significance level in the distant future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anytime valid sequential tests permit us to stop and continue testing based\non the current data, without invalidating the inference. Given a maximum number\nof observations $N$, one may believe this must come at the cost of power when\ncompared to a conventional test that waits until all $N$ observations have\narrived. Our first contribution is to show that this is false: for any valid\ntest based on $N$ observations, we derive an anytime valid sequential test that\nmatches it after $N$ observations. Our second contribution is that the outcome\nof a continuously-interpreted test can be used as a significance level in\nsubsequent testing, leading to an overall procedure that is valid at the\noriginal significance level. This shows anytime validity and optional\ncontinuation are readily available in traditional testing, without requiring\nexplicit use of e-values. We illustrate this by deriving the anytime valid\nsequentialized $z$-test and $t$-test, which at time $N$ coincide with the\ntraditional $z$-test and $t$-test. Lastly, we show the popular log-optimal\nsequential $z$-test can be interpreted as desiring a rejection by the\ntraditional $z$-test at some tiny significance level in the distant future."
                },
                "authors": [
                    {
                        "name": "Nick W. Koning"
                    },
                    {
                        "name": "Sam van Meer"
                    }
                ],
                "author_detail": {
                    "name": "Sam van Meer"
                },
                "author": "Sam van Meer",
                "arxiv_comment": "Minor changes and textual polishing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03982v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03982v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07384v1",
                "updated": "2025-03-10T14:32:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    32,
                    56,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:32:56Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    32,
                    56,
                    0,
                    69,
                    0
                ],
                "title": "Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs"
                },
                "summary": "This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies."
                },
                "authors": [
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Daniel de Alcala"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Aythami Morales"
                    }
                ],
                "author_detail": {
                    "name": "Aythami Morales"
                },
                "author": "Aythami Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07377v1",
                "updated": "2025-03-10T14:31:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    31,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:31:00Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    31,
                    0,
                    0,
                    69,
                    0
                ],
                "title": "Process-Supervised LLM Recommenders via Flow-guided Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-Supervised LLM Recommenders via Flow-guided Tuning"
                },
                "summary": "While large language models (LLMs) are increasingly adapted for\nrecommendation systems via supervised fine-tuning (SFT), this approach\namplifies popularity bias due to its likelihood maximization objective,\ncompromising recommendation diversity and fairness. To address this, we present\nFlow-guided fine-tuning recommender (Flower), which replaces SFT with a\nGenerative Flow Network (GFlowNet) framework that enacts process supervision\nthrough token-level reward propagation. Flower's key innovation lies in\ndecomposing item-level rewards into constituent token rewards, enabling direct\nalignment between token generation probabilities and their reward signals. This\nmechanism achieves three critical advancements: (1) popularity bias mitigation\nand fairness enhancement through empirical distribution matching, (2)\npreservation of diversity through GFlowNet's proportional sampling, and (3)\nflexible integration of personalized preferences via adaptable token rewards.\nExperiments demonstrate Flower's superior distribution-fitting capability and\nits significant advantages over traditional SFT in terms of fairness,\ndiversity, and accuracy, highlighting its potential to improve LLM-based\nrecommendation systems. The implementation is available via\nhttps://github.com/Mr-Peach0301/Flower",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly adapted for\nrecommendation systems via supervised fine-tuning (SFT), this approach\namplifies popularity bias due to its likelihood maximization objective,\ncompromising recommendation diversity and fairness. To address this, we present\nFlow-guided fine-tuning recommender (Flower), which replaces SFT with a\nGenerative Flow Network (GFlowNet) framework that enacts process supervision\nthrough token-level reward propagation. Flower's key innovation lies in\ndecomposing item-level rewards into constituent token rewards, enabling direct\nalignment between token generation probabilities and their reward signals. This\nmechanism achieves three critical advancements: (1) popularity bias mitigation\nand fairness enhancement through empirical distribution matching, (2)\npreservation of diversity through GFlowNet's proportional sampling, and (3)\nflexible integration of personalized preferences via adaptable token rewards.\nExperiments demonstrate Flower's superior distribution-fitting capability and\nits significant advantages over traditional SFT in terms of fairness,\ndiversity, and accuracy, highlighting its potential to improve LLM-based\nrecommendation systems. The implementation is available via\nhttps://github.com/Mr-Peach0301/Flower"
                },
                "authors": [
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Mengyao Gao"
                    },
                    {
                        "name": "Chenxiao Fan"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Wentao Shi"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07365v1",
                "updated": "2025-03-10T14:23:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    23,
                    12,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:23:12Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    23,
                    12,
                    0,
                    69,
                    0
                ],
                "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning"
                },
                "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA"
                },
                "authors": [
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Lingxiao Du"
                    },
                    {
                        "name": "Zongkai Liu"
                    },
                    {
                        "name": "Zhixiang Zhou"
                    },
                    {
                        "name": "Quanfeng Lu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Shao"
                },
                "author": "Wenqi Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05139v2",
                "updated": "2025-03-10T14:21:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    21,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T04:43:39Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    4,
                    43,
                    39,
                    4,
                    66,
                    0
                ],
                "title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without\n  Premium GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without\n  Premium GPUs"
                },
                "summary": "In this technical report, we tackle the challenges of training large-scale\nMixture of Experts (MoE) models, focusing on overcoming cost inefficiency and\nresource limitations prevalent in such systems. To address these issues, we\npresent two differently sized MoE large language models (LLMs), namely\nLing-Lite and Ling-Plus (referred to as \"Bailing\" in Chinese, spelled\nB\\v{a}il\\'ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75\nbillion activated parameters, while Ling-Plus boasts 290 billion parameters\nwith 28.8 billion activated parameters. Both models exhibit comparable\nperformance to leading industry benchmarks. This report offers actionable\ninsights to improve the efficiency and accessibility of AI development in\nresource-constrained settings, promoting more scalable and sustainable\ntechnologies. Specifically, to reduce training costs for large-scale MoE\nmodels, we propose innovative methods for (1) optimization of model\narchitecture and training processes, (2) refinement of training anomaly\nhandling, and (3) enhancement of model evaluation efficiency. Additionally,\nleveraging high-quality data generated from knowledge graphs, our models\ndemonstrate superior capabilities in tool use compared to other models.\nUltimately, our experimental findings demonstrate that a 300B MoE LLM can be\neffectively trained on lower-performance devices while achieving comparable\nperformance to models of a similar scale, including dense and MoE models.\nCompared to high-performance devices, utilizing a lower-specification hardware\nsystem during the pre-training phase demonstrates significant cost savings,\nreducing computing costs by approximately 20%. The models can be accessed at\nhttps://huggingface.co/inclusionAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this technical report, we tackle the challenges of training large-scale\nMixture of Experts (MoE) models, focusing on overcoming cost inefficiency and\nresource limitations prevalent in such systems. To address these issues, we\npresent two differently sized MoE large language models (LLMs), namely\nLing-Lite and Ling-Plus (referred to as \"Bailing\" in Chinese, spelled\nB\\v{a}il\\'ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75\nbillion activated parameters, while Ling-Plus boasts 290 billion parameters\nwith 28.8 billion activated parameters. Both models exhibit comparable\nperformance to leading industry benchmarks. This report offers actionable\ninsights to improve the efficiency and accessibility of AI development in\nresource-constrained settings, promoting more scalable and sustainable\ntechnologies. Specifically, to reduce training costs for large-scale MoE\nmodels, we propose innovative methods for (1) optimization of model\narchitecture and training processes, (2) refinement of training anomaly\nhandling, and (3) enhancement of model evaluation efficiency. Additionally,\nleveraging high-quality data generated from knowledge graphs, our models\ndemonstrate superior capabilities in tool use compared to other models.\nUltimately, our experimental findings demonstrate that a 300B MoE LLM can be\neffectively trained on lower-performance devices while achieving comparable\nperformance to models of a similar scale, including dense and MoE models.\nCompared to high-performance devices, utilizing a lower-specification hardware\nsystem during the pre-training phase demonstrates significant cost savings,\nreducing computing costs by approximately 20%. The models can be accessed at\nhttps://huggingface.co/inclusionAI."
                },
                "authors": [
                    {
                        "name": "Ling Team"
                    },
                    {
                        "name": "Binwei Zeng"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Changxin Tian"
                    },
                    {
                        "name": "Cong Chen"
                    },
                    {
                        "name": "Dingnan Jin"
                    },
                    {
                        "name": "Feng Yu"
                    },
                    {
                        "name": "Feng Zhu"
                    },
                    {
                        "name": "Feng Yuan"
                    },
                    {
                        "name": "Fakang Wang"
                    },
                    {
                        "name": "Gangshan Wang"
                    },
                    {
                        "name": "Guangyao Zhai"
                    },
                    {
                        "name": "Haitao Zhang"
                    },
                    {
                        "name": "Huizhong Li"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Junpeng Fang"
                    },
                    {
                        "name": "Junjie Ou"
                    },
                    {
                        "name": "Jun Hu"
                    },
                    {
                        "name": "Ji Luo"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Jian Sha"
                    },
                    {
                        "name": "Jianxue Qian"
                    },
                    {
                        "name": "Jiewei Wu"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Jubao Feng"
                    },
                    {
                        "name": "Jingchao Di"
                    },
                    {
                        "name": "Junming Xu"
                    },
                    {
                        "name": "Jinghua Yao"
                    },
                    {
                        "name": "Kuan Xu"
                    },
                    {
                        "name": "Kewei Du"
                    },
                    {
                        "name": "Longfei Li"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Lu Yu"
                    },
                    {
                        "name": "Li Tang"
                    },
                    {
                        "name": "Lin Ju"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Qing Cui"
                    },
                    {
                        "name": "Song Liu"
                    },
                    {
                        "name": "Shicheng Li"
                    },
                    {
                        "name": "Shun Song"
                    },
                    {
                        "name": "Song Yan"
                    },
                    {
                        "name": "Tengwei Cai"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Ting Guo"
                    },
                    {
                        "name": "Ting Huang"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Xueming Yang"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Xiaobo Hu"
                    },
                    {
                        "name": "Xin Lin"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yilong Wang"
                    },
                    {
                        "name": "Yongzhen Guo"
                    },
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Yuhao Fu"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Yanzhe Li"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhaoxin Huan"
                    },
                    {
                        "name": "Zujie Wen"
                    },
                    {
                        "name": "Zhenhang Sun"
                    },
                    {
                        "name": "Zhuoxuan Du"
                    },
                    {
                        "name": "Zhengyu He"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyu He"
                },
                "author": "Zhengyu He",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07358v1",
                "updated": "2025-03-10T14:16:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    16,
                    8,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:16:08Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    16,
                    8,
                    0,
                    69,
                    0
                ],
                "title": "RepoST: Scalable Repository-Level Coding Environment Construction with\n  Sandbox Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoST: Scalable Repository-Level Coding Environment Construction with\n  Sandbox Testing"
                },
                "summary": "We present RepoST, a scalable method to construct environments that provide\nexecution feedback for repository-level code generation for both training and\nevaluation. Unlike existing works that aim to build entire repositories for\nexecution, which is challenging for both human and LLMs, we provide execution\nfeedback with sandbox testing, which isolates a given target function and its\ndependencies to a separate script for testing. Sandbox testing reduces the\ncomplexity of external dependencies and enables constructing environments at a\nlarge scale. We use our method to construct RepoST-Train, a large-scale train\nset with 7,415 functions from 832 repositories. Training with the execution\nfeedback provided by RepoST-Train leads to a performance gain of 5.5% Pass@1 on\nHumanEval and 3.5% Pass@1 on RepoEval. We also build an evaluation dataset,\nRepoST-Eval, and benchmark 12 code generation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RepoST, a scalable method to construct environments that provide\nexecution feedback for repository-level code generation for both training and\nevaluation. Unlike existing works that aim to build entire repositories for\nexecution, which is challenging for both human and LLMs, we provide execution\nfeedback with sandbox testing, which isolates a given target function and its\ndependencies to a separate script for testing. Sandbox testing reduces the\ncomplexity of external dependencies and enables constructing environments at a\nlarge scale. We use our method to construct RepoST-Train, a large-scale train\nset with 7,415 functions from 832 repositories. Training with the execution\nfeedback provided by RepoST-Train leads to a performance gain of 5.5% Pass@1 on\nHumanEval and 3.5% Pass@1 on RepoEval. We also build an evaluation dataset,\nRepoST-Eval, and benchmark 12 code generation models."
                },
                "authors": [
                    {
                        "name": "Yiqing Xie"
                    },
                    {
                        "name": "Alex Xie"
                    },
                    {
                        "name": "Divyanshu Sheth"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Daniel Fried"
                    },
                    {
                        "name": "Carolyn Rose"
                    }
                ],
                "author_detail": {
                    "name": "Carolyn Rose"
                },
                "author": "Carolyn Rose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04870v2",
                "updated": "2025-03-10T14:04:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    4,
                    38,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-06T16:04:01Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    4,
                    1,
                    3,
                    65,
                    0
                ],
                "title": "Leveraging Large Language Models to Address Data Scarcity in Machine\n  Learning: Applications in Graphene Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Address Data Scarcity in Machine\n  Learning: Applications in Graphene Synthesis"
                },
                "summary": "Machine learning in materials science faces challenges due to limited\nexperimental data, as generating synthesis data is costly and time-consuming,\nespecially with in-house experiments. Mining data from existing literature\nintroduces issues like mixed data quality, inconsistent formats, and variations\nin reporting experimental parameters, complicating the creation of consistent\nfeatures for the learning algorithm. Additionally, combining continuous and\ndiscrete features can hinder the learning process with limited data. Here, we\npropose strategies that utilize large language models (LLMs) to enhance machine\nlearning performance on a limited, heterogeneous dataset of graphene chemical\nvapor deposition synthesis compiled from existing literature. These strategies\ninclude prompting modalities for imputing missing data points and leveraging\nlarge language model embeddings to encode the complex nomenclature of\nsubstrates reported in chemical vapor deposition experiments. The proposed\nstrategies enhance graphene layer classification using a support vector machine\n(SVM) model, increasing binary classification accuracy from 39% to 65% and\nternary accuracy from 52% to 72%. We compare the performance of the SVM and a\nGPT-4 model, both trained and fine-tuned on the same data. Our results\ndemonstrate that the numerical classifier, when combined with LLM-driven data\nenhancements, outperforms the standalone LLM predictor, highlighting that in\ndata-scarce scenarios, improving predictive learning with LLM strategies\nrequires more than simple fine-tuning on datasets. Instead, it necessitates\nsophisticated approaches for data imputation and feature space homogenization\nto achieve optimal performance. The proposed strategies emphasize data\nenhancement techniques, offering a broadly applicable framework for improving\nmachine learning performance on scarce, inhomogeneous datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning in materials science faces challenges due to limited\nexperimental data, as generating synthesis data is costly and time-consuming,\nespecially with in-house experiments. Mining data from existing literature\nintroduces issues like mixed data quality, inconsistent formats, and variations\nin reporting experimental parameters, complicating the creation of consistent\nfeatures for the learning algorithm. Additionally, combining continuous and\ndiscrete features can hinder the learning process with limited data. Here, we\npropose strategies that utilize large language models (LLMs) to enhance machine\nlearning performance on a limited, heterogeneous dataset of graphene chemical\nvapor deposition synthesis compiled from existing literature. These strategies\ninclude prompting modalities for imputing missing data points and leveraging\nlarge language model embeddings to encode the complex nomenclature of\nsubstrates reported in chemical vapor deposition experiments. The proposed\nstrategies enhance graphene layer classification using a support vector machine\n(SVM) model, increasing binary classification accuracy from 39% to 65% and\nternary accuracy from 52% to 72%. We compare the performance of the SVM and a\nGPT-4 model, both trained and fine-tuned on the same data. Our results\ndemonstrate that the numerical classifier, when combined with LLM-driven data\nenhancements, outperforms the standalone LLM predictor, highlighting that in\ndata-scarce scenarios, improving predictive learning with LLM strategies\nrequires more than simple fine-tuning on datasets. Instead, it necessitates\nsophisticated approaches for data imputation and feature space homogenization\nto achieve optimal performance. The proposed strategies emphasize data\nenhancement techniques, offering a broadly applicable framework for improving\nmachine learning performance on scarce, inhomogeneous datasets."
                },
                "authors": [
                    {
                        "name": "Devi Dutta Biswajeet"
                    },
                    {
                        "name": "Sara Kadkhodaei"
                    }
                ],
                "author_detail": {
                    "name": "Sara Kadkhodaei"
                },
                "author": "Sara Kadkhodaei",
                "arxiv_comment": "20 pages, 10 figures, 4 tables; Supplementary Material with 13\n  figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16457v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16457v3",
                "updated": "2025-03-10T14:00:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    0,
                    39,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-23T06:16:23Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    6,
                    16,
                    23,
                    6,
                    54,
                    0
                ],
                "title": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge"
                },
                "summary": "Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Ji Hoon Hong"
                    },
                    {
                        "name": "Dong Won Jeon"
                    },
                    {
                        "name": "Ga-Yeon Baek"
                    },
                    {
                        "name": "Kyeong-Won Kwak"
                    },
                    {
                        "name": "Dong-Hee Lee"
                    },
                    {
                        "name": "Jisu Bae"
                    },
                    {
                        "name": "Chihoon Lee"
                    },
                    {
                        "name": "Yunseo Kim"
                    },
                    {
                        "name": "Seon-Jin Choi"
                    },
                    {
                        "name": "Jin-Seong Park"
                    },
                    {
                        "name": "Sung Beom Cho"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16457v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16457v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16821v3",
                "updated": "2025-03-10T13:58:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    58,
                    19,
                    0,
                    69,
                    0
                ],
                "published": "2024-11-25T17:15:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    15,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "KL-geodesics flow matching with a novel sampling scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KL-geodesics flow matching with a novel sampling scheme"
                },
                "summary": "Non-autoregressive language models generate all tokens simultaneously,\noffering potential speed advantages over traditional autoregressive models, but\nthey face challenges in modeling the complex dependencies inherent in text\ndata. In this work, we investigate a conditional flow matching approach for\ntext generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional\nsimplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which\ncorrespond to linear interpolation in logit space. We provide a theoretical\njustification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid\nx_t, t)\\) yields the exact flow matching velocity under logit interpolation. To\naddress the suboptimal performance of basic inference, we propose a novel\nempirical sampling scheme that iteratively samples from the conditional\ndistribution and introduces additional noise, significantly improving results\ndespite lacking full theoretical underpinnings. Furthermore, we propose a\nhybrid inference method that combines the basic approach with the sampling\nscheme. This method demonstrates superior performance on both conditional and\nunconditional text generation experiments compared to previous SOTA method for\ndiscrete flow matching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-autoregressive language models generate all tokens simultaneously,\noffering potential speed advantages over traditional autoregressive models, but\nthey face challenges in modeling the complex dependencies inherent in text\ndata. In this work, we investigate a conditional flow matching approach for\ntext generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional\nsimplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which\ncorrespond to linear interpolation in logit space. We provide a theoretical\njustification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid\nx_t, t)\\) yields the exact flow matching velocity under logit interpolation. To\naddress the suboptimal performance of basic inference, we propose a novel\nempirical sampling scheme that iteratively samples from the conditional\ndistribution and introduces additional noise, significantly improving results\ndespite lacking full theoretical underpinnings. Furthermore, we propose a\nhybrid inference method that combines the basic approach with the sampling\nscheme. This method demonstrates superior performance on both conditional and\nunconditional text generation experiments compared to previous SOTA method for\ndiscrete flow matching."
                },
                "authors": [
                    {
                        "name": "Egor Sevriugov"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03612v2",
                "updated": "2025-03-10T13:58:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    58,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-05T15:51:25Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    51,
                    25,
                    2,
                    64,
                    0
                ],
                "title": "Large language models in finance : what is financial sentiment?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models in finance : what is financial sentiment?"
                },
                "summary": "Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance."
                },
                "authors": [
                    {
                        "name": "Kemal Kirtac"
                    },
                    {
                        "name": "Guido Germano"
                    }
                ],
                "author_detail": {
                    "name": "Guido Germano"
                },
                "author": "Guido Germano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; H.3.4; J.4; J.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07802v2",
                "updated": "2025-03-10T13:54:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    54,
                    40,
                    0,
                    69,
                    0
                ],
                "published": "2024-11-12T13:57:13Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    13,
                    57,
                    13,
                    1,
                    317,
                    0
                ],
                "title": "Large-scale Remote Sensing Image Target Recognition and Automatic\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Remote Sensing Image Target Recognition and Automatic\n  Annotation"
                },
                "summary": "This paper presents a method for object recognition and automatic labeling in\nlarge-area remote sensing images called LRSAA. The method integrates YOLOv11\nand MobileNetV3-SSD object detection algorithms through ensemble learning to\nenhance model performance. Furthermore, it employs Poisson disk sampling\nsegmentation techniques and the EIOU metric to optimize the training and\ninference processes of segmented images, followed by the integration of\nresults. This approach not only reduces the demand for computational resources\nbut also achieves a good balance between accuracy and speed. The source code\nfor this project has been made publicly available on\nhttps://github.com/anaerovane/LRSAA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method for object recognition and automatic labeling in\nlarge-area remote sensing images called LRSAA. The method integrates YOLOv11\nand MobileNetV3-SSD object detection algorithms through ensemble learning to\nenhance model performance. Furthermore, it employs Poisson disk sampling\nsegmentation techniques and the EIOU metric to optimize the training and\ninference processes of segmented images, followed by the integration of\nresults. This approach not only reduces the demand for computational resources\nbut also achieves a good balance between accuracy and speed. The source code\nfor this project has been made publicly available on\nhttps://github.com/anaerovane/LRSAA."
                },
                "authors": [
                    {
                        "name": "Wuzheng Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wuzheng Dong"
                },
                "author": "Wuzheng Dong",
                "arxiv_comment": "My team members have already submitted another version of this paper\n  to arXiv (arXiv:2411.15808). We have to withdraw this version to avoid\n  resubmitting the same paper content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05923v2",
                "updated": "2025-03-10T13:53:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    53,
                    2,
                    0,
                    69,
                    0
                ],
                "published": "2024-12-08T12:37:29Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    12,
                    37,
                    29,
                    6,
                    343,
                    0
                ],
                "title": "Implications of latest NICER data for the neutron star equation of state",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implications of latest NICER data for the neutron star equation of state"
                },
                "summary": "As an update to our previously performed Bayesian inference analyses of the\nneutron star matter equation-of-state and related quantities, the additional\nimpact of the recently published NICER data of PSR J0437-4751 is examined.\nIncluding the mass and radius distributions of this pulsar in our data base\nresults in modest shifts from previously inferred median posterior values of\nradii $R$ and central densities $n_c$ for representative $1.4\\,M_\\odot$ and\n$2.1\\,M_\\odot$ neutron stars: radii are reduced by about $0.2-0.3$ km to values\nof $R_{1.4} = 12.1\\pm 0.5$ km and $R_{2.1} = 11.9^{+0.5}_{-0.6}$ km (at the\n68\\% level), and central densities increase slightly to values of\n$n_c(1.4\\,M_\\odot)/n_0 = 2.8\\pm 0.3$ and $n_c(2.1\\,M_\\odot)/n_0 =\n3.8_{-0.7}^{+0.6}$ (in units of equilibrium nuclear matter density, $n_0 =\n0.16$ fm$^{-3}$), i.e., they still fall below five times nuclear saturation\ndensity at the 68\\% level. As a further significant result, the evidence\nestablished by analyzing Bayes factors for a negative trace anomaly measure,\n$\\Delta = 1/3-P/\\varepsilon < 0$, inside heavy neutron stars is raised to\nstrong.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an update to our previously performed Bayesian inference analyses of the\nneutron star matter equation-of-state and related quantities, the additional\nimpact of the recently published NICER data of PSR J0437-4751 is examined.\nIncluding the mass and radius distributions of this pulsar in our data base\nresults in modest shifts from previously inferred median posterior values of\nradii $R$ and central densities $n_c$ for representative $1.4\\,M_\\odot$ and\n$2.1\\,M_\\odot$ neutron stars: radii are reduced by about $0.2-0.3$ km to values\nof $R_{1.4} = 12.1\\pm 0.5$ km and $R_{2.1} = 11.9^{+0.5}_{-0.6}$ km (at the\n68\\% level), and central densities increase slightly to values of\n$n_c(1.4\\,M_\\odot)/n_0 = 2.8\\pm 0.3$ and $n_c(2.1\\,M_\\odot)/n_0 =\n3.8_{-0.7}^{+0.6}$ (in units of equilibrium nuclear matter density, $n_0 =\n0.16$ fm$^{-3}$), i.e., they still fall below five times nuclear saturation\ndensity at the 68\\% level. As a further significant result, the evidence\nestablished by analyzing Bayes factors for a negative trace anomaly measure,\n$\\Delta = 1/3-P/\\varepsilon < 0$, inside heavy neutron stars is raised to\nstrong."
                },
                "authors": [
                    {
                        "name": "Len Brandes"
                    },
                    {
                        "name": "Wolfram Weise"
                    }
                ],
                "author_detail": {
                    "name": "Wolfram Weise"
                },
                "author": "Wolfram Weise",
                "arxiv_doi": "10.1103/PhysRevD.111.034005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.034005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.05923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "published version",
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07338v1",
                "updated": "2025-03-10T13:50:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    50,
                    23,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:50:23Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    50,
                    23,
                    0,
                    69,
                    0
                ],
                "title": "Temporal Triplane Transformers as Occupancy World Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Triplane Transformers as Occupancy World Models"
                },
                "summary": "Recent years have seen significant advances in world models, which primarily\nfocus on learning fine-grained correlations between an agent's motion\ntrajectory and the resulting changes in its surrounding environment. However,\nexisting methods often struggle to capture such fine-grained correlations and\nachieve real-time predictions. To address this, we propose a new 4D occupancy\nworld model for autonomous driving, termed T$^3$Former. T$^3$Former begins by\npre-training a compact triplane representation that efficiently compresses the\n3D semantically occupied environment. Next, T$^3$Former extracts multi-scale\ntemporal motion features from the historical triplane and employs an\nautoregressive approach to iteratively predict the next triplane changes.\nFinally, T$^3$Former combines the triplane changes with the previous ones to\ndecode them into future occupancy results and ego-motion trajectories.\nExperimental results demonstrate the superiority of T$^3$Former, achieving\n1.44$\\times$ faster inference speed (26 FPS), while improving the mean IoU to\n36.09 and reducing the mean absolute planning error to 1.0 meters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen significant advances in world models, which primarily\nfocus on learning fine-grained correlations between an agent's motion\ntrajectory and the resulting changes in its surrounding environment. However,\nexisting methods often struggle to capture such fine-grained correlations and\nachieve real-time predictions. To address this, we propose a new 4D occupancy\nworld model for autonomous driving, termed T$^3$Former. T$^3$Former begins by\npre-training a compact triplane representation that efficiently compresses the\n3D semantically occupied environment. Next, T$^3$Former extracts multi-scale\ntemporal motion features from the historical triplane and employs an\nautoregressive approach to iteratively predict the next triplane changes.\nFinally, T$^3$Former combines the triplane changes with the previous ones to\ndecode them into future occupancy results and ego-motion trajectories.\nExperimental results demonstrate the superiority of T$^3$Former, achieving\n1.44$\\times$ faster inference speed (26 FPS), while improving the mean IoU to\n36.09 and reducing the mean absolute planning error to 1.0 meters."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Peixi Peng"
                    },
                    {
                        "name": "Guang Tan"
                    },
                    {
                        "name": "Yiqian Chang"
                    },
                    {
                        "name": "Yisen Zhao"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07334v1",
                "updated": "2025-03-10T13:49:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    49,
                    28,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:49:28Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    49,
                    28,
                    0,
                    69,
                    0
                ],
                "title": "Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment"
                },
                "summary": "We present Autoregressive Representation Alignment (ARRA), a new training\nframework that unlocks global-coherent text-to-image generation in\nautoregressive LLMs without architectural changes. Unlike prior work that\nrequires complex architectural redesigns, ARRA aligns LLM hidden states with\nvisual representations from external visual foundational models via a global\nvisual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual\nconstraints: local next-token prediction and global semantic distillation,\nenabling LLMs to implicitly learn spatial and contextual coherence while\nretaining their original autoregressive paradigm. Extensive experiments\nvalidate ARRA's plug-and-play versatility. When training from\ntext-generation-only LLMs or random initialization, ARRA reduces FID by 25.5%\n(MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive\nLLMs like Chameleon and LlamaGen, all without framework modifications. For\ndomain adaption, ARRA aligns general-purpose LLMs with specialized models\n(e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on\nmedical imaging (MIMIC-CXR). By demonstrating that training objective redesign\n-- not just architectural innovation -- can resolve cross-modal global\ncoherence challenges, ARRA offers a complementary paradigm for advancing\nautoregressive models. Code and models will be released to advance\nautoregressive image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Autoregressive Representation Alignment (ARRA), a new training\nframework that unlocks global-coherent text-to-image generation in\nautoregressive LLMs without architectural changes. Unlike prior work that\nrequires complex architectural redesigns, ARRA aligns LLM hidden states with\nvisual representations from external visual foundational models via a global\nvisual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual\nconstraints: local next-token prediction and global semantic distillation,\nenabling LLMs to implicitly learn spatial and contextual coherence while\nretaining their original autoregressive paradigm. Extensive experiments\nvalidate ARRA's plug-and-play versatility. When training from\ntext-generation-only LLMs or random initialization, ARRA reduces FID by 25.5%\n(MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive\nLLMs like Chameleon and LlamaGen, all without framework modifications. For\ndomain adaption, ARRA aligns general-purpose LLMs with specialized models\n(e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on\nmedical imaging (MIMIC-CXR). By demonstrating that training objective redesign\n-- not just architectural innovation -- can resolve cross-modal global\ncoherence challenges, ARRA offers a complementary paradigm for advancing\nautoregressive models. Code and models will be released to advance\nautoregressive image generation."
                },
                "authors": [
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Ziyue Lin"
                    },
                    {
                        "name": "Huijie Fan"
                    },
                    {
                        "name": "Zhi Han"
                    },
                    {
                        "name": "Yandong Tang"
                    },
                    {
                        "name": "Liangqiong Qu"
                    }
                ],
                "author_detail": {
                    "name": "Liangqiong Qu"
                },
                "author": "Liangqiong Qu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07329v1",
                "updated": "2025-03-10T13:42:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    42,
                    4,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:42:04Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    42,
                    4,
                    0,
                    69,
                    0
                ],
                "title": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning\n  Large Language Models"
                },
                "summary": "The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation."
                },
                "authors": [
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Guergana Savova"
                    },
                    {
                        "name": "Lijing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijing Wang"
                },
                "author": "Lijing Wang",
                "arxiv_comment": "7 pages, 5 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07323v1",
                "updated": "2025-03-10T13:39:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    39,
                    9,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:39:09Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    39,
                    9,
                    0,
                    69,
                    0
                ],
                "title": "Dynamic Path Navigation for Motion Agents with LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Path Navigation for Motion Agents with LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong generalizable reasoning\nand planning capabilities. However, their efficacies in spatial path planning\nand obstacle-free trajectory generation remain underexplored. Leveraging LLMs\nfor navigation holds significant potential, given LLMs' ability to handle\nunseen scenarios, support user-agent interactions, and provide global control\nacross complex systems, making them well-suited for agentic planning and\nhumanoid motion generation. As one of the first studies in this domain, we\nexplore the zero-shot navigation and path generation capabilities of LLMs by\nconstructing a dataset and proposing an evaluation protocol. Specifically, we\nrepresent paths using anchor points connected by straight lines, enabling\nmovement in various directions. This approach offers greater flexibility and\npracticality compared to previous methods while remaining simple and intuitive\nfor LLMs. We demonstrate that, when tasks are well-structured in this manner,\nmodern LLMs exhibit substantial planning proficiency in avoiding obstacles\nwhile autonomously refining navigation with the generated motion to reach the\ntarget. Further, this spatial reasoning ability of a single LLM motion agent\ninteracting in a static environment can be seamlessly generalized in\nmulti-motion agents coordination in dynamic environments. Unlike traditional\napproaches that rely on single-step planning or local policies, our\ntraining-free LLM-based method enables global, dynamic, closed-loop planning,\nand autonomously resolving collision issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong generalizable reasoning\nand planning capabilities. However, their efficacies in spatial path planning\nand obstacle-free trajectory generation remain underexplored. Leveraging LLMs\nfor navigation holds significant potential, given LLMs' ability to handle\nunseen scenarios, support user-agent interactions, and provide global control\nacross complex systems, making them well-suited for agentic planning and\nhumanoid motion generation. As one of the first studies in this domain, we\nexplore the zero-shot navigation and path generation capabilities of LLMs by\nconstructing a dataset and proposing an evaluation protocol. Specifically, we\nrepresent paths using anchor points connected by straight lines, enabling\nmovement in various directions. This approach offers greater flexibility and\npracticality compared to previous methods while remaining simple and intuitive\nfor LLMs. We demonstrate that, when tasks are well-structured in this manner,\nmodern LLMs exhibit substantial planning proficiency in avoiding obstacles\nwhile autonomously refining navigation with the generated motion to reach the\ntarget. Further, this spatial reasoning ability of a single LLM motion agent\ninteracting in a static environment can be seamlessly generalized in\nmulti-motion agents coordination in dynamic environments. Unlike traditional\napproaches that rely on single-step planning or local policies, our\ntraining-free LLM-based method enables global, dynamic, closed-loop planning,\nand autonomously resolving collision issues."
                },
                "authors": [
                    {
                        "name": "Yubo Zhao"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Keung Tang"
                },
                "author": "Chi-Keung Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03831v2",
                "updated": "2025-03-10T13:38:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    38,
                    35,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-06T07:31:18Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    31,
                    18,
                    3,
                    37,
                    0
                ],
                "title": "Optimizing Bayesian model selection for equation of state of cold\n  neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Bayesian model selection for equation of state of cold\n  neutron stars"
                },
                "summary": "We introduce a computational framework, Bayesian Evidence calculation fOr\nModel Selection (BEOMS) to evaluate multiple Bayesian model selection methods\nin the context of determining the equation of state (EOS) for cold neutron star\n(NS), focusing on their performance with current and next-generation\ngravitational wave (GW) observatories. We conduct a systematic comparison of\nvarious EOS models by using posterior distributions obtained from EOS-agnostic\nBayesian inference of binary parameters applied to GWs from a population of\nbinary neutron star (BNS) mergers. The cumulative evidence for each model is\ncalculated in a multi-dimensional parameter space characterized by neutron star\nmasses and tidal deformabilities. Our findings indicate that Bayesian model\nselection is most effective when performed in the two-dimensional subspace of\ncomponent mass and tidal deformability, requiring fewer events to distinguish\nbetween EOS models with high confidence. Furthermore, we establish a\nrelationship between the precision of tidal deformability measurements and the\naccuracy of model selection, taking into account the evolving sensitivities of\ncurrent and planned GW observatories. BEOMS offers computational efficiency and\ncan be adapted to execute model selection for gravitational wave data from\nother sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a computational framework, Bayesian Evidence calculation fOr\nModel Selection (BEOMS) to evaluate multiple Bayesian model selection methods\nin the context of determining the equation of state (EOS) for cold neutron star\n(NS), focusing on their performance with current and next-generation\ngravitational wave (GW) observatories. We conduct a systematic comparison of\nvarious EOS models by using posterior distributions obtained from EOS-agnostic\nBayesian inference of binary parameters applied to GWs from a population of\nbinary neutron star (BNS) mergers. The cumulative evidence for each model is\ncalculated in a multi-dimensional parameter space characterized by neutron star\nmasses and tidal deformabilities. Our findings indicate that Bayesian model\nselection is most effective when performed in the two-dimensional subspace of\ncomponent mass and tidal deformability, requiring fewer events to distinguish\nbetween EOS models with high confidence. Furthermore, we establish a\nrelationship between the precision of tidal deformability measurements and the\naccuracy of model selection, taking into account the evolving sensitivities of\ncurrent and planned GW observatories. BEOMS offers computational efficiency and\ncan be adapted to execute model selection for gravitational wave data from\nother sources."
                },
                "authors": [
                    {
                        "name": "Rahul Kashyap"
                    },
                    {
                        "name": "Ish Gupta"
                    },
                    {
                        "name": "Arnab Dhani"
                    },
                    {
                        "name": "Monica Bapna"
                    },
                    {
                        "name": "Bangalore Sathyaprakash"
                    }
                ],
                "author_detail": {
                    "name": "Bangalore Sathyaprakash"
                },
                "author": "Bangalore Sathyaprakash",
                "arxiv_comment": "LVK PnP approved, 22 Page, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07320v1",
                "updated": "2025-03-10T13:37:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    37,
                    36,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:37:36Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    37,
                    36,
                    0,
                    69,
                    0
                ],
                "title": "Experimental Exploration: Investigating Cooperative Interaction Behavior\n  Between Humans and Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental Exploration: Investigating Cooperative Interaction Behavior\n  Between Humans and Large Language Model Agents"
                },
                "summary": "With the rise of large language models (LLMs), AI agents as autonomous\ndecision-makers present significant opportunities and challenges for human-AI\ncooperation. While many studies have explored human cooperation with AI as\ntools, the role of LLM-augmented autonomous agents in competitive-cooperative\ninteractions remains under-examined. This study investigates human cooperative\nbehavior by engaging 30 participants who interacted with LLM agents exhibiting\ndifferent characteristics (purported human, purported rule-based AI agent, and\nLLM agent) in repeated Prisoner's Dilemma games. Findings show significant\ndifferences in cooperative behavior based on the agents' purported\ncharacteristics and the interaction effect of participants' genders and\npurported characteristics. We also analyzed human response patterns, including\ngame completion time, proactive favorable behavior, and acceptance of repair\nefforts. These insights offer a new perspective on human interactions with LLM\nagents in competitive cooperation contexts, such as virtual avatars or future\nphysical entities. The study underscores the importance of understanding human\nbiases toward AI agents and how observed behaviors can influence future\nhuman-AI cooperation dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs), AI agents as autonomous\ndecision-makers present significant opportunities and challenges for human-AI\ncooperation. While many studies have explored human cooperation with AI as\ntools, the role of LLM-augmented autonomous agents in competitive-cooperative\ninteractions remains under-examined. This study investigates human cooperative\nbehavior by engaging 30 participants who interacted with LLM agents exhibiting\ndifferent characteristics (purported human, purported rule-based AI agent, and\nLLM agent) in repeated Prisoner's Dilemma games. Findings show significant\ndifferences in cooperative behavior based on the agents' purported\ncharacteristics and the interaction effect of participants' genders and\npurported characteristics. We also analyzed human response patterns, including\ngame completion time, proactive favorable behavior, and acceptance of repair\nefforts. These insights offer a new perspective on human interactions with LLM\nagents in competitive cooperation contexts, such as virtual avatars or future\nphysical entities. The study underscores the importance of understanding human\nbiases toward AI agents and how observed behaviors can influence future\nhuman-AI cooperation dynamics."
                },
                "authors": [
                    {
                        "name": "Guanxuan Jiang"
                    },
                    {
                        "name": "Yuyang Wang"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13654v2",
                "updated": "2025-03-10T13:37:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    37,
                    13,
                    0,
                    69,
                    0
                ],
                "published": "2024-12-18T09:33:20Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    33,
                    20,
                    2,
                    353,
                    0
                ],
                "title": "GAGS: Granularity-Aware Feature Distillation for Language Gaussian\n  Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAGS: Granularity-Aware Feature Distillation for Language Gaussian\n  Splatting"
                },
                "summary": "3D open-vocabulary scene understanding, which accurately perceives complex\nsemantic properties of objects in space, has gained significant attention in\nrecent years. In this paper, we propose GAGS, a framework that distills 2D CLIP\nfeatures into 3D Gaussian splatting, enabling open-vocabulary queries for\nrenderings on arbitrary viewpoints. The main challenge of distilling 2D\nfeatures for 3D fields lies in the multiview inconsistency of extracted 2D\nfeatures, which provides unstable supervision for the 3D feature field. GAGS\naddresses this challenge with two novel strategies. First, GAGS associates the\nprompt point density of SAM with the camera distances, which significantly\nimproves the multiview consistency of segmentation results. Second, GAGS\nfurther decodes a granularity factor to guide the distillation process and this\ngranularity factor can be learned in a unsupervised manner to only select the\nmultiview consistent 2D features in the distillation process. Experimental\nresults on two datasets demonstrate significant performance and stability\nimprovements of GAGS in visual grounding and semantic segmentation, with an\ninference speed 2$\\times$ faster than baseline methods. The code and additional\nresults are available at https://pz0826.github.io/GAGS-Webpage/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D open-vocabulary scene understanding, which accurately perceives complex\nsemantic properties of objects in space, has gained significant attention in\nrecent years. In this paper, we propose GAGS, a framework that distills 2D CLIP\nfeatures into 3D Gaussian splatting, enabling open-vocabulary queries for\nrenderings on arbitrary viewpoints. The main challenge of distilling 2D\nfeatures for 3D fields lies in the multiview inconsistency of extracted 2D\nfeatures, which provides unstable supervision for the 3D feature field. GAGS\naddresses this challenge with two novel strategies. First, GAGS associates the\nprompt point density of SAM with the camera distances, which significantly\nimproves the multiview consistency of segmentation results. Second, GAGS\nfurther decodes a granularity factor to guide the distillation process and this\ngranularity factor can be learned in a unsupervised manner to only select the\nmultiview consistent 2D features in the distillation process. Experimental\nresults on two datasets demonstrate significant performance and stability\nimprovements of GAGS in visual grounding and semantic segmentation, with an\ninference speed 2$\\times$ faster than baseline methods. The code and additional\nresults are available at https://pz0826.github.io/GAGS-Webpage/ ."
                },
                "authors": [
                    {
                        "name": "Yuning Peng"
                    },
                    {
                        "name": "Haiping Wang"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Chenglu Wen"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Bisheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bisheng Yang"
                },
                "author": "Bisheng Yang",
                "arxiv_comment": "Project page: https://pz0826.github.io/GAGS-Webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07317v1",
                "updated": "2025-03-10T13:35:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    35,
                    51,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:35:51Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    35,
                    51,
                    0,
                    69,
                    0
                ],
                "title": "Self-Corrective Task Planning by Inverse Prompting with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Corrective Task Planning by Inverse Prompting with Large Language\n  Models"
                },
                "summary": "In robot task planning, large language models (LLMs) have shown significant\npromise in generating complex and long-horizon action sequences. However, it is\nobserved that LLMs often produce responses that sound plausible but are not\naccurate. To address these problems, existing methods typically employ\npredefined error sets or external knowledge sources, requiring human efforts\nand computation resources. Recently, self-correction approaches have emerged,\nwhere LLM generates and refines plans, identifying errors by itself. Despite\ntheir effectiveness, they are more prone to failures in correction due to\ninsufficient reasoning. In this paper, we introduce InversePrompt, a novel\nself-corrective task planning approach that leverages inverse prompting to\nenhance interpretability. Our method incorporates reasoning steps to provide\nclear, interpretable feedback. It generates inverse actions corresponding to\nthe initially generated actions and verifies whether these inverse actions can\nrestore the system to its original state, explicitly validating the logical\ncoherence of the generated plans. The results on benchmark datasets show an\naverage 16.3% higher success rate over existing LLM-based task planning\nmethods. Our approach offers clearer justifications for feedback in real-world\nenvironments, resulting in more successful task completion than existing\nself-correction approaches across various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robot task planning, large language models (LLMs) have shown significant\npromise in generating complex and long-horizon action sequences. However, it is\nobserved that LLMs often produce responses that sound plausible but are not\naccurate. To address these problems, existing methods typically employ\npredefined error sets or external knowledge sources, requiring human efforts\nand computation resources. Recently, self-correction approaches have emerged,\nwhere LLM generates and refines plans, identifying errors by itself. Despite\ntheir effectiveness, they are more prone to failures in correction due to\ninsufficient reasoning. In this paper, we introduce InversePrompt, a novel\nself-corrective task planning approach that leverages inverse prompting to\nenhance interpretability. Our method incorporates reasoning steps to provide\nclear, interpretable feedback. It generates inverse actions corresponding to\nthe initially generated actions and verifies whether these inverse actions can\nrestore the system to its original state, explicitly validating the logical\ncoherence of the generated plans. The results on benchmark datasets show an\naverage 16.3% higher success rate over existing LLM-based task planning\nmethods. Our approach offers clearer justifications for feedback in real-world\nenvironments, resulting in more successful task completion than existing\nself-correction approaches across various scenarios."
                },
                "authors": [
                    {
                        "name": "Jiho Lee"
                    },
                    {
                        "name": "Hayun Lee"
                    },
                    {
                        "name": "Jonghyeon Kim"
                    },
                    {
                        "name": "Kyungjae Lee"
                    },
                    {
                        "name": "Eunwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Eunwoo Kim"
                },
                "author": "Eunwoo Kim",
                "arxiv_comment": "7 pages, 5 figures, IEEE International Conference on Robotics and\n  Automation (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07314v1",
                "updated": "2025-03-10T13:33:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    33,
                    27,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:33:27Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    33,
                    27,
                    0,
                    69,
                    0
                ],
                "title": "Automated Movie Generation via Multi-Agent CoT Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Movie Generation via Multi-Agent CoT Planning"
                },
                "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent."
                },
                "authors": [
                    {
                        "name": "Weijia Wu"
                    },
                    {
                        "name": "Zeyu Zhu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "The code and project website are available at:\n  https://github.com/showlab/MovieAgent and\n  https://weijiawu.github.io/MovieAgent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07307v1",
                "updated": "2025-03-10T13:28:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    28,
                    36,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:28:36Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    28,
                    36,
                    0,
                    69,
                    0
                ],
                "title": "AttenST: A Training-Free Attention-Driven Style Transfer Framework with\n  Pre-Trained Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttenST: A Training-Free Attention-Driven Style Transfer Framework with\n  Pre-Trained Diffusion Models"
                },
                "summary": "While diffusion models have achieved remarkable progress in style transfer\ntasks, existing methods typically rely on fine-tuning or optimizing pre-trained\nmodels during inference, leading to high computational costs and challenges in\nbalancing content preservation with style integration. To address these\nlimitations, we introduce AttenST, a training-free attention-driven style\ntransfer framework. Specifically, we propose a style-guided self-attention\nmechanism that conditions self-attention on the reference style by retaining\nthe query of the content image while substituting its key and value with those\nfrom the style image, enabling effective style feature integration. To mitigate\nstyle information loss during inversion, we introduce a style-preserving\ninversion strategy that refines inversion accuracy through multiple resampling\nsteps. Additionally, we propose a content-aware adaptive instance\nnormalization, which integrates content statistics into the normalization\nprocess to optimize style fusion while mitigating the content degradation.\nFurthermore, we introduce a dual-feature cross-attention mechanism to fuse\ncontent and style features, ensuring a harmonious synthesis of structural\nfidelity and stylistic expression. Extensive experiments demonstrate that\nAttenST outperforms existing methods, achieving state-of-the-art performance in\nstyle transfer dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion models have achieved remarkable progress in style transfer\ntasks, existing methods typically rely on fine-tuning or optimizing pre-trained\nmodels during inference, leading to high computational costs and challenges in\nbalancing content preservation with style integration. To address these\nlimitations, we introduce AttenST, a training-free attention-driven style\ntransfer framework. Specifically, we propose a style-guided self-attention\nmechanism that conditions self-attention on the reference style by retaining\nthe query of the content image while substituting its key and value with those\nfrom the style image, enabling effective style feature integration. To mitigate\nstyle information loss during inversion, we introduce a style-preserving\ninversion strategy that refines inversion accuracy through multiple resampling\nsteps. Additionally, we propose a content-aware adaptive instance\nnormalization, which integrates content statistics into the normalization\nprocess to optimize style fusion while mitigating the content degradation.\nFurthermore, we introduce a dual-feature cross-attention mechanism to fuse\ncontent and style features, ensuring a harmonious synthesis of structural\nfidelity and stylistic expression. Extensive experiments demonstrate that\nAttenST outperforms existing methods, achieving state-of-the-art performance in\nstyle transfer dataset."
                },
                "authors": [
                    {
                        "name": "Bo Huang"
                    },
                    {
                        "name": "Wenlun Xu"
                    },
                    {
                        "name": "Qizhuo Han"
                    },
                    {
                        "name": "Haodong Jing"
                    },
                    {
                        "name": "Ying Li"
                    }
                ],
                "author_detail": {
                    "name": "Ying Li"
                },
                "author": "Ying Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07306v1",
                "updated": "2025-03-10T13:28:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    28,
                    25,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:28:25Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    28,
                    25,
                    0,
                    69,
                    0
                ],
                "title": "Benchmarking Chinese Medical LLMs: A Medbench-based Analysis of\n  Performance Gaps and Hierarchical Optimization Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Chinese Medical LLMs: A Medbench-based Analysis of\n  Performance Gaps and Hierarchical Optimization Strategies"
                },
                "summary": "The evaluation and improvement of medical large language models (LLMs) are\ncritical for their real-world deployment, particularly in ensuring accuracy,\nsafety, and ethical alignment. Existing frameworks inadequately dissect\ndomain-specific error patterns or address cross-modal challenges. This study\nintroduces a granular error taxonomy through systematic analysis of top 10\nmodels on MedBench, categorizing incorrect responses into eight types:\nOmissions, Hallucination, Format Mismatch, Causal Reasoning Deficiency,\nContextual Inconsistency, Unanswered, Output Error, and Deficiency in Medical\nLanguage Generation. Evaluation of 10 leading models reveals vulnerabilities:\ndespite achieving 0.86 accuracy in medical knowledge recall, critical reasoning\ntasks show 96.3% omission, while safety ethics evaluations expose alarming\ninconsistency (robustness score: 0.79) under option shuffled. Our analysis\nuncovers systemic weaknesses in knowledge boundary enforcement and multi-step\nreasoning. To address these, we propose a tiered optimization strategy spanning\nfour levels, from prompt engineering and knowledge-augmented retrieval to\nhybrid neuro-symbolic architectures and causal reasoning frameworks. This work\nestablishes an actionable roadmap for developing clinically robust LLMs while\nredefining evaluation paradigms through error-driven insights, ultimately\nadvancing the safety and trustworthiness of AI in high-stakes medical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation and improvement of medical large language models (LLMs) are\ncritical for their real-world deployment, particularly in ensuring accuracy,\nsafety, and ethical alignment. Existing frameworks inadequately dissect\ndomain-specific error patterns or address cross-modal challenges. This study\nintroduces a granular error taxonomy through systematic analysis of top 10\nmodels on MedBench, categorizing incorrect responses into eight types:\nOmissions, Hallucination, Format Mismatch, Causal Reasoning Deficiency,\nContextual Inconsistency, Unanswered, Output Error, and Deficiency in Medical\nLanguage Generation. Evaluation of 10 leading models reveals vulnerabilities:\ndespite achieving 0.86 accuracy in medical knowledge recall, critical reasoning\ntasks show 96.3% omission, while safety ethics evaluations expose alarming\ninconsistency (robustness score: 0.79) under option shuffled. Our analysis\nuncovers systemic weaknesses in knowledge boundary enforcement and multi-step\nreasoning. To address these, we propose a tiered optimization strategy spanning\nfour levels, from prompt engineering and knowledge-augmented retrieval to\nhybrid neuro-symbolic architectures and causal reasoning frameworks. This work\nestablishes an actionable roadmap for developing clinically robust LLMs while\nredefining evaluation paradigms through error-driven insights, ultimately\nadvancing the safety and trustworthiness of AI in high-stakes medical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Luyi Jiang"
                    },
                    {
                        "name": "Jiayuan Chen"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Xinwei Peng"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Jie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Xu"
                },
                "author": "Jie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07303v1",
                "updated": "2025-03-10T13:24:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    24,
                    46,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:24:46Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    24,
                    46,
                    0,
                    69,
                    0
                ],
                "title": "An Information-Theoretic Approach to Identifying Formulaic Clusters in\n  Textual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Information-Theoretic Approach to Identifying Formulaic Clusters in\n  Textual Data"
                },
                "summary": "Texts, whether literary or historical, exhibit structural and stylistic\npatterns shaped by their purpose, authorship, and cultural context. Formulaic\ntexts, characterized by repetition and constrained expression, tend to have\nlower variability in self-information compared to more dynamic compositions.\nIdentifying such patterns in historical documents, particularly multi-author\ntexts like the Hebrew Bible provides insights into their origins, purpose, and\ntransmission.\n  This study aims to identify formulaic clusters -- sections exhibiting\nsystematic repetition and structural constraints -- by analyzing recurring\nphrases, syntactic structures, and stylistic markers. However, distinguishing\nformulaic from non-formulaic elements in an unsupervised manner presents a\ncomputational challenge, especially in high-dimensional textual spaces where\npatterns must be inferred without predefined labels.\n  To address this, we develop an information-theoretic algorithm leveraging\nweighted self-information distributions to detect structured patterns in text,\nunlike covariance-based methods, which become unstable in small-sample,\nhigh-dimensional settings, our approach directly models variations in\nself-information to identify formulaicity. By extending classical discrete\nself-information measures with a continuous formulation based on differential\nself-information, our method remains applicable across different types of\ntextual representations, including neural embeddings under Gaussian priors.\n  Applied to hypothesized authorial divisions in the Hebrew Bible, our approach\nsuccessfully isolates stylistic layers, providing a quantitative framework for\ntextual stratification. This method enhances our ability to analyze\ncompositional patterns, offering deeper insights into the literary and cultural\nevolution of texts shaped by complex authorship and editorial processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Texts, whether literary or historical, exhibit structural and stylistic\npatterns shaped by their purpose, authorship, and cultural context. Formulaic\ntexts, characterized by repetition and constrained expression, tend to have\nlower variability in self-information compared to more dynamic compositions.\nIdentifying such patterns in historical documents, particularly multi-author\ntexts like the Hebrew Bible provides insights into their origins, purpose, and\ntransmission.\n  This study aims to identify formulaic clusters -- sections exhibiting\nsystematic repetition and structural constraints -- by analyzing recurring\nphrases, syntactic structures, and stylistic markers. However, distinguishing\nformulaic from non-formulaic elements in an unsupervised manner presents a\ncomputational challenge, especially in high-dimensional textual spaces where\npatterns must be inferred without predefined labels.\n  To address this, we develop an information-theoretic algorithm leveraging\nweighted self-information distributions to detect structured patterns in text,\nunlike covariance-based methods, which become unstable in small-sample,\nhigh-dimensional settings, our approach directly models variations in\nself-information to identify formulaicity. By extending classical discrete\nself-information measures with a continuous formulation based on differential\nself-information, our method remains applicable across different types of\ntextual representations, including neural embeddings under Gaussian priors.\n  Applied to hypothesized authorial divisions in the Hebrew Bible, our approach\nsuccessfully isolates stylistic layers, providing a quantitative framework for\ntextual stratification. This method enhances our ability to analyze\ncompositional patterns, offering deeper insights into the literary and cultural\nevolution of texts shaped by complex authorship and editorial processes."
                },
                "authors": [
                    {
                        "name": "Gideon Yoffe"
                    },
                    {
                        "name": "Yair Segev"
                    },
                    {
                        "name": "Barak Sober"
                    }
                ],
                "author_detail": {
                    "name": "Barak Sober"
                },
                "author": "Barak Sober",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06096v2",
                "updated": "2025-03-10T13:20:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    20,
                    58,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-10T02:01:30Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    2,
                    1,
                    30,
                    0,
                    41,
                    0
                ],
                "title": "Post-detection inference for sequential changepoint localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-detection inference for sequential changepoint localization"
                },
                "summary": "This paper addresses a fundamental but largely unexplored challenge in\nsequential changepoint analysis: conducting inference following a detected\nchange. We study the problem of localizing the changepoint using only the data\nobserved up to a data-dependent stopping time at which a sequential detection\nalgorithm $\\mathcal A$ declares a change. We first construct confidence sets\nfor the unknown changepoint when pre- and post-change distributions are assumed\nto be known. We then extend our framework to composite pre- and post-change\nscenarios. We impose no conditions on the observation space or on $\\mathcal A$\n-- we only need to be able to run $\\mathcal A$ on simulated data sequences. In\nsummary, this work offers both theoretically sound and practically effective\ntools for sequential changepoint localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses a fundamental but largely unexplored challenge in\nsequential changepoint analysis: conducting inference following a detected\nchange. We study the problem of localizing the changepoint using only the data\nobserved up to a data-dependent stopping time at which a sequential detection\nalgorithm $\\mathcal A$ declares a change. We first construct confidence sets\nfor the unknown changepoint when pre- and post-change distributions are assumed\nto be known. We then extend our framework to composite pre- and post-change\nscenarios. We impose no conditions on the observation space or on $\\mathcal A$\n-- we only need to be able to run $\\mathcal A$ on simulated data sequences. In\nsummary, this work offers both theoretically sound and practically effective\ntools for sequential changepoint localization."
                },
                "authors": [
                    {
                        "name": "Aytijhya Saha"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    }
                ],
                "author_detail": {
                    "name": "Aaditya Ramdas"
                },
                "author": "Aaditya Ramdas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07298v1",
                "updated": "2025-03-10T13:18:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    18,
                    5,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:18:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    18,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "ALLVB: All-in-One Long Video Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLVB: All-in-One Long Video Understanding Benchmark"
                },
                "summary": "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding."
                },
                "authors": [
                    {
                        "name": "Xichen Tan"
                    },
                    {
                        "name": "Yuanjing Luo"
                    },
                    {
                        "name": "Yunfan Ye"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Zhiping Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhiping Cai"
                },
                "author": "Zhiping Cai",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21037v2",
                "updated": "2025-03-10T13:02:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    2,
                    48,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-28T13:29:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    29,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "The amplifier effect of artificial agents in social contagion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The amplifier effect of artificial agents in social contagion"
                },
                "summary": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways."
                },
                "authors": [
                    {
                        "name": "Eric Hitz"
                    },
                    {
                        "name": "Mingmin Feng"
                    },
                    {
                        "name": "Radu Tanase"
                    },
                    {
                        "name": "René Algesheimer"
                    },
                    {
                        "name": "Manuel S. Mariani"
                    }
                ],
                "author_detail": {
                    "name": "Manuel S. Mariani"
                },
                "author": "Manuel S. Mariani",
                "arxiv_comment": "Main text pp. 1-4; Supplementary Material pp. 5-10",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07282v1",
                "updated": "2025-03-10T13:02:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    2,
                    29,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:02:29Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    2,
                    29,
                    0,
                    69,
                    0
                ],
                "title": "A Graph-based Verification Framework for Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-based Verification Framework for Fact-Checking"
                },
                "summary": "Fact-checking plays a crucial role in combating misinformation. Existing\nmethods using large language models (LLMs) for claim decomposition face two key\nlimitations: (1) insufficient decomposition, introducing unnecessary complexity\nto the verification process, and (2) ambiguity of mentions, leading to\nincorrect verification results. To address these challenges, we suggest\nintroducing a claim graph consisting of triplets to address the insufficient\ndecomposition problem and reduce mention ambiguity through graph structure.\nBased on this core idea, we propose a graph-based framework, GraphFC, for\nfact-checking. The framework features three key components: graph construction,\nwhich builds both claim and evidence graphs; graph-guided planning, which\nprioritizes the triplet verification order; and graph-guided checking, which\nverifies the triples one by one between claim and evidence graphs. Extensive\nexperiments show that GraphFC enables fine-grained decomposition while\nresolving referential ambiguities through relational constraints, achieving\nstate-of-the-art performance across three datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking plays a crucial role in combating misinformation. Existing\nmethods using large language models (LLMs) for claim decomposition face two key\nlimitations: (1) insufficient decomposition, introducing unnecessary complexity\nto the verification process, and (2) ambiguity of mentions, leading to\nincorrect verification results. To address these challenges, we suggest\nintroducing a claim graph consisting of triplets to address the insufficient\ndecomposition problem and reduce mention ambiguity through graph structure.\nBased on this core idea, we propose a graph-based framework, GraphFC, for\nfact-checking. The framework features three key components: graph construction,\nwhich builds both claim and evidence graphs; graph-guided planning, which\nprioritizes the triplet verification order; and graph-guided checking, which\nverifies the triples one by one between claim and evidence graphs. Extensive\nexperiments show that GraphFC enables fine-grained decomposition while\nresolving referential ambiguities through relational constraints, achieving\nstate-of-the-art performance across three datasets."
                },
                "authors": [
                    {
                        "name": "Yani Huang"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Junfan Chen"
                    },
                    {
                        "name": "Xuefeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuefeng Zhang"
                },
                "author": "Xuefeng Zhang",
                "arxiv_comment": "13pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04479v2",
                "updated": "2025-03-10T13:01:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    1,
                    58,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-06T14:29:52Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    29,
                    52,
                    3,
                    65,
                    0
                ],
                "title": "ToolFuzz -- Automated Agent Tool Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolFuzz -- Automated Agent Tool Testing"
                },
                "summary": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents."
                },
                "authors": [
                    {
                        "name": "Ivan Milev"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04299v2",
                "updated": "2025-03-10T13:00:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    0,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-06T10:39:47Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    39,
                    47,
                    3,
                    65,
                    0
                ],
                "title": "Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation"
                },
                "summary": "The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment."
                },
                "authors": [
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Pierre-François Gimenez"
                    },
                    {
                        "name": "Simeon Campos"
                    }
                ],
                "author_detail": {
                    "name": "Simeon Campos"
                },
                "author": "Simeon Campos",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07276v1",
                "updated": "2025-03-10T12:57:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    57,
                    43,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:57:43Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    57,
                    43,
                    0,
                    69,
                    0
                ],
                "title": "A Systematic Review of ECG Arrhythmia Classification: Adherence to\n  Standards, Fair Evaluation, and Embedded Feasibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Review of ECG Arrhythmia Classification: Adherence to\n  Standards, Fair Evaluation, and Embedded Feasibility"
                },
                "summary": "The classification of electrocardiogram (ECG) signals is crucial for early\ndetection of arrhythmias and other cardiac conditions. However, despite\nadvances in machine learning, many studies fail to follow standardization\nprotocols, leading to inconsistencies in performance evaluation and real-world\napplicability. Additionally, hardware constraints essential for practical\ndeployment, such as in pacemakers, Holter monitors, and wearable ECG patches,\nare often overlooked. Since real-world impact depends on feasibility in\nresource-constrained devices, ensuring efficient deployment is critical for\ncontinuous monitoring. This review systematically analyzes ECG classification\nstudies published between 2017 and 2024, focusing on those adhering to the E3C\n(Embedded, Clinical, and Comparative Criteria), which include inter-patient\nparadigm implementation, compliance with Association for the Advancement of\nMedical Instrumentation (AAMI) recommendations, and model feasibility for\nembedded systems. While many studies report high accuracy, few properly\nconsider patient-independent partitioning and hardware limitations. We identify\nstate-of-the-art methods meeting E3C criteria and conduct a comparative\nanalysis of accuracy, inference time, energy consumption, and memory usage.\nFinally, we propose standardized reporting practices to ensure fair comparisons\nand practical applicability of ECG classification models. By addressing these\ngaps, this study aims to guide future research toward more robust and\nclinically viable ECG classification systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classification of electrocardiogram (ECG) signals is crucial for early\ndetection of arrhythmias and other cardiac conditions. However, despite\nadvances in machine learning, many studies fail to follow standardization\nprotocols, leading to inconsistencies in performance evaluation and real-world\napplicability. Additionally, hardware constraints essential for practical\ndeployment, such as in pacemakers, Holter monitors, and wearable ECG patches,\nare often overlooked. Since real-world impact depends on feasibility in\nresource-constrained devices, ensuring efficient deployment is critical for\ncontinuous monitoring. This review systematically analyzes ECG classification\nstudies published between 2017 and 2024, focusing on those adhering to the E3C\n(Embedded, Clinical, and Comparative Criteria), which include inter-patient\nparadigm implementation, compliance with Association for the Advancement of\nMedical Instrumentation (AAMI) recommendations, and model feasibility for\nembedded systems. While many studies report high accuracy, few properly\nconsider patient-independent partitioning and hardware limitations. We identify\nstate-of-the-art methods meeting E3C criteria and conduct a comparative\nanalysis of accuracy, inference time, energy consumption, and memory usage.\nFinally, we propose standardized reporting practices to ensure fair comparisons\nand practical applicability of ECG classification models. By addressing these\ngaps, this study aims to guide future research toward more robust and\nclinically viable ECG classification systems."
                },
                "authors": [
                    {
                        "name": "Guilherme Silva"
                    },
                    {
                        "name": "Pedro Silva"
                    },
                    {
                        "name": "Gladston Moreira"
                    },
                    {
                        "name": "Vander Freitas"
                    },
                    {
                        "name": "Jadson Gertrudes"
                    },
                    {
                        "name": "Eduardo Luz"
                    }
                ],
                "author_detail": {
                    "name": "Eduardo Luz"
                },
                "author": "Eduardo Luz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05870v4",
                "updated": "2025-03-10T12:56:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    56,
                    54,
                    0,
                    69,
                    0
                ],
                "published": "2024-06-09T17:55:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    17,
                    55,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents"
                },
                "summary": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database and applying an LLM to the\nretrieved documents. We demonstrate that RAG systems that operate on databases\nwith untrusted content are vulnerable to denial-of-service attacks we call\njamming. An adversary can add a single ``blocker'' document to the database\nthat will be retrieved in response to a specific query and result in the RAG\nsystem not answering this query, ostensibly because it lacks relevant\ninformation or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. Our\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not employ an auxiliary LLM.\n  We evaluate jamming attacks on several embeddings and LLMs and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database and applying an LLM to the\nretrieved documents. We demonstrate that RAG systems that operate on databases\nwith untrusted content are vulnerable to denial-of-service attacks we call\njamming. An adversary can add a single ``blocker'' document to the database\nthat will be retrieved in response to a specific query and result in the RAG\nsystem not answering this query, ostensibly because it lacks relevant\ninformation or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. Our\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not employ an auxiliary LLM.\n  We evaluate jamming attacks on several embeddings and LLMs and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents."
                },
                "authors": [
                    {
                        "name": "Avital Shafran"
                    },
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07274v1",
                "updated": "2025-03-10T12:55:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    55,
                    8,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:55:08Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    55,
                    8,
                    0,
                    69,
                    0
                ],
                "title": "Efficient Distillation of Classifier-Free Guidance using Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distillation of Classifier-Free Guidance using Adapters"
                },
                "summary": "While classifier-free guidance (CFG) is essential for conditional diffusion\nmodels, it doubles the number of neural function evaluations (NFEs) per\ninference step. To mitigate this inefficiency, we introduce adapter guidance\ndistillation (AGD), a novel approach that simulates CFG in a single forward\npass. AGD leverages lightweight adapters to approximate CFG, effectively\ndoubling the sampling speed while maintaining or even improving sample quality.\nUnlike prior guidance distillation methods that tune the entire model, AGD\nkeeps the base model frozen and only trains minimal additional parameters\n($\\sim$2%) to significantly reduce the resource requirement of the distillation\nphase. Additionally, this approach preserves the original model weights and\nenables the adapters to be seamlessly combined with other checkpoints derived\nfrom the same base model. We also address a key mismatch between training and\ninference in existing guidance distillation methods by training on CFG-guided\ntrajectories instead of standard diffusion trajectories. Through extensive\nexperiments, we show that AGD achieves comparable or superior FID to CFG across\nmultiple architectures with only half the NFEs. Notably, our method enables the\ndistillation of large models ($\\sim$2.6B parameters) on a single consumer GPU\nwith 24 GB of VRAM, making it more accessible than previous approaches that\nrequire multiple high-end GPUs. We will publicly release the implementation of\nour method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While classifier-free guidance (CFG) is essential for conditional diffusion\nmodels, it doubles the number of neural function evaluations (NFEs) per\ninference step. To mitigate this inefficiency, we introduce adapter guidance\ndistillation (AGD), a novel approach that simulates CFG in a single forward\npass. AGD leverages lightweight adapters to approximate CFG, effectively\ndoubling the sampling speed while maintaining or even improving sample quality.\nUnlike prior guidance distillation methods that tune the entire model, AGD\nkeeps the base model frozen and only trains minimal additional parameters\n($\\sim$2%) to significantly reduce the resource requirement of the distillation\nphase. Additionally, this approach preserves the original model weights and\nenables the adapters to be seamlessly combined with other checkpoints derived\nfrom the same base model. We also address a key mismatch between training and\ninference in existing guidance distillation methods by training on CFG-guided\ntrajectories instead of standard diffusion trajectories. Through extensive\nexperiments, we show that AGD achieves comparable or superior FID to CFG across\nmultiple architectures with only half the NFEs. Notably, our method enables the\ndistillation of large models ($\\sim$2.6B parameters) on a single consumer GPU\nwith 24 GB of VRAM, making it more accessible than previous approaches that\nrequire multiple high-end GPUs. We will publicly release the implementation of\nour method."
                },
                "authors": [
                    {
                        "name": "Cristian Perez Jensen"
                    },
                    {
                        "name": "Seyedmorteza Sadat"
                    }
                ],
                "author_detail": {
                    "name": "Seyedmorteza Sadat"
                },
                "author": "Seyedmorteza Sadat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07258v1",
                "updated": "2025-03-10T12:41:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    41,
                    19,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:41:19Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    41,
                    19,
                    0,
                    69,
                    0
                ],
                "title": "MC-GRU:a Multi-Channel GRU network for generalized nonlinear structural\n  response prediction across structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-GRU:a Multi-Channel GRU network for generalized nonlinear structural\n  response prediction across structures"
                },
                "summary": "Accurate prediction of seismic responses and quantification of structural\ndamage are critical in civil engineering. Traditional approaches such as finite\nelement analysis could lack computational efficiency, especially for complex\nstructural systems under extreme hazards. Recently, artificial intelligence has\nprovided an alternative to efficiently model highly nonlinear behaviors.\nHowever, existing models face challenges in generalizing across diverse\nstructural systems. This paper proposes a novel multi-channel gated recurrent\nunit (MC-GRU) network aimed at achieving generalized nonlinear structural\nresponse prediction for varying structures. The key concept lies in the\nintegration of a multi-channel input mechanism to GRU with an extra input of\nstructural information to the candidate hidden state, which enables the network\nto learn the dynamic characteristics of diverse structures and thus empower the\ngeneralizability and adaptiveness to unseen structures. The performance of the\nproposed MC-GRU is validated through a series of case studies, including a\nsingle-degree-of-freedom linear system, a hysteretic Bouc-Wen system, and a\nnonlinear reinforced concrete column from experimental testing. Results\nindicate that the proposed MC-GRU overcomes the major generalizability issues\nof existing methods, with capability of accurately inferring seismic responses\nof varying structures. Additionally, it demonstrates enhanced capabilities in\nrepresenting nonlinear structural dynamics compared to traditional models such\nas GRU and LSTM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of seismic responses and quantification of structural\ndamage are critical in civil engineering. Traditional approaches such as finite\nelement analysis could lack computational efficiency, especially for complex\nstructural systems under extreme hazards. Recently, artificial intelligence has\nprovided an alternative to efficiently model highly nonlinear behaviors.\nHowever, existing models face challenges in generalizing across diverse\nstructural systems. This paper proposes a novel multi-channel gated recurrent\nunit (MC-GRU) network aimed at achieving generalized nonlinear structural\nresponse prediction for varying structures. The key concept lies in the\nintegration of a multi-channel input mechanism to GRU with an extra input of\nstructural information to the candidate hidden state, which enables the network\nto learn the dynamic characteristics of diverse structures and thus empower the\ngeneralizability and adaptiveness to unseen structures. The performance of the\nproposed MC-GRU is validated through a series of case studies, including a\nsingle-degree-of-freedom linear system, a hysteretic Bouc-Wen system, and a\nnonlinear reinforced concrete column from experimental testing. Results\nindicate that the proposed MC-GRU overcomes the major generalizability issues\nof existing methods, with capability of accurately inferring seismic responses\nof varying structures. Additionally, it demonstrates enhanced capabilities in\nrepresenting nonlinear structural dynamics compared to traditional models such\nas GRU and LSTM."
                },
                "authors": [
                    {
                        "name": "Shan He"
                    },
                    {
                        "name": "Ruiyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiyang Zhang"
                },
                "author": "Ruiyang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07256v1",
                "updated": "2025-03-10T12:38:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    38,
                    10,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:38:10Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    38,
                    10,
                    0,
                    69,
                    0
                ],
                "title": "Exploring $Δ$-resonance in neutron stars: implications from\n  astrophysical and nuclear observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring $Δ$-resonance in neutron stars: implications from\n  astrophysical and nuclear observations"
                },
                "summary": "This study presents the first comprehensive Bayesian inference of neutron\nstar matter, incorporating $\\Delta$-resonances alongside hyperons and nucleons\nwithin a density-dependent relativistic hadron (DDRH) framework. Using\nconstraints from nuclear saturation properties, chiral effective field theory\n($\\chi$EFT), NICER radius measurements, and tidal deformability data from\nGW170817, we systematically explore the impact of $\\Delta$-resonances on the\nequation of state (EoS) of dense matter and neutron star observables. Our\nresults demonstrate that the inclusion of $\\Delta$-baryons softens the EoS at\nlow densities while maintaining sufficient stiffness at high densities to\nsupport $2M_{\\odot}$ neutron stars. This naturally reconciles neutron star\nradius constraints with the recent observation of the low-mass compact object\nin HESS J1731-347 while simultaneously exhibiting excellent agreement with\nGW170817 tidal deformability constraints, reinforcing the astrophysical\nviability of $\\Delta$-admixed neutron stars. Additionally, $\\Delta$-resonances\nare found to populate the outer layers of the neutron star core, which may have\nimplications for neutron star mergers and their cooling. Furthermore, we show\nthat the presence of $\\Delta$-baryons might significantly influence neutron\nstar cooling via the direct Urca process. We also investigate quasi-normal\n$f$-mode oscillations within a fully general relativistic framework, revealing\nstrong correlations between the $f$-mode frequency, neutron star compactness,\nand tidal deformability. With the inclusion of $\\Delta$-resonances and\nadherence to astrophysical constraints, we obtain $f_{1.4} =\n1.97^{+0.17}_{-0.22}$ kHz and the damping time $\\tau_{f_{1.4}} =\n0.19^{+0.05}_{-0.03}$ s at the $1\\sigma$ confidence level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents the first comprehensive Bayesian inference of neutron\nstar matter, incorporating $\\Delta$-resonances alongside hyperons and nucleons\nwithin a density-dependent relativistic hadron (DDRH) framework. Using\nconstraints from nuclear saturation properties, chiral effective field theory\n($\\chi$EFT), NICER radius measurements, and tidal deformability data from\nGW170817, we systematically explore the impact of $\\Delta$-resonances on the\nequation of state (EoS) of dense matter and neutron star observables. Our\nresults demonstrate that the inclusion of $\\Delta$-baryons softens the EoS at\nlow densities while maintaining sufficient stiffness at high densities to\nsupport $2M_{\\odot}$ neutron stars. This naturally reconciles neutron star\nradius constraints with the recent observation of the low-mass compact object\nin HESS J1731-347 while simultaneously exhibiting excellent agreement with\nGW170817 tidal deformability constraints, reinforcing the astrophysical\nviability of $\\Delta$-admixed neutron stars. Additionally, $\\Delta$-resonances\nare found to populate the outer layers of the neutron star core, which may have\nimplications for neutron star mergers and their cooling. Furthermore, we show\nthat the presence of $\\Delta$-baryons might significantly influence neutron\nstar cooling via the direct Urca process. We also investigate quasi-normal\n$f$-mode oscillations within a fully general relativistic framework, revealing\nstrong correlations between the $f$-mode frequency, neutron star compactness,\nand tidal deformability. With the inclusion of $\\Delta$-resonances and\nadherence to astrophysical constraints, we obtain $f_{1.4} =\n1.97^{+0.17}_{-0.22}$ kHz and the damping time $\\tau_{f_{1.4}} =\n0.19^{+0.05}_{-0.03}$ s at the $1\\sigma$ confidence level."
                },
                "authors": [
                    {
                        "name": "Vishal Parmar"
                    },
                    {
                        "name": "Vivek Baruah Thapa"
                    },
                    {
                        "name": "Monika Sinha"
                    },
                    {
                        "name": "Ignazio Bombaci"
                    }
                ],
                "author_detail": {
                    "name": "Ignazio Bombaci"
                },
                "author": "Ignazio Bombaci",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20076v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20076v5",
                "updated": "2025-03-10T12:34:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    34,
                    24,
                    0,
                    69,
                    0
                ],
                "published": "2024-06-28T17:38:18Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    38,
                    18,
                    4,
                    180,
                    0
                ],
                "title": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model"
                },
                "summary": "Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Lianghui Zhu"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Heng Liu"
                    },
                    {
                        "name": "Longjin Ran"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Preprint. Update: (1) better performance and (2) versatile\n  segmentation. Code and models are available at:\n  https://github.com/hustvl/EVF-SAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20076v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20076v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09481v2",
                "updated": "2025-03-10T12:27:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    27,
                    10,
                    0,
                    69,
                    0
                ],
                "published": "2025-01-16T11:35:22Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    11,
                    35,
                    22,
                    3,
                    16,
                    0
                ],
                "title": "MonoSOWA: Scalable monocular 3D Object detector Without human\n  Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoSOWA: Scalable monocular 3D Object detector Without human\n  Annotations"
                },
                "summary": "Inferring object 3D position and orientation from a single RGB camera is a\nfoundational task in computer vision with many important applications.\nTraditionally, 3D object detection methods are trained in a fully-supervised\nsetup, requiring LiDAR and vast amounts of human annotations, which are\nlaborious, costly, and do not scale well with the ever-increasing amounts of\ndata being captured.\n  We present a novel method to train a 3D object detector from a single RGB\ncamera without domain-specific human annotations, making orders of magnitude\nmore data available for training. The method uses newly proposed Local Object\nMotion Model to disentangle object movement source between subsequent frames,\nis approximately 700 times faster than previous work and compensates camera\nfocal length differences to aggregate multiple datasets.\n  The method is evaluated on three public datasets, where despite using no\nhuman labels, it outperforms prior work by a significant margin. It also shows\nits versatility as a pre-training tool for fully-supervised training and shows\nthat combining pseudo-labels from multiple datasets can achieve comparable\naccuracy to using human labels from a single dataset. The source code and model\nwill be published soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring object 3D position and orientation from a single RGB camera is a\nfoundational task in computer vision with many important applications.\nTraditionally, 3D object detection methods are trained in a fully-supervised\nsetup, requiring LiDAR and vast amounts of human annotations, which are\nlaborious, costly, and do not scale well with the ever-increasing amounts of\ndata being captured.\n  We present a novel method to train a 3D object detector from a single RGB\ncamera without domain-specific human annotations, making orders of magnitude\nmore data available for training. The method uses newly proposed Local Object\nMotion Model to disentangle object movement source between subsequent frames,\nis approximately 700 times faster than previous work and compensates camera\nfocal length differences to aggregate multiple datasets.\n  The method is evaluated on three public datasets, where despite using no\nhuman labels, it outperforms prior work by a significant margin. It also shows\nits versatility as a pre-training tool for fully-supervised training and shows\nthat combining pseudo-labels from multiple datasets can achieve comparable\naccuracy to using human labels from a single dataset. The source code and model\nwill be published soon."
                },
                "authors": [
                    {
                        "name": "Jan Skvrna"
                    },
                    {
                        "name": "Lukas Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Neumann"
                },
                "author": "Lukas Neumann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07238v1",
                "updated": "2025-03-10T12:20:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    20,
                    29,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:20:29Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    20,
                    29,
                    0,
                    69,
                    0
                ],
                "title": "Learning and planning for optimal synergistic human-robot coordination\n  in manufacturing contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning and planning for optimal synergistic human-robot coordination\n  in manufacturing contexts"
                },
                "summary": "Collaborative robotics cells leverage heterogeneous agents to provide agile\nproduction solutions. Effective coordination is essential to prevent\ninefficiencies and risks for human operators working alongside robots. This\npaper proposes a human-aware task allocation and scheduling model based on\nMixed Integer Nonlinear Programming to optimize efficiency and safety starting\nfrom task planning stages. The approach exploits synergies that encode the\ncoupling effects between pairs of tasks executed in parallel by the agents,\narising from the safety constraints imposed on robot agents. These terms are\nlearned from previous executions using a Bayesian estimation; the inference of\nthe posterior probability distribution of the synergy coefficients is performed\nusing the Markov Chain Monte Carlo method. The synergy enhances task planning\nby adapting the nominal duration of the plan according to the effect of the\noperator's presence. Simulations and experimental results demonstrate that the\nproposed method produces improved human-aware task plans, reducing unuseful\ninterference between agents, increasing human-robot distance, and achieving up\nto an 18\\% reduction in process execution time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative robotics cells leverage heterogeneous agents to provide agile\nproduction solutions. Effective coordination is essential to prevent\ninefficiencies and risks for human operators working alongside robots. This\npaper proposes a human-aware task allocation and scheduling model based on\nMixed Integer Nonlinear Programming to optimize efficiency and safety starting\nfrom task planning stages. The approach exploits synergies that encode the\ncoupling effects between pairs of tasks executed in parallel by the agents,\narising from the safety constraints imposed on robot agents. These terms are\nlearned from previous executions using a Bayesian estimation; the inference of\nthe posterior probability distribution of the synergy coefficients is performed\nusing the Markov Chain Monte Carlo method. The synergy enhances task planning\nby adapting the nominal duration of the plan according to the effect of the\noperator's presence. Simulations and experimental results demonstrate that the\nproposed method produces improved human-aware task plans, reducing unuseful\ninterference between agents, increasing human-robot distance, and achieving up\nto an 18\\% reduction in process execution time."
                },
                "authors": [
                    {
                        "name": "Samuele Sandrini"
                    },
                    {
                        "name": "Marco Faroni"
                    },
                    {
                        "name": "Nicola Pedrocchi"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Pedrocchi"
                },
                "author": "Nicola Pedrocchi",
                "arxiv_comment": "Accepted for publication in Robotics and Computer-Integrated\n  Manufacturing (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07237v1",
                "updated": "2025-03-10T12:20:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    20,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:20:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    20,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "LLM-C3MOD: A Human-LLM Collaborative System for Cross-Cultural Hate\n  Speech Moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-C3MOD: A Human-LLM Collaborative System for Cross-Cultural Hate\n  Speech Moderation"
                },
                "summary": "Content moderation is a global challenge, yet major tech platforms prioritize\nhigh-resource languages, leaving low-resource languages with scarce native\nmoderators. Since effective moderation depends on understanding contextual\ncues, this imbalance increases the risk of improper moderation due to\nnon-native moderators' limited cultural understanding. Through a user study, we\nidentify that non-native moderators struggle with interpreting\nculturally-specific knowledge, sentiment, and internet culture in the hate\nspeech moderation. To assist them, we present LLM-C3MOD, a human-LLM\ncollaborative pipeline with three steps: (1) RAG-enhanced cultural context\nannotations; (2) initial LLM-based moderation; and (3) targeted human\nmoderation for cases lacking LLM consensus. Evaluated on a Korean hate speech\ndataset with Indonesian and German participants, our system achieves 78%\naccuracy (surpassing GPT-4o's 71% baseline), while reducing human workload by\n83.6%. Notably, human moderators excel at nuanced contents where LLMs struggle.\nOur findings suggest that non-native moderators, when properly supported by\nLLMs, can effectively contribute to cross-cultural hate speech moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content moderation is a global challenge, yet major tech platforms prioritize\nhigh-resource languages, leaving low-resource languages with scarce native\nmoderators. Since effective moderation depends on understanding contextual\ncues, this imbalance increases the risk of improper moderation due to\nnon-native moderators' limited cultural understanding. Through a user study, we\nidentify that non-native moderators struggle with interpreting\nculturally-specific knowledge, sentiment, and internet culture in the hate\nspeech moderation. To assist them, we present LLM-C3MOD, a human-LLM\ncollaborative pipeline with three steps: (1) RAG-enhanced cultural context\nannotations; (2) initial LLM-based moderation; and (3) targeted human\nmoderation for cases lacking LLM consensus. Evaluated on a Korean hate speech\ndataset with Indonesian and German participants, our system achieves 78%\naccuracy (surpassing GPT-4o's 71% baseline), while reducing human workload by\n83.6%. Notably, human moderators excel at nuanced contents where LLMs struggle.\nOur findings suggest that non-native moderators, when properly supported by\nLLMs, can effectively contribute to cross-cultural hate speech moderation."
                },
                "authors": [
                    {
                        "name": "Junyeong Park"
                    },
                    {
                        "name": "Seogyeong Jeong"
                    },
                    {
                        "name": "Seyoung Song"
                    },
                    {
                        "name": "Yohan Lee"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "arxiv_comment": "Accepted to NAACL 2025 Workshop - C3NLP (Workshop on Cross-Cultural\n  Considerations in NLP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11926v2",
                "updated": "2025-03-10T12:20:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    20,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-17T15:39:50Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    15,
                    39,
                    50,
                    0,
                    48,
                    0
                ],
                "title": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages"
                },
                "summary": "People worldwide use language in subtle and complex ways to express emotions.\nWhile emotion recognition -- an umbrella term for several NLP tasks --\nsignificantly impacts different applications in NLP and other fields, most work\nin the area is focused on high-resource languages. Therefore, this has led to\nmajor disparities in research and proposed solutions, especially for\nlow-resource languages that suffer from the lack of high-quality datasets. In\nthis paper, we present BRIGHTER -- a collection of multilabeled\nemotion-annotated datasets in 28 different languages. BRIGHTER covers\npredominantly low-resource languages from Africa, Asia, Eastern Europe, and\nLatin America, with instances from various domains annotated by fluent\nspeakers. We describe the data collection and annotation processes and the\nchallenges of building these datasets. Then, we report different experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as intensity-level emotion recognition. We investigate results with and\nwithout using LLMs and analyse the large variability in performance across\nlanguages and text domains. We show that BRIGHTER datasets are a step towards\nbridging the gap in text-based emotion recognition and discuss their impact and\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People worldwide use language in subtle and complex ways to express emotions.\nWhile emotion recognition -- an umbrella term for several NLP tasks --\nsignificantly impacts different applications in NLP and other fields, most work\nin the area is focused on high-resource languages. Therefore, this has led to\nmajor disparities in research and proposed solutions, especially for\nlow-resource languages that suffer from the lack of high-quality datasets. In\nthis paper, we present BRIGHTER -- a collection of multilabeled\nemotion-annotated datasets in 28 different languages. BRIGHTER covers\npredominantly low-resource languages from Africa, Asia, Eastern Europe, and\nLatin America, with instances from various domains annotated by fluent\nspeakers. We describe the data collection and annotation processes and the\nchallenges of building these datasets. Then, we report different experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as intensity-level emotion recognition. We investigate results with and\nwithout using LLMs and analyse the large variability in performance across\nlanguages and text domains. We show that BRIGHTER datasets are a step towards\nbridging the gap in text-based emotion recognition and discuss their impact and\nutility."
                },
                "authors": [
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Meriem Beloucif"
                    },
                    {
                        "name": "Christine de Kock"
                    },
                    {
                        "name": "Nirmal Surange"
                    },
                    {
                        "name": "Daniela Teodorescu"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Felermino D. M. A. Ali"
                    },
                    {
                        "name": "Ilseyar Alimova"
                    },
                    {
                        "name": "Vladimir Araujo"
                    },
                    {
                        "name": "Nikolay Babakov"
                    },
                    {
                        "name": "Naomi Baes"
                    },
                    {
                        "name": "Ana-Maria Bucur"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "Guanqun Cao"
                    },
                    {
                        "name": "Rodrigo Tufino Cardenas"
                    },
                    {
                        "name": "Rendi Chevi"
                    },
                    {
                        "name": "Chiamaka Ijeoma Chukwuneke"
                    },
                    {
                        "name": "Alexandra Ciobotaru"
                    },
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Murja Sani Gadanya"
                    },
                    {
                        "name": "Robert Geislinger"
                    },
                    {
                        "name": "Bela Gipp"
                    },
                    {
                        "name": "Oumaima Hourrane"
                    },
                    {
                        "name": "Oana Ignat"
                    },
                    {
                        "name": "Falalu Ibrahim Lawan"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Rahmad Mahendra"
                    },
                    {
                        "name": "Vukosi Marivate"
                    },
                    {
                        "name": "Andrew Piper"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Charles Henrique Porto Ferreira"
                    },
                    {
                        "name": "Vitaly Protasov"
                    },
                    {
                        "name": "Samuel Rutunda"
                    },
                    {
                        "name": "Manish Shrivastava"
                    },
                    {
                        "name": "Aura Cristina Udrea"
                    },
                    {
                        "name": "Lilian Diana Awuor Wanzare"
                    },
                    {
                        "name": "Sophie Wu"
                    },
                    {
                        "name": "Florian Valentin Wunderlich"
                    },
                    {
                        "name": "Hanif Muhammad Zhafran"
                    },
                    {
                        "name": "Tianhui Zhang"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Saif M. Mohammad"
                    }
                ],
                "author_detail": {
                    "name": "Saif M. Mohammad"
                },
                "author": "Saif M. Mohammad",
                "arxiv_comment": "20 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07234v1",
                "updated": "2025-03-10T12:17:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    17,
                    38,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:17:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    17,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs\n  and Chain-of-Thought Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs\n  and Chain-of-Thought Prompting"
                },
                "summary": "Accurate motion forecasting is crucial for safe autonomous driving (AD). This\nstudy proposes CoT-Drive, a novel approach that enhances motion forecasting by\nleveraging large language models (LLMs) and a chain-of-thought (CoT) prompting\nmethod. We introduce a teacher-student knowledge distillation strategy to\neffectively transfer LLMs' advanced scene understanding capabilities to\nlightweight language models (LMs), ensuring that CoT-Drive operates in\nreal-time on edge devices while maintaining comprehensive scene understanding\nand generalization capabilities. By leveraging CoT prompting techniques for\nLLMs without additional training, CoT-Drive generates semantic annotations that\nsignificantly improve the understanding of complex traffic environments,\nthereby boosting the accuracy and robustness of predictions. Additionally, we\npresent two new scene description datasets, Highway-Text and Urban-Text,\ndesigned for fine-tuning lightweight LMs to generate context-specific semantic\nannotations. Comprehensive evaluations of five real-world datasets demonstrate\nthat CoT-Drive outperforms existing models, highlighting its effectiveness and\nefficiency in handling complex traffic scenarios. Overall, this study is the\nfirst to consider the practical application of LLMs in this field. It pioneers\nthe training and use of a lightweight LLM surrogate for motion forecasting,\nsetting a new benchmark and showcasing the potential of integrating LLMs into\nAD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate motion forecasting is crucial for safe autonomous driving (AD). This\nstudy proposes CoT-Drive, a novel approach that enhances motion forecasting by\nleveraging large language models (LLMs) and a chain-of-thought (CoT) prompting\nmethod. We introduce a teacher-student knowledge distillation strategy to\neffectively transfer LLMs' advanced scene understanding capabilities to\nlightweight language models (LMs), ensuring that CoT-Drive operates in\nreal-time on edge devices while maintaining comprehensive scene understanding\nand generalization capabilities. By leveraging CoT prompting techniques for\nLLMs without additional training, CoT-Drive generates semantic annotations that\nsignificantly improve the understanding of complex traffic environments,\nthereby boosting the accuracy and robustness of predictions. Additionally, we\npresent two new scene description datasets, Highway-Text and Urban-Text,\ndesigned for fine-tuning lightweight LMs to generate context-specific semantic\nannotations. Comprehensive evaluations of five real-world datasets demonstrate\nthat CoT-Drive outperforms existing models, highlighting its effectiveness and\nefficiency in handling complex traffic scenarios. Overall, this study is the\nfirst to consider the practical application of LLMs in this field. It pioneers\nthe training and use of a lightweight LLM surrogate for motion forecasting,\nsetting a new benchmark and showcasing the potential of integrating LLMs into\nAD systems."
                },
                "authors": [
                    {
                        "name": "Haicheng Liao"
                    },
                    {
                        "name": "Hanlin Kong"
                    },
                    {
                        "name": "Bonan Wang"
                    },
                    {
                        "name": "Chengyue Wang"
                    },
                    {
                        "name": "Wang Ye"
                    },
                    {
                        "name": "Zhengbing He"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Zhenning Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhenning Li"
                },
                "author": "Zhenning Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03279v2",
                "updated": "2025-03-10T12:16:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    16,
                    16,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-05T15:35:06Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    35,
                    6,
                    2,
                    36,
                    0
                ],
                "title": "Posterior SBC: Simulation-Based Calibration Checking Conditional on Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posterior SBC: Simulation-Based Calibration Checking Conditional on Data"
                },
                "summary": "Simulation-based calibration checking (SBC) refers to the validation of an\ninference algorithm and model implementation through repeated inference on data\nsimulated from a generative model. In the original and commonly used approach,\nthe generative model uses parameters drawn from the prior, and thus the\napproach is testing whether the inference works for simulated data generated\nwith parameter values plausible under that prior. This approach is natural and\ndesirable when we want to test whether the inference works for a wide range of\ndatasets we might observe. However, after observing data, we are interested in\nanswering whether the inference works conditional on that particular data. In\nthis paper, we propose posterior SBC and demonstrate how it can be used to\nvalidate the inference conditionally on observed data. We illustrate the\nutility of posterior SBC in three case studies: (1) A simple multilevel model;\n(2) a model that is governed by differential equations; and (3) a joint\nintegrative neuroscience model which is approximated via amortized Bayesian\ninference with neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based calibration checking (SBC) refers to the validation of an\ninference algorithm and model implementation through repeated inference on data\nsimulated from a generative model. In the original and commonly used approach,\nthe generative model uses parameters drawn from the prior, and thus the\napproach is testing whether the inference works for simulated data generated\nwith parameter values plausible under that prior. This approach is natural and\ndesirable when we want to test whether the inference works for a wide range of\ndatasets we might observe. However, after observing data, we are interested in\nanswering whether the inference works conditional on that particular data. In\nthis paper, we propose posterior SBC and demonstrate how it can be used to\nvalidate the inference conditionally on observed data. We illustrate the\nutility of posterior SBC in three case studies: (1) A simple multilevel model;\n(2) a model that is governed by differential equations; and (3) a joint\nintegrative neuroscience model which is approximated via amortized Bayesian\ninference with neural networks."
                },
                "authors": [
                    {
                        "name": "Teemu Säilynoja"
                    },
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    },
                    {
                        "name": "Aki Vehtari"
                    }
                ],
                "author_detail": {
                    "name": "Aki Vehtari"
                },
                "author": "Aki Vehtari",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07227v1",
                "updated": "2025-03-10T12:14:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    14,
                    2,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:14:02Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    14,
                    2,
                    0,
                    69,
                    0
                ],
                "title": "Coreset Spectral Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreset Spectral Clustering"
                },
                "summary": "Coresets have become an invaluable tool for solving $k$-means and kernel\n$k$-means clustering problems on large datasets with small numbers of clusters.\nOn the other hand, spectral clustering works well on sparse graphs and has\nrecently been extended to scale efficiently to large numbers of clusters. We\nexploit the connection between kernel $k$-means and the normalised cut problem\nto combine the benefits of both. Our main result is a coreset spectral\nclustering algorithm for graphs that clusters a coreset graph to infer a good\nlabelling of the original graph. We prove that an $\\alpha$-approximation for\nthe normalised cut problem on the coreset graph is an $O(\\alpha)$-approximation\non the original. We also improve the running time of the state-of-the-art\ncoreset algorithm for kernel $k$-means on sparse kernels, from $\\tilde{O}(nk)$\nto $\\tilde{O}(n\\cdot \\min \\{k, d_{avg}\\})$, where $d_{avg}$ is the average\nnumber of non-zero entries in each row of the $n\\times n$ kernel matrix. Our\nexperiments confirm our coreset algorithm is asymptotically faster on large\nreal-world graphs with many clusters, and show that our clustering algorithm\novercomes the main challenge faced by coreset kernel $k$-means on sparse\nkernels which is getting stuck in local optima.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coresets have become an invaluable tool for solving $k$-means and kernel\n$k$-means clustering problems on large datasets with small numbers of clusters.\nOn the other hand, spectral clustering works well on sparse graphs and has\nrecently been extended to scale efficiently to large numbers of clusters. We\nexploit the connection between kernel $k$-means and the normalised cut problem\nto combine the benefits of both. Our main result is a coreset spectral\nclustering algorithm for graphs that clusters a coreset graph to infer a good\nlabelling of the original graph. We prove that an $\\alpha$-approximation for\nthe normalised cut problem on the coreset graph is an $O(\\alpha)$-approximation\non the original. We also improve the running time of the state-of-the-art\ncoreset algorithm for kernel $k$-means on sparse kernels, from $\\tilde{O}(nk)$\nto $\\tilde{O}(n\\cdot \\min \\{k, d_{avg}\\})$, where $d_{avg}$ is the average\nnumber of non-zero entries in each row of the $n\\times n$ kernel matrix. Our\nexperiments confirm our coreset algorithm is asymptotically faster on large\nreal-world graphs with many clusters, and show that our clustering algorithm\novercomes the main challenge faced by coreset kernel $k$-means on sparse\nkernels which is getting stuck in local optima."
                },
                "authors": [
                    {
                        "name": "Ben Jourdan"
                    },
                    {
                        "name": "Gregory Schwartzman"
                    },
                    {
                        "name": "Peter Macgregor"
                    },
                    {
                        "name": "He Sun"
                    }
                ],
                "author_detail": {
                    "name": "He Sun"
                },
                "author": "He Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13178v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13178v4",
                "updated": "2025-03-10T12:13:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    13,
                    9,
                    0,
                    69,
                    0
                ],
                "published": "2024-12-17T18:55:58Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents"
                },
                "summary": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to understand and plan complicated natural language\ninstructions. However, a foreseeable issue is that those embodied agents can\nalso flawlessly execute some hazardous tasks, potentially causing damages in\nthe real world. Existing benchmarks predominantly overlook critical safety\nrisks, focusing solely on planning performance, while a few evaluate LLMs'\nsafety awareness only on non-interactive image-text data. To address this gap,\nwe present SafeAgentBench-the first benchmark for safety-aware task planning of\nembodied LLM agents in interactive simulation environments. SafeAgentBench\nincludes: (1) an executable, diverse, and high-quality dataset of 750 tasks,\nrigorously curated to cover 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that, although\nagents based on different design frameworks exhibit substantial differences in\ntask success rates, their overall safety awareness remains weak. The most\nsafety-conscious baseline achieves only a 10\\% rejection rate for detailed\nhazardous tasks. Moreover, simply replacing the LLM driving the agent does not\nlead to notable improvements in safety awareness. More details and code are\navailable at https://github.com/shengyin1224/SafeAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to understand and plan complicated natural language\ninstructions. However, a foreseeable issue is that those embodied agents can\nalso flawlessly execute some hazardous tasks, potentially causing damages in\nthe real world. Existing benchmarks predominantly overlook critical safety\nrisks, focusing solely on planning performance, while a few evaluate LLMs'\nsafety awareness only on non-interactive image-text data. To address this gap,\nwe present SafeAgentBench-the first benchmark for safety-aware task planning of\nembodied LLM agents in interactive simulation environments. SafeAgentBench\nincludes: (1) an executable, diverse, and high-quality dataset of 750 tasks,\nrigorously curated to cover 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that, although\nagents based on different design frameworks exhibit substantial differences in\ntask success rates, their overall safety awareness remains weak. The most\nsafety-conscious baseline achieves only a 10\\% rejection rate for detailed\nhazardous tasks. Moreover, simply replacing the LLM driving the agent does not\nlead to notable improvements in safety awareness. More details and code are\navailable at https://github.com/shengyin1224/SafeAgentBench."
                },
                "authors": [
                    {
                        "name": "Sheng Yin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Yuanzhuo Ding"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Yutong Bi"
                    },
                    {
                        "name": "Yichen Xiong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "23 pages, 17 tables, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13178v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13178v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17506v2",
                "updated": "2025-03-10T12:11:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    11,
                    58,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-22T00:12:52Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    0,
                    12,
                    52,
                    5,
                    53,
                    0
                ],
                "title": "RAG-Enhanced Collaborative LLM Agents for Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Enhanced Collaborative LLM Agents for Drug Discovery"
                },
                "summary": "Recent advances in large language models (LLMs) have shown great potential to\naccelerate drug discovery. However, the specialized nature of biochemical data\noften necessitates costly domain-specific fine-tuning, posing critical\nchallenges. First, it hinders the application of more flexible general-purpose\nLLMs in cutting-edge drug discovery tasks. More importantly, it impedes the\nrapid integration of the vast amounts of scientific data continuously generated\nthrough experiments and research. To investigate these challenges, we propose\nCLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored\nto drug discovery tasks. Through the collaboration of multiple LLM agents,\nCLADD dynamically retrieves information from biomedical knowledge bases,\ncontextualizes query molecules, and integrates relevant evidence to generate\nresponses -- all without the need for domain-specific fine-tuning. Crucially,\nwe tackle key obstacles in applying RAG workflows to biochemical data,\nincluding data heterogeneity, ambiguity, and multi-source integration. We\ndemonstrate the flexibility and effectiveness of this framework across a\nvariety of drug discovery tasks, showing that it outperforms general-purpose\nand domain-specific LLMs as well as traditional deep learning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown great potential to\naccelerate drug discovery. However, the specialized nature of biochemical data\noften necessitates costly domain-specific fine-tuning, posing critical\nchallenges. First, it hinders the application of more flexible general-purpose\nLLMs in cutting-edge drug discovery tasks. More importantly, it impedes the\nrapid integration of the vast amounts of scientific data continuously generated\nthrough experiments and research. To investigate these challenges, we propose\nCLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored\nto drug discovery tasks. Through the collaboration of multiple LLM agents,\nCLADD dynamically retrieves information from biomedical knowledge bases,\ncontextualizes query molecules, and integrates relevant evidence to generate\nresponses -- all without the need for domain-specific fine-tuning. Crucially,\nwe tackle key obstacles in applying RAG workflows to biochemical data,\nincluding data heterogeneity, ambiguity, and multi-source integration. We\ndemonstrate the flexibility and effectiveness of this framework across a\nvariety of drug discovery tasks, showing that it outperforms general-purpose\nand domain-specific LLMs as well as traditional deep learning approaches."
                },
                "authors": [
                    {
                        "name": "Namkyeong Lee"
                    },
                    {
                        "name": "Edward De Brouwer"
                    },
                    {
                        "name": "Ehsan Hajiramezanali"
                    },
                    {
                        "name": "Tommaso Biancalani"
                    },
                    {
                        "name": "Chanyoung Park"
                    },
                    {
                        "name": "Gabriele Scalia"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Scalia"
                },
                "author": "Gabriele Scalia",
                "arxiv_comment": "Machine Learning, Drug Discovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04090v2",
                "updated": "2025-03-10T12:01:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    1,
                    1,
                    0,
                    69,
                    0
                ],
                "published": "2024-12-05T11:52:20Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    52,
                    20,
                    3,
                    340,
                    0
                ],
                "title": "LossAgent: Towards Any Optimization Objectives for Image Processing with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LossAgent: Towards Any Optimization Objectives for Image Processing with\n  LLM Agents"
                },
                "summary": "We present the first loss agent, dubbed LossAgent, for low-level image\nprocessing tasks, e.g., image super-resolution and restoration, intending to\nachieve any customized optimization objectives of low-level image processing in\ndifferent practical applications. Notably, not all optimization objectives,\nsuch as complex hand-crafted perceptual metrics, text description, and\nintricate human feedback, can be instantiated with existing low-level losses,\ne.g., MSE loss, which presents a crucial challenge in optimizing image\nprocessing networks in an end-to-end manner. To eliminate this, our LossAgent\nintroduces the powerful large language model (LLM) as the loss agent, where the\nrich textual understanding of prior knowledge empowers the loss agent with the\npotential to understand complex optimization objectives, trajectory, and state\nfeedback from external environments in the optimization process of the\nlow-level image processing networks. In particular, we establish the loss\nrepository by incorporating existing loss functions that support the end-to-end\noptimization for low-level image processing. Then, we design the\noptimization-oriented prompt engineering for the loss agent to actively and\nintelligently decide the compositional weights for each loss in the repository\nat each optimization interaction, thereby achieving the required optimization\ntrajectory for any customized optimization objectives. Extensive experiments on\nthree typical low-level image processing tasks and multiple optimization\nobjectives have shown the effectiveness and applicability of our proposed\nLossAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first loss agent, dubbed LossAgent, for low-level image\nprocessing tasks, e.g., image super-resolution and restoration, intending to\nachieve any customized optimization objectives of low-level image processing in\ndifferent practical applications. Notably, not all optimization objectives,\nsuch as complex hand-crafted perceptual metrics, text description, and\nintricate human feedback, can be instantiated with existing low-level losses,\ne.g., MSE loss, which presents a crucial challenge in optimizing image\nprocessing networks in an end-to-end manner. To eliminate this, our LossAgent\nintroduces the powerful large language model (LLM) as the loss agent, where the\nrich textual understanding of prior knowledge empowers the loss agent with the\npotential to understand complex optimization objectives, trajectory, and state\nfeedback from external environments in the optimization process of the\nlow-level image processing networks. In particular, we establish the loss\nrepository by incorporating existing loss functions that support the end-to-end\noptimization for low-level image processing. Then, we design the\noptimization-oriented prompt engineering for the loss agent to actively and\nintelligently decide the compositional weights for each loss in the repository\nat each optimization interaction, thereby achieving the required optimization\ntrajectory for any customized optimization objectives. Extensive experiments on\nthree typical low-level image processing tasks and multiple optimization\nobjectives have shown the effectiveness and applicability of our proposed\nLossAgent."
                },
                "authors": [
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Update format",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07217v1",
                "updated": "2025-03-10T11:57:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    57,
                    55,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T11:57:55Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    57,
                    55,
                    0,
                    69,
                    0
                ],
                "title": "ReelWave: A Multi-Agent Framework Toward Professional Movie Sound\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReelWave: A Multi-Agent Framework Toward Professional Movie Sound\n  Generation"
                },
                "summary": "Film production is an important application for generative audio, where\nricher context is provided through multiple scenes. In ReelWave, we propose a\nmulti-agent framework for audio generation inspired by the professional movie\nproduction process. We first capture semantic and temporal synchronized\n\"on-screen\" sound by training a prediction model that predicts three\ninterpretable time-varying audio control signals comprising loudness, pitch,\nand timbre. These three parameters are subsequently specified as conditions by\na cross-attention module. Then, our framework infers \"off-screen\" sound to\ncomplement the generation through cooperative interaction between communicative\nagents. Each agent takes up specific roles similar to the movie production team\nand is supervised by an agent called the director. Besides, we investigate when\nthe conditional video consists of multiple scenes, a case frequently seen in\nvideos extracted from movies of considerable length. Consequently, our\nframework can capture a richer context of audio generation conditioned on video\nclips extracted from movies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Film production is an important application for generative audio, where\nricher context is provided through multiple scenes. In ReelWave, we propose a\nmulti-agent framework for audio generation inspired by the professional movie\nproduction process. We first capture semantic and temporal synchronized\n\"on-screen\" sound by training a prediction model that predicts three\ninterpretable time-varying audio control signals comprising loudness, pitch,\nand timbre. These three parameters are subsequently specified as conditions by\na cross-attention module. Then, our framework infers \"off-screen\" sound to\ncomplement the generation through cooperative interaction between communicative\nagents. Each agent takes up specific roles similar to the movie production team\nand is supervised by an agent called the director. Besides, we investigate when\nthe conditional video consists of multiple scenes, a case frequently seen in\nvideos extracted from movies of considerable length. Consequently, our\nframework can capture a richer context of audio generation conditioned on video\nclips extracted from movies."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Wing Tai"
                },
                "author": "Yu-Wing Tai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07216v1",
                "updated": "2025-03-10T11:55:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    55,
                    50,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T11:55:50Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    55,
                    50,
                    0,
                    69,
                    0
                ],
                "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates"
                },
                "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Byungjoo Kim"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07215v1",
                "updated": "2025-03-10T11:52:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    52,
                    48,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T11:52:48Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    52,
                    48,
                    0,
                    69,
                    0
                ],
                "title": "Control Flow-Augmented Decompiler based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control Flow-Augmented Decompiler based on Large Language Model"
                },
                "summary": "Binary decompilation plays a crucial role in various tasks related to\nsecurity threat analysis and software engineering, such as binary vulnerability\ndetection and software supply chain analysis. Current prevalent binary\ndecompilation methods primarily rely on large language models (LLMs) and can be\nbroadly classified into two main approaches: prompt-based decompilation and\nend-toend decompilation. Prompt-based methods typically require significant\neffort to analyze and summarize the predicted data to extract aspect-specific\nexpert knowledge, which is then fed into a general purpose large language model\nto address specific decompilation tasks. End-to-end methods, on the other hand,\ncarefully construct training datasets or neural networks to perform\npost-training on general-purpose large language models, thereby obtaining\ndomain-specific large language models for decompiling the predicted data.\nHowever, both existing approaches still face significant challenges, including\nthe absence of rich semantic representations of the input code and the neglect\nof control flow information, which is crucial for accurate decompilation.\nFurthermore, most current decompilation techniques are specifically tailored\nfor the x86 architecture, making it difficult to efficiently adapt and\ngeneralize them to other bit width or instruction architectures. To address\nthese limitations, we propose a novel end-to-end decompilation LLM, CFADecLLM,\nwhich aims to enhance existing end-to-end decompilation methods. We conduct\nextensive experiments on the public dataset Humaneval and Exebench across four\noptimization levels, and results demonstrate that our approach outperforms\nexisting methods in multiple metrics, validating its effectiveness and\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary decompilation plays a crucial role in various tasks related to\nsecurity threat analysis and software engineering, such as binary vulnerability\ndetection and software supply chain analysis. Current prevalent binary\ndecompilation methods primarily rely on large language models (LLMs) and can be\nbroadly classified into two main approaches: prompt-based decompilation and\nend-toend decompilation. Prompt-based methods typically require significant\neffort to analyze and summarize the predicted data to extract aspect-specific\nexpert knowledge, which is then fed into a general purpose large language model\nto address specific decompilation tasks. End-to-end methods, on the other hand,\ncarefully construct training datasets or neural networks to perform\npost-training on general-purpose large language models, thereby obtaining\ndomain-specific large language models for decompiling the predicted data.\nHowever, both existing approaches still face significant challenges, including\nthe absence of rich semantic representations of the input code and the neglect\nof control flow information, which is crucial for accurate decompilation.\nFurthermore, most current decompilation techniques are specifically tailored\nfor the x86 architecture, making it difficult to efficiently adapt and\ngeneralize them to other bit width or instruction architectures. To address\nthese limitations, we propose a novel end-to-end decompilation LLM, CFADecLLM,\nwhich aims to enhance existing end-to-end decompilation methods. We conduct\nextensive experiments on the public dataset Humaneval and Exebench across four\noptimization levels, and results demonstrate that our approach outperforms\nexisting methods in multiple metrics, validating its effectiveness and\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Peipei Liu"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Zhaoteng Yan"
                    },
                    {
                        "name": "Peizheng Zhang"
                    },
                    {
                        "name": "Dapeng Sun"
                    },
                    {
                        "name": "Dawei Wang"
                    },
                    {
                        "name": "Dan Li"
                    }
                ],
                "author_detail": {
                    "name": "Dan Li"
                },
                "author": "Dan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07202v1",
                "updated": "2025-03-10T11:38:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    38,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T11:38:21Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    38,
                    21,
                    0,
                    69,
                    0
                ],
                "title": "A Zero-shot Learning Method Based on Large Language Models for\n  Multi-modal Knowledge Graph Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-shot Learning Method Based on Large Language Models for\n  Multi-modal Knowledge Graph Embedding"
                },
                "summary": "Zero-shot learning (ZL) is crucial for tasks involving unseen categories,\nsuch as natural language processing, image classification, and cross-lingual\ntransfer. Current applications often fail to accurately infer and handle new\nrelations or entities involving unseen categories, severely limiting their\nscalability and practicality in open-domain scenarios. ZL learning faces the\nchallenge of effectively transferring semantic information of unseen categories\nin multi-modal knowledge graph (MMKG) embedding representation learning. In\nthis paper, we propose ZSLLM, a framework for zero-shot embedding learning of\nMMKGs using large language models (LLMs). We leverage textual modality\ninformation of unseen categories as prompts to fully utilize the reasoning\ncapabilities of LLMs, enabling semantic information transfer across different\nmodalities for unseen categories. Through model-based learning, the embedding\nrepresentation of unseen categories in MMKG is enhanced. Extensive experiments\nconducted on multiple real-world datasets demonstrate the superiority of our\napproach compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot learning (ZL) is crucial for tasks involving unseen categories,\nsuch as natural language processing, image classification, and cross-lingual\ntransfer. Current applications often fail to accurately infer and handle new\nrelations or entities involving unseen categories, severely limiting their\nscalability and practicality in open-domain scenarios. ZL learning faces the\nchallenge of effectively transferring semantic information of unseen categories\nin multi-modal knowledge graph (MMKG) embedding representation learning. In\nthis paper, we propose ZSLLM, a framework for zero-shot embedding learning of\nMMKGs using large language models (LLMs). We leverage textual modality\ninformation of unseen categories as prompts to fully utilize the reasoning\ncapabilities of LLMs, enabling semantic information transfer across different\nmodalities for unseen categories. Through model-based learning, the embedding\nrepresentation of unseen categories in MMKG is enhanced. Extensive experiments\nconducted on multiple real-world datasets demonstrate the superiority of our\napproach compared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Bingchen Liu"
                    },
                    {
                        "name": "Jingchen Li"
                    },
                    {
                        "name": "Naixing Xu"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11278v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11278v3",
                "updated": "2025-03-11T05:22:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    5,
                    22,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-18T04:35:20Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    4,
                    35,
                    20,
                    0,
                    323,
                    0
                ],
                "title": "Towards Open-Vocabulary Audio-Visual Event Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Open-Vocabulary Audio-Visual Event Localization"
                },
                "summary": "The Audio-Visual Event Localization (AVEL) task aims to temporally locate and\nclassify video events that are both audible and visible. Most research in this\nfield assumes a closed-set setting, which restricts these models' ability to\nhandle test data containing event categories absent (unseen) during training.\nRecently, a few studies have explored AVEL in an open-set setting, enabling the\nrecognition of unseen events as ``unknown'', but without providing\ncategory-specific semantics. In this paper, we advance the field by introducing\nthe Open-Vocabulary Audio-Visual Event Localization (OV-AVEL) problem, which\nrequires localizing audio-visual events and predicting explicit categories for\nboth seen and unseen data at inference. To address this new task, we propose\nthe OV-AVEBench dataset, comprising 24,800 videos across 67 real-life\naudio-visual scenes (seen:unseen = 46:21), each with manual segment-level\nannotation. We also establish three evaluation metrics for this task. Moreover,\nwe investigate two baseline approaches, one training-free and one using a\nfurther fine-tuning paradigm. Specifically, we utilize the unified multimodal\nspace from the pretrained ImageBind model to extract audio, visual, and textual\n(event classes) features. The training-free baseline then determines\npredictions by comparing the consistency of audio-text and visual-text feature\nsimilarities. The fine-tuning baseline incorporates lightweight temporal layers\nto encode temporal relations within the audio and visual modalities, using\nOV-AVEBench training data for model fine-tuning. We evaluate these baselines on\nthe proposed OV-AVEBench dataset and discuss potential directions for future\nwork in this new field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Audio-Visual Event Localization (AVEL) task aims to temporally locate and\nclassify video events that are both audible and visible. Most research in this\nfield assumes a closed-set setting, which restricts these models' ability to\nhandle test data containing event categories absent (unseen) during training.\nRecently, a few studies have explored AVEL in an open-set setting, enabling the\nrecognition of unseen events as ``unknown'', but without providing\ncategory-specific semantics. In this paper, we advance the field by introducing\nthe Open-Vocabulary Audio-Visual Event Localization (OV-AVEL) problem, which\nrequires localizing audio-visual events and predicting explicit categories for\nboth seen and unseen data at inference. To address this new task, we propose\nthe OV-AVEBench dataset, comprising 24,800 videos across 67 real-life\naudio-visual scenes (seen:unseen = 46:21), each with manual segment-level\nannotation. We also establish three evaluation metrics for this task. Moreover,\nwe investigate two baseline approaches, one training-free and one using a\nfurther fine-tuning paradigm. Specifically, we utilize the unified multimodal\nspace from the pretrained ImageBind model to extract audio, visual, and textual\n(event classes) features. The training-free baseline then determines\npredictions by comparing the consistency of audio-text and visual-text feature\nsimilarities. The fine-tuning baseline incorporates lightweight temporal layers\nto encode temporal relations within the audio and visual modalities, using\nOV-AVEBench training data for model fine-tuning. We evaluate these baselines on\nthe proposed OV-AVEBench dataset and discuss potential directions for future\nwork in this new field."
                },
                "authors": [
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Dan Guo"
                    },
                    {
                        "name": "Ruohao Guo"
                    },
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Jingjing Hu"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "accepted by CVPR 2025; Project page:\n  https://github.com/jasongief/OV-AVEL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11278v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11278v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07195v1",
                "updated": "2025-03-10T11:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    23,
                    44,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T11:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    23,
                    44,
                    0,
                    69,
                    0
                ],
                "title": "Contextual Cues in Machine Translation: Investigating the Potential of\n  Multi-Source Input Strategies in LLMs and NMT Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Cues in Machine Translation: Investigating the Potential of\n  Multi-Source Input Strategies in LLMs and NMT Systems"
                },
                "summary": "We explore the impact of multi-source input strategies on machine translation\n(MT) quality, comparing GPT-4o, a large language model (LLM), with a\ntraditional multilingual neural machine translation (NMT) system. Using\nintermediate language translations as contextual cues, we evaluate their\neffectiveness in enhancing English and Chinese translations into Portuguese.\nResults suggest that contextual information significantly improves translation\nquality for domain-specific datasets and potentially for linguistically distant\nlanguage pairs, with diminishing returns observed in benchmarks with high\nlinguistic variability. Additionally, we demonstrate that shallow fusion, a\nmulti-source approach we apply within the NMT system, shows improved results\nwhen using high-resource languages as context for other translation pairs,\nhighlighting the importance of strategic context language selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the impact of multi-source input strategies on machine translation\n(MT) quality, comparing GPT-4o, a large language model (LLM), with a\ntraditional multilingual neural machine translation (NMT) system. Using\nintermediate language translations as contextual cues, we evaluate their\neffectiveness in enhancing English and Chinese translations into Portuguese.\nResults suggest that contextual information significantly improves translation\nquality for domain-specific datasets and potentially for linguistically distant\nlanguage pairs, with diminishing returns observed in benchmarks with high\nlinguistic variability. Additionally, we demonstrate that shallow fusion, a\nmulti-source approach we apply within the NMT system, shows improved results\nwhen using high-resource languages as context for other translation pairs,\nhighlighting the importance of strategic context language selection."
                },
                "authors": [
                    {
                        "name": "Lia Shahnazaryan"
                    },
                    {
                        "name": "Patrick Simianer"
                    },
                    {
                        "name": "Joern Wuebker"
                    }
                ],
                "author_detail": {
                    "name": "Joern Wuebker"
                },
                "author": "Joern Wuebker",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12279v4",
                "updated": "2025-03-10T11:08:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    8,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2024-11-19T06:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "title": "HouseTune: Two-Stage Floorplan Generation with LLM Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HouseTune: Two-Stage Floorplan Generation with LLM Assistance"
                },
                "summary": "This paper proposes a two-stage text-to-floorplan generation framework that\ncombines the reasoning capability of Large Language Models (LLMs) with the\ngenerative power of diffusion models. In the first stage, we leverage a\nChain-of-Thought (CoT) prompting strategy to guide an LLM in generating an\ninitial layout (Layout-Init) from natural language descriptions, which ensures\na user-friendly and intuitive design process. However, Layout-Init may lack\nprecise geometric alignment and fine-grained structural details. To address\nthis, the second stage employs a conditional diffusion model to refine\nLayout-Init into a final floorplan (Layout-Final) that better adheres to\nphysical constraints and user requirements. Unlike prior methods, our approach\neffectively reduces the difficulty of floorplan generation learning without the\nneed for extensive domain-specific training data. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, which validates its effectiveness in practical home design\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a two-stage text-to-floorplan generation framework that\ncombines the reasoning capability of Large Language Models (LLMs) with the\ngenerative power of diffusion models. In the first stage, we leverage a\nChain-of-Thought (CoT) prompting strategy to guide an LLM in generating an\ninitial layout (Layout-Init) from natural language descriptions, which ensures\na user-friendly and intuitive design process. However, Layout-Init may lack\nprecise geometric alignment and fine-grained structural details. To address\nthis, the second stage employs a conditional diffusion model to refine\nLayout-Init into a final floorplan (Layout-Final) that better adheres to\nphysical constraints and user requirements. Unlike prior methods, our approach\neffectively reduces the difficulty of floorplan generation learning without the\nneed for extensive domain-specific training data. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, which validates its effectiveness in practical home design\napplications."
                },
                "authors": [
                    {
                        "name": "Ziyang Zong"
                    },
                    {
                        "name": "Guanying Chen"
                    },
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Fengcheng Yu"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11843v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11843v4",
                "updated": "2025-03-10T10:50:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    50,
                    44,
                    0,
                    69,
                    0
                ],
                "published": "2024-09-23T08:39:16Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    39,
                    16,
                    0,
                    267,
                    0
                ],
                "title": "From Commands to Prompts: LLM-based Semantic File System for AIOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Commands to Prompts: LLM-based Semantic File System for AIOS"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Chaoji Zuo"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Yujie Ren"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Dong Deng"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_journal_ref": "ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11843v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11843v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07173v1",
                "updated": "2025-03-10T10:50:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    50,
                    33,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T10:50:33Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    50,
                    33,
                    0,
                    69,
                    0
                ],
                "title": "Towards Spatial Transcriptomics-guided Pathological Image Recognition\n  with Batch-Agnostic Encoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Spatial Transcriptomics-guided Pathological Image Recognition\n  with Batch-Agnostic Encoder"
                },
                "summary": "Spatial transcriptomics (ST) is a novel technique that simultaneously\ncaptures pathological images and gene expression profiling with spatial\ncoordinates. Since ST is closely related to pathological features such as\ndisease subtypes, it may be valuable to augment image representation with\npathological information. However, there are no attempts to leverage ST for\nimage recognition ({\\it i.e,} patch-level classification of subtypes of\npathological image.). One of the big challenges is significant batch effects in\nspatial transcriptomics that make it difficult to extract pathological features\nof images from ST. In this paper, we propose a batch-agnostic contrastive\nlearning framework that can extract consistent signals from gene expression of\nST in multiple patients. To extract consistent signals from ST, we utilize the\nbatch-agnostic gene encoder that is trained in a variational inference manner.\nExperiments demonstrated the effectiveness of our framework on a publicly\navailable dataset. Code is publicly available at\nhttps://github.com/naivete5656/TPIRBAE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial transcriptomics (ST) is a novel technique that simultaneously\ncaptures pathological images and gene expression profiling with spatial\ncoordinates. Since ST is closely related to pathological features such as\ndisease subtypes, it may be valuable to augment image representation with\npathological information. However, there are no attempts to leverage ST for\nimage recognition ({\\it i.e,} patch-level classification of subtypes of\npathological image.). One of the big challenges is significant batch effects in\nspatial transcriptomics that make it difficult to extract pathological features\nof images from ST. In this paper, we propose a batch-agnostic contrastive\nlearning framework that can extract consistent signals from gene expression of\nST in multiple patients. To extract consistent signals from ST, we utilize the\nbatch-agnostic gene encoder that is trained in a variational inference manner.\nExperiments demonstrated the effectiveness of our framework on a publicly\navailable dataset. Code is publicly available at\nhttps://github.com/naivete5656/TPIRBAE"
                },
                "authors": [
                    {
                        "name": "Kazuya Nishimura"
                    },
                    {
                        "name": "Ryoma Bise"
                    },
                    {
                        "name": "Yasuhiro Kojima"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhiro Kojima"
                },
                "author": "Yasuhiro Kojima",
                "arxiv_comment": "Accepted to ISBI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11995v2",
                "updated": "2025-03-10T10:48:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    48,
                    57,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-17T16:35:15Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    16,
                    35,
                    15,
                    0,
                    48,
                    0
                ],
                "title": "Presumed Cultural Identity: How Names Shape LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Presumed Cultural Identity: How Names Shape LLM Responses"
                },
                "summary": "Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation."
                },
                "authors": [
                    {
                        "name": "Siddhesh Pawar"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Lucie-Aimée Kaffee"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "23 Pages, 13 Figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07158v1",
                "updated": "2025-03-10T10:33:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    33,
                    31,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T10:33:31Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    33,
                    31,
                    0,
                    69,
                    0
                ],
                "title": "Generative AI in Transportation Planning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI in Transportation Planning: A Survey"
                },
                "summary": "The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning."
                },
                "authors": [
                    {
                        "name": "Longchao Da"
                    },
                    {
                        "name": "Tiejin Chen"
                    },
                    {
                        "name": "Zhuoheng Li"
                    },
                    {
                        "name": "Shreyas Bachiraju"
                    },
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Dongjie Wang"
                    },
                    {
                        "name": "Xuanyu"
                    },
                    {
                        "name": "Zhou"
                    },
                    {
                        "name": "Ram Pendyala"
                    },
                    {
                        "name": "Benjamin Stabler"
                    },
                    {
                        "name": "Yezhou Yang"
                    },
                    {
                        "name": "Xuesong Zhou"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "arxiv_affiliation": "Ben",
                "author": "Hua Wei",
                "arxiv_comment": "56 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T99, 90B06",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; I.6.3; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07154v1",
                "updated": "2025-03-10T10:27:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    27,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T10:27:30Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    27,
                    30,
                    0,
                    69,
                    0
                ],
                "title": "Ideas in Inference-time Scaling can Benefit Generative Pre-training\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ideas in Inference-time Scaling can Benefit Generative Pre-training\n  Algorithms"
                },
                "summary": "Recent years have seen significant advancements in foundation models through\ngenerative pre-training, yet algorithmic innovation in this space has largely\nstagnated around autoregressive models for discrete signals and diffusion\nmodels for continuous signals. This stagnation creates a bottleneck that\nprevents us from fully unlocking the potential of rich multi-modal data, which\nin turn limits the progress on multimodal intelligence. We argue that an\ninference-first perspective, which prioritizes scaling efficiency during\ninference time across sequence length and refinement steps, can inspire novel\ngenerative pre-training algorithms. Using Inductive Moment Matching (IMM) as a\nconcrete example, we demonstrate how addressing limitations in diffusion\nmodels' inference process through targeted modifications yields a stable,\nsingle-stage algorithm that achieves superior sample quality with over an order\nof magnitude greater inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen significant advancements in foundation models through\ngenerative pre-training, yet algorithmic innovation in this space has largely\nstagnated around autoregressive models for discrete signals and diffusion\nmodels for continuous signals. This stagnation creates a bottleneck that\nprevents us from fully unlocking the potential of rich multi-modal data, which\nin turn limits the progress on multimodal intelligence. We argue that an\ninference-first perspective, which prioritizes scaling efficiency during\ninference time across sequence length and refinement steps, can inspire novel\ngenerative pre-training algorithms. Using Inductive Moment Matching (IMM) as a\nconcrete example, we demonstrate how addressing limitations in diffusion\nmodels' inference process through targeted modifications yields a stable,\nsingle-stage algorithm that achieves superior sample quality with over an order\nof magnitude greater inference efficiency."
                },
                "authors": [
                    {
                        "name": "Jiaming Song"
                    },
                    {
                        "name": "Linqi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Zhou"
                },
                "author": "Linqi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07152v1",
                "updated": "2025-03-10T10:26:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    26,
                    8,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T10:26:08Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    26,
                    8,
                    0,
                    69,
                    0
                ],
                "title": "Controllable 3D Outdoor Scene Generation via Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable 3D Outdoor Scene Generation via Scene Graphs"
                },
                "summary": "Three-dimensional scene generation is crucial in computer vision, with\napplications spanning autonomous driving, gaming and the metaverse. Current\nmethods either lack user control or rely on imprecise, non-intuitive\nconditions. In this work, we propose a method that uses, scene graphs, an\naccessible, user friendly control format to generate outdoor 3D scenes. We\ndevelop an interactive system that transforms a sparse scene graph into a dense\nBEV (Bird's Eye View) Embedding Map, which guides a conditional diffusion model\nto generate 3D scenes that match the scene graph description. During inference,\nusers can easily create or modify scene graphs to generate large-scale outdoor\nscenes. We create a large-scale dataset with paired scene graphs and 3D\nsemantic scenes to train the BEV embedding and diffusion models. Experimental\nresults show that our approach consistently produces high-quality 3D urban\nscenes closely aligned with the input scene graphs. To the best of our\nknowledge, this is the first approach to generate 3D outdoor scenes conditioned\non scene graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three-dimensional scene generation is crucial in computer vision, with\napplications spanning autonomous driving, gaming and the metaverse. Current\nmethods either lack user control or rely on imprecise, non-intuitive\nconditions. In this work, we propose a method that uses, scene graphs, an\naccessible, user friendly control format to generate outdoor 3D scenes. We\ndevelop an interactive system that transforms a sparse scene graph into a dense\nBEV (Bird's Eye View) Embedding Map, which guides a conditional diffusion model\nto generate 3D scenes that match the scene graph description. During inference,\nusers can easily create or modify scene graphs to generate large-scale outdoor\nscenes. We create a large-scale dataset with paired scene graphs and 3D\nsemantic scenes to train the BEV embedding and diffusion models. Experimental\nresults show that our approach consistently produces high-quality 3D urban\nscenes closely aligned with the input scene graphs. To the best of our\nknowledge, this is the first approach to generate 3D outdoor scenes conditioned\non scene graphs."
                },
                "authors": [
                    {
                        "name": "Yuheng Liu"
                    },
                    {
                        "name": "Xinke Li"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Wenping Wang"
                    },
                    {
                        "name": "Chongshou Li"
                    },
                    {
                        "name": "Xueting Li"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "arxiv_comment": "Project Page: https://yuheng.ink/project-page/control-3d-scene/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07144v1",
                "updated": "2025-03-10T10:20:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    20,
                    5,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T10:20:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    20,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "MRCEval: A Comprehensive, Challenging and Accessible Machine Reading\n  Comprehension Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRCEval: A Comprehensive, Challenging and Accessible Machine Reading\n  Comprehension Benchmark"
                },
                "summary": "Machine Reading Comprehension (MRC) is an essential task in evaluating\nnatural language understanding. Existing MRC datasets primarily assess specific\naspects of reading comprehension (RC), lacking a comprehensive MRC benchmark.\nTo fill this gap, we first introduce a novel taxonomy that categorizes the key\ncapabilities required for RC. Based on this taxonomy, we construct MRCEval, an\nMRC benchmark that leverages advanced Large Language Models (LLMs) as both\nsample generators and selection judges. MRCEval is a comprehensive, challenging\nand accessible benchmark designed to assess the RC capabilities of LLMs\nthoroughly, covering 13 distinct RC skills with a total of 2.1K high-quality\nmulti-choice questions. We perform an extensive evaluation of 28 widely used\nopen-source and proprietary models, highlighting that MRC continues to present\nsignificant challenges even in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Reading Comprehension (MRC) is an essential task in evaluating\nnatural language understanding. Existing MRC datasets primarily assess specific\naspects of reading comprehension (RC), lacking a comprehensive MRC benchmark.\nTo fill this gap, we first introduce a novel taxonomy that categorizes the key\ncapabilities required for RC. Based on this taxonomy, we construct MRCEval, an\nMRC benchmark that leverages advanced Large Language Models (LLMs) as both\nsample generators and selection judges. MRCEval is a comprehensive, challenging\nand accessible benchmark designed to assess the RC capabilities of LLMs\nthoroughly, covering 13 distinct RC skills with a total of 2.1K high-quality\nmulti-choice questions. We perform an extensive evaluation of 28 widely used\nopen-source and proprietary models, highlighting that MRC continues to present\nsignificant challenges even in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Ma"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07133v1",
                "updated": "2025-03-10T10:03:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    3,
                    23,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T10:03:23Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    3,
                    23,
                    0,
                    69,
                    0
                ],
                "title": "A Light Perspective for 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Light Perspective for 3D Object Detection"
                },
                "summary": "Comprehending the environment and accurately detecting objects in 3D space\nare essential for advancing autonomous vehicle technologies. Integrating Camera\nand LIDAR data has emerged as an effective approach for achieving high accuracy\nin 3D Object Detection models. However, existing methodologies often rely on\nheavy, traditional backbones that are computationally demanding. This paper\nintroduces a novel approach that incorporates cutting-edge Deep Learning\ntechniques into the feature extraction process, aiming to create more efficient\nmodels without compromising performance. Our model, NextBEV, surpasses\nestablished feature extractors like ResNet50 and MobileNetV2. On the KITTI 3D\nMonocular detection benchmark, NextBEV achieves an accuracy improvement of\n2.39%, having less than 10% of the MobileNetV3 parameters. Moreover, we propose\nchanges in LIDAR backbones that decreased the original inference time to 10 ms.\nAdditionally, by fusing these lightweight proposals, we have enhanced the\naccuracy of the VoxelNet-based model by 2.93% and improved the F1-score of the\nPointPillar-based model by approximately 20%. Therefore, this work contributes\nto establishing lightweight and powerful models for individual or fusion\ntechniques, making them more suitable for onboard implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehending the environment and accurately detecting objects in 3D space\nare essential for advancing autonomous vehicle technologies. Integrating Camera\nand LIDAR data has emerged as an effective approach for achieving high accuracy\nin 3D Object Detection models. However, existing methodologies often rely on\nheavy, traditional backbones that are computationally demanding. This paper\nintroduces a novel approach that incorporates cutting-edge Deep Learning\ntechniques into the feature extraction process, aiming to create more efficient\nmodels without compromising performance. Our model, NextBEV, surpasses\nestablished feature extractors like ResNet50 and MobileNetV2. On the KITTI 3D\nMonocular detection benchmark, NextBEV achieves an accuracy improvement of\n2.39%, having less than 10% of the MobileNetV3 parameters. Moreover, we propose\nchanges in LIDAR backbones that decreased the original inference time to 10 ms.\nAdditionally, by fusing these lightweight proposals, we have enhanced the\naccuracy of the VoxelNet-based model by 2.93% and improved the F1-score of the\nPointPillar-based model by approximately 20%. Therefore, this work contributes\nto establishing lightweight and powerful models for individual or fusion\ntechniques, making them more suitable for onboard implementations."
                },
                "authors": [
                    {
                        "name": "Marcelo Eduardo Pederiva"
                    },
                    {
                        "name": "José Mario De Martino"
                    },
                    {
                        "name": "Alessandro Zimmer"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Zimmer"
                },
                "author": "Alessandro Zimmer",
                "arxiv_doi": "10.1117/12.3055035",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1117/12.3055035",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.07133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. SPIE 13517, Seventeenth International Conference on Machine\n  Vision (ICMV 2024), 135170J (24 February 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07125v1",
                "updated": "2025-03-10T09:54:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    54,
                    40,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:54:40Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    54,
                    40,
                    0,
                    69,
                    0
                ],
                "title": "Learning A Zero-shot Occupancy Network from Vision Foundation Models via\n  Self-supervised Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning A Zero-shot Occupancy Network from Vision Foundation Models via\n  Self-supervised Adaptation"
                },
                "summary": "Estimating the 3D world from 2D monocular images is a fundamental yet\nchallenging task due to the labour-intensive nature of 3D annotations. To\nsimplify label acquisition, this work proposes a novel approach that bridges 2D\nvision foundation models (VFMs) with 3D tasks by decoupling 3D supervision into\nan ensemble of image-level primitives, e.g., semantic and geometric components.\nAs a key motivator, we leverage the zero-shot capabilities of vision-language\nmodels for image semantics. However, due to the notorious ill-posed problem -\nmultiple distinct 3D scenes can produce identical 2D projections, directly\ninferring metric depth from a monocular image in a zero-shot manner is\nunsuitable. In contrast, 2D VFMs provide promising sources of relative depth,\nwhich theoretically aligns with metric depth when properly scaled and offset.\nThus, we adapt the relative depth derived from VFMs into metric depth by\noptimising the scale and offset using temporal consistency, also known as novel\nview synthesis, without access to ground-truth metric depth. Consequently, we\nproject the semantics into 3D space using the reconstructed metric depth,\nthereby providing 3D supervision. Extensive experiments on nuScenes and\nSemanticKITTI demonstrate the effectiveness of our framework. For instance, the\nproposed method surpasses the current state-of-the-art by 3.34% mIoU on\nnuScenes for voxel occupancy prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the 3D world from 2D monocular images is a fundamental yet\nchallenging task due to the labour-intensive nature of 3D annotations. To\nsimplify label acquisition, this work proposes a novel approach that bridges 2D\nvision foundation models (VFMs) with 3D tasks by decoupling 3D supervision into\nan ensemble of image-level primitives, e.g., semantic and geometric components.\nAs a key motivator, we leverage the zero-shot capabilities of vision-language\nmodels for image semantics. However, due to the notorious ill-posed problem -\nmultiple distinct 3D scenes can produce identical 2D projections, directly\ninferring metric depth from a monocular image in a zero-shot manner is\nunsuitable. In contrast, 2D VFMs provide promising sources of relative depth,\nwhich theoretically aligns with metric depth when properly scaled and offset.\nThus, we adapt the relative depth derived from VFMs into metric depth by\noptimising the scale and offset using temporal consistency, also known as novel\nview synthesis, without access to ground-truth metric depth. Consequently, we\nproject the semantics into 3D space using the reconstructed metric depth,\nthereby providing 3D supervision. Extensive experiments on nuScenes and\nSemanticKITTI demonstrate the effectiveness of our framework. For instance, the\nproposed method surpasses the current state-of-the-art by 3.34% mIoU on\nnuScenes for voxel occupancy prediction."
                },
                "authors": [
                    {
                        "name": "Sihao Lin"
                    },
                    {
                        "name": "Daqi Liu"
                    },
                    {
                        "name": "Ruochong Fu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Andy Song"
                    },
                    {
                        "name": "Hongwei Xie"
                    },
                    {
                        "name": "Zhihui Li"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Xiaojun Chang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Chang"
                },
                "author": "Xiaojun Chang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03946v2",
                "updated": "2025-03-10T09:52:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    52,
                    25,
                    0,
                    69,
                    0
                ],
                "published": "2024-09-06T00:02:09Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    2,
                    9,
                    4,
                    250,
                    0
                ],
                "title": "On The Role of Prompt Construction In Enhancing Efficacy and Efficiency\n  of LLM-Based Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Role of Prompt Construction In Enhancing Efficacy and Efficiency\n  of LLM-Based Tabular Data Generation"
                },
                "summary": "LLM-based data generation for real-world tabular data can be challenged by\nthe lack of sufficient semantic context in feature names used to describe\ncolumns. We hypothesize that enriching prompts with domain-specific insights\ncan improve both the quality and efficiency of data generation. To test this\nhypothesis, we explore three prompt construction protocols: Expert-guided,\nLLM-guided, and Novel-Mapping. Through empirical studies with the recently\nproposed GReaT framework, we find that context-enriched prompts lead to\nsignificantly improved data generation quality and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based data generation for real-world tabular data can be challenged by\nthe lack of sufficient semantic context in feature names used to describe\ncolumns. We hypothesize that enriching prompts with domain-specific insights\ncan improve both the quality and efficiency of data generation. To test this\nhypothesis, we explore three prompt construction protocols: Expert-guided,\nLLM-guided, and Novel-Mapping. Through empirical studies with the recently\nproposed GReaT framework, we find that context-enriched prompts lead to\nsignificantly improved data generation quality and training efficiency."
                },
                "authors": [
                    {
                        "name": "Banooqa Banday"
                    },
                    {
                        "name": "Kowshik Thopalli"
                    },
                    {
                        "name": "Tanzima Z. Islam"
                    },
                    {
                        "name": "Jayaraman J. Thiagarajan"
                    }
                ],
                "author_detail": {
                    "name": "Jayaraman J. Thiagarajan"
                },
                "author": "Jayaraman J. Thiagarajan",
                "arxiv_comment": "Accepted to IEEE ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12827v3",
                "updated": "2025-03-10T09:51:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    51,
                    28,
                    0,
                    69,
                    0
                ],
                "published": "2024-04-19T12:04:32Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    4,
                    32,
                    4,
                    110,
                    0
                ],
                "title": "An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical\n  Trial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical\n  Trial Results"
                },
                "summary": "Adverse drug events (ADEs) are a major safety issue in clinical trials. Thus,\npredicting ADEs is key to developing safer medications and enhancing patient\noutcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel\nADE prediction in monopharmacy treatments. CT-ADE encompasses 2,497 drugs and\n168,984 drug-ADE pairs from clinical trial results, annotated using the MedDRA\nontology. Unlike existing resources, CT-ADE integrates treatment and target\npopulation data, enabling comparative analyses under varying conditions, such\nas dosage, administration route, and demographics. In addition, CT-ADE\nsystematically collects all ADEs in the study population, including positive\nand negative cases. To provide a baseline for ADE prediction performance using\nthe CT-ADE dataset, we conducted analyses using large language models (LLMs).\nThe best LLM achieved an F1-score of 56%, with models incorporating treatment\nand patient information outperforming by 21%-38% those relying solely on the\nchemical structure. These findings underscore the importance of contextual\ninformation in ADE prediction and establish CT-ADE as a robust resource for\nsafety risk assessment in pharmaceutical research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adverse drug events (ADEs) are a major safety issue in clinical trials. Thus,\npredicting ADEs is key to developing safer medications and enhancing patient\noutcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel\nADE prediction in monopharmacy treatments. CT-ADE encompasses 2,497 drugs and\n168,984 drug-ADE pairs from clinical trial results, annotated using the MedDRA\nontology. Unlike existing resources, CT-ADE integrates treatment and target\npopulation data, enabling comparative analyses under varying conditions, such\nas dosage, administration route, and demographics. In addition, CT-ADE\nsystematically collects all ADEs in the study population, including positive\nand negative cases. To provide a baseline for ADE prediction performance using\nthe CT-ADE dataset, we conducted analyses using large language models (LLMs).\nThe best LLM achieved an F1-score of 56%, with models incorporating treatment\nand patient information outperforming by 21%-38% those relying solely on the\nchemical structure. These findings underscore the importance of contextual\ninformation in ADE prediction and establish CT-ADE as a robust resource for\nsafety risk assessment in pharmaceutical research and development."
                },
                "authors": [
                    {
                        "name": "Anthony Yazdani"
                    },
                    {
                        "name": "Alban Bornet"
                    },
                    {
                        "name": "Philipp Khlebnikov"
                    },
                    {
                        "name": "Boya Zhang"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Poorya Amini"
                    },
                    {
                        "name": "Douglas Teodoro"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Teodoro"
                },
                "author": "Douglas Teodoro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03135v2",
                "updated": "2025-03-10T09:51:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    51,
                    5,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-05T03:15:38Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    3,
                    15,
                    38,
                    2,
                    64,
                    0
                ],
                "title": "Bridging Molecular Graphs and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Molecular Graphs and Large Language Models"
                },
                "summary": "While Large Language Models (LLMs) have shown exceptional generalization\ncapabilities, their ability to process graph data, such as molecular\nstructures, remains limited. To bridge this gap, this paper proposes\nGraph2Token, an efficient solution that aligns graph tokens to LLM tokens. The\nkey idea is to represent a graph token with the LLM token vocabulary, without\nfine-tuning the LLM backbone. To achieve this goal, we first construct a\nmolecule-text paired dataset from multisources, including CHEBI and HMDB, to\ntrain a graph structure encoder, which reduces the distance between graphs and\ntexts representations in the feature space. Then, we propose a novel alignment\nstrategy that associates a graph token with LLM tokens. To further unleash the\npotential of LLMs, we collect molecular IUPAC name identifiers, which are\nincorporated into the LLM prompts. By aligning molecular graphs as special\ntokens, we can activate LLM generalization ability to molecular few-shot\nlearning. Extensive experiments on molecular classification and regression\ntasks demonstrate the effectiveness of our proposed Graph2Token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown exceptional generalization\ncapabilities, their ability to process graph data, such as molecular\nstructures, remains limited. To bridge this gap, this paper proposes\nGraph2Token, an efficient solution that aligns graph tokens to LLM tokens. The\nkey idea is to represent a graph token with the LLM token vocabulary, without\nfine-tuning the LLM backbone. To achieve this goal, we first construct a\nmolecule-text paired dataset from multisources, including CHEBI and HMDB, to\ntrain a graph structure encoder, which reduces the distance between graphs and\ntexts representations in the feature space. Then, we propose a novel alignment\nstrategy that associates a graph token with LLM tokens. To further unleash the\npotential of LLMs, we collect molecular IUPAC name identifiers, which are\nincorporated into the LLM prompts. By aligning molecular graphs as special\ntokens, we can activate LLM generalization ability to molecular few-shot\nlearning. Extensive experiments on molecular classification and regression\ntasks demonstrate the effectiveness of our proposed Graph2Token."
                },
                "authors": [
                    {
                        "name": "Runze Wang"
                    },
                    {
                        "name": "Mingqi Yang"
                    },
                    {
                        "name": "Yanming Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yanming Shen"
                },
                "author": "Yanming Shen",
                "arxiv_comment": "AAAI 2025 camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07119v1",
                "updated": "2025-03-10T09:48:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    48,
                    52,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:48:52Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    48,
                    52,
                    0,
                    69,
                    0
                ],
                "title": "Improving Deep Ensembles by Estimating Confusion Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Deep Ensembles by Estimating Confusion Matrices"
                },
                "summary": "Ensembling in deep learning improves accuracy and calibration over single\nnetworks. The traditional aggregation approach, ensemble averaging, treats all\nindividual networks equally by averaging their outputs. Inspired by\ncrowdsourcing we propose an aggregation method called soft Dawid Skene for deep\nensembles that estimates confusion matrices of ensemble members and weighs them\naccording to their inferred performance. Soft Dawid Skene aggregates soft\nlabels in contrast to hard labels often used in crowdsourcing. We empirically\nshow the superiority of soft Dawid Skene in accuracy, calibration and out of\ndistribution detection in comparison to ensemble averaging in extensive\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensembling in deep learning improves accuracy and calibration over single\nnetworks. The traditional aggregation approach, ensemble averaging, treats all\nindividual networks equally by averaging their outputs. Inspired by\ncrowdsourcing we propose an aggregation method called soft Dawid Skene for deep\nensembles that estimates confusion matrices of ensemble members and weighs them\naccording to their inferred performance. Soft Dawid Skene aggregates soft\nlabels in contrast to hard labels often used in crowdsourcing. We empirically\nshow the superiority of soft Dawid Skene in accuracy, calibration and out of\ndistribution detection in comparison to ensemble averaging in extensive\nexperiments."
                },
                "authors": [
                    {
                        "name": "Danil Kuzin"
                    },
                    {
                        "name": "Olga Isupova"
                    },
                    {
                        "name": "Steven Reece"
                    },
                    {
                        "name": "Brooke D Simmons"
                    }
                ],
                "author_detail": {
                    "name": "Brooke D Simmons"
                },
                "author": "Brooke D Simmons",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07114v1",
                "updated": "2025-03-10T09:38:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    38,
                    35,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:38:35Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    38,
                    35,
                    0,
                    69,
                    0
                ],
                "title": "Sequential Function-Space Variational Inference via Gaussian Mixture\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Function-Space Variational Inference via Gaussian Mixture\n  Approximation"
                },
                "summary": "Continual learning is learning from a sequence of tasks with the aim of\nlearning new tasks without forgetting old tasks. Sequential function-space\nvariational inference (SFSVI) is a continual learning method based on\nvariational inference which uses a Gaussian variational distribution to\napproximate the distribution of the outputs of a finite number of selected\ninducing points. Since the posterior distribution of a neural network is\nmulti-modal, a Gaussian distribution could only match one mode of the posterior\ndistribution, and a Gaussian mixture distribution could be used to better\napproximate the posterior distribution. We propose an SFSVI method which uses a\nGaussian mixture variational distribution. We also compare different types of\nvariational inference methods with and without a fixed pre-trained feature\nextractor. We find that in terms of final average accuracy, Gaussian mixture\nmethods perform better than Gaussian methods and likelihood-focused methods\nperform better than prior-focused methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning is learning from a sequence of tasks with the aim of\nlearning new tasks without forgetting old tasks. Sequential function-space\nvariational inference (SFSVI) is a continual learning method based on\nvariational inference which uses a Gaussian variational distribution to\napproximate the distribution of the outputs of a finite number of selected\ninducing points. Since the posterior distribution of a neural network is\nmulti-modal, a Gaussian distribution could only match one mode of the posterior\ndistribution, and a Gaussian mixture distribution could be used to better\napproximate the posterior distribution. We propose an SFSVI method which uses a\nGaussian mixture variational distribution. We also compare different types of\nvariational inference methods with and without a fixed pre-trained feature\nextractor. We find that in terms of final average accuracy, Gaussian mixture\nmethods perform better than Gaussian methods and likelihood-focused methods\nperform better than prior-focused methods."
                },
                "authors": [
                    {
                        "name": "Menghao Waiyan William Zhu"
                    },
                    {
                        "name": "Pengcheng Hao"
                    },
                    {
                        "name": "Ercan Engin Kuruoğlu"
                    }
                ],
                "author_detail": {
                    "name": "Ercan Engin Kuruoğlu"
                },
                "author": "Ercan Engin Kuruoğlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14362v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14362v5",
                "updated": "2025-03-10T09:35:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    35,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2024-03-21T12:45:01Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    12,
                    45,
                    1,
                    3,
                    81,
                    0
                ],
                "title": "Enabling Generalized Zero-shot Learning Towards Unseen Domains by\n  Intrinsic Learning from Redundant LLM Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Generalized Zero-shot Learning Towards Unseen Domains by\n  Intrinsic Learning from Redundant LLM Semantics"
                },
                "summary": "Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we study cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods,\nCDGZSL constructs a common feature space across domains and acquires the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class similarity alignment, which eliminates the\nnon-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and unseen-class meta generation, which\npreserves intrinsic semantics to maintain connectivity between seen and unseen\nclasses by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\ntwo datasets, Office-Home and Mini-DomainNet, and we have shared the LLM-based\nsemantics for these datasets as a benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we study cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods,\nCDGZSL constructs a common feature space across domains and acquires the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class similarity alignment, which eliminates the\nnon-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and unseen-class meta generation, which\npreserves intrinsic semantics to maintain connectivity between seen and unseen\nclasses by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\ntwo datasets, Office-Home and Mini-DomainNet, and we have shared the LLM-based\nsemantics for these datasets as a benchmark."
                },
                "authors": [
                    {
                        "name": "Jiaqi Yue"
                    },
                    {
                        "name": "Chunhui Zhao"
                    },
                    {
                        "name": "Jiancheng Zhao"
                    },
                    {
                        "name": "Biao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Biao Huang"
                },
                "author": "Biao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14362v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14362v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17477v2",
                "updated": "2025-03-10T09:32:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    32,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2024-01-30T22:22:55Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    22,
                    22,
                    55,
                    1,
                    30,
                    0
                ],
                "title": "Detecting mental disorder on social media: a ChatGPT-augmented\n  explainable approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting mental disorder on social media: a ChatGPT-augmented\n  explainable approach"
                },
                "summary": "In the digital era, the prevalence of depressive symptoms expressed on social\nmedia has raised serious concerns, necessitating advanced methodologies for\ntimely detection. This paper addresses the challenge of interpretable\ndepression detection by proposing a novel methodology that effectively combines\nLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\nconversational agents like ChatGPT. In our methodology, explanations are\nachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\nnovel self-explanatory model, namely BERT-XDD, capable of providing both\nclassification and explanations via masked attention. The interpretability is\nfurther enhanced using ChatGPT to transform technical explanations into\nhuman-readable commentaries. By introducing an effective and modular approach\nfor interpretable depression detection, our methodology can contribute to the\ndevelopment of socially responsible digital platforms, fostering early\nintervention and support for mental health challenges under the guidance of\nqualified healthcare professionals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital era, the prevalence of depressive symptoms expressed on social\nmedia has raised serious concerns, necessitating advanced methodologies for\ntimely detection. This paper addresses the challenge of interpretable\ndepression detection by proposing a novel methodology that effectively combines\nLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\nconversational agents like ChatGPT. In our methodology, explanations are\nachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\nnovel self-explanatory model, namely BERT-XDD, capable of providing both\nclassification and explanations via masked attention. The interpretability is\nfurther enhanced using ChatGPT to transform technical explanations into\nhuman-readable commentaries. By introducing an effective and modular approach\nfor interpretable depression detection, our methodology can contribute to the\ndevelopment of socially responsible digital platforms, fostering early\nintervention and support for mental health challenges under the guidance of\nqualified healthcare professionals."
                },
                "authors": [
                    {
                        "name": "Loris Belcastro"
                    },
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Fabrizio Marozzo"
                    },
                    {
                        "name": "Domenico Talia"
                    },
                    {
                        "name": "Paolo Trunfio"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Trunfio"
                },
                "author": "Paolo Trunfio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07107v1",
                "updated": "2025-03-10T09:31:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    31,
                    32,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:31:32Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    31,
                    32,
                    0,
                    69,
                    0
                ],
                "title": "Towards Experience Replay for Class-Incremental Learning in Fully-Binary\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Experience Replay for Class-Incremental Learning in Fully-Binary\n  Networks"
                },
                "summary": "Binary Neural Networks (BNNs) are a promising approach to enable Artificial\nNeural Network (ANN) implementation on ultra-low power edge devices. Such\ndevices may compute data in highly dynamic environments, in which the classes\ntargeted for inference can evolve or even novel classes may arise, requiring\ncontinual learning. Class Incremental Learning (CIL) is a common type of\ncontinual learning for classification problems, that has been scarcely\naddressed in the context of BNNs. Furthermore, most of existing BNNs models are\nnot fully binary, as they require several real-valued network layers, at the\ninput, the output, and for batch normalization. This paper goes a step further,\nenabling class incremental learning in Fully-Binarized NNs (FBNNs) through four\nmain contributions. We firstly revisit the FBNN design and its training\nprocedure that is suitable to CIL. Secondly, we explore loss balancing, a\nmethod to trade-off the performance of past and current classes. Thirdly, we\npropose a semi-supervised method to pre-train the feature extractor of the FBNN\nfor transferable representations. Fourthly, two conventional CIL methods, \\ie,\nLatent and Native replay, are thoroughly compared. These contributions are\nexemplified first on the CIFAR100 dataset, before being scaled up to address\nthe CORE50 continual learning benchmark. The final results based on our 3Mb\nFBNN on CORE50 exhibit at par and better performance than conventional\nreal-valued larger NN models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary Neural Networks (BNNs) are a promising approach to enable Artificial\nNeural Network (ANN) implementation on ultra-low power edge devices. Such\ndevices may compute data in highly dynamic environments, in which the classes\ntargeted for inference can evolve or even novel classes may arise, requiring\ncontinual learning. Class Incremental Learning (CIL) is a common type of\ncontinual learning for classification problems, that has been scarcely\naddressed in the context of BNNs. Furthermore, most of existing BNNs models are\nnot fully binary, as they require several real-valued network layers, at the\ninput, the output, and for batch normalization. This paper goes a step further,\nenabling class incremental learning in Fully-Binarized NNs (FBNNs) through four\nmain contributions. We firstly revisit the FBNN design and its training\nprocedure that is suitable to CIL. Secondly, we explore loss balancing, a\nmethod to trade-off the performance of past and current classes. Thirdly, we\npropose a semi-supervised method to pre-train the feature extractor of the FBNN\nfor transferable representations. Fourthly, two conventional CIL methods, \\ie,\nLatent and Native replay, are thoroughly compared. These contributions are\nexemplified first on the CIFAR100 dataset, before being scaled up to address\nthe CORE50 continual learning benchmark. The final results based on our 3Mb\nFBNN on CORE50 exhibit at par and better performance than conventional\nreal-valued larger NN models."
                },
                "authors": [
                    {
                        "name": "Yanis Basso-Bert"
                    },
                    {
                        "name": "Anca Molnos"
                    },
                    {
                        "name": "Romain Lemaire"
                    },
                    {
                        "name": "William Guicquero"
                    },
                    {
                        "name": "Antoine Dupret"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Dupret"
                },
                "author": "Antoine Dupret",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07104v1",
                "updated": "2025-03-10T09:27:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    27,
                    7,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:27:07Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    27,
                    7,
                    0,
                    69,
                    0
                ],
                "title": "Global Context Is All You Need for Parallel Efficient Tractography\n  Parcellation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Context Is All You Need for Parallel Efficient Tractography\n  Parcellation"
                },
                "summary": "Whole-brain tractography in diffusion MRI is often followed by a parcellation\nin which each streamline is classified as belonging to a specific white matter\nbundle, or discarded as a false positive. Efficient parcellation is important\nboth in large-scale studies, which have to process huge amounts of data, and in\nthe clinic, where computational resources are often limited. TractCloud is a\nstate-of-the-art approach that aims to maximize accuracy with a local-global\nrepresentation. We demonstrate that the local context does not contribute to\nthe accuracy of that approach, and is even detrimental when dealing with\npathological cases. Based on this observation, we propose PETParc, a new method\nfor Parallel Efficient Tractography Parcellation. PETParc is a\ntransformer-based architecture in which the whole-brain tractogram is randomly\npartitioned into sub-tractograms whose streamlines are classified in parallel,\nwhile serving as global context for each other. This leads to a speedup of up\nto two orders of magnitude relative to TractCloud, and permits inference even\non clinical workstations without a GPU. PETParc accounts for the lack of\nstreamline orientation either via a novel flip-invariant embedding, or by\nsimply using flips as part of data augmentation. Despite the speedup, results\nare often even better than those of prior methods. The code and pretrained\nmodel will be made public upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole-brain tractography in diffusion MRI is often followed by a parcellation\nin which each streamline is classified as belonging to a specific white matter\nbundle, or discarded as a false positive. Efficient parcellation is important\nboth in large-scale studies, which have to process huge amounts of data, and in\nthe clinic, where computational resources are often limited. TractCloud is a\nstate-of-the-art approach that aims to maximize accuracy with a local-global\nrepresentation. We demonstrate that the local context does not contribute to\nthe accuracy of that approach, and is even detrimental when dealing with\npathological cases. Based on this observation, we propose PETParc, a new method\nfor Parallel Efficient Tractography Parcellation. PETParc is a\ntransformer-based architecture in which the whole-brain tractogram is randomly\npartitioned into sub-tractograms whose streamlines are classified in parallel,\nwhile serving as global context for each other. This leads to a speedup of up\nto two orders of magnitude relative to TractCloud, and permits inference even\non clinical workstations without a GPU. PETParc accounts for the lack of\nstreamline orientation either via a novel flip-invariant embedding, or by\nsimply using flips as part of data augmentation. Despite the speedup, results\nare often even better than those of prior methods. The code and pretrained\nmodel will be made public upon acceptance."
                },
                "authors": [
                    {
                        "name": "Valentin von Bornhaupt"
                    },
                    {
                        "name": "Johannes Grün"
                    },
                    {
                        "name": "and Justus Bisten"
                    },
                    {
                        "name": "Tobias Bauer"
                    },
                    {
                        "name": "Theodor Rüber"
                    },
                    {
                        "name": "Thomas Schultz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Schultz"
                },
                "author": "Thomas Schultz",
                "arxiv_comment": "8 pages, 2 pages references, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T10, 92C55, 65Y05, 68U10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.5.4; J.3; C.1.4; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04475v2",
                "updated": "2025-03-10T09:27:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    27,
                    3,
                    0,
                    69,
                    0
                ],
                "published": "2024-04-06T02:29:02Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    2,
                    29,
                    2,
                    5,
                    97,
                    0
                ],
                "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic\n  Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic\n  Evaluators"
                },
                "summary": "LLM-based auto-annotators have become a key component of the LLM development\nprocess due to their cost-effectiveness and scalability compared to human-based\nevaluation. However, these auto-annotators can introduce biases that are hard\nto remove. Even simple, known confounders such as preference for longer outputs\nremain in existing automated evaluation metrics. We propose a simple regression\nanalysis approach for controlling biases in auto-evaluations. As a real case\nstudy, we focus on reducing the length bias of AlpacaEval, a fast and\naffordable benchmark for instruction-tuned LLMs that uses LLMs to estimate\nresponse quality. Despite being highly correlated with human preferences,\nAlpacaEval is known to favor models that generate longer outputs. We introduce\na length-controlled AlpacaEval that aims to answer the counterfactual question:\n\"What would the preference be if the model's and baseline's output had the same\nlength?\" To achieve this, we first fit a generalized linear model to predict\nthe biased auto-annotator's preferences based on the mediators we want to\ncontrol for (length difference) and other relevant features. We then obtain\nlength-controlled preferences by predicting preferences while conditioning the\nGLM with a zero difference in lengths. Length-controlling not only improves the\nrobustness of the metric to manipulations in model verbosity, but we also find\nthat it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94\nto 0.98.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based auto-annotators have become a key component of the LLM development\nprocess due to their cost-effectiveness and scalability compared to human-based\nevaluation. However, these auto-annotators can introduce biases that are hard\nto remove. Even simple, known confounders such as preference for longer outputs\nremain in existing automated evaluation metrics. We propose a simple regression\nanalysis approach for controlling biases in auto-evaluations. As a real case\nstudy, we focus on reducing the length bias of AlpacaEval, a fast and\naffordable benchmark for instruction-tuned LLMs that uses LLMs to estimate\nresponse quality. Despite being highly correlated with human preferences,\nAlpacaEval is known to favor models that generate longer outputs. We introduce\na length-controlled AlpacaEval that aims to answer the counterfactual question:\n\"What would the preference be if the model's and baseline's output had the same\nlength?\" To achieve this, we first fit a generalized linear model to predict\nthe biased auto-annotator's preferences based on the mediators we want to\ncontrol for (length difference) and other relevant features. We then obtain\nlength-controlled preferences by predicting preferences while conditioning the\nGLM with a zero difference in lengths. Length-controlling not only improves the\nrobustness of the metric to manipulations in model verbosity, but we also find\nthat it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94\nto 0.98."
                },
                "authors": [
                    {
                        "name": "Yann Dubois"
                    },
                    {
                        "name": "Balázs Galambosi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori B. Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori B. Hashimoto"
                },
                "author": "Tatsunori B. Hashimoto",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07103v1",
                "updated": "2025-03-10T09:26:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    26,
                    8,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:26:08Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    26,
                    8,
                    0,
                    69,
                    0
                ],
                "title": "Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication"
                },
                "summary": "Large Language Models (LLMs) have shown an impressive capability in code\ngeneration and, specifically, to automatically implement requirements described\nin natural language. The LLM effectiveness generally increases with its size:\nThe higher the number of LLM's trainable parameters the better its ability to\nimplement code. However, when it comes to deploying LLM-based code generators,\nlarger LLMs pose significant challenges related to their memory (and,\nconsequently, carbon) footprint. A previous work by Wei et al. proposed to\nleverage quantization techniques to reduce the memory footprint of LLM-based\ncode generators without substantially degrading their effectiveness. In short,\nthey studied LLMs featuring up to 16B parameters, quantizing their precision\nfrom floating point 32 bits down to int 8 bits and showing their limited impact\non code generation performance. Given the fast pace at which LLM capabilities\nand quantization techniques are evolving, in this work we present a\ndifferentiated replication of the work by Wei et al. in which we consider (i)\non the one side, more recent and larger code-related LLMs, of up to 34B\nparameters; (ii) the latest advancements in model quantization techniques,\nwhich allow pushing the compression to the extreme quantization level of 2 bits\nper model parameter and; (iii) different types of calibration datasets to guide\nthe quantization process, including code-specific ones. Our empirical\nevaluation reveals that the new frontier for LLM quantization is 4-bit\nprecision, resulting in an average memory footprint reduction of 70% compared\nto the original model without observing any significant decrease in\nperformance. Additionally, when the quantization becomes even more extreme (3\nand 2 bits), a code-specific calibration dataset helps to limit the loss of\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown an impressive capability in code\ngeneration and, specifically, to automatically implement requirements described\nin natural language. The LLM effectiveness generally increases with its size:\nThe higher the number of LLM's trainable parameters the better its ability to\nimplement code. However, when it comes to deploying LLM-based code generators,\nlarger LLMs pose significant challenges related to their memory (and,\nconsequently, carbon) footprint. A previous work by Wei et al. proposed to\nleverage quantization techniques to reduce the memory footprint of LLM-based\ncode generators without substantially degrading their effectiveness. In short,\nthey studied LLMs featuring up to 16B parameters, quantizing their precision\nfrom floating point 32 bits down to int 8 bits and showing their limited impact\non code generation performance. Given the fast pace at which LLM capabilities\nand quantization techniques are evolving, in this work we present a\ndifferentiated replication of the work by Wei et al. in which we consider (i)\non the one side, more recent and larger code-related LLMs, of up to 34B\nparameters; (ii) the latest advancements in model quantization techniques,\nwhich allow pushing the compression to the extreme quantization level of 2 bits\nper model parameter and; (iii) different types of calibration datasets to guide\nthe quantization process, including code-specific ones. Our empirical\nevaluation reveals that the new frontier for LLM quantization is 4-bit\nprecision, resulting in an average memory footprint reduction of 70% compared\nto the original model without observing any significant decrease in\nperformance. Additionally, when the quantization becomes even more extreme (3\nand 2 bits), a code-specific calibration dataset helps to limit the loss of\nperformance."
                },
                "authors": [
                    {
                        "name": "Alessandro Giagnorio"
                    },
                    {
                        "name": "Antonio Mastropaolo"
                    },
                    {
                        "name": "Saima Afrin"
                    },
                    {
                        "name": "Massimiliano Di Penta"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16498v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16498v4",
                "updated": "2025-03-10T09:20:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    20,
                    24,
                    0,
                    69,
                    0
                ],
                "published": "2024-05-26T09:20:47Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    9,
                    20,
                    47,
                    6,
                    147,
                    0
                ],
                "title": "On Sequential Maximum a Posteriori Inference for Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Sequential Maximum a Posteriori Inference for Continual Learning"
                },
                "summary": "We formulate sequential maximum a posteriori inference as a recursion of loss\nfunctions and reduce the problem of continual learning to approximating the\nprevious loss function. We then propose two coreset-free methods: autodiff\nquadratic consolidation, which uses an accurate and full quadratic\napproximation, and neural consolidation, which uses a neural network\napproximation. These methods are not scalable with respect to the neural\nnetwork size, and we study them for classification tasks in combination with a\nfixed pre-trained feature extractor. We also introduce simple but challenging\nclassical task sequences based on Iris and Wine datasets. We find that neural\nconsolidation performs well in the classical task sequences, where the input\ndimension is small, while autodiff quadratic consolidation performs\nconsistently well in image task sequences with a fixed pre-trained feature\nextractor, achieving comparable performance to joint maximum a posteriori\ntraining in many cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formulate sequential maximum a posteriori inference as a recursion of loss\nfunctions and reduce the problem of continual learning to approximating the\nprevious loss function. We then propose two coreset-free methods: autodiff\nquadratic consolidation, which uses an accurate and full quadratic\napproximation, and neural consolidation, which uses a neural network\napproximation. These methods are not scalable with respect to the neural\nnetwork size, and we study them for classification tasks in combination with a\nfixed pre-trained feature extractor. We also introduce simple but challenging\nclassical task sequences based on Iris and Wine datasets. We find that neural\nconsolidation performs well in the classical task sequences, where the input\ndimension is small, while autodiff quadratic consolidation performs\nconsistently well in image task sequences with a fixed pre-trained feature\nextractor, achieving comparable performance to joint maximum a posteriori\ntraining in many cases."
                },
                "authors": [
                    {
                        "name": "Menghao Waiyan William Zhu"
                    },
                    {
                        "name": "Ercan Engin Kuruoğlu"
                    }
                ],
                "author_detail": {
                    "name": "Ercan Engin Kuruoğlu"
                },
                "author": "Ercan Engin Kuruoğlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16498v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16498v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07094v1",
                "updated": "2025-03-10T09:19:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    19,
                    55,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:19:55Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    19,
                    55,
                    0,
                    69,
                    0
                ],
                "title": "A Novel Ophthalmic Benchmark for Evaluating Multimodal Large Language\n  Models with Fundus Photographs and OCT Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Ophthalmic Benchmark for Evaluating Multimodal Large Language\n  Models with Fundus Photographs and OCT Images"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated remarkable\npotential across various medical applications. Building on this foundation,\nmultimodal large language models (MLLMs) integrate LLMs with visual models to\nprocess diverse inputs, including clinical data and medical images. In\nophthalmology, LLMs have been explored for analyzing optical coherence\ntomography (OCT) reports, assisting in disease classification, and even\npredicting treatment outcomes. However, existing MLLM benchmarks often fail to\ncapture the complexities of real-world clinical practice, particularly in the\nanalysis of OCT images. Many suffer from limitations such as small sample\nsizes, a lack of diverse OCT datasets, and insufficient expert validation.\nThese shortcomings hinder the accurate assessment of MLLMs' ability to\ninterpret OCT scans and their broader applicability in ophthalmology. Our\ndataset, curated through rigorous quality control and expert annotation,\nconsists of 439 fundus images and 75 OCT images. Using a standardized API-based\nframework, we assessed seven mainstream MLLMs and observed significant\nvariability in diagnostic accuracy across different diseases. While some models\nperformed well in diagnosing conditions such as diabetic retinopathy and\nage-related macular degeneration, they struggled with others, including\nchoroidal neovascularization and myopia, highlighting inconsistencies in\nperformance and the need for further refinement. Our findings emphasize the\nimportance of developing clinically relevant benchmarks to provide a more\naccurate assessment of MLLMs' capabilities. By refining these models and\nexpanding their scope, we can enhance their potential to transform ophthalmic\ndiagnosis and treatment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated remarkable\npotential across various medical applications. Building on this foundation,\nmultimodal large language models (MLLMs) integrate LLMs with visual models to\nprocess diverse inputs, including clinical data and medical images. In\nophthalmology, LLMs have been explored for analyzing optical coherence\ntomography (OCT) reports, assisting in disease classification, and even\npredicting treatment outcomes. However, existing MLLM benchmarks often fail to\ncapture the complexities of real-world clinical practice, particularly in the\nanalysis of OCT images. Many suffer from limitations such as small sample\nsizes, a lack of diverse OCT datasets, and insufficient expert validation.\nThese shortcomings hinder the accurate assessment of MLLMs' ability to\ninterpret OCT scans and their broader applicability in ophthalmology. Our\ndataset, curated through rigorous quality control and expert annotation,\nconsists of 439 fundus images and 75 OCT images. Using a standardized API-based\nframework, we assessed seven mainstream MLLMs and observed significant\nvariability in diagnostic accuracy across different diseases. While some models\nperformed well in diagnosing conditions such as diabetic retinopathy and\nage-related macular degeneration, they struggled with others, including\nchoroidal neovascularization and myopia, highlighting inconsistencies in\nperformance and the need for further refinement. Our findings emphasize the\nimportance of developing clinically relevant benchmarks to provide a more\naccurate assessment of MLLMs' capabilities. By refining these models and\nexpanding their scope, we can enhance their potential to transform ophthalmic\ndiagnosis and treatment."
                },
                "authors": [
                    {
                        "name": "Xiaoyi Liang"
                    },
                    {
                        "name": "Mouxiao Bian"
                    },
                    {
                        "name": "Moxin Chen"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Lin Li"
                    }
                ],
                "author_detail": {
                    "name": "Lin Li"
                },
                "author": "Lin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14297v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14297v4",
                "updated": "2025-03-10T09:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    17,
                    56,
                    0,
                    69,
                    0
                ],
                "published": "2024-05-23T08:18:30Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    8,
                    18,
                    30,
                    3,
                    144,
                    0
                ],
                "title": "Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient\n  Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient\n  Transformer Models"
                },
                "summary": "The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the\nefficiency of training and inference for Transformer-based foundational models,\nyielding promising results.However, the performance of SMoE heavily depends on\nthe choice of hyper-parameters, such as the number of experts and the number of\nexperts to be activated (referred to as top-k), resulting in significant\ncomputational overhead due to the extensive model training by searching over\nvarious hyper-parameter configurations. As a remedy, we introduce the Dynamic\nMixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating\nmethod that enables each token to automatically determine the number of experts\nto activate. (2) An adaptive process automatically adjusts the number of\nexperts during training. Extensive numerical results across Vision, Language,\nand Vision-Language tasks demonstrate the effectiveness of our approach to\nachieve competitive performance compared to GMoE for vision and language tasks,\nand MoE-LLaVA for vision-language tasks, while maintaining efficiency by\nactivating fewer parameters. Our code is available at\nhttps://github.com/LINs-lab/DynMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the\nefficiency of training and inference for Transformer-based foundational models,\nyielding promising results.However, the performance of SMoE heavily depends on\nthe choice of hyper-parameters, such as the number of experts and the number of\nexperts to be activated (referred to as top-k), resulting in significant\ncomputational overhead due to the extensive model training by searching over\nvarious hyper-parameter configurations. As a remedy, we introduce the Dynamic\nMixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating\nmethod that enables each token to automatically determine the number of experts\nto activate. (2) An adaptive process automatically adjusts the number of\nexperts during training. Extensive numerical results across Vision, Language,\nand Vision-Language tasks demonstrate the effectiveness of our approach to\nachieve competitive performance compared to GMoE for vision and language tasks,\nand MoE-LLaVA for vision-language tasks, while maintaining efficiency by\nactivating fewer parameters. Our code is available at\nhttps://github.com/LINs-lab/DynMoE."
                },
                "authors": [
                    {
                        "name": "Yongxin Guo"
                    },
                    {
                        "name": "Zhenglin Cheng"
                    },
                    {
                        "name": "Xiaoying Tang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14297v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14297v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07078v1",
                "updated": "2025-03-10T09:00:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    0,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:00:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    0,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Linguistic Knowledge Transfer Learning for Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Knowledge Transfer Learning for Speech Enhancement"
                },
                "summary": "Linguistic knowledge plays a crucial role in spoken language comprehension.\nIt provides essential semantic and syntactic context for speech perception in\nnoisy environments. However, most speech enhancement (SE) methods predominantly\nrely on acoustic features to learn the mapping relationship between noisy and\nclean speech, with limited exploration of linguistic integration. While\ntext-informed SE approaches have been investigated, they often require explicit\nspeech-text alignment or externally provided textual data, constraining their\npracticality in real-world scenarios. Additionally, using text as input poses\nchallenges in aligning linguistic and acoustic representations due to their\ninherent differences. In this study, we propose the Cross-Modality Knowledge\nTransfer (CMKT) learning framework, which leverages pre-trained large language\nmodels (LLMs) to infuse linguistic knowledge into SE models without requiring\ntext input or LLMs during inference. Furthermore, we introduce a misalignment\nstrategy to improve knowledge transfer. This strategy applies controlled\ntemporal shifts, encouraging the model to learn more robust representations.\nExperimental evaluations demonstrate that CMKT consistently outperforms\nbaseline models across various SE architectures and LLM embeddings,\nhighlighting its adaptability to different configurations. Additionally,\nresults on Mandarin and English datasets confirm its effectiveness across\ndiverse linguistic conditions, further validating its robustness. Moreover,\nCMKT remains effective even in scenarios without textual data, underscoring its\npracticality for real-world applications. By bridging the gap between\nlinguistic and acoustic modalities, CMKT offers a scalable and innovative\nsolution for integrating linguistic knowledge into SE models, leading to\nsubstantial improvements in both intelligibility and enhancement performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic knowledge plays a crucial role in spoken language comprehension.\nIt provides essential semantic and syntactic context for speech perception in\nnoisy environments. However, most speech enhancement (SE) methods predominantly\nrely on acoustic features to learn the mapping relationship between noisy and\nclean speech, with limited exploration of linguistic integration. While\ntext-informed SE approaches have been investigated, they often require explicit\nspeech-text alignment or externally provided textual data, constraining their\npracticality in real-world scenarios. Additionally, using text as input poses\nchallenges in aligning linguistic and acoustic representations due to their\ninherent differences. In this study, we propose the Cross-Modality Knowledge\nTransfer (CMKT) learning framework, which leverages pre-trained large language\nmodels (LLMs) to infuse linguistic knowledge into SE models without requiring\ntext input or LLMs during inference. Furthermore, we introduce a misalignment\nstrategy to improve knowledge transfer. This strategy applies controlled\ntemporal shifts, encouraging the model to learn more robust representations.\nExperimental evaluations demonstrate that CMKT consistently outperforms\nbaseline models across various SE architectures and LLM embeddings,\nhighlighting its adaptability to different configurations. Additionally,\nresults on Mandarin and English datasets confirm its effectiveness across\ndiverse linguistic conditions, further validating its robustness. Moreover,\nCMKT remains effective even in scenarios without textual data, underscoring its\npracticality for real-world applications. By bridging the gap between\nlinguistic and acoustic modalities, CMKT offers a scalable and innovative\nsolution for integrating linguistic knowledge into SE models, leading to\nsubstantial improvements in both intelligibility and enhancement performance."
                },
                "authors": [
                    {
                        "name": "Kuo-Hsuan Hung"
                    },
                    {
                        "name": "Xugang Lu"
                    },
                    {
                        "name": "Szu-Wei Fu"
                    },
                    {
                        "name": "Huan-Hsin Tseng"
                    },
                    {
                        "name": "Hsin-Yi Lin"
                    },
                    {
                        "name": "Chii-Wann Lin"
                    },
                    {
                        "name": "Yu Tsao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tsao"
                },
                "author": "Yu Tsao",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07076v1",
                "updated": "2025-03-10T08:59:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    59,
                    10,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:59:10Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    59,
                    10,
                    0,
                    69,
                    0
                ],
                "title": "NFIG: Autoregressive Image Generation with Next-Frequency Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NFIG: Autoregressive Image Generation with Next-Frequency Prediction"
                },
                "summary": "Autoregressive models have achieved promising results in natural language\nprocessing. However, for image generation tasks, they encounter substantial\nchallenges in effectively capturing long-range dependencies, managing\ncomputational costs, and most crucially, defining meaningful autoregressive\nsequences that reflect natural image hierarchies. To address these issues, we\npresent \\textbf{N}ext-\\textbf{F}requency \\textbf{I}mage \\textbf{G}eneration\n(\\textbf{NFIG}), a novel framework that decomposes the image generation process\ninto multiple frequency-guided stages. Our approach first generates\nlow-frequency components to establish global structure with fewer tokens, then\nprogressively adds higher-frequency details, following the natural spectral\nhierarchy of images. This principled autoregressive sequence not only improves\nthe quality of generated images by better capturing true causal relationships\nbetween image components, but also significantly reduces computational overhead\nduring inference. Extensive experiments demonstrate that NFIG achieves\nstate-of-the-art performance with fewer steps, offering a more efficient\nsolution for image generation, with 1.25$\\times$ speedup compared to VAR-d20\nwhile achieving better performance (FID: 2.81) on the ImageNet-256 benchmark.\nWe hope that our insight of incorporating frequency-domain knowledge to guide\nautoregressive sequence design will shed light on future research. We will make\nour code publicly available upon acceptance of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models have achieved promising results in natural language\nprocessing. However, for image generation tasks, they encounter substantial\nchallenges in effectively capturing long-range dependencies, managing\ncomputational costs, and most crucially, defining meaningful autoregressive\nsequences that reflect natural image hierarchies. To address these issues, we\npresent \\textbf{N}ext-\\textbf{F}requency \\textbf{I}mage \\textbf{G}eneration\n(\\textbf{NFIG}), a novel framework that decomposes the image generation process\ninto multiple frequency-guided stages. Our approach first generates\nlow-frequency components to establish global structure with fewer tokens, then\nprogressively adds higher-frequency details, following the natural spectral\nhierarchy of images. This principled autoregressive sequence not only improves\nthe quality of generated images by better capturing true causal relationships\nbetween image components, but also significantly reduces computational overhead\nduring inference. Extensive experiments demonstrate that NFIG achieves\nstate-of-the-art performance with fewer steps, offering a more efficient\nsolution for image generation, with 1.25$\\times$ speedup compared to VAR-d20\nwhile achieving better performance (FID: 2.81) on the ImageNet-256 benchmark.\nWe hope that our insight of incorporating frequency-domain knowledge to guide\nautoregressive sequence design will shed light on future research. We will make\nour code publicly available upon acceptance of the paper."
                },
                "authors": [
                    {
                        "name": "Zhihao Huang"
                    },
                    {
                        "name": "Xi Qiu"
                    },
                    {
                        "name": "Yukuo Ma"
                    },
                    {
                        "name": "Yifu Zhou"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "10 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00486v3",
                "updated": "2025-03-10T08:56:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    56,
                    52,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-01T13:24:09Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    13,
                    24,
                    9,
                    5,
                    60,
                    0
                ],
                "title": "Conformal Lyapunov Optimization: Optimal Resource Allocation under\n  Deterministic Reliability Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Lyapunov Optimization: Optimal Resource Allocation under\n  Deterministic Reliability Constraints"
                },
                "summary": "This paper introduces conformal Lyapunov optimization (CLO), a novel resource\nallocation framework for networked systems that optimizes average long-term\nobjectives, while satisfying deterministic long-term reliability constraints.\nUnlike traditional Lyapunov optimization (LO), which addresses resource\nallocation tasks under average long-term constraints, CLO provides formal\nworst-case deterministic reliability guarantees. This is achieved by\nintegrating the standard LO optimization framework with online conformal risk\ncontrol (O-CRC), an adaptive update mechanism controlling long-term risks. The\neffectiveness of CLO is verified via experiments for hierarchal edge inference\ntargeting image segmentation tasks in a networked computing architecture.\nSpecifically, simulation results confirm that CLO can control reliability\nconstraints, measured via the false negative rate of all the segmentation\ndecisions made in the network, while at the same time minimizing the weighted\nsum of energy consumption and imprecision, with the latter accounting for the\nrate of false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces conformal Lyapunov optimization (CLO), a novel resource\nallocation framework for networked systems that optimizes average long-term\nobjectives, while satisfying deterministic long-term reliability constraints.\nUnlike traditional Lyapunov optimization (LO), which addresses resource\nallocation tasks under average long-term constraints, CLO provides formal\nworst-case deterministic reliability guarantees. This is achieved by\nintegrating the standard LO optimization framework with online conformal risk\ncontrol (O-CRC), an adaptive update mechanism controlling long-term risks. The\neffectiveness of CLO is verified via experiments for hierarchal edge inference\ntargeting image segmentation tasks in a networked computing architecture.\nSpecifically, simulation results confirm that CLO can control reliability\nconstraints, measured via the false negative rate of all the segmentation\ndecisions made in the network, while at the same time minimizing the weighted\nsum of energy consumption and imprecision, with the latter accounting for the\nrate of false positives."
                },
                "authors": [
                    {
                        "name": "Francesco Binucci"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Paolo Banelli"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Banelli"
                },
                "author": "Paolo Banelli",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07070v1",
                "updated": "2025-03-10T08:53:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    53,
                    11,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:53:11Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    53,
                    11,
                    0,
                    69,
                    0
                ],
                "title": "PIED: Physics-Informed Experimental Design for Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIED: Physics-Informed Experimental Design for Inverse Problems"
                },
                "summary": "In many science and engineering settings, system dynamics are characterized\nby governing PDEs, and a major challenge is to solve inverse problems (IPs)\nwhere unknown PDE parameters are inferred based on observational data gathered\nunder limited budget. Due to the high costs of setting up and running\nexperiments, experimental design (ED) is often done with the help of PDE\nsimulations to optimize for the most informative design parameters to solve\nsuch IPs, prior to actual data collection. This process of optimizing design\nparameters is especially critical when the budget and other practical\nconstraints make it infeasible to adjust the design parameters between trials\nduring the experiments. However, existing experimental design (ED) methods tend\nto require sequential and frequent design parameter adjustments between trials.\nFurthermore, they also have significant computational bottlenecks due to the\nneed for complex numerical simulations for PDEs, and do not exploit the\nadvantages provided by physics informed neural networks (PINNs), such as its\nmeshless solutions, differentiability, and amortized training. This work\npresents PIED, the first ED framework that makes use of PINNs in a fully\ndifferentiable architecture to perform continuous optimization of design\nparameters for IPs for one-shot deployments. PIED overcomes existing methods'\ncomputational bottlenecks through parallelized computation and meta-learning of\nPINN parameter initialization, and proposes novel methods to effectively take\ninto account PINN training dynamics in optimizing the ED parameters. Through\nexperiments based on noisy simulated data and even real world experimental\ndata, we empirically show that given limited observation budget, PIED\nsignificantly outperforms existing ED methods in solving IPs, including\nchallenging settings where the inverse parameters are unknown functions rather\nthan just finite-dimensional.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many science and engineering settings, system dynamics are characterized\nby governing PDEs, and a major challenge is to solve inverse problems (IPs)\nwhere unknown PDE parameters are inferred based on observational data gathered\nunder limited budget. Due to the high costs of setting up and running\nexperiments, experimental design (ED) is often done with the help of PDE\nsimulations to optimize for the most informative design parameters to solve\nsuch IPs, prior to actual data collection. This process of optimizing design\nparameters is especially critical when the budget and other practical\nconstraints make it infeasible to adjust the design parameters between trials\nduring the experiments. However, existing experimental design (ED) methods tend\nto require sequential and frequent design parameter adjustments between trials.\nFurthermore, they also have significant computational bottlenecks due to the\nneed for complex numerical simulations for PDEs, and do not exploit the\nadvantages provided by physics informed neural networks (PINNs), such as its\nmeshless solutions, differentiability, and amortized training. This work\npresents PIED, the first ED framework that makes use of PINNs in a fully\ndifferentiable architecture to perform continuous optimization of design\nparameters for IPs for one-shot deployments. PIED overcomes existing methods'\ncomputational bottlenecks through parallelized computation and meta-learning of\nPINN parameter initialization, and proposes novel methods to effectively take\ninto account PINN training dynamics in optimizing the ED parameters. Through\nexperiments based on noisy simulated data and even real world experimental\ndata, we empirically show that given limited observation budget, PIED\nsignificantly outperforms existing ED methods in solving IPs, including\nchallenging settings where the inverse parameters are unknown functions rather\nthan just finite-dimensional."
                },
                "authors": [
                    {
                        "name": "Apivich Hemachandra"
                    },
                    {
                        "name": "Gregory Kang Ruey Lau"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "Accepted to 13th International Conference on Learning Representations\n  (ICLR 2025), 31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07067v1",
                "updated": "2025-03-10T08:51:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    51,
                    32,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:51:32Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    51,
                    32,
                    0,
                    69,
                    0
                ],
                "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs"
                },
                "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types."
                },
                "authors": [
                    {
                        "name": "Jongwoo Ko"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Tianyu Ding"
                    },
                    {
                        "name": "Luming Liang"
                    },
                    {
                        "name": "Ilya Zharkov"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "The code will be available soon at\n  https://github.com/jongwooko/distillm-2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07066v1",
                "updated": "2025-03-10T08:50:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    50,
                    55,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:50:55Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    50,
                    55,
                    0,
                    69,
                    0
                ],
                "title": "You Only Debias Once: Towards Flexible Accuracy-Fairness Trade-offs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Debias Once: Towards Flexible Accuracy-Fairness Trade-offs at\n  Inference Time"
                },
                "summary": "Deep neural networks are prone to various bias issues, jeopardizing their\napplications for high-stake decision-making. Existing fairness methods\ntypically offer a fixed accuracy-fairness trade-off, since the weight of the\nwell-trained model is a fixed point (fairness-optimum) in the weight space.\nNevertheless, more flexible accuracy-fairness trade-offs at inference time are\npractically desired since: 1) stakes of the same downstream task can vary for\ndifferent individuals, and 2) different regions have diverse laws or\nregularization for fairness. If using the previous fairness methods, we have to\ntrain multiple models, each offering a specific level of accuracy-fairness\ntrade-off. This is often computationally expensive, time-consuming, and\ndifficult to deploy, making it less practical for real-world applications. To\naddress this problem, we propose You Only Debias Once (YODO) to achieve in-situ\nflexible accuracy-fairness trade-offs at inference time, using a single model\nthat trained only once. Instead of pursuing one individual fixed point\n(fairness-optimum) in the weight space, we aim to find a \"line\" in the weight\nspace that connects the accuracy-optimum and fairness-optimum points using a\nsingle model. Points (models) on this line implement varying levels of\naccuracy-fairness trade-offs. At inference time, by manually selecting the\nspecific position of the learned \"line\", our proposed method can achieve\narbitrary accuracy-fairness trade-offs for different end-users and scenarios.\nExperimental results on tabular and image datasets show that YODO achieves\nflexible trade-offs between model accuracy and fairness, at ultra-low\noverheads. For example, if we need $100$ levels of trade-off on the \\acse\ndataset, YODO takes $3.53$ seconds while training $100$ fixed models consumes\n$425$ seconds. The code is available at https://github.com/ahxt/yodo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks are prone to various bias issues, jeopardizing their\napplications for high-stake decision-making. Existing fairness methods\ntypically offer a fixed accuracy-fairness trade-off, since the weight of the\nwell-trained model is a fixed point (fairness-optimum) in the weight space.\nNevertheless, more flexible accuracy-fairness trade-offs at inference time are\npractically desired since: 1) stakes of the same downstream task can vary for\ndifferent individuals, and 2) different regions have diverse laws or\nregularization for fairness. If using the previous fairness methods, we have to\ntrain multiple models, each offering a specific level of accuracy-fairness\ntrade-off. This is often computationally expensive, time-consuming, and\ndifficult to deploy, making it less practical for real-world applications. To\naddress this problem, we propose You Only Debias Once (YODO) to achieve in-situ\nflexible accuracy-fairness trade-offs at inference time, using a single model\nthat trained only once. Instead of pursuing one individual fixed point\n(fairness-optimum) in the weight space, we aim to find a \"line\" in the weight\nspace that connects the accuracy-optimum and fairness-optimum points using a\nsingle model. Points (models) on this line implement varying levels of\naccuracy-fairness trade-offs. At inference time, by manually selecting the\nspecific position of the learned \"line\", our proposed method can achieve\narbitrary accuracy-fairness trade-offs for different end-users and scenarios.\nExperimental results on tabular and image datasets show that YODO achieves\nflexible trade-offs between model accuracy and fairness, at ultra-low\noverheads. For example, if we need $100$ levels of trade-off on the \\acse\ndataset, YODO takes $3.53$ seconds while training $100$ fixed models consumes\n$425$ seconds. The code is available at https://github.com/ahxt/yodo."
                },
                "authors": [
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Zhimeng Jiang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_comment": "CPAL2025(Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13432v3",
                "updated": "2025-03-10T08:49:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    49,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2024-12-18T02:07:21Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    2,
                    7,
                    21,
                    2,
                    353,
                    0
                ],
                "title": "Large Language Model Enhanced Recommender Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enhanced Recommender Systems: A Survey"
                },
                "summary": "Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Feng Tian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Tian"
                },
                "author": "Feng Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07065v1",
                "updated": "2025-03-10T08:48:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    48,
                    50,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:48:50Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    48,
                    50,
                    0,
                    69,
                    0
                ],
                "title": "Boosting the Generalization and Reasoning of Vision Language Models with\n  Curriculum Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting the Generalization and Reasoning of Vision Language Models with\n  Curriculum Reinforcement Learning"
                },
                "summary": "While state-of-the-art vision-language models (VLMs) have demonstrated\nremarkable capabilities in complex visual-text tasks, their success heavily\nrelies on massive model scaling, limiting their practical deployment.\nSmall-scale VLMs offer a more practical alternative but face significant\nchallenges when trained with traditional supervised fine-tuning (SFT),\nparticularly in two aspects: out-of-domain (OOD) generalization and reasoning\nabilities, which significantly lags behind the contemporary Large language\nmodels (LLMs). To address these challenges, we propose Curriculum Reinforcement\nFinetuning (Curr-ReFT), a novel post-training paradigm specifically designed\nfor small-scale VLMs. Inspired by the success of reinforcement learning in\nLLMs, Curr-ReFT comprises two sequential stages: (1) Curriculum Reinforcement\nLearning, which ensures steady progression of model capabilities through\ndifficulty-aware reward design, transitioning from basic visual perception to\ncomplex reasoning tasks; and (2) Rejected Sampling-based Self-improvement,\nwhich maintains the fundamental capabilities of VLMs through selective learning\nfrom high-quality multimodal and language examples. Extensive experiments\ndemonstrate that models trained with Curr-ReFT paradigm achieve\nstate-of-the-art performance across various visual tasks in both in-domain and\nout-of-domain settings. Moreover, our Curr-ReFT enhanced 3B model matches the\nperformance of 32B-parameter models, demonstrating that efficient training\nparadigms can effectively bridge the gap between small and large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While state-of-the-art vision-language models (VLMs) have demonstrated\nremarkable capabilities in complex visual-text tasks, their success heavily\nrelies on massive model scaling, limiting their practical deployment.\nSmall-scale VLMs offer a more practical alternative but face significant\nchallenges when trained with traditional supervised fine-tuning (SFT),\nparticularly in two aspects: out-of-domain (OOD) generalization and reasoning\nabilities, which significantly lags behind the contemporary Large language\nmodels (LLMs). To address these challenges, we propose Curriculum Reinforcement\nFinetuning (Curr-ReFT), a novel post-training paradigm specifically designed\nfor small-scale VLMs. Inspired by the success of reinforcement learning in\nLLMs, Curr-ReFT comprises two sequential stages: (1) Curriculum Reinforcement\nLearning, which ensures steady progression of model capabilities through\ndifficulty-aware reward design, transitioning from basic visual perception to\ncomplex reasoning tasks; and (2) Rejected Sampling-based Self-improvement,\nwhich maintains the fundamental capabilities of VLMs through selective learning\nfrom high-quality multimodal and language examples. Extensive experiments\ndemonstrate that models trained with Curr-ReFT paradigm achieve\nstate-of-the-art performance across various visual tasks in both in-domain and\nout-of-domain settings. Moreover, our Curr-ReFT enhanced 3B model matches the\nperformance of 32B-parameter models, demonstrating that efficient training\nparadigms can effectively bridge the gap between small and large models."
                },
                "authors": [
                    {
                        "name": "Huilin Deng"
                    },
                    {
                        "name": "Ding Zou"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Hongchen Luo"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Yu Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Kang"
                },
                "author": "Yu Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09307v2",
                "updated": "2025-03-10T08:46:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    46,
                    11,
                    0,
                    69,
                    0
                ],
                "published": "2025-01-16T05:40:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    40,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "RoboReflect: A Robotic Reflective Reasoning Framework for Grasping\n  Ambiguous-Condition Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboReflect: A Robotic Reflective Reasoning Framework for Grasping\n  Ambiguous-Condition Objects"
                },
                "summary": "As robotic technology rapidly develops, robots are being employed in an\nincreasing number of fields. However, due to the complexity of deployment\nenvironments or the prevalence of ambiguous-condition objects, the practical\napplication of robotics still faces many challenges, leading to frequent\nerrors. Traditional methods and some LLM-based approaches, although improved,\nstill require substantial human intervention and struggle with autonomous error\ncorrection in complex scenarios. In this work, we propose RoboReflect, a novel\nframework leveraging large vision-language models (LVLMs) to enable\nself-reflection and autonomous error correction in robotic grasping tasks.\nRoboReflect allows robots to automatically adjust their strategies based on\nunsuccessful attempts until successful execution is achieved. The corrected\nstrategies are saved in the memory for future task reference. We evaluate\nRoboReflect through extensive testing on eight common objects prone to\nambiguous conditions of three categories. Our results demonstrate that\nRoboReflect not only outperforms existing grasp pose estimation methods like\nAnyGrasp and high-level action planning techniques ReKep with GPT-4V but also\nsignificantly enhances the robot's capability to adapt and correct errors\nindependently. These findings underscore the critical importance of autonomous\nself-reflection in robotic systems while effectively addressing the challenges\nposed by ambiguous-condition environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robotic technology rapidly develops, robots are being employed in an\nincreasing number of fields. However, due to the complexity of deployment\nenvironments or the prevalence of ambiguous-condition objects, the practical\napplication of robotics still faces many challenges, leading to frequent\nerrors. Traditional methods and some LLM-based approaches, although improved,\nstill require substantial human intervention and struggle with autonomous error\ncorrection in complex scenarios. In this work, we propose RoboReflect, a novel\nframework leveraging large vision-language models (LVLMs) to enable\nself-reflection and autonomous error correction in robotic grasping tasks.\nRoboReflect allows robots to automatically adjust their strategies based on\nunsuccessful attempts until successful execution is achieved. The corrected\nstrategies are saved in the memory for future task reference. We evaluate\nRoboReflect through extensive testing on eight common objects prone to\nambiguous conditions of three categories. Our results demonstrate that\nRoboReflect not only outperforms existing grasp pose estimation methods like\nAnyGrasp and high-level action planning techniques ReKep with GPT-4V but also\nsignificantly enhances the robot's capability to adapt and correct errors\nindependently. These findings underscore the critical importance of autonomous\nself-reflection in robotic systems while effectively addressing the challenges\nposed by ambiguous-condition environments."
                },
                "authors": [
                    {
                        "name": "Zhen Luo"
                    },
                    {
                        "name": "Yixuan Yang"
                    },
                    {
                        "name": "Yanfu Zhang"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10998v2",
                "updated": "2025-03-10T08:45:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    45,
                    53,
                    0,
                    69,
                    0
                ],
                "published": "2024-06-25T09:55:22Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    9,
                    55,
                    22,
                    1,
                    177,
                    0
                ],
                "title": "Discrete Diffusion Language Model for Efficient Text Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete Diffusion Language Model for Efficient Text Summarization"
                },
                "summary": "While diffusion models excel at conditional generating high-quality images,\nprior works in discrete diffusion models were not evaluated on conditional\nlong-text generation. In this work, we address the limitations of prior\ndiscrete diffusion models for conditional long-text generation, particularly in\nlong sequence-to-sequence tasks such as abstractive summarization. Despite fast\ndecoding speeds compared to autoregressive methods, previous diffusion models\nfailed on the abstractive summarization task due to the incompatibility between\nthe backbone architectures and the random noising process. To overcome these\nchallenges, we introduce a novel semantic-aware noising process that enables\nTransformer backbones to handle long sequences effectively. Additionally, we\npropose CrossMamba, an adaptation of the Mamba model to the encoder-decoder\nparadigm, which integrates seamlessly with the random absorbing noising\nprocess. Our approaches achieve state-of-the-art performance on three benchmark\nsummarization datasets: Gigaword, CNN/DailyMail, and Arxiv, outperforming\nexisting discrete diffusion models on ROUGE metrics as well as possessing much\nfaster speed in inference compared to autoregressive models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion models excel at conditional generating high-quality images,\nprior works in discrete diffusion models were not evaluated on conditional\nlong-text generation. In this work, we address the limitations of prior\ndiscrete diffusion models for conditional long-text generation, particularly in\nlong sequence-to-sequence tasks such as abstractive summarization. Despite fast\ndecoding speeds compared to autoregressive methods, previous diffusion models\nfailed on the abstractive summarization task due to the incompatibility between\nthe backbone architectures and the random noising process. To overcome these\nchallenges, we introduce a novel semantic-aware noising process that enables\nTransformer backbones to handle long sequences effectively. Additionally, we\npropose CrossMamba, an adaptation of the Mamba model to the encoder-decoder\nparadigm, which integrates seamlessly with the random absorbing noising\nprocess. Our approaches achieve state-of-the-art performance on three benchmark\nsummarization datasets: Gigaword, CNN/DailyMail, and Arxiv, outperforming\nexisting discrete diffusion models on ROUGE metrics as well as possessing much\nfaster speed in inference compared to autoregressive models."
                },
                "authors": [
                    {
                        "name": "Do Huu Dat"
                    },
                    {
                        "name": "Do Duc Anh"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "Wray Buntine"
                    }
                ],
                "author_detail": {
                    "name": "Wray Buntine"
                },
                "author": "Wray Buntine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.07493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07493v1",
                "updated": "2025-03-10T16:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    12,
                    50,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    12,
                    50,
                    0,
                    69,
                    0
                ],
                "title": "V2Flow: Unifying Visual Tokenization and Large Language Model\n  Vocabularies for Autoregressive Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V2Flow: Unifying Visual Tokenization and Large Language Model\n  Vocabularies for Autoregressive Image Generation"
                },
                "summary": "We propose V2Flow, a novel tokenizer that produces discrete visual tokens\ncapable of high-fidelity reconstruction, while ensuring structural and latent\ndistribution alignment with the vocabulary space of large language models\n(LLMs). Leveraging this tight visual-vocabulary coupling, V2Flow enables\nautoregressive visual generation on top of existing LLMs. Our approach\nformulates visual tokenization as a flow-matching problem, aiming to learn a\nmapping from a standard normal prior to the continuous image distribution,\nconditioned on token sequences embedded within the LLMs vocabulary space. The\neffectiveness of V2Flow stems from two core designs. First, we propose a Visual\nVocabulary resampler, which compresses visual data into compact token\nsequences, with each represented as a soft categorical distribution over LLM's\nvocabulary. This allows seamless integration of visual tokens into existing\nLLMs for autoregressive visual generation. Second, we present a masked\nautoregressive Rectified-Flow decoder, employing a masked transformer\nencoder-decoder to refine visual tokens into contextually enriched embeddings.\nThese embeddings then condition a dedicated velocity field for precise\nreconstruction. Additionally, an autoregressive rectified-flow sampling\nstrategy is incorporated, ensuring flexible sequence lengths while preserving\ncompetitive reconstruction quality. Extensive experiments show that V2Flow\noutperforms mainstream VQ-based tokenizers and facilitates autoregressive\nvisual generation on top of existing. https://github.com/zhangguiwei610/V2Flow",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose V2Flow, a novel tokenizer that produces discrete visual tokens\ncapable of high-fidelity reconstruction, while ensuring structural and latent\ndistribution alignment with the vocabulary space of large language models\n(LLMs). Leveraging this tight visual-vocabulary coupling, V2Flow enables\nautoregressive visual generation on top of existing LLMs. Our approach\nformulates visual tokenization as a flow-matching problem, aiming to learn a\nmapping from a standard normal prior to the continuous image distribution,\nconditioned on token sequences embedded within the LLMs vocabulary space. The\neffectiveness of V2Flow stems from two core designs. First, we propose a Visual\nVocabulary resampler, which compresses visual data into compact token\nsequences, with each represented as a soft categorical distribution over LLM's\nvocabulary. This allows seamless integration of visual tokens into existing\nLLMs for autoregressive visual generation. Second, we present a masked\nautoregressive Rectified-Flow decoder, employing a masked transformer\nencoder-decoder to refine visual tokens into contextually enriched embeddings.\nThese embeddings then condition a dedicated velocity field for precise\nreconstruction. Additionally, an autoregressive rectified-flow sampling\nstrategy is incorporated, ensuring flexible sequence lengths while preserving\ncompetitive reconstruction quality. Extensive experiments show that V2Flow\noutperforms mainstream VQ-based tokenizers and facilitates autoregressive\nvisual generation on top of existing. https://github.com/zhangguiwei610/V2Flow"
                },
                "authors": [
                    {
                        "name": "Guiwei Zhang"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Mohan Zhou"
                    },
                    {
                        "name": "Yalong Bai"
                    },
                    {
                        "name": "Biye Li"
                    }
                ],
                "author_detail": {
                    "name": "Biye Li"
                },
                "author": "Biye Li",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07465v1",
                "updated": "2025-03-10T15:42:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    42,
                    59,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:42:59Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    42,
                    59,
                    0,
                    69,
                    0
                ],
                "title": "YOLOE: Real-Time Seeing Anything",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLOE: Real-Time Seeing Anything"
                },
                "summary": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3$\\times$\nless training cost and 1.4$\\times$ inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP$^b$ and 0.4 AP$^m$ gains over closed-set YOLOv8-L with nearly 4$\\times$ less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3$\\times$\nless training cost and 1.4$\\times$ inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP$^b$ and 0.4 AP$^m$ gains over closed-set YOLOv8-L with nearly 4$\\times$ less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "15 pages, 9 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07463v1",
                "updated": "2025-03-10T15:42:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    42,
                    7,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:42:07Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    42,
                    7,
                    0,
                    69,
                    0
                ],
                "title": "GenAIReading: Augmenting Human Cognition with Interactive Digital\n  Textbooks Using Large Language Models and Image Generation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAIReading: Augmenting Human Cognition with Interactive Digital\n  Textbooks Using Large Language Models and Image Generation Models"
                },
                "summary": "Cognitive augmentation is a cornerstone in advancing education, particularly\nthrough personalized learning. However, personalizing extensive textual\nmaterials, such as narratives and academic textbooks, remains challenging due\nto their heavy use, which can hinder learner engagement and understanding.\nBuilding on cognitive theories like Dual Coding Theory -- which posits that\ncombining textual and visual information enhances comprehension and memory --\nthis study explores the potential of Generative AI (GenAI) to enrich\neducational materials. We utilized large language models (LLMs) to generate\nconcise text summaries and image generation models (IGMs) to create visually\naligned content from textual inputs. After recruiting 24 participants, we\nverified that integrating AI-generated supplementary materials significantly\nimproved learning outcomes, increasing post-reading test scores by 7.50%. These\nfindings underscore GenAI's transformative potential in creating adaptive\nlearning environments that enhance cognitive augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive augmentation is a cornerstone in advancing education, particularly\nthrough personalized learning. However, personalizing extensive textual\nmaterials, such as narratives and academic textbooks, remains challenging due\nto their heavy use, which can hinder learner engagement and understanding.\nBuilding on cognitive theories like Dual Coding Theory -- which posits that\ncombining textual and visual information enhances comprehension and memory --\nthis study explores the potential of Generative AI (GenAI) to enrich\neducational materials. We utilized large language models (LLMs) to generate\nconcise text summaries and image generation models (IGMs) to create visually\naligned content from textual inputs. After recruiting 24 participants, we\nverified that integrating AI-generated supplementary materials significantly\nimproved learning outcomes, increasing post-reading test scores by 7.50%. These\nfindings underscore GenAI's transformative potential in creating adaptive\nlearning environments that enhance cognitive augmentation."
                },
                "authors": [
                    {
                        "name": "Ryugo Morita"
                    },
                    {
                        "name": "Ko Watanabe"
                    },
                    {
                        "name": "Jinjia Zhou"
                    },
                    {
                        "name": "Andreas Dengel"
                    },
                    {
                        "name": "Shoya Ishimaru"
                    }
                ],
                "author_detail": {
                    "name": "Shoya Ishimaru"
                },
                "author": "Shoya Ishimaru",
                "arxiv_comment": "Accepted at AHs2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07459v1",
                "updated": "2025-03-10T15:38:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    38,
                    44,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:38:44Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    38,
                    44,
                    0,
                    69,
                    0
                ],
                "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark."
                },
                "authors": [
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Daniel Shao"
                    },
                    {
                        "name": "Jiwoong Sohn"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Chenglin Wu"
                    },
                    {
                        "name": "Wenqi Shi"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Mark Gerstein"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gerstein"
                },
                "author": "Mark Gerstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07457v1",
                "updated": "2025-03-10T15:37:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    37,
                    7,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:37:07Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    37,
                    7,
                    0,
                    69,
                    0
                ],
                "title": "LLMs syntactically adapt their language use to their conversational\n  partner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs syntactically adapt their language use to their conversational\n  partner"
                },
                "summary": "It has been frequently observed that human speakers align their language use\nwith each other during conversations. In this paper, we study empirically\nwhether large language models (LLMs) exhibit the same behavior of\nconversational adaptation. We construct a corpus of conversations between LLMs\nand find that two LLM agents end up making more similar syntactic choices as\nconversations go on, confirming that modern LLMs adapt their language use to\ntheir conversational partners in at least a rudimentary way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been frequently observed that human speakers align their language use\nwith each other during conversations. In this paper, we study empirically\nwhether large language models (LLMs) exhibit the same behavior of\nconversational adaptation. We construct a corpus of conversations between LLMs\nand find that two LLM agents end up making more similar syntactic choices as\nconversations go on, confirming that modern LLMs adapt their language use to\ntheir conversational partners in at least a rudimentary way."
                },
                "authors": [
                    {
                        "name": "Florian Kandra"
                    },
                    {
                        "name": "Vera Demberg"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "4 pages, 1 table, 1 figure, submitted to ACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07456v1",
                "updated": "2025-03-10T15:36:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    36,
                    49,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:36:49Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    36,
                    49,
                    0,
                    69,
                    0
                ],
                "title": "Anatomy-Aware Conditional Image-Text Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anatomy-Aware Conditional Image-Text Retrieval"
                },
                "summary": "Image-Text Retrieval (ITR) finds broad applications in healthcare, aiding\nclinicians and radiologists by automatically retrieving relevant patient cases\nin the database given the query image and/or report, for more efficient\nclinical diagnosis and treatment, especially for rare diseases. However\nconventional ITR systems typically only rely on global image or text\nrepresentations for measuring patient image/report similarities, which overlook\nlocal distinctiveness across patient cases. This often results in suboptimal\nretrieval performance. In this paper, we propose an Anatomical\nLocation-Conditioned Image-Text Retrieval (ALC-ITR) framework, which, given a\nquery image and the associated suspicious anatomical region(s), aims to\nretrieve similar patient cases exhibiting the same disease or symptoms in the\nsame anatomical region. To perform location-conditioned multimodal retrieval,\nwe learn a medical Relevance-Region-Aligned Vision Language (RRA-VL) model with\nsemantic global-level and region-/word-level alignment to produce\ngeneralizable, well-aligned multi-modal representations. Additionally, we\nperform location-conditioned contrastive learning to further utilize cross-pair\nregion-level contrastiveness for improved multi-modal retrieval. We show that\nour proposed RRA-VL achieves state-of-the-art localization performance in\nphase-grounding tasks, and satisfying multi-modal retrieval performance with or\nwithout location conditioning. Finally, we thoroughly investigate the\ngeneralizability and explainability of our proposed ALC-ITR system in providing\nexplanations and preliminary diagnosis reports given retrieved patient cases\n(conditioned on anatomical regions), with proper off-the-shelf LLM prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image-Text Retrieval (ITR) finds broad applications in healthcare, aiding\nclinicians and radiologists by automatically retrieving relevant patient cases\nin the database given the query image and/or report, for more efficient\nclinical diagnosis and treatment, especially for rare diseases. However\nconventional ITR systems typically only rely on global image or text\nrepresentations for measuring patient image/report similarities, which overlook\nlocal distinctiveness across patient cases. This often results in suboptimal\nretrieval performance. In this paper, we propose an Anatomical\nLocation-Conditioned Image-Text Retrieval (ALC-ITR) framework, which, given a\nquery image and the associated suspicious anatomical region(s), aims to\nretrieve similar patient cases exhibiting the same disease or symptoms in the\nsame anatomical region. To perform location-conditioned multimodal retrieval,\nwe learn a medical Relevance-Region-Aligned Vision Language (RRA-VL) model with\nsemantic global-level and region-/word-level alignment to produce\ngeneralizable, well-aligned multi-modal representations. Additionally, we\nperform location-conditioned contrastive learning to further utilize cross-pair\nregion-level contrastiveness for improved multi-modal retrieval. We show that\nour proposed RRA-VL achieves state-of-the-art localization performance in\nphase-grounding tasks, and satisfying multi-modal retrieval performance with or\nwithout location conditioning. Finally, we thoroughly investigate the\ngeneralizability and explainability of our proposed ALC-ITR system in providing\nexplanations and preliminary diagnosis reports given retrieved patient cases\n(conditioned on anatomical regions), with proper off-the-shelf LLM prompts."
                },
                "authors": [
                    {
                        "name": "Meng Zheng"
                    },
                    {
                        "name": "Jiajin Zhang"
                    },
                    {
                        "name": "Benjamin Planche"
                    },
                    {
                        "name": "Zhongpai Gao"
                    },
                    {
                        "name": "Terrence Chen"
                    },
                    {
                        "name": "Ziyan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyan Wu"
                },
                "author": "Ziyan Wu",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07450v1",
                "updated": "2025-03-10T15:30:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    30,
                    5,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    30,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper"
                },
                "summary": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted."
                },
                "authors": [
                    {
                        "name": "Sargam Yadav"
                    },
                    {
                        "name": "Asifa Mehmood Qureshi"
                    },
                    {
                        "name": "Abhishek Kaushik"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Roisin Loughran"
                    },
                    {
                        "name": "Subramaniam Kazhuparambil"
                    },
                    {
                        "name": "Andrew Shaw"
                    },
                    {
                        "name": "Mohammed Sabry"
                    },
                    {
                        "name": "Niamh St John Lynch"
                    },
                    {
                        "name": ". Nikhil Singh"
                    },
                    {
                        "name": "Padraic O'Hara"
                    },
                    {
                        "name": "Pranay Jaiswal"
                    },
                    {
                        "name": "Roshan Chandru"
                    },
                    {
                        "name": "David Lillis"
                    }
                ],
                "author_detail": {
                    "name": "David Lillis"
                },
                "arxiv_affiliation": "University College Dublin",
                "author": "David Lillis",
                "arxiv_comment": "The project is partially supported by the DkIT Postgraduate\n  Scholarship, Research Ireland under Grant number 13/RC/2094_2, and Grant\n  number 21/FFP-A/925",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05206v2",
                "updated": "2025-03-10T15:24:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    24,
                    39,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T07:54:43Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    7,
                    54,
                    43,
                    4,
                    66,
                    0
                ],
                "title": "Operationalizing Cybersecurity Knowledge: Design, Implementation &\n  Evaluation of a Knowledge Management System for CACAO Playbooks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operationalizing Cybersecurity Knowledge: Design, Implementation &\n  Evaluation of a Knowledge Management System for CACAO Playbooks"
                },
                "summary": "Modern cybersecurity threats are growing in complexity, targeting\nincreasingly intricate & interconnected systems. To effectively defend against\nthese evolving threats, security teams utilize automation & orchestration to\nenhance response efficiency and consistency. In that sense, cybersecurity\nplaybooks are key enablers, providing a structured, reusable, and continuously\nimproving approach to incident response, enabling organizations to codify\nrequirements, domain expertise, and best practices and automate decision-making\nprocesses to the extent possible. The emerging Collaborative Automated Course\nof Action Operations (CACAO) standard defines a common machine-processable\nschema for cybersecurity playbooks, facilitating interoperability for their\nexchange and ensuring the ability to orchestrate and automate cybersecurity\noperations. However, despite its potential and the fact that it is a relatively\nnew standardization work, there is a lack of tools to support its adoption and,\nin particular, the management & lifecycle development of CACAO playbooks,\nlimiting their practical deployment. Motivated by the above, this work presents\nthe design, development, and evaluation of a Knowledge Management System (KMS)\nfor managing CACAO cybersecurity playbooks throughout their lifecycle,\nproviding essential tools to streamline playbook management. Using open\ntechnologies & standards, the proposed approach fosters standards-based\ninteroperability & enhances the usability of state-of-the-art cybersecurity\norchestration & automation primitives. To encourage adoption, the resulting\nimplementation is released as open-source, which, to the extent of our\nknowledge, comprises the first publicly available & documented work in this\ndomain, supporting the broader uptake of CACAO playbooks & promoting the\nwidespread use of interoperable automation and orchestration mechanisms in\ncybersecurity operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cybersecurity threats are growing in complexity, targeting\nincreasingly intricate & interconnected systems. To effectively defend against\nthese evolving threats, security teams utilize automation & orchestration to\nenhance response efficiency and consistency. In that sense, cybersecurity\nplaybooks are key enablers, providing a structured, reusable, and continuously\nimproving approach to incident response, enabling organizations to codify\nrequirements, domain expertise, and best practices and automate decision-making\nprocesses to the extent possible. The emerging Collaborative Automated Course\nof Action Operations (CACAO) standard defines a common machine-processable\nschema for cybersecurity playbooks, facilitating interoperability for their\nexchange and ensuring the ability to orchestrate and automate cybersecurity\noperations. However, despite its potential and the fact that it is a relatively\nnew standardization work, there is a lack of tools to support its adoption and,\nin particular, the management & lifecycle development of CACAO playbooks,\nlimiting their practical deployment. Motivated by the above, this work presents\nthe design, development, and evaluation of a Knowledge Management System (KMS)\nfor managing CACAO cybersecurity playbooks throughout their lifecycle,\nproviding essential tools to streamline playbook management. Using open\ntechnologies & standards, the proposed approach fosters standards-based\ninteroperability & enhances the usability of state-of-the-art cybersecurity\norchestration & automation primitives. To encourage adoption, the resulting\nimplementation is released as open-source, which, to the extent of our\nknowledge, comprises the first publicly available & documented work in this\ndomain, supporting the broader uptake of CACAO playbooks & promoting the\nwidespread use of interoperable automation and orchestration mechanisms in\ncybersecurity operations."
                },
                "authors": [
                    {
                        "name": "Orestis Tsirakis"
                    },
                    {
                        "name": "Konstantinos Fysarakis"
                    },
                    {
                        "name": "Vasileios Mavroeidis"
                    },
                    {
                        "name": "Ioannis Papaefstathiou"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Papaefstathiou"
                },
                "author": "Ioannis Papaefstathiou",
                "arxiv_comment": "This preprint has not been peer-reviewed. It is a preliminary version\n  of a research article that has been submitted for journal publication. The\n  final, peer-reviewed version may differ from this preprint. Associated GitHub\n  page available at: https://github.com/Orestistsira/cacao-knowledge-base",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07429v1",
                "updated": "2025-03-10T15:13:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    13,
                    38,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:13:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    13,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector\n  Graphics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector\n  Graphics"
                },
                "summary": "Advances in large language models (LLMs) offer new possibilities for\nenhancing math education by automating support for both teachers and students.\nWhile prior work has focused on generating math problems and high-quality\ndistractors, the role of visualization in math learning remains under-explored.\nDiagrams are essential for mathematical thinking and problem-solving, yet\nmanually creating them is time-consuming and requires domain-specific\nexpertise, limiting scalability. Recent research on using LLMs to generate\nScalable Vector Graphics (SVG) presents a promising approach to automating\ndiagram creation. Unlike pixel-based images, SVGs represent geometric figures\nusing XML, allowing seamless scaling and adaptability. Educational platforms\nsuch as Khan Academy and IXL already use SVGs to display math problems and\nhints. In this paper, we explore the use of LLMs to generate math-related\ndiagrams that accompany textual hints via intermediate SVG representations. We\naddress three research questions: (1) how to automatically generate math\ndiagrams in problem-solving hints and evaluate their quality, (2) whether SVG\nis an effective intermediate representation for math diagrams, and (3) what\nprompting strategies and formats are required for LLMs to generate accurate\nSVG-based diagrams. Our contributions include defining the task of\nautomatically generating SVG-based diagrams for math hints, developing an LLM\nprompting-based pipeline, and identifying key strategies for improving diagram\ngeneration. Additionally, we introduce a Visual Question Answering-based\nevaluation setup and conduct ablation studies to assess different pipeline\nvariations. By automating the math diagram creation, we aim to provide students\nand teachers with accurate, conceptually relevant visual aids that enhance\nproblem-solving and learning experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in large language models (LLMs) offer new possibilities for\nenhancing math education by automating support for both teachers and students.\nWhile prior work has focused on generating math problems and high-quality\ndistractors, the role of visualization in math learning remains under-explored.\nDiagrams are essential for mathematical thinking and problem-solving, yet\nmanually creating them is time-consuming and requires domain-specific\nexpertise, limiting scalability. Recent research on using LLMs to generate\nScalable Vector Graphics (SVG) presents a promising approach to automating\ndiagram creation. Unlike pixel-based images, SVGs represent geometric figures\nusing XML, allowing seamless scaling and adaptability. Educational platforms\nsuch as Khan Academy and IXL already use SVGs to display math problems and\nhints. In this paper, we explore the use of LLMs to generate math-related\ndiagrams that accompany textual hints via intermediate SVG representations. We\naddress three research questions: (1) how to automatically generate math\ndiagrams in problem-solving hints and evaluate their quality, (2) whether SVG\nis an effective intermediate representation for math diagrams, and (3) what\nprompting strategies and formats are required for LLMs to generate accurate\nSVG-based diagrams. Our contributions include defining the task of\nautomatically generating SVG-based diagrams for math hints, developing an LLM\nprompting-based pipeline, and identifying key strategies for improving diagram\ngeneration. Additionally, we introduce a Visual Question Answering-based\nevaluation setup and conduct ablation studies to assess different pipeline\nvariations. By automating the math diagram creation, we aim to provide students\nand teachers with accurate, conceptually relevant visual aids that enhance\nproblem-solving and learning experiences."
                },
                "authors": [
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Jeongah Lee"
                    },
                    {
                        "name": "Wanyong Feng"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07426v1",
                "updated": "2025-03-10T15:11:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    11,
                    7,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:11:07Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    11,
                    7,
                    0,
                    69,
                    0
                ],
                "title": "RePO: ReLU-based Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePO: ReLU-based Preference Optimization"
                },
                "summary": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter $\\beta$, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters ($\\beta$, $\\gamma$). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates $\\beta$ via two\nadvances: (1) retaining SimPO's reference-free margins but removing $\\beta$\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case ($\\beta \\to \\infty$), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter $\\beta$, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters ($\\beta$, $\\gamma$). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates $\\beta$ via two\nadvances: (1) retaining SimPO's reference-free margins but removing $\\beta$\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case ($\\beta \\to \\infty$), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune."
                },
                "authors": [
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Xiangnan He"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07420v1",
                "updated": "2025-03-10T15:07:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    7,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:07:30Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    7,
                    30,
                    0,
                    69,
                    0
                ],
                "title": "The Interplay of AI-and-RAN: Dynamic Resource Allocation for Converged\n  6G Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Interplay of AI-and-RAN: Dynamic Resource Allocation for Converged\n  6G Platform"
                },
                "summary": "The concept of AI-RAN as specified by the AI-RAN alliance is geared to\nexplore a converged 6G platform that can support management, orchestration, and\ndeployment of both AI and RAN workloads. This concept is central to the\ndevelopment of a 6G architecture that aims to exploit the accelerated compute\ncapabilities for supporting both real-time signal processing and offloading of\nGenerative AI (GenAI) workloads. However, both the architectural framework\nrequired to support this vision and the dynamic resource allocation strategy\nare still in their infancy. The O-RAN architecture intrinsically allows\ncloud-native disaggregated implementation. Consequently, we explore a framework\nthat can allow orchestration of AI-and-RAN workloads by expanding the Near\nReal-Time RAN Intelligent Controller (NRT-RIC) within O-RAN. The framework\nincorporates a monitoring xApp that tracks RAN KPIs and exposes radio analytics\nto the proposed E2E orchestrator via a recently introduced Y1 interface. The\norchestrator implements a Soft Actor-Critic (SAC) reinforcement learning\nalgorithm to dynamically allocate critical computing resources, e.g.,\nMulti-Instance GPUs (MIGs), between latency-sensitive RAN network functions and\ncomputationally intensive AI workloads on shared RAN infrastructure. The\nproposed framework provides insight on how the traditional RAN architecture can\nbe evolved to inherently support emerging GenAI workloads. Our framework\nprioritizes the real-time requirements of RAN workloads while maintaining\nefficient resource sharing for AI applications. The simulation results\ndemonstrate the benefits of the proposed framework, as it meets nearly 99% of\nthe requests for RAN workload while effectively supporting AI workloads and\nachieving 100% utilization of the RAN infrastructure resources in a dynamic\nenvironment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of AI-RAN as specified by the AI-RAN alliance is geared to\nexplore a converged 6G platform that can support management, orchestration, and\ndeployment of both AI and RAN workloads. This concept is central to the\ndevelopment of a 6G architecture that aims to exploit the accelerated compute\ncapabilities for supporting both real-time signal processing and offloading of\nGenerative AI (GenAI) workloads. However, both the architectural framework\nrequired to support this vision and the dynamic resource allocation strategy\nare still in their infancy. The O-RAN architecture intrinsically allows\ncloud-native disaggregated implementation. Consequently, we explore a framework\nthat can allow orchestration of AI-and-RAN workloads by expanding the Near\nReal-Time RAN Intelligent Controller (NRT-RIC) within O-RAN. The framework\nincorporates a monitoring xApp that tracks RAN KPIs and exposes radio analytics\nto the proposed E2E orchestrator via a recently introduced Y1 interface. The\norchestrator implements a Soft Actor-Critic (SAC) reinforcement learning\nalgorithm to dynamically allocate critical computing resources, e.g.,\nMulti-Instance GPUs (MIGs), between latency-sensitive RAN network functions and\ncomputationally intensive AI workloads on shared RAN infrastructure. The\nproposed framework provides insight on how the traditional RAN architecture can\nbe evolved to inherently support emerging GenAI workloads. Our framework\nprioritizes the real-time requirements of RAN workloads while maintaining\nefficient resource sharing for AI applications. The simulation results\ndemonstrate the benefits of the proposed framework, as it meets nearly 99% of\nthe requests for RAN workload while effectively supporting AI workloads and\nachieving 100% utilization of the RAN infrastructure resources in a dynamic\nenvironment."
                },
                "authors": [
                    {
                        "name": "Syed Danial Ali Shah"
                    },
                    {
                        "name": "Zeinab Nezami"
                    },
                    {
                        "name": "Maryam Hafeez"
                    },
                    {
                        "name": "Syed Ali Raza Zaidi"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ali Raza Zaidi"
                },
                "author": "Syed Ali Raza Zaidi",
                "arxiv_comment": "This paper has been accepted for presentation at the IEEE INFOCOM\n  2025 Workshop and will appear in the IEEE INFOCOM 2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07404v1",
                "updated": "2025-03-10T14:55:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    55,
                    9,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:55:09Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    55,
                    9,
                    0,
                    69,
                    0
                ],
                "title": "Towards Safe Robot Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safe Robot Foundation Models"
                },
                "summary": "Robot foundation models hold the potential for deployment across diverse\nenvironments, from industrial applications to household tasks. While current\nresearch focuses primarily on the policies' generalization capabilities across\na variety of tasks, it fails to address safety, a critical requirement for\ndeployment on real-world systems. In this paper, we introduce a safety layer\ndesigned to constrain the action space of any generalist policy appropriately.\nOur approach uses ATACOM, a safe reinforcement learning algorithm that creates\na safe action space and, therefore, ensures safe state transitions. By\nextending ATACOM to generalist policies, our method facilitates their\ndeployment in safety-critical scenarios without requiring any specific safety\nfine-tuning. We demonstrate the effectiveness of this safety layer in an air\nhockey environment, where it prevents a puck-hitting agent from colliding with\nits surroundings, a failure observed in generalist policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot foundation models hold the potential for deployment across diverse\nenvironments, from industrial applications to household tasks. While current\nresearch focuses primarily on the policies' generalization capabilities across\na variety of tasks, it fails to address safety, a critical requirement for\ndeployment on real-world systems. In this paper, we introduce a safety layer\ndesigned to constrain the action space of any generalist policy appropriately.\nOur approach uses ATACOM, a safe reinforcement learning algorithm that creates\na safe action space and, therefore, ensures safe state transitions. By\nextending ATACOM to generalist policies, our method facilitates their\ndeployment in safety-critical scenarios without requiring any specific safety\nfine-tuning. We demonstrate the effectiveness of this safety layer in an air\nhockey environment, where it prevents a puck-hitting agent from colliding with\nits surroundings, a failure observed in generalist policies."
                },
                "authors": [
                    {
                        "name": "Maximilian Tölle"
                    },
                    {
                        "name": "Theo Gruner"
                    },
                    {
                        "name": "Daniel Palenicek"
                    },
                    {
                        "name": "Jonas Günster"
                    },
                    {
                        "name": "Puze Liu"
                    },
                    {
                        "name": "Joe Watson"
                    },
                    {
                        "name": "Davide Tateo"
                    },
                    {
                        "name": "Jan Peters"
                    }
                ],
                "author_detail": {
                    "name": "Jan Peters"
                },
                "author": "Jan Peters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07384v1",
                "updated": "2025-03-10T14:32:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    32,
                    56,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:32:56Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    32,
                    56,
                    0,
                    69,
                    0
                ],
                "title": "Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is My Text in Your AI Model? Gradient-based Membership Inference Test\n  applied to LLMs"
                },
                "summary": "This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies."
                },
                "authors": [
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Daniel de Alcala"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Aythami Morales"
                    }
                ],
                "author_detail": {
                    "name": "Aythami Morales"
                },
                "author": "Aythami Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07377v1",
                "updated": "2025-03-10T14:31:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    31,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:31:00Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    31,
                    0,
                    0,
                    69,
                    0
                ],
                "title": "Process-Supervised LLM Recommenders via Flow-guided Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-Supervised LLM Recommenders via Flow-guided Tuning"
                },
                "summary": "While large language models (LLMs) are increasingly adapted for\nrecommendation systems via supervised fine-tuning (SFT), this approach\namplifies popularity bias due to its likelihood maximization objective,\ncompromising recommendation diversity and fairness. To address this, we present\nFlow-guided fine-tuning recommender (Flower), which replaces SFT with a\nGenerative Flow Network (GFlowNet) framework that enacts process supervision\nthrough token-level reward propagation. Flower's key innovation lies in\ndecomposing item-level rewards into constituent token rewards, enabling direct\nalignment between token generation probabilities and their reward signals. This\nmechanism achieves three critical advancements: (1) popularity bias mitigation\nand fairness enhancement through empirical distribution matching, (2)\npreservation of diversity through GFlowNet's proportional sampling, and (3)\nflexible integration of personalized preferences via adaptable token rewards.\nExperiments demonstrate Flower's superior distribution-fitting capability and\nits significant advantages over traditional SFT in terms of fairness,\ndiversity, and accuracy, highlighting its potential to improve LLM-based\nrecommendation systems. The implementation is available via\nhttps://github.com/Mr-Peach0301/Flower",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly adapted for\nrecommendation systems via supervised fine-tuning (SFT), this approach\namplifies popularity bias due to its likelihood maximization objective,\ncompromising recommendation diversity and fairness. To address this, we present\nFlow-guided fine-tuning recommender (Flower), which replaces SFT with a\nGenerative Flow Network (GFlowNet) framework that enacts process supervision\nthrough token-level reward propagation. Flower's key innovation lies in\ndecomposing item-level rewards into constituent token rewards, enabling direct\nalignment between token generation probabilities and their reward signals. This\nmechanism achieves three critical advancements: (1) popularity bias mitigation\nand fairness enhancement through empirical distribution matching, (2)\npreservation of diversity through GFlowNet's proportional sampling, and (3)\nflexible integration of personalized preferences via adaptable token rewards.\nExperiments demonstrate Flower's superior distribution-fitting capability and\nits significant advantages over traditional SFT in terms of fairness,\ndiversity, and accuracy, highlighting its potential to improve LLM-based\nrecommendation systems. The implementation is available via\nhttps://github.com/Mr-Peach0301/Flower"
                },
                "authors": [
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Mengyao Gao"
                    },
                    {
                        "name": "Chenxiao Fan"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Wentao Shi"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07375v1",
                "updated": "2025-03-10T14:30:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    30,
                    56,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:30:56Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    30,
                    56,
                    0,
                    69,
                    0
                ],
                "title": "Probabilistic Segmentation for Robust Field of View Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Segmentation for Robust Field of View Estimation"
                },
                "summary": "Attacks on sensing and perception threaten the safe deployment of autonomous\nvehicles (AVs). Security-aware sensor fusion helps mitigate threats but\nrequires accurate field of view (FOV) estimation which has not been evaluated\nautonomy. To address this gap, we adapt classical computer graphics algorithms\nto develop the first autonomy-relevant FOV estimators and create the first\ndatasets with ground truth FOV labels. Unfortunately, we find that these\napproaches are themselves highly vulnerable to attacks on sensing. To improve\nrobustness of FOV estimation against attacks, we propose a learning-based\nsegmentation model that captures FOV features, integrates Monte Carlo dropout\n(MCD) for uncertainty quantification, and performs anomaly detection on\nconfidence maps. We illustrate through comprehensive evaluations attack\nresistance and strong generalization across environments. Architecture trade\nstudies demonstrate the model is feasible for real-time deployment in multiple\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacks on sensing and perception threaten the safe deployment of autonomous\nvehicles (AVs). Security-aware sensor fusion helps mitigate threats but\nrequires accurate field of view (FOV) estimation which has not been evaluated\nautonomy. To address this gap, we adapt classical computer graphics algorithms\nto develop the first autonomy-relevant FOV estimators and create the first\ndatasets with ground truth FOV labels. Unfortunately, we find that these\napproaches are themselves highly vulnerable to attacks on sensing. To improve\nrobustness of FOV estimation against attacks, we propose a learning-based\nsegmentation model that captures FOV features, integrates Monte Carlo dropout\n(MCD) for uncertainty quantification, and performs anomaly detection on\nconfidence maps. We illustrate through comprehensive evaluations attack\nresistance and strong generalization across environments. Architecture trade\nstudies demonstrate the model is feasible for real-time deployment in multiple\napplications."
                },
                "authors": [
                    {
                        "name": "R. Spencer Hallyburton"
                    },
                    {
                        "name": "David Hunt"
                    },
                    {
                        "name": "Yiwei He"
                    },
                    {
                        "name": "Judy He"
                    },
                    {
                        "name": "Miroslav Pajic"
                    }
                ],
                "author_detail": {
                    "name": "Miroslav Pajic"
                },
                "author": "Miroslav Pajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07365v1",
                "updated": "2025-03-10T14:23:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    23,
                    12,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:23:12Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    23,
                    12,
                    0,
                    69,
                    0
                ],
                "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning"
                },
                "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA"
                },
                "authors": [
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Lingxiao Du"
                    },
                    {
                        "name": "Zongkai Liu"
                    },
                    {
                        "name": "Zhixiang Zhou"
                    },
                    {
                        "name": "Quanfeng Lu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Shao"
                },
                "author": "Wenqi Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05139v2",
                "updated": "2025-03-10T14:21:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    21,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T04:43:39Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    4,
                    43,
                    39,
                    4,
                    66,
                    0
                ],
                "title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without\n  Premium GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without\n  Premium GPUs"
                },
                "summary": "In this technical report, we tackle the challenges of training large-scale\nMixture of Experts (MoE) models, focusing on overcoming cost inefficiency and\nresource limitations prevalent in such systems. To address these issues, we\npresent two differently sized MoE large language models (LLMs), namely\nLing-Lite and Ling-Plus (referred to as \"Bailing\" in Chinese, spelled\nB\\v{a}il\\'ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75\nbillion activated parameters, while Ling-Plus boasts 290 billion parameters\nwith 28.8 billion activated parameters. Both models exhibit comparable\nperformance to leading industry benchmarks. This report offers actionable\ninsights to improve the efficiency and accessibility of AI development in\nresource-constrained settings, promoting more scalable and sustainable\ntechnologies. Specifically, to reduce training costs for large-scale MoE\nmodels, we propose innovative methods for (1) optimization of model\narchitecture and training processes, (2) refinement of training anomaly\nhandling, and (3) enhancement of model evaluation efficiency. Additionally,\nleveraging high-quality data generated from knowledge graphs, our models\ndemonstrate superior capabilities in tool use compared to other models.\nUltimately, our experimental findings demonstrate that a 300B MoE LLM can be\neffectively trained on lower-performance devices while achieving comparable\nperformance to models of a similar scale, including dense and MoE models.\nCompared to high-performance devices, utilizing a lower-specification hardware\nsystem during the pre-training phase demonstrates significant cost savings,\nreducing computing costs by approximately 20%. The models can be accessed at\nhttps://huggingface.co/inclusionAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this technical report, we tackle the challenges of training large-scale\nMixture of Experts (MoE) models, focusing on overcoming cost inefficiency and\nresource limitations prevalent in such systems. To address these issues, we\npresent two differently sized MoE large language models (LLMs), namely\nLing-Lite and Ling-Plus (referred to as \"Bailing\" in Chinese, spelled\nB\\v{a}il\\'ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75\nbillion activated parameters, while Ling-Plus boasts 290 billion parameters\nwith 28.8 billion activated parameters. Both models exhibit comparable\nperformance to leading industry benchmarks. This report offers actionable\ninsights to improve the efficiency and accessibility of AI development in\nresource-constrained settings, promoting more scalable and sustainable\ntechnologies. Specifically, to reduce training costs for large-scale MoE\nmodels, we propose innovative methods for (1) optimization of model\narchitecture and training processes, (2) refinement of training anomaly\nhandling, and (3) enhancement of model evaluation efficiency. Additionally,\nleveraging high-quality data generated from knowledge graphs, our models\ndemonstrate superior capabilities in tool use compared to other models.\nUltimately, our experimental findings demonstrate that a 300B MoE LLM can be\neffectively trained on lower-performance devices while achieving comparable\nperformance to models of a similar scale, including dense and MoE models.\nCompared to high-performance devices, utilizing a lower-specification hardware\nsystem during the pre-training phase demonstrates significant cost savings,\nreducing computing costs by approximately 20%. The models can be accessed at\nhttps://huggingface.co/inclusionAI."
                },
                "authors": [
                    {
                        "name": "Ling Team"
                    },
                    {
                        "name": "Binwei Zeng"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Changxin Tian"
                    },
                    {
                        "name": "Cong Chen"
                    },
                    {
                        "name": "Dingnan Jin"
                    },
                    {
                        "name": "Feng Yu"
                    },
                    {
                        "name": "Feng Zhu"
                    },
                    {
                        "name": "Feng Yuan"
                    },
                    {
                        "name": "Fakang Wang"
                    },
                    {
                        "name": "Gangshan Wang"
                    },
                    {
                        "name": "Guangyao Zhai"
                    },
                    {
                        "name": "Haitao Zhang"
                    },
                    {
                        "name": "Huizhong Li"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Junpeng Fang"
                    },
                    {
                        "name": "Junjie Ou"
                    },
                    {
                        "name": "Jun Hu"
                    },
                    {
                        "name": "Ji Luo"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Jian Sha"
                    },
                    {
                        "name": "Jianxue Qian"
                    },
                    {
                        "name": "Jiewei Wu"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Jubao Feng"
                    },
                    {
                        "name": "Jingchao Di"
                    },
                    {
                        "name": "Junming Xu"
                    },
                    {
                        "name": "Jinghua Yao"
                    },
                    {
                        "name": "Kuan Xu"
                    },
                    {
                        "name": "Kewei Du"
                    },
                    {
                        "name": "Longfei Li"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Lu Yu"
                    },
                    {
                        "name": "Li Tang"
                    },
                    {
                        "name": "Lin Ju"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Qing Cui"
                    },
                    {
                        "name": "Song Liu"
                    },
                    {
                        "name": "Shicheng Li"
                    },
                    {
                        "name": "Shun Song"
                    },
                    {
                        "name": "Song Yan"
                    },
                    {
                        "name": "Tengwei Cai"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Ting Guo"
                    },
                    {
                        "name": "Ting Huang"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Xueming Yang"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Xiaobo Hu"
                    },
                    {
                        "name": "Xin Lin"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yilong Wang"
                    },
                    {
                        "name": "Yongzhen Guo"
                    },
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Yuhao Fu"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Yanzhe Li"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhaoxin Huan"
                    },
                    {
                        "name": "Zujie Wen"
                    },
                    {
                        "name": "Zhenhang Sun"
                    },
                    {
                        "name": "Zhuoxuan Du"
                    },
                    {
                        "name": "Zhengyu He"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyu He"
                },
                "author": "Zhengyu He",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07358v1",
                "updated": "2025-03-10T14:16:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    16,
                    8,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:16:08Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    16,
                    8,
                    0,
                    69,
                    0
                ],
                "title": "RepoST: Scalable Repository-Level Coding Environment Construction with\n  Sandbox Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoST: Scalable Repository-Level Coding Environment Construction with\n  Sandbox Testing"
                },
                "summary": "We present RepoST, a scalable method to construct environments that provide\nexecution feedback for repository-level code generation for both training and\nevaluation. Unlike existing works that aim to build entire repositories for\nexecution, which is challenging for both human and LLMs, we provide execution\nfeedback with sandbox testing, which isolates a given target function and its\ndependencies to a separate script for testing. Sandbox testing reduces the\ncomplexity of external dependencies and enables constructing environments at a\nlarge scale. We use our method to construct RepoST-Train, a large-scale train\nset with 7,415 functions from 832 repositories. Training with the execution\nfeedback provided by RepoST-Train leads to a performance gain of 5.5% Pass@1 on\nHumanEval and 3.5% Pass@1 on RepoEval. We also build an evaluation dataset,\nRepoST-Eval, and benchmark 12 code generation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RepoST, a scalable method to construct environments that provide\nexecution feedback for repository-level code generation for both training and\nevaluation. Unlike existing works that aim to build entire repositories for\nexecution, which is challenging for both human and LLMs, we provide execution\nfeedback with sandbox testing, which isolates a given target function and its\ndependencies to a separate script for testing. Sandbox testing reduces the\ncomplexity of external dependencies and enables constructing environments at a\nlarge scale. We use our method to construct RepoST-Train, a large-scale train\nset with 7,415 functions from 832 repositories. Training with the execution\nfeedback provided by RepoST-Train leads to a performance gain of 5.5% Pass@1 on\nHumanEval and 3.5% Pass@1 on RepoEval. We also build an evaluation dataset,\nRepoST-Eval, and benchmark 12 code generation models."
                },
                "authors": [
                    {
                        "name": "Yiqing Xie"
                    },
                    {
                        "name": "Alex Xie"
                    },
                    {
                        "name": "Divyanshu Sheth"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Daniel Fried"
                    },
                    {
                        "name": "Carolyn Rose"
                    }
                ],
                "author_detail": {
                    "name": "Carolyn Rose"
                },
                "author": "Carolyn Rose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07357v1",
                "updated": "2025-03-10T14:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    14,
                    35,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T14:14:35Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    14,
                    35,
                    0,
                    69,
                    0
                ],
                "title": "Impact of Microphone Array Mismatches to Learning-based Replay Speech\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Microphone Array Mismatches to Learning-based Replay Speech\n  Detection"
                },
                "summary": "In this work, we investigate the generalization of a multi-channel\nlearning-based replay speech detector, which employs adaptive beamforming and\ndetection, across different microphone arrays. In general, deep neural\nnetwork-based microphone array processing techniques generalize poorly to\nunseen array types, i.e., showing a significant training-test mismatch of\nperformance. We employ the ReMASC dataset to analyze performance degradation\ndue to inter- and intra-device mismatches, assessing both single- and\nmulti-channel configurations. Furthermore, we explore fine-tuning to mitigate\nthe performance loss when transitioning to unseen microphone arrays. Our\nfindings reveal that array mismatches significantly decrease detection\naccuracy, with intra-device generalization being more robust than inter-device.\nHowever, fine-tuning with as little as ten minutes of target data can\neffectively recover performance, providing insights for practical deployment of\nreplay detection systems in heterogeneous automatic speaker verification\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate the generalization of a multi-channel\nlearning-based replay speech detector, which employs adaptive beamforming and\ndetection, across different microphone arrays. In general, deep neural\nnetwork-based microphone array processing techniques generalize poorly to\nunseen array types, i.e., showing a significant training-test mismatch of\nperformance. We employ the ReMASC dataset to analyze performance degradation\ndue to inter- and intra-device mismatches, assessing both single- and\nmulti-channel configurations. Furthermore, we explore fine-tuning to mitigate\nthe performance loss when transitioning to unseen microphone arrays. Our\nfindings reveal that array mismatches significantly decrease detection\naccuracy, with intra-device generalization being more robust than inter-device.\nHowever, fine-tuning with as little as ten minutes of target data can\neffectively recover performance, providing insights for practical deployment of\nreplay detection systems in heterogeneous automatic speaker verification\nenvironments."
                },
                "authors": [
                    {
                        "name": "Michael Neri"
                    },
                    {
                        "name": "Tuomas Virtanen"
                    }
                ],
                "author_detail": {
                    "name": "Tuomas Virtanen"
                },
                "author": "Tuomas Virtanen",
                "arxiv_comment": "Submitted to EUSIPCO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04870v2",
                "updated": "2025-03-10T14:04:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    4,
                    38,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-06T16:04:01Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    4,
                    1,
                    3,
                    65,
                    0
                ],
                "title": "Leveraging Large Language Models to Address Data Scarcity in Machine\n  Learning: Applications in Graphene Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Address Data Scarcity in Machine\n  Learning: Applications in Graphene Synthesis"
                },
                "summary": "Machine learning in materials science faces challenges due to limited\nexperimental data, as generating synthesis data is costly and time-consuming,\nespecially with in-house experiments. Mining data from existing literature\nintroduces issues like mixed data quality, inconsistent formats, and variations\nin reporting experimental parameters, complicating the creation of consistent\nfeatures for the learning algorithm. Additionally, combining continuous and\ndiscrete features can hinder the learning process with limited data. Here, we\npropose strategies that utilize large language models (LLMs) to enhance machine\nlearning performance on a limited, heterogeneous dataset of graphene chemical\nvapor deposition synthesis compiled from existing literature. These strategies\ninclude prompting modalities for imputing missing data points and leveraging\nlarge language model embeddings to encode the complex nomenclature of\nsubstrates reported in chemical vapor deposition experiments. The proposed\nstrategies enhance graphene layer classification using a support vector machine\n(SVM) model, increasing binary classification accuracy from 39% to 65% and\nternary accuracy from 52% to 72%. We compare the performance of the SVM and a\nGPT-4 model, both trained and fine-tuned on the same data. Our results\ndemonstrate that the numerical classifier, when combined with LLM-driven data\nenhancements, outperforms the standalone LLM predictor, highlighting that in\ndata-scarce scenarios, improving predictive learning with LLM strategies\nrequires more than simple fine-tuning on datasets. Instead, it necessitates\nsophisticated approaches for data imputation and feature space homogenization\nto achieve optimal performance. The proposed strategies emphasize data\nenhancement techniques, offering a broadly applicable framework for improving\nmachine learning performance on scarce, inhomogeneous datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning in materials science faces challenges due to limited\nexperimental data, as generating synthesis data is costly and time-consuming,\nespecially with in-house experiments. Mining data from existing literature\nintroduces issues like mixed data quality, inconsistent formats, and variations\nin reporting experimental parameters, complicating the creation of consistent\nfeatures for the learning algorithm. Additionally, combining continuous and\ndiscrete features can hinder the learning process with limited data. Here, we\npropose strategies that utilize large language models (LLMs) to enhance machine\nlearning performance on a limited, heterogeneous dataset of graphene chemical\nvapor deposition synthesis compiled from existing literature. These strategies\ninclude prompting modalities for imputing missing data points and leveraging\nlarge language model embeddings to encode the complex nomenclature of\nsubstrates reported in chemical vapor deposition experiments. The proposed\nstrategies enhance graphene layer classification using a support vector machine\n(SVM) model, increasing binary classification accuracy from 39% to 65% and\nternary accuracy from 52% to 72%. We compare the performance of the SVM and a\nGPT-4 model, both trained and fine-tuned on the same data. Our results\ndemonstrate that the numerical classifier, when combined with LLM-driven data\nenhancements, outperforms the standalone LLM predictor, highlighting that in\ndata-scarce scenarios, improving predictive learning with LLM strategies\nrequires more than simple fine-tuning on datasets. Instead, it necessitates\nsophisticated approaches for data imputation and feature space homogenization\nto achieve optimal performance. The proposed strategies emphasize data\nenhancement techniques, offering a broadly applicable framework for improving\nmachine learning performance on scarce, inhomogeneous datasets."
                },
                "authors": [
                    {
                        "name": "Devi Dutta Biswajeet"
                    },
                    {
                        "name": "Sara Kadkhodaei"
                    }
                ],
                "author_detail": {
                    "name": "Sara Kadkhodaei"
                },
                "author": "Sara Kadkhodaei",
                "arxiv_comment": "20 pages, 10 figures, 4 tables; Supplementary Material with 13\n  figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16457v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16457v3",
                "updated": "2025-03-10T14:00:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    14,
                    0,
                    39,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-23T06:16:23Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    6,
                    16,
                    23,
                    6,
                    54,
                    0
                ],
                "title": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge"
                },
                "summary": "Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Ji Hoon Hong"
                    },
                    {
                        "name": "Dong Won Jeon"
                    },
                    {
                        "name": "Ga-Yeon Baek"
                    },
                    {
                        "name": "Kyeong-Won Kwak"
                    },
                    {
                        "name": "Dong-Hee Lee"
                    },
                    {
                        "name": "Jisu Bae"
                    },
                    {
                        "name": "Chihoon Lee"
                    },
                    {
                        "name": "Yunseo Kim"
                    },
                    {
                        "name": "Seon-Jin Choi"
                    },
                    {
                        "name": "Jin-Seong Park"
                    },
                    {
                        "name": "Sung Beom Cho"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16457v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16457v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03612v2",
                "updated": "2025-03-10T13:58:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    58,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-05T15:51:25Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    51,
                    25,
                    2,
                    64,
                    0
                ],
                "title": "Large language models in finance : what is financial sentiment?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models in finance : what is financial sentiment?"
                },
                "summary": "Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance."
                },
                "authors": [
                    {
                        "name": "Kemal Kirtac"
                    },
                    {
                        "name": "Guido Germano"
                    }
                ],
                "author_detail": {
                    "name": "Guido Germano"
                },
                "author": "Guido Germano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; H.3.4; J.4; J.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07334v1",
                "updated": "2025-03-10T13:49:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    49,
                    28,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:49:28Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    49,
                    28,
                    0,
                    69,
                    0
                ],
                "title": "Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment"
                },
                "summary": "We present Autoregressive Representation Alignment (ARRA), a new training\nframework that unlocks global-coherent text-to-image generation in\nautoregressive LLMs without architectural changes. Unlike prior work that\nrequires complex architectural redesigns, ARRA aligns LLM hidden states with\nvisual representations from external visual foundational models via a global\nvisual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual\nconstraints: local next-token prediction and global semantic distillation,\nenabling LLMs to implicitly learn spatial and contextual coherence while\nretaining their original autoregressive paradigm. Extensive experiments\nvalidate ARRA's plug-and-play versatility. When training from\ntext-generation-only LLMs or random initialization, ARRA reduces FID by 25.5%\n(MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive\nLLMs like Chameleon and LlamaGen, all without framework modifications. For\ndomain adaption, ARRA aligns general-purpose LLMs with specialized models\n(e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on\nmedical imaging (MIMIC-CXR). By demonstrating that training objective redesign\n-- not just architectural innovation -- can resolve cross-modal global\ncoherence challenges, ARRA offers a complementary paradigm for advancing\nautoregressive models. Code and models will be released to advance\nautoregressive image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Autoregressive Representation Alignment (ARRA), a new training\nframework that unlocks global-coherent text-to-image generation in\nautoregressive LLMs without architectural changes. Unlike prior work that\nrequires complex architectural redesigns, ARRA aligns LLM hidden states with\nvisual representations from external visual foundational models via a global\nvisual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual\nconstraints: local next-token prediction and global semantic distillation,\nenabling LLMs to implicitly learn spatial and contextual coherence while\nretaining their original autoregressive paradigm. Extensive experiments\nvalidate ARRA's plug-and-play versatility. When training from\ntext-generation-only LLMs or random initialization, ARRA reduces FID by 25.5%\n(MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive\nLLMs like Chameleon and LlamaGen, all without framework modifications. For\ndomain adaption, ARRA aligns general-purpose LLMs with specialized models\n(e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on\nmedical imaging (MIMIC-CXR). By demonstrating that training objective redesign\n-- not just architectural innovation -- can resolve cross-modal global\ncoherence challenges, ARRA offers a complementary paradigm for advancing\nautoregressive models. Code and models will be released to advance\nautoregressive image generation."
                },
                "authors": [
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Ziyue Lin"
                    },
                    {
                        "name": "Huijie Fan"
                    },
                    {
                        "name": "Zhi Han"
                    },
                    {
                        "name": "Yandong Tang"
                    },
                    {
                        "name": "Liangqiong Qu"
                    }
                ],
                "author_detail": {
                    "name": "Liangqiong Qu"
                },
                "author": "Liangqiong Qu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07329v1",
                "updated": "2025-03-10T13:42:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    42,
                    4,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:42:04Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    42,
                    4,
                    0,
                    69,
                    0
                ],
                "title": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning\n  Large Language Models"
                },
                "summary": "The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation."
                },
                "authors": [
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Guergana Savova"
                    },
                    {
                        "name": "Lijing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijing Wang"
                },
                "author": "Lijing Wang",
                "arxiv_comment": "7 pages, 5 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07323v1",
                "updated": "2025-03-10T13:39:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    39,
                    9,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:39:09Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    39,
                    9,
                    0,
                    69,
                    0
                ],
                "title": "Dynamic Path Navigation for Motion Agents with LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Path Navigation for Motion Agents with LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong generalizable reasoning\nand planning capabilities. However, their efficacies in spatial path planning\nand obstacle-free trajectory generation remain underexplored. Leveraging LLMs\nfor navigation holds significant potential, given LLMs' ability to handle\nunseen scenarios, support user-agent interactions, and provide global control\nacross complex systems, making them well-suited for agentic planning and\nhumanoid motion generation. As one of the first studies in this domain, we\nexplore the zero-shot navigation and path generation capabilities of LLMs by\nconstructing a dataset and proposing an evaluation protocol. Specifically, we\nrepresent paths using anchor points connected by straight lines, enabling\nmovement in various directions. This approach offers greater flexibility and\npracticality compared to previous methods while remaining simple and intuitive\nfor LLMs. We demonstrate that, when tasks are well-structured in this manner,\nmodern LLMs exhibit substantial planning proficiency in avoiding obstacles\nwhile autonomously refining navigation with the generated motion to reach the\ntarget. Further, this spatial reasoning ability of a single LLM motion agent\ninteracting in a static environment can be seamlessly generalized in\nmulti-motion agents coordination in dynamic environments. Unlike traditional\napproaches that rely on single-step planning or local policies, our\ntraining-free LLM-based method enables global, dynamic, closed-loop planning,\nand autonomously resolving collision issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong generalizable reasoning\nand planning capabilities. However, their efficacies in spatial path planning\nand obstacle-free trajectory generation remain underexplored. Leveraging LLMs\nfor navigation holds significant potential, given LLMs' ability to handle\nunseen scenarios, support user-agent interactions, and provide global control\nacross complex systems, making them well-suited for agentic planning and\nhumanoid motion generation. As one of the first studies in this domain, we\nexplore the zero-shot navigation and path generation capabilities of LLMs by\nconstructing a dataset and proposing an evaluation protocol. Specifically, we\nrepresent paths using anchor points connected by straight lines, enabling\nmovement in various directions. This approach offers greater flexibility and\npracticality compared to previous methods while remaining simple and intuitive\nfor LLMs. We demonstrate that, when tasks are well-structured in this manner,\nmodern LLMs exhibit substantial planning proficiency in avoiding obstacles\nwhile autonomously refining navigation with the generated motion to reach the\ntarget. Further, this spatial reasoning ability of a single LLM motion agent\ninteracting in a static environment can be seamlessly generalized in\nmulti-motion agents coordination in dynamic environments. Unlike traditional\napproaches that rely on single-step planning or local policies, our\ntraining-free LLM-based method enables global, dynamic, closed-loop planning,\nand autonomously resolving collision issues."
                },
                "authors": [
                    {
                        "name": "Yubo Zhao"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Keung Tang"
                },
                "author": "Chi-Keung Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07320v1",
                "updated": "2025-03-10T13:37:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    37,
                    36,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:37:36Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    37,
                    36,
                    0,
                    69,
                    0
                ],
                "title": "Experimental Exploration: Investigating Cooperative Interaction Behavior\n  Between Humans and Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental Exploration: Investigating Cooperative Interaction Behavior\n  Between Humans and Large Language Model Agents"
                },
                "summary": "With the rise of large language models (LLMs), AI agents as autonomous\ndecision-makers present significant opportunities and challenges for human-AI\ncooperation. While many studies have explored human cooperation with AI as\ntools, the role of LLM-augmented autonomous agents in competitive-cooperative\ninteractions remains under-examined. This study investigates human cooperative\nbehavior by engaging 30 participants who interacted with LLM agents exhibiting\ndifferent characteristics (purported human, purported rule-based AI agent, and\nLLM agent) in repeated Prisoner's Dilemma games. Findings show significant\ndifferences in cooperative behavior based on the agents' purported\ncharacteristics and the interaction effect of participants' genders and\npurported characteristics. We also analyzed human response patterns, including\ngame completion time, proactive favorable behavior, and acceptance of repair\nefforts. These insights offer a new perspective on human interactions with LLM\nagents in competitive cooperation contexts, such as virtual avatars or future\nphysical entities. The study underscores the importance of understanding human\nbiases toward AI agents and how observed behaviors can influence future\nhuman-AI cooperation dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs), AI agents as autonomous\ndecision-makers present significant opportunities and challenges for human-AI\ncooperation. While many studies have explored human cooperation with AI as\ntools, the role of LLM-augmented autonomous agents in competitive-cooperative\ninteractions remains under-examined. This study investigates human cooperative\nbehavior by engaging 30 participants who interacted with LLM agents exhibiting\ndifferent characteristics (purported human, purported rule-based AI agent, and\nLLM agent) in repeated Prisoner's Dilemma games. Findings show significant\ndifferences in cooperative behavior based on the agents' purported\ncharacteristics and the interaction effect of participants' genders and\npurported characteristics. We also analyzed human response patterns, including\ngame completion time, proactive favorable behavior, and acceptance of repair\nefforts. These insights offer a new perspective on human interactions with LLM\nagents in competitive cooperation contexts, such as virtual avatars or future\nphysical entities. The study underscores the importance of understanding human\nbiases toward AI agents and how observed behaviors can influence future\nhuman-AI cooperation dynamics."
                },
                "authors": [
                    {
                        "name": "Guanxuan Jiang"
                    },
                    {
                        "name": "Yuyang Wang"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07317v1",
                "updated": "2025-03-10T13:35:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    35,
                    51,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:35:51Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    35,
                    51,
                    0,
                    69,
                    0
                ],
                "title": "Self-Corrective Task Planning by Inverse Prompting with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Corrective Task Planning by Inverse Prompting with Large Language\n  Models"
                },
                "summary": "In robot task planning, large language models (LLMs) have shown significant\npromise in generating complex and long-horizon action sequences. However, it is\nobserved that LLMs often produce responses that sound plausible but are not\naccurate. To address these problems, existing methods typically employ\npredefined error sets or external knowledge sources, requiring human efforts\nand computation resources. Recently, self-correction approaches have emerged,\nwhere LLM generates and refines plans, identifying errors by itself. Despite\ntheir effectiveness, they are more prone to failures in correction due to\ninsufficient reasoning. In this paper, we introduce InversePrompt, a novel\nself-corrective task planning approach that leverages inverse prompting to\nenhance interpretability. Our method incorporates reasoning steps to provide\nclear, interpretable feedback. It generates inverse actions corresponding to\nthe initially generated actions and verifies whether these inverse actions can\nrestore the system to its original state, explicitly validating the logical\ncoherence of the generated plans. The results on benchmark datasets show an\naverage 16.3% higher success rate over existing LLM-based task planning\nmethods. Our approach offers clearer justifications for feedback in real-world\nenvironments, resulting in more successful task completion than existing\nself-correction approaches across various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robot task planning, large language models (LLMs) have shown significant\npromise in generating complex and long-horizon action sequences. However, it is\nobserved that LLMs often produce responses that sound plausible but are not\naccurate. To address these problems, existing methods typically employ\npredefined error sets or external knowledge sources, requiring human efforts\nand computation resources. Recently, self-correction approaches have emerged,\nwhere LLM generates and refines plans, identifying errors by itself. Despite\ntheir effectiveness, they are more prone to failures in correction due to\ninsufficient reasoning. In this paper, we introduce InversePrompt, a novel\nself-corrective task planning approach that leverages inverse prompting to\nenhance interpretability. Our method incorporates reasoning steps to provide\nclear, interpretable feedback. It generates inverse actions corresponding to\nthe initially generated actions and verifies whether these inverse actions can\nrestore the system to its original state, explicitly validating the logical\ncoherence of the generated plans. The results on benchmark datasets show an\naverage 16.3% higher success rate over existing LLM-based task planning\nmethods. Our approach offers clearer justifications for feedback in real-world\nenvironments, resulting in more successful task completion than existing\nself-correction approaches across various scenarios."
                },
                "authors": [
                    {
                        "name": "Jiho Lee"
                    },
                    {
                        "name": "Hayun Lee"
                    },
                    {
                        "name": "Jonghyeon Kim"
                    },
                    {
                        "name": "Kyungjae Lee"
                    },
                    {
                        "name": "Eunwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Eunwoo Kim"
                },
                "author": "Eunwoo Kim",
                "arxiv_comment": "7 pages, 5 figures, IEEE International Conference on Robotics and\n  Automation (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07315v1",
                "updated": "2025-03-10T13:34:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    34,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:34:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    34,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Group-robust Sample Reweighting for Subpopulation Shifts via Influence\n  Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group-robust Sample Reweighting for Subpopulation Shifts via Influence\n  Functions"
                },
                "summary": "Machine learning models often have uneven performance among subpopulations\n(a.k.a., groups) in the data distributions. This poses a significant challenge\nfor the models to generalize when the proportions of the groups shift during\ndeployment. To improve robustness to such shifts, existing approaches have\ndeveloped strategies that train models or perform hyperparameter tuning using\nthe group-labeled data to minimize the worst-case loss over groups. However, a\nnon-trivial amount of high-quality labels is often required to obtain\nnoticeable improvements. Given the costliness of the labels, we propose to\nadopt a different paradigm to enhance group label efficiency: utilizing the\ngroup-labeled data as a target set to optimize the weights of other\ngroup-unlabeled data. We introduce Group-robust Sample Reweighting (GSR), a\ntwo-stage approach that first learns the representations from group-unlabeled\ndata, and then tinkers the model by iteratively retraining its last layer on\nthe reweighted data using influence functions. Our GSR is theoretically sound,\npractically lightweight, and effective in improving the robustness to\nsubpopulation shifts. In particular, GSR outperforms the previous\nstate-of-the-art approaches that require the same amount or even more group\nlabels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models often have uneven performance among subpopulations\n(a.k.a., groups) in the data distributions. This poses a significant challenge\nfor the models to generalize when the proportions of the groups shift during\ndeployment. To improve robustness to such shifts, existing approaches have\ndeveloped strategies that train models or perform hyperparameter tuning using\nthe group-labeled data to minimize the worst-case loss over groups. However, a\nnon-trivial amount of high-quality labels is often required to obtain\nnoticeable improvements. Given the costliness of the labels, we propose to\nadopt a different paradigm to enhance group label efficiency: utilizing the\ngroup-labeled data as a target set to optimize the weights of other\ngroup-unlabeled data. We introduce Group-robust Sample Reweighting (GSR), a\ntwo-stage approach that first learns the representations from group-unlabeled\ndata, and then tinkers the model by iteratively retraining its last layer on\nthe reweighted data using influence functions. Our GSR is theoretically sound,\npractically lightweight, and effective in improving the robustness to\nsubpopulation shifts. In particular, GSR outperforms the previous\nstate-of-the-art approaches that require the same amount or even more group\nlabels."
                },
                "authors": [
                    {
                        "name": "Rui Qiao"
                    },
                    {
                        "name": "Zhaoxuan Wu"
                    },
                    {
                        "name": "Jingtan Wang"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "Accepted to the 13th International Conference on Learning\n  Representations (ICLR 2025). Code is available at\n  https://github.com/qiaoruiyt/GSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07314v1",
                "updated": "2025-03-10T13:33:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    33,
                    27,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:33:27Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    33,
                    27,
                    0,
                    69,
                    0
                ],
                "title": "Automated Movie Generation via Multi-Agent CoT Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Movie Generation via Multi-Agent CoT Planning"
                },
                "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent."
                },
                "authors": [
                    {
                        "name": "Weijia Wu"
                    },
                    {
                        "name": "Zeyu Zhu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "The code and project website are available at:\n  https://github.com/showlab/MovieAgent and\n  https://weijiawu.github.io/MovieAgent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07306v1",
                "updated": "2025-03-10T13:28:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    28,
                    25,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:28:25Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    28,
                    25,
                    0,
                    69,
                    0
                ],
                "title": "Benchmarking Chinese Medical LLMs: A Medbench-based Analysis of\n  Performance Gaps and Hierarchical Optimization Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Chinese Medical LLMs: A Medbench-based Analysis of\n  Performance Gaps and Hierarchical Optimization Strategies"
                },
                "summary": "The evaluation and improvement of medical large language models (LLMs) are\ncritical for their real-world deployment, particularly in ensuring accuracy,\nsafety, and ethical alignment. Existing frameworks inadequately dissect\ndomain-specific error patterns or address cross-modal challenges. This study\nintroduces a granular error taxonomy through systematic analysis of top 10\nmodels on MedBench, categorizing incorrect responses into eight types:\nOmissions, Hallucination, Format Mismatch, Causal Reasoning Deficiency,\nContextual Inconsistency, Unanswered, Output Error, and Deficiency in Medical\nLanguage Generation. Evaluation of 10 leading models reveals vulnerabilities:\ndespite achieving 0.86 accuracy in medical knowledge recall, critical reasoning\ntasks show 96.3% omission, while safety ethics evaluations expose alarming\ninconsistency (robustness score: 0.79) under option shuffled. Our analysis\nuncovers systemic weaknesses in knowledge boundary enforcement and multi-step\nreasoning. To address these, we propose a tiered optimization strategy spanning\nfour levels, from prompt engineering and knowledge-augmented retrieval to\nhybrid neuro-symbolic architectures and causal reasoning frameworks. This work\nestablishes an actionable roadmap for developing clinically robust LLMs while\nredefining evaluation paradigms through error-driven insights, ultimately\nadvancing the safety and trustworthiness of AI in high-stakes medical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation and improvement of medical large language models (LLMs) are\ncritical for their real-world deployment, particularly in ensuring accuracy,\nsafety, and ethical alignment. Existing frameworks inadequately dissect\ndomain-specific error patterns or address cross-modal challenges. This study\nintroduces a granular error taxonomy through systematic analysis of top 10\nmodels on MedBench, categorizing incorrect responses into eight types:\nOmissions, Hallucination, Format Mismatch, Causal Reasoning Deficiency,\nContextual Inconsistency, Unanswered, Output Error, and Deficiency in Medical\nLanguage Generation. Evaluation of 10 leading models reveals vulnerabilities:\ndespite achieving 0.86 accuracy in medical knowledge recall, critical reasoning\ntasks show 96.3% omission, while safety ethics evaluations expose alarming\ninconsistency (robustness score: 0.79) under option shuffled. Our analysis\nuncovers systemic weaknesses in knowledge boundary enforcement and multi-step\nreasoning. To address these, we propose a tiered optimization strategy spanning\nfour levels, from prompt engineering and knowledge-augmented retrieval to\nhybrid neuro-symbolic architectures and causal reasoning frameworks. This work\nestablishes an actionable roadmap for developing clinically robust LLMs while\nredefining evaluation paradigms through error-driven insights, ultimately\nadvancing the safety and trustworthiness of AI in high-stakes medical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Luyi Jiang"
                    },
                    {
                        "name": "Jiayuan Chen"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Xinwei Peng"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Jie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Xu"
                },
                "author": "Jie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07298v1",
                "updated": "2025-03-10T13:18:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    18,
                    5,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:18:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    18,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "ALLVB: All-in-One Long Video Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLVB: All-in-One Long Video Understanding Benchmark"
                },
                "summary": "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding."
                },
                "authors": [
                    {
                        "name": "Xichen Tan"
                    },
                    {
                        "name": "Yuanjing Luo"
                    },
                    {
                        "name": "Yunfan Ye"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Zhiping Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhiping Cai"
                },
                "author": "Zhiping Cai",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21037v2",
                "updated": "2025-03-10T13:02:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    2,
                    48,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-28T13:29:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    29,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "The amplifier effect of artificial agents in social contagion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The amplifier effect of artificial agents in social contagion"
                },
                "summary": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways."
                },
                "authors": [
                    {
                        "name": "Eric Hitz"
                    },
                    {
                        "name": "Mingmin Feng"
                    },
                    {
                        "name": "Radu Tanase"
                    },
                    {
                        "name": "René Algesheimer"
                    },
                    {
                        "name": "Manuel S. Mariani"
                    }
                ],
                "author_detail": {
                    "name": "Manuel S. Mariani"
                },
                "author": "Manuel S. Mariani",
                "arxiv_comment": "Main text pp. 1-4; Supplementary Material pp. 5-10",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07282v1",
                "updated": "2025-03-10T13:02:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    2,
                    29,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T13:02:29Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    2,
                    29,
                    0,
                    69,
                    0
                ],
                "title": "A Graph-based Verification Framework for Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-based Verification Framework for Fact-Checking"
                },
                "summary": "Fact-checking plays a crucial role in combating misinformation. Existing\nmethods using large language models (LLMs) for claim decomposition face two key\nlimitations: (1) insufficient decomposition, introducing unnecessary complexity\nto the verification process, and (2) ambiguity of mentions, leading to\nincorrect verification results. To address these challenges, we suggest\nintroducing a claim graph consisting of triplets to address the insufficient\ndecomposition problem and reduce mention ambiguity through graph structure.\nBased on this core idea, we propose a graph-based framework, GraphFC, for\nfact-checking. The framework features three key components: graph construction,\nwhich builds both claim and evidence graphs; graph-guided planning, which\nprioritizes the triplet verification order; and graph-guided checking, which\nverifies the triples one by one between claim and evidence graphs. Extensive\nexperiments show that GraphFC enables fine-grained decomposition while\nresolving referential ambiguities through relational constraints, achieving\nstate-of-the-art performance across three datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking plays a crucial role in combating misinformation. Existing\nmethods using large language models (LLMs) for claim decomposition face two key\nlimitations: (1) insufficient decomposition, introducing unnecessary complexity\nto the verification process, and (2) ambiguity of mentions, leading to\nincorrect verification results. To address these challenges, we suggest\nintroducing a claim graph consisting of triplets to address the insufficient\ndecomposition problem and reduce mention ambiguity through graph structure.\nBased on this core idea, we propose a graph-based framework, GraphFC, for\nfact-checking. The framework features three key components: graph construction,\nwhich builds both claim and evidence graphs; graph-guided planning, which\nprioritizes the triplet verification order; and graph-guided checking, which\nverifies the triples one by one between claim and evidence graphs. Extensive\nexperiments show that GraphFC enables fine-grained decomposition while\nresolving referential ambiguities through relational constraints, achieving\nstate-of-the-art performance across three datasets."
                },
                "authors": [
                    {
                        "name": "Yani Huang"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Junfan Chen"
                    },
                    {
                        "name": "Xuefeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuefeng Zhang"
                },
                "author": "Xuefeng Zhang",
                "arxiv_comment": "13pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04479v2",
                "updated": "2025-03-10T13:01:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    1,
                    58,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-06T14:29:52Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    29,
                    52,
                    3,
                    65,
                    0
                ],
                "title": "ToolFuzz -- Automated Agent Tool Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolFuzz -- Automated Agent Tool Testing"
                },
                "summary": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents."
                },
                "authors": [
                    {
                        "name": "Ivan Milev"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04299v2",
                "updated": "2025-03-10T13:00:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    13,
                    0,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-06T10:39:47Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    39,
                    47,
                    3,
                    65,
                    0
                ],
                "title": "Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation"
                },
                "summary": "The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment."
                },
                "authors": [
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Pierre-François Gimenez"
                    },
                    {
                        "name": "Simeon Campos"
                    }
                ],
                "author_detail": {
                    "name": "Simeon Campos"
                },
                "author": "Simeon Campos",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07276v1",
                "updated": "2025-03-10T12:57:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    57,
                    43,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:57:43Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    57,
                    43,
                    0,
                    69,
                    0
                ],
                "title": "A Systematic Review of ECG Arrhythmia Classification: Adherence to\n  Standards, Fair Evaluation, and Embedded Feasibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Review of ECG Arrhythmia Classification: Adherence to\n  Standards, Fair Evaluation, and Embedded Feasibility"
                },
                "summary": "The classification of electrocardiogram (ECG) signals is crucial for early\ndetection of arrhythmias and other cardiac conditions. However, despite\nadvances in machine learning, many studies fail to follow standardization\nprotocols, leading to inconsistencies in performance evaluation and real-world\napplicability. Additionally, hardware constraints essential for practical\ndeployment, such as in pacemakers, Holter monitors, and wearable ECG patches,\nare often overlooked. Since real-world impact depends on feasibility in\nresource-constrained devices, ensuring efficient deployment is critical for\ncontinuous monitoring. This review systematically analyzes ECG classification\nstudies published between 2017 and 2024, focusing on those adhering to the E3C\n(Embedded, Clinical, and Comparative Criteria), which include inter-patient\nparadigm implementation, compliance with Association for the Advancement of\nMedical Instrumentation (AAMI) recommendations, and model feasibility for\nembedded systems. While many studies report high accuracy, few properly\nconsider patient-independent partitioning and hardware limitations. We identify\nstate-of-the-art methods meeting E3C criteria and conduct a comparative\nanalysis of accuracy, inference time, energy consumption, and memory usage.\nFinally, we propose standardized reporting practices to ensure fair comparisons\nand practical applicability of ECG classification models. By addressing these\ngaps, this study aims to guide future research toward more robust and\nclinically viable ECG classification systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classification of electrocardiogram (ECG) signals is crucial for early\ndetection of arrhythmias and other cardiac conditions. However, despite\nadvances in machine learning, many studies fail to follow standardization\nprotocols, leading to inconsistencies in performance evaluation and real-world\napplicability. Additionally, hardware constraints essential for practical\ndeployment, such as in pacemakers, Holter monitors, and wearable ECG patches,\nare often overlooked. Since real-world impact depends on feasibility in\nresource-constrained devices, ensuring efficient deployment is critical for\ncontinuous monitoring. This review systematically analyzes ECG classification\nstudies published between 2017 and 2024, focusing on those adhering to the E3C\n(Embedded, Clinical, and Comparative Criteria), which include inter-patient\nparadigm implementation, compliance with Association for the Advancement of\nMedical Instrumentation (AAMI) recommendations, and model feasibility for\nembedded systems. While many studies report high accuracy, few properly\nconsider patient-independent partitioning and hardware limitations. We identify\nstate-of-the-art methods meeting E3C criteria and conduct a comparative\nanalysis of accuracy, inference time, energy consumption, and memory usage.\nFinally, we propose standardized reporting practices to ensure fair comparisons\nand practical applicability of ECG classification models. By addressing these\ngaps, this study aims to guide future research toward more robust and\nclinically viable ECG classification systems."
                },
                "authors": [
                    {
                        "name": "Guilherme Silva"
                    },
                    {
                        "name": "Pedro Silva"
                    },
                    {
                        "name": "Gladston Moreira"
                    },
                    {
                        "name": "Vander Freitas"
                    },
                    {
                        "name": "Jadson Gertrudes"
                    },
                    {
                        "name": "Eduardo Luz"
                    }
                ],
                "author_detail": {
                    "name": "Eduardo Luz"
                },
                "author": "Eduardo Luz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05870v4",
                "updated": "2025-03-10T12:56:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    56,
                    54,
                    0,
                    69,
                    0
                ],
                "published": "2024-06-09T17:55:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    17,
                    55,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents"
                },
                "summary": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database and applying an LLM to the\nretrieved documents. We demonstrate that RAG systems that operate on databases\nwith untrusted content are vulnerable to denial-of-service attacks we call\njamming. An adversary can add a single ``blocker'' document to the database\nthat will be retrieved in response to a specific query and result in the RAG\nsystem not answering this query, ostensibly because it lacks relevant\ninformation or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. Our\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not employ an auxiliary LLM.\n  We evaluate jamming attacks on several embeddings and LLMs and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database and applying an LLM to the\nretrieved documents. We demonstrate that RAG systems that operate on databases\nwith untrusted content are vulnerable to denial-of-service attacks we call\njamming. An adversary can add a single ``blocker'' document to the database\nthat will be retrieved in response to a specific query and result in the RAG\nsystem not answering this query, ostensibly because it lacks relevant\ninformation or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. Our\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not employ an auxiliary LLM.\n  We evaluate jamming attacks on several embeddings and LLMs and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents."
                },
                "authors": [
                    {
                        "name": "Avital Shafran"
                    },
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20076v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20076v5",
                "updated": "2025-03-10T12:34:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    34,
                    24,
                    0,
                    69,
                    0
                ],
                "published": "2024-06-28T17:38:18Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    38,
                    18,
                    4,
                    180,
                    0
                ],
                "title": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model"
                },
                "summary": "Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Lianghui Zhu"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Heng Liu"
                    },
                    {
                        "name": "Longjin Ran"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Preprint. Update: (1) better performance and (2) versatile\n  segmentation. Code and models are available at:\n  https://github.com/hustvl/EVF-SAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20076v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20076v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02662v2",
                "updated": "2025-03-10T12:26:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    26,
                    22,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-04T14:25:51Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    25,
                    51,
                    1,
                    63,
                    0
                ],
                "title": "10K is Enough: An Ultra-Lightweight Binarized Network for Infrared\n  Small-Target Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "10K is Enough: An Ultra-Lightweight Binarized Network for Infrared\n  Small-Target Detection"
                },
                "summary": "The widespread deployment of Infrared Small-Target Detection (IRSTD)\nalgorithms on edge devices necessitates the exploration of model compression\ntechniques. Binarized neural networks (BNNs) are distinguished by their\nexceptional efficiency in model compression. However, the small size of\ninfrared targets introduces stringent precision requirements for the IRSTD\ntask, while the inherent precision loss during binarization presents a\nsignificant challenge. To address this, we propose the Binarized Infrared\nSmall-Target Detection Network (BiisNet), which preserves the core operations\nof binarized convolutions while integrating full-precision features into the\nnetwork's information flow. Specifically, we propose the Dot Binary\nConvolution, which retains fine-grained semantic information in feature maps\nwhile still leveraging the binarized convolution operations. In addition, we\nintroduce a smooth and adaptive Dynamic Softsign function, which provides more\ncomprehensive and progressively finer gradient during backpropagation,\nenhancing model stability and promoting an optimal weight distribution.\nExperimental results demonstrate that BiisNet not only significantly\noutperforms other binary architectures but also has strong competitiveness\namong state-of-the-art full-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of Infrared Small-Target Detection (IRSTD)\nalgorithms on edge devices necessitates the exploration of model compression\ntechniques. Binarized neural networks (BNNs) are distinguished by their\nexceptional efficiency in model compression. However, the small size of\ninfrared targets introduces stringent precision requirements for the IRSTD\ntask, while the inherent precision loss during binarization presents a\nsignificant challenge. To address this, we propose the Binarized Infrared\nSmall-Target Detection Network (BiisNet), which preserves the core operations\nof binarized convolutions while integrating full-precision features into the\nnetwork's information flow. Specifically, we propose the Dot Binary\nConvolution, which retains fine-grained semantic information in feature maps\nwhile still leveraging the binarized convolution operations. In addition, we\nintroduce a smooth and adaptive Dynamic Softsign function, which provides more\ncomprehensive and progressively finer gradient during backpropagation,\nenhancing model stability and promoting an optimal weight distribution.\nExperimental results demonstrate that BiisNet not only significantly\noutperforms other binary architectures but also has strong competitiveness\namong state-of-the-art full-precision models."
                },
                "authors": [
                    {
                        "name": "Biqiao Xin"
                    },
                    {
                        "name": "Qianchen Mao"
                    },
                    {
                        "name": "Bingshu Wang"
                    },
                    {
                        "name": "Jiangbin Zheng"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "C. L. Philip Chen"
                    }
                ],
                "author_detail": {
                    "name": "C. L. Philip Chen"
                },
                "author": "C. L. Philip Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07237v1",
                "updated": "2025-03-10T12:20:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    20,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:20:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    20,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "LLM-C3MOD: A Human-LLM Collaborative System for Cross-Cultural Hate\n  Speech Moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-C3MOD: A Human-LLM Collaborative System for Cross-Cultural Hate\n  Speech Moderation"
                },
                "summary": "Content moderation is a global challenge, yet major tech platforms prioritize\nhigh-resource languages, leaving low-resource languages with scarce native\nmoderators. Since effective moderation depends on understanding contextual\ncues, this imbalance increases the risk of improper moderation due to\nnon-native moderators' limited cultural understanding. Through a user study, we\nidentify that non-native moderators struggle with interpreting\nculturally-specific knowledge, sentiment, and internet culture in the hate\nspeech moderation. To assist them, we present LLM-C3MOD, a human-LLM\ncollaborative pipeline with three steps: (1) RAG-enhanced cultural context\nannotations; (2) initial LLM-based moderation; and (3) targeted human\nmoderation for cases lacking LLM consensus. Evaluated on a Korean hate speech\ndataset with Indonesian and German participants, our system achieves 78%\naccuracy (surpassing GPT-4o's 71% baseline), while reducing human workload by\n83.6%. Notably, human moderators excel at nuanced contents where LLMs struggle.\nOur findings suggest that non-native moderators, when properly supported by\nLLMs, can effectively contribute to cross-cultural hate speech moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content moderation is a global challenge, yet major tech platforms prioritize\nhigh-resource languages, leaving low-resource languages with scarce native\nmoderators. Since effective moderation depends on understanding contextual\ncues, this imbalance increases the risk of improper moderation due to\nnon-native moderators' limited cultural understanding. Through a user study, we\nidentify that non-native moderators struggle with interpreting\nculturally-specific knowledge, sentiment, and internet culture in the hate\nspeech moderation. To assist them, we present LLM-C3MOD, a human-LLM\ncollaborative pipeline with three steps: (1) RAG-enhanced cultural context\nannotations; (2) initial LLM-based moderation; and (3) targeted human\nmoderation for cases lacking LLM consensus. Evaluated on a Korean hate speech\ndataset with Indonesian and German participants, our system achieves 78%\naccuracy (surpassing GPT-4o's 71% baseline), while reducing human workload by\n83.6%. Notably, human moderators excel at nuanced contents where LLMs struggle.\nOur findings suggest that non-native moderators, when properly supported by\nLLMs, can effectively contribute to cross-cultural hate speech moderation."
                },
                "authors": [
                    {
                        "name": "Junyeong Park"
                    },
                    {
                        "name": "Seogyeong Jeong"
                    },
                    {
                        "name": "Seyoung Song"
                    },
                    {
                        "name": "Yohan Lee"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "arxiv_comment": "Accepted to NAACL 2025 Workshop - C3NLP (Workshop on Cross-Cultural\n  Considerations in NLP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11926v2",
                "updated": "2025-03-10T12:20:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    20,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-17T15:39:50Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    15,
                    39,
                    50,
                    0,
                    48,
                    0
                ],
                "title": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages"
                },
                "summary": "People worldwide use language in subtle and complex ways to express emotions.\nWhile emotion recognition -- an umbrella term for several NLP tasks --\nsignificantly impacts different applications in NLP and other fields, most work\nin the area is focused on high-resource languages. Therefore, this has led to\nmajor disparities in research and proposed solutions, especially for\nlow-resource languages that suffer from the lack of high-quality datasets. In\nthis paper, we present BRIGHTER -- a collection of multilabeled\nemotion-annotated datasets in 28 different languages. BRIGHTER covers\npredominantly low-resource languages from Africa, Asia, Eastern Europe, and\nLatin America, with instances from various domains annotated by fluent\nspeakers. We describe the data collection and annotation processes and the\nchallenges of building these datasets. Then, we report different experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as intensity-level emotion recognition. We investigate results with and\nwithout using LLMs and analyse the large variability in performance across\nlanguages and text domains. We show that BRIGHTER datasets are a step towards\nbridging the gap in text-based emotion recognition and discuss their impact and\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People worldwide use language in subtle and complex ways to express emotions.\nWhile emotion recognition -- an umbrella term for several NLP tasks --\nsignificantly impacts different applications in NLP and other fields, most work\nin the area is focused on high-resource languages. Therefore, this has led to\nmajor disparities in research and proposed solutions, especially for\nlow-resource languages that suffer from the lack of high-quality datasets. In\nthis paper, we present BRIGHTER -- a collection of multilabeled\nemotion-annotated datasets in 28 different languages. BRIGHTER covers\npredominantly low-resource languages from Africa, Asia, Eastern Europe, and\nLatin America, with instances from various domains annotated by fluent\nspeakers. We describe the data collection and annotation processes and the\nchallenges of building these datasets. Then, we report different experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as intensity-level emotion recognition. We investigate results with and\nwithout using LLMs and analyse the large variability in performance across\nlanguages and text domains. We show that BRIGHTER datasets are a step towards\nbridging the gap in text-based emotion recognition and discuss their impact and\nutility."
                },
                "authors": [
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Meriem Beloucif"
                    },
                    {
                        "name": "Christine de Kock"
                    },
                    {
                        "name": "Nirmal Surange"
                    },
                    {
                        "name": "Daniela Teodorescu"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Felermino D. M. A. Ali"
                    },
                    {
                        "name": "Ilseyar Alimova"
                    },
                    {
                        "name": "Vladimir Araujo"
                    },
                    {
                        "name": "Nikolay Babakov"
                    },
                    {
                        "name": "Naomi Baes"
                    },
                    {
                        "name": "Ana-Maria Bucur"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "Guanqun Cao"
                    },
                    {
                        "name": "Rodrigo Tufino Cardenas"
                    },
                    {
                        "name": "Rendi Chevi"
                    },
                    {
                        "name": "Chiamaka Ijeoma Chukwuneke"
                    },
                    {
                        "name": "Alexandra Ciobotaru"
                    },
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Murja Sani Gadanya"
                    },
                    {
                        "name": "Robert Geislinger"
                    },
                    {
                        "name": "Bela Gipp"
                    },
                    {
                        "name": "Oumaima Hourrane"
                    },
                    {
                        "name": "Oana Ignat"
                    },
                    {
                        "name": "Falalu Ibrahim Lawan"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Rahmad Mahendra"
                    },
                    {
                        "name": "Vukosi Marivate"
                    },
                    {
                        "name": "Andrew Piper"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Charles Henrique Porto Ferreira"
                    },
                    {
                        "name": "Vitaly Protasov"
                    },
                    {
                        "name": "Samuel Rutunda"
                    },
                    {
                        "name": "Manish Shrivastava"
                    },
                    {
                        "name": "Aura Cristina Udrea"
                    },
                    {
                        "name": "Lilian Diana Awuor Wanzare"
                    },
                    {
                        "name": "Sophie Wu"
                    },
                    {
                        "name": "Florian Valentin Wunderlich"
                    },
                    {
                        "name": "Hanif Muhammad Zhafran"
                    },
                    {
                        "name": "Tianhui Zhang"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Saif M. Mohammad"
                    }
                ],
                "author_detail": {
                    "name": "Saif M. Mohammad"
                },
                "author": "Saif M. Mohammad",
                "arxiv_comment": "20 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07234v1",
                "updated": "2025-03-10T12:17:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    17,
                    38,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T12:17:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    17,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs\n  and Chain-of-Thought Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs\n  and Chain-of-Thought Prompting"
                },
                "summary": "Accurate motion forecasting is crucial for safe autonomous driving (AD). This\nstudy proposes CoT-Drive, a novel approach that enhances motion forecasting by\nleveraging large language models (LLMs) and a chain-of-thought (CoT) prompting\nmethod. We introduce a teacher-student knowledge distillation strategy to\neffectively transfer LLMs' advanced scene understanding capabilities to\nlightweight language models (LMs), ensuring that CoT-Drive operates in\nreal-time on edge devices while maintaining comprehensive scene understanding\nand generalization capabilities. By leveraging CoT prompting techniques for\nLLMs without additional training, CoT-Drive generates semantic annotations that\nsignificantly improve the understanding of complex traffic environments,\nthereby boosting the accuracy and robustness of predictions. Additionally, we\npresent two new scene description datasets, Highway-Text and Urban-Text,\ndesigned for fine-tuning lightweight LMs to generate context-specific semantic\nannotations. Comprehensive evaluations of five real-world datasets demonstrate\nthat CoT-Drive outperforms existing models, highlighting its effectiveness and\nefficiency in handling complex traffic scenarios. Overall, this study is the\nfirst to consider the practical application of LLMs in this field. It pioneers\nthe training and use of a lightweight LLM surrogate for motion forecasting,\nsetting a new benchmark and showcasing the potential of integrating LLMs into\nAD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate motion forecasting is crucial for safe autonomous driving (AD). This\nstudy proposes CoT-Drive, a novel approach that enhances motion forecasting by\nleveraging large language models (LLMs) and a chain-of-thought (CoT) prompting\nmethod. We introduce a teacher-student knowledge distillation strategy to\neffectively transfer LLMs' advanced scene understanding capabilities to\nlightweight language models (LMs), ensuring that CoT-Drive operates in\nreal-time on edge devices while maintaining comprehensive scene understanding\nand generalization capabilities. By leveraging CoT prompting techniques for\nLLMs without additional training, CoT-Drive generates semantic annotations that\nsignificantly improve the understanding of complex traffic environments,\nthereby boosting the accuracy and robustness of predictions. Additionally, we\npresent two new scene description datasets, Highway-Text and Urban-Text,\ndesigned for fine-tuning lightweight LMs to generate context-specific semantic\nannotations. Comprehensive evaluations of five real-world datasets demonstrate\nthat CoT-Drive outperforms existing models, highlighting its effectiveness and\nefficiency in handling complex traffic scenarios. Overall, this study is the\nfirst to consider the practical application of LLMs in this field. It pioneers\nthe training and use of a lightweight LLM surrogate for motion forecasting,\nsetting a new benchmark and showcasing the potential of integrating LLMs into\nAD systems."
                },
                "authors": [
                    {
                        "name": "Haicheng Liao"
                    },
                    {
                        "name": "Hanlin Kong"
                    },
                    {
                        "name": "Bonan Wang"
                    },
                    {
                        "name": "Chengyue Wang"
                    },
                    {
                        "name": "Wang Ye"
                    },
                    {
                        "name": "Zhengbing He"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Zhenning Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhenning Li"
                },
                "author": "Zhenning Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13178v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13178v4",
                "updated": "2025-03-10T12:13:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    13,
                    9,
                    0,
                    69,
                    0
                ],
                "published": "2024-12-17T18:55:58Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents"
                },
                "summary": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to understand and plan complicated natural language\ninstructions. However, a foreseeable issue is that those embodied agents can\nalso flawlessly execute some hazardous tasks, potentially causing damages in\nthe real world. Existing benchmarks predominantly overlook critical safety\nrisks, focusing solely on planning performance, while a few evaluate LLMs'\nsafety awareness only on non-interactive image-text data. To address this gap,\nwe present SafeAgentBench-the first benchmark for safety-aware task planning of\nembodied LLM agents in interactive simulation environments. SafeAgentBench\nincludes: (1) an executable, diverse, and high-quality dataset of 750 tasks,\nrigorously curated to cover 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that, although\nagents based on different design frameworks exhibit substantial differences in\ntask success rates, their overall safety awareness remains weak. The most\nsafety-conscious baseline achieves only a 10\\% rejection rate for detailed\nhazardous tasks. Moreover, simply replacing the LLM driving the agent does not\nlead to notable improvements in safety awareness. More details and code are\navailable at https://github.com/shengyin1224/SafeAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to understand and plan complicated natural language\ninstructions. However, a foreseeable issue is that those embodied agents can\nalso flawlessly execute some hazardous tasks, potentially causing damages in\nthe real world. Existing benchmarks predominantly overlook critical safety\nrisks, focusing solely on planning performance, while a few evaluate LLMs'\nsafety awareness only on non-interactive image-text data. To address this gap,\nwe present SafeAgentBench-the first benchmark for safety-aware task planning of\nembodied LLM agents in interactive simulation environments. SafeAgentBench\nincludes: (1) an executable, diverse, and high-quality dataset of 750 tasks,\nrigorously curated to cover 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that, although\nagents based on different design frameworks exhibit substantial differences in\ntask success rates, their overall safety awareness remains weak. The most\nsafety-conscious baseline achieves only a 10\\% rejection rate for detailed\nhazardous tasks. Moreover, simply replacing the LLM driving the agent does not\nlead to notable improvements in safety awareness. More details and code are\navailable at https://github.com/shengyin1224/SafeAgentBench."
                },
                "authors": [
                    {
                        "name": "Sheng Yin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Yuanzhuo Ding"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Yutong Bi"
                    },
                    {
                        "name": "Yichen Xiong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "23 pages, 17 tables, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13178v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13178v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17506v2",
                "updated": "2025-03-10T12:11:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    11,
                    58,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-22T00:12:52Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    0,
                    12,
                    52,
                    5,
                    53,
                    0
                ],
                "title": "RAG-Enhanced Collaborative LLM Agents for Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Enhanced Collaborative LLM Agents for Drug Discovery"
                },
                "summary": "Recent advances in large language models (LLMs) have shown great potential to\naccelerate drug discovery. However, the specialized nature of biochemical data\noften necessitates costly domain-specific fine-tuning, posing critical\nchallenges. First, it hinders the application of more flexible general-purpose\nLLMs in cutting-edge drug discovery tasks. More importantly, it impedes the\nrapid integration of the vast amounts of scientific data continuously generated\nthrough experiments and research. To investigate these challenges, we propose\nCLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored\nto drug discovery tasks. Through the collaboration of multiple LLM agents,\nCLADD dynamically retrieves information from biomedical knowledge bases,\ncontextualizes query molecules, and integrates relevant evidence to generate\nresponses -- all without the need for domain-specific fine-tuning. Crucially,\nwe tackle key obstacles in applying RAG workflows to biochemical data,\nincluding data heterogeneity, ambiguity, and multi-source integration. We\ndemonstrate the flexibility and effectiveness of this framework across a\nvariety of drug discovery tasks, showing that it outperforms general-purpose\nand domain-specific LLMs as well as traditional deep learning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown great potential to\naccelerate drug discovery. However, the specialized nature of biochemical data\noften necessitates costly domain-specific fine-tuning, posing critical\nchallenges. First, it hinders the application of more flexible general-purpose\nLLMs in cutting-edge drug discovery tasks. More importantly, it impedes the\nrapid integration of the vast amounts of scientific data continuously generated\nthrough experiments and research. To investigate these challenges, we propose\nCLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored\nto drug discovery tasks. Through the collaboration of multiple LLM agents,\nCLADD dynamically retrieves information from biomedical knowledge bases,\ncontextualizes query molecules, and integrates relevant evidence to generate\nresponses -- all without the need for domain-specific fine-tuning. Crucially,\nwe tackle key obstacles in applying RAG workflows to biochemical data,\nincluding data heterogeneity, ambiguity, and multi-source integration. We\ndemonstrate the flexibility and effectiveness of this framework across a\nvariety of drug discovery tasks, showing that it outperforms general-purpose\nand domain-specific LLMs as well as traditional deep learning approaches."
                },
                "authors": [
                    {
                        "name": "Namkyeong Lee"
                    },
                    {
                        "name": "Edward De Brouwer"
                    },
                    {
                        "name": "Ehsan Hajiramezanali"
                    },
                    {
                        "name": "Tommaso Biancalani"
                    },
                    {
                        "name": "Chanyoung Park"
                    },
                    {
                        "name": "Gabriele Scalia"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Scalia"
                },
                "author": "Gabriele Scalia",
                "arxiv_comment": "Machine Learning, Drug Discovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04090v2",
                "updated": "2025-03-10T12:01:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    1,
                    1,
                    0,
                    69,
                    0
                ],
                "published": "2024-12-05T11:52:20Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    52,
                    20,
                    3,
                    340,
                    0
                ],
                "title": "LossAgent: Towards Any Optimization Objectives for Image Processing with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LossAgent: Towards Any Optimization Objectives for Image Processing with\n  LLM Agents"
                },
                "summary": "We present the first loss agent, dubbed LossAgent, for low-level image\nprocessing tasks, e.g., image super-resolution and restoration, intending to\nachieve any customized optimization objectives of low-level image processing in\ndifferent practical applications. Notably, not all optimization objectives,\nsuch as complex hand-crafted perceptual metrics, text description, and\nintricate human feedback, can be instantiated with existing low-level losses,\ne.g., MSE loss, which presents a crucial challenge in optimizing image\nprocessing networks in an end-to-end manner. To eliminate this, our LossAgent\nintroduces the powerful large language model (LLM) as the loss agent, where the\nrich textual understanding of prior knowledge empowers the loss agent with the\npotential to understand complex optimization objectives, trajectory, and state\nfeedback from external environments in the optimization process of the\nlow-level image processing networks. In particular, we establish the loss\nrepository by incorporating existing loss functions that support the end-to-end\noptimization for low-level image processing. Then, we design the\noptimization-oriented prompt engineering for the loss agent to actively and\nintelligently decide the compositional weights for each loss in the repository\nat each optimization interaction, thereby achieving the required optimization\ntrajectory for any customized optimization objectives. Extensive experiments on\nthree typical low-level image processing tasks and multiple optimization\nobjectives have shown the effectiveness and applicability of our proposed\nLossAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first loss agent, dubbed LossAgent, for low-level image\nprocessing tasks, e.g., image super-resolution and restoration, intending to\nachieve any customized optimization objectives of low-level image processing in\ndifferent practical applications. Notably, not all optimization objectives,\nsuch as complex hand-crafted perceptual metrics, text description, and\nintricate human feedback, can be instantiated with existing low-level losses,\ne.g., MSE loss, which presents a crucial challenge in optimizing image\nprocessing networks in an end-to-end manner. To eliminate this, our LossAgent\nintroduces the powerful large language model (LLM) as the loss agent, where the\nrich textual understanding of prior knowledge empowers the loss agent with the\npotential to understand complex optimization objectives, trajectory, and state\nfeedback from external environments in the optimization process of the\nlow-level image processing networks. In particular, we establish the loss\nrepository by incorporating existing loss functions that support the end-to-end\noptimization for low-level image processing. Then, we design the\noptimization-oriented prompt engineering for the loss agent to actively and\nintelligently decide the compositional weights for each loss in the repository\nat each optimization interaction, thereby achieving the required optimization\ntrajectory for any customized optimization objectives. Extensive experiments on\nthree typical low-level image processing tasks and multiple optimization\nobjectives have shown the effectiveness and applicability of our proposed\nLossAgent."
                },
                "authors": [
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Update format",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07215v1",
                "updated": "2025-03-10T11:52:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    52,
                    48,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T11:52:48Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    52,
                    48,
                    0,
                    69,
                    0
                ],
                "title": "Control Flow-Augmented Decompiler based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control Flow-Augmented Decompiler based on Large Language Model"
                },
                "summary": "Binary decompilation plays a crucial role in various tasks related to\nsecurity threat analysis and software engineering, such as binary vulnerability\ndetection and software supply chain analysis. Current prevalent binary\ndecompilation methods primarily rely on large language models (LLMs) and can be\nbroadly classified into two main approaches: prompt-based decompilation and\nend-toend decompilation. Prompt-based methods typically require significant\neffort to analyze and summarize the predicted data to extract aspect-specific\nexpert knowledge, which is then fed into a general purpose large language model\nto address specific decompilation tasks. End-to-end methods, on the other hand,\ncarefully construct training datasets or neural networks to perform\npost-training on general-purpose large language models, thereby obtaining\ndomain-specific large language models for decompiling the predicted data.\nHowever, both existing approaches still face significant challenges, including\nthe absence of rich semantic representations of the input code and the neglect\nof control flow information, which is crucial for accurate decompilation.\nFurthermore, most current decompilation techniques are specifically tailored\nfor the x86 architecture, making it difficult to efficiently adapt and\ngeneralize them to other bit width or instruction architectures. To address\nthese limitations, we propose a novel end-to-end decompilation LLM, CFADecLLM,\nwhich aims to enhance existing end-to-end decompilation methods. We conduct\nextensive experiments on the public dataset Humaneval and Exebench across four\noptimization levels, and results demonstrate that our approach outperforms\nexisting methods in multiple metrics, validating its effectiveness and\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary decompilation plays a crucial role in various tasks related to\nsecurity threat analysis and software engineering, such as binary vulnerability\ndetection and software supply chain analysis. Current prevalent binary\ndecompilation methods primarily rely on large language models (LLMs) and can be\nbroadly classified into two main approaches: prompt-based decompilation and\nend-toend decompilation. Prompt-based methods typically require significant\neffort to analyze and summarize the predicted data to extract aspect-specific\nexpert knowledge, which is then fed into a general purpose large language model\nto address specific decompilation tasks. End-to-end methods, on the other hand,\ncarefully construct training datasets or neural networks to perform\npost-training on general-purpose large language models, thereby obtaining\ndomain-specific large language models for decompiling the predicted data.\nHowever, both existing approaches still face significant challenges, including\nthe absence of rich semantic representations of the input code and the neglect\nof control flow information, which is crucial for accurate decompilation.\nFurthermore, most current decompilation techniques are specifically tailored\nfor the x86 architecture, making it difficult to efficiently adapt and\ngeneralize them to other bit width or instruction architectures. To address\nthese limitations, we propose a novel end-to-end decompilation LLM, CFADecLLM,\nwhich aims to enhance existing end-to-end decompilation methods. We conduct\nextensive experiments on the public dataset Humaneval and Exebench across four\noptimization levels, and results demonstrate that our approach outperforms\nexisting methods in multiple metrics, validating its effectiveness and\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Peipei Liu"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Zhaoteng Yan"
                    },
                    {
                        "name": "Peizheng Zhang"
                    },
                    {
                        "name": "Dapeng Sun"
                    },
                    {
                        "name": "Dawei Wang"
                    },
                    {
                        "name": "Dan Li"
                    }
                ],
                "author_detail": {
                    "name": "Dan Li"
                },
                "author": "Dan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07202v1",
                "updated": "2025-03-10T11:38:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    38,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T11:38:21Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    38,
                    21,
                    0,
                    69,
                    0
                ],
                "title": "A Zero-shot Learning Method Based on Large Language Models for\n  Multi-modal Knowledge Graph Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-shot Learning Method Based on Large Language Models for\n  Multi-modal Knowledge Graph Embedding"
                },
                "summary": "Zero-shot learning (ZL) is crucial for tasks involving unseen categories,\nsuch as natural language processing, image classification, and cross-lingual\ntransfer. Current applications often fail to accurately infer and handle new\nrelations or entities involving unseen categories, severely limiting their\nscalability and practicality in open-domain scenarios. ZL learning faces the\nchallenge of effectively transferring semantic information of unseen categories\nin multi-modal knowledge graph (MMKG) embedding representation learning. In\nthis paper, we propose ZSLLM, a framework for zero-shot embedding learning of\nMMKGs using large language models (LLMs). We leverage textual modality\ninformation of unseen categories as prompts to fully utilize the reasoning\ncapabilities of LLMs, enabling semantic information transfer across different\nmodalities for unseen categories. Through model-based learning, the embedding\nrepresentation of unseen categories in MMKG is enhanced. Extensive experiments\nconducted on multiple real-world datasets demonstrate the superiority of our\napproach compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot learning (ZL) is crucial for tasks involving unseen categories,\nsuch as natural language processing, image classification, and cross-lingual\ntransfer. Current applications often fail to accurately infer and handle new\nrelations or entities involving unseen categories, severely limiting their\nscalability and practicality in open-domain scenarios. ZL learning faces the\nchallenge of effectively transferring semantic information of unseen categories\nin multi-modal knowledge graph (MMKG) embedding representation learning. In\nthis paper, we propose ZSLLM, a framework for zero-shot embedding learning of\nMMKGs using large language models (LLMs). We leverage textual modality\ninformation of unseen categories as prompts to fully utilize the reasoning\ncapabilities of LLMs, enabling semantic information transfer across different\nmodalities for unseen categories. Through model-based learning, the embedding\nrepresentation of unseen categories in MMKG is enhanced. Extensive experiments\nconducted on multiple real-world datasets demonstrate the superiority of our\napproach compared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Bingchen Liu"
                    },
                    {
                        "name": "Jingchen Li"
                    },
                    {
                        "name": "Naixing Xu"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07195v1",
                "updated": "2025-03-10T11:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    23,
                    44,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T11:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    23,
                    44,
                    0,
                    69,
                    0
                ],
                "title": "Contextual Cues in Machine Translation: Investigating the Potential of\n  Multi-Source Input Strategies in LLMs and NMT Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Cues in Machine Translation: Investigating the Potential of\n  Multi-Source Input Strategies in LLMs and NMT Systems"
                },
                "summary": "We explore the impact of multi-source input strategies on machine translation\n(MT) quality, comparing GPT-4o, a large language model (LLM), with a\ntraditional multilingual neural machine translation (NMT) system. Using\nintermediate language translations as contextual cues, we evaluate their\neffectiveness in enhancing English and Chinese translations into Portuguese.\nResults suggest that contextual information significantly improves translation\nquality for domain-specific datasets and potentially for linguistically distant\nlanguage pairs, with diminishing returns observed in benchmarks with high\nlinguistic variability. Additionally, we demonstrate that shallow fusion, a\nmulti-source approach we apply within the NMT system, shows improved results\nwhen using high-resource languages as context for other translation pairs,\nhighlighting the importance of strategic context language selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the impact of multi-source input strategies on machine translation\n(MT) quality, comparing GPT-4o, a large language model (LLM), with a\ntraditional multilingual neural machine translation (NMT) system. Using\nintermediate language translations as contextual cues, we evaluate their\neffectiveness in enhancing English and Chinese translations into Portuguese.\nResults suggest that contextual information significantly improves translation\nquality for domain-specific datasets and potentially for linguistically distant\nlanguage pairs, with diminishing returns observed in benchmarks with high\nlinguistic variability. Additionally, we demonstrate that shallow fusion, a\nmulti-source approach we apply within the NMT system, shows improved results\nwhen using high-resource languages as context for other translation pairs,\nhighlighting the importance of strategic context language selection."
                },
                "authors": [
                    {
                        "name": "Lia Shahnazaryan"
                    },
                    {
                        "name": "Patrick Simianer"
                    },
                    {
                        "name": "Joern Wuebker"
                    }
                ],
                "author_detail": {
                    "name": "Joern Wuebker"
                },
                "author": "Joern Wuebker",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12279v4",
                "updated": "2025-03-10T11:08:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    8,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2024-11-19T06:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "title": "HouseTune: Two-Stage Floorplan Generation with LLM Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HouseTune: Two-Stage Floorplan Generation with LLM Assistance"
                },
                "summary": "This paper proposes a two-stage text-to-floorplan generation framework that\ncombines the reasoning capability of Large Language Models (LLMs) with the\ngenerative power of diffusion models. In the first stage, we leverage a\nChain-of-Thought (CoT) prompting strategy to guide an LLM in generating an\ninitial layout (Layout-Init) from natural language descriptions, which ensures\na user-friendly and intuitive design process. However, Layout-Init may lack\nprecise geometric alignment and fine-grained structural details. To address\nthis, the second stage employs a conditional diffusion model to refine\nLayout-Init into a final floorplan (Layout-Final) that better adheres to\nphysical constraints and user requirements. Unlike prior methods, our approach\neffectively reduces the difficulty of floorplan generation learning without the\nneed for extensive domain-specific training data. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, which validates its effectiveness in practical home design\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a two-stage text-to-floorplan generation framework that\ncombines the reasoning capability of Large Language Models (LLMs) with the\ngenerative power of diffusion models. In the first stage, we leverage a\nChain-of-Thought (CoT) prompting strategy to guide an LLM in generating an\ninitial layout (Layout-Init) from natural language descriptions, which ensures\na user-friendly and intuitive design process. However, Layout-Init may lack\nprecise geometric alignment and fine-grained structural details. To address\nthis, the second stage employs a conditional diffusion model to refine\nLayout-Init into a final floorplan (Layout-Final) that better adheres to\nphysical constraints and user requirements. Unlike prior methods, our approach\neffectively reduces the difficulty of floorplan generation learning without the\nneed for extensive domain-specific training data. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, which validates its effectiveness in practical home design\napplications."
                },
                "authors": [
                    {
                        "name": "Ziyang Zong"
                    },
                    {
                        "name": "Guanying Chen"
                    },
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Fengcheng Yu"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11843v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11843v4",
                "updated": "2025-03-10T10:50:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    50,
                    44,
                    0,
                    69,
                    0
                ],
                "published": "2024-09-23T08:39:16Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    39,
                    16,
                    0,
                    267,
                    0
                ],
                "title": "From Commands to Prompts: LLM-based Semantic File System for AIOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Commands to Prompts: LLM-based Semantic File System for AIOS"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Chaoji Zuo"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Yujie Ren"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Dong Deng"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_journal_ref": "ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11843v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11843v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11995v2",
                "updated": "2025-03-10T10:48:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    48,
                    57,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-17T16:35:15Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    16,
                    35,
                    15,
                    0,
                    48,
                    0
                ],
                "title": "Presumed Cultural Identity: How Names Shape LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Presumed Cultural Identity: How Names Shape LLM Responses"
                },
                "summary": "Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation."
                },
                "authors": [
                    {
                        "name": "Siddhesh Pawar"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Lucie-Aimée Kaffee"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "23 Pages, 13 Figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07169v1",
                "updated": "2025-03-10T10:47:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    47,
                    27,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T10:47:27Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    47,
                    27,
                    0,
                    69,
                    0
                ],
                "title": "Reducing Friction in Cloud Migration of Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Friction in Cloud Migration of Services"
                },
                "summary": "Public cloud services are integral to modern software development, offering\nscalability and flexibility to organizations. Based on customer requests, a\nlarge-scale product development organization considered migrating the\nmicroservice-based product deployments of a large customer to a public cloud\nprovider.\n  We conducted an exploratory single-case study, utilizing quantitative and\nqualitative data analysis to understand how and why deployment costs would\nchange when transitioning the product from a private to a public cloud\nenvironment while preserving the software architecture. We also isolated the\nmajor factors driving the changes in deployment costs.\n  We found that switching to the customer-chosen public cloud provider would\nincrease costs by up to 50\\%, even when sharing some resources between\ndeployments, and limiting the use of expensive cloud services such as security\nlog analyzers. A large part of the cost was related to the sizing and license\ncosts of the existing relational database, which was running on Virtual\nMachines in the cloud. We also found that existing system integrators, using\nthe product via its API, were likely to use the product inefficiently, in many\ncases causing at least 10\\% more load to the system than needed.\n  From a deployment cost perspective, successful migration to a public cloud\nrequires considering the entire system architecture, including services like\nrelational databases, value-added cloud services, and enabled product features.\nOur study highlights the importance of leveraging end-to-end usage data to\nassess and manage these cost drivers effectively, especially in environments\nwith elastic costs, such as public cloud deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public cloud services are integral to modern software development, offering\nscalability and flexibility to organizations. Based on customer requests, a\nlarge-scale product development organization considered migrating the\nmicroservice-based product deployments of a large customer to a public cloud\nprovider.\n  We conducted an exploratory single-case study, utilizing quantitative and\nqualitative data analysis to understand how and why deployment costs would\nchange when transitioning the product from a private to a public cloud\nenvironment while preserving the software architecture. We also isolated the\nmajor factors driving the changes in deployment costs.\n  We found that switching to the customer-chosen public cloud provider would\nincrease costs by up to 50\\%, even when sharing some resources between\ndeployments, and limiting the use of expensive cloud services such as security\nlog analyzers. A large part of the cost was related to the sizing and license\ncosts of the existing relational database, which was running on Virtual\nMachines in the cloud. We also found that existing system integrators, using\nthe product via its API, were likely to use the product inefficiently, in many\ncases causing at least 10\\% more load to the system than needed.\n  From a deployment cost perspective, successful migration to a public cloud\nrequires considering the entire system architecture, including services like\nrelational databases, value-added cloud services, and enabled product features.\nOur study highlights the importance of leveraging end-to-end usage data to\nassess and manage these cost drivers effectively, especially in environments\nwith elastic costs, such as public cloud deployments."
                },
                "authors": [
                    {
                        "name": "Anders Sundelin"
                    },
                    {
                        "name": "Javier Gonzalez-Huerta"
                    },
                    {
                        "name": "Krzysztof Wnuk"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Wnuk"
                },
                "author": "Krzysztof Wnuk",
                "arxiv_comment": "Submitted to JSS In-Practice track Mars 8, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07144v1",
                "updated": "2025-03-10T10:20:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    20,
                    5,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T10:20:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    20,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "MRCEval: A Comprehensive, Challenging and Accessible Machine Reading\n  Comprehension Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRCEval: A Comprehensive, Challenging and Accessible Machine Reading\n  Comprehension Benchmark"
                },
                "summary": "Machine Reading Comprehension (MRC) is an essential task in evaluating\nnatural language understanding. Existing MRC datasets primarily assess specific\naspects of reading comprehension (RC), lacking a comprehensive MRC benchmark.\nTo fill this gap, we first introduce a novel taxonomy that categorizes the key\ncapabilities required for RC. Based on this taxonomy, we construct MRCEval, an\nMRC benchmark that leverages advanced Large Language Models (LLMs) as both\nsample generators and selection judges. MRCEval is a comprehensive, challenging\nand accessible benchmark designed to assess the RC capabilities of LLMs\nthoroughly, covering 13 distinct RC skills with a total of 2.1K high-quality\nmulti-choice questions. We perform an extensive evaluation of 28 widely used\nopen-source and proprietary models, highlighting that MRC continues to present\nsignificant challenges even in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Reading Comprehension (MRC) is an essential task in evaluating\nnatural language understanding. Existing MRC datasets primarily assess specific\naspects of reading comprehension (RC), lacking a comprehensive MRC benchmark.\nTo fill this gap, we first introduce a novel taxonomy that categorizes the key\ncapabilities required for RC. Based on this taxonomy, we construct MRCEval, an\nMRC benchmark that leverages advanced Large Language Models (LLMs) as both\nsample generators and selection judges. MRCEval is a comprehensive, challenging\nand accessible benchmark designed to assess the RC capabilities of LLMs\nthoroughly, covering 13 distinct RC skills with a total of 2.1K high-quality\nmulti-choice questions. We perform an extensive evaluation of 28 widely used\nopen-source and proprietary models, highlighting that MRC continues to present\nsignificant challenges even in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Ma"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07137v1",
                "updated": "2025-03-10T10:08:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    8,
                    55,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T10:08:55Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    8,
                    55,
                    0,
                    69,
                    0
                ],
                "title": "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and\n  Applications"
                },
                "summary": "Artificial intelligence (AI) has achieved astonishing successes in many\ndomains, especially with the recent breakthroughs in the development of\nfoundational large models. These large models, leveraging their extensive\ntraining data, provide versatile solutions for a wide range of downstream\ntasks. However, as modern datasets become increasingly diverse and complex, the\ndevelopment of large AI models faces two major challenges: (1) the enormous\nconsumption of computational resources and deployment difficulties, and (2) the\ndifficulty in fitting heterogeneous and complex data, which limits the\nusability of the models. Mixture of Experts (MoE) models has recently attracted\nmuch attention in addressing these challenges, by dynamically selecting and\nactivating the most relevant sub-models to process input data. It has been\nshown that MoEs can significantly improve model performance and efficiency with\nfewer resources, particularly excelling in handling large-scale, multimodal\ndata. Given the tremendous potential MoE has demonstrated across various\ndomains, it is urgent to provide a comprehensive summary of recent advancements\nof MoEs in many important fields. Existing surveys on MoE have their\nlimitations, e.g., being outdated or lacking discussion on certain key areas,\nand we aim to address these gaps. In this paper, we first introduce the basic\ndesign of MoE, including gating functions, expert networks, routing mechanisms,\ntraining strategies, and system design. We then explore the algorithm design of\nMoE in important machine learning paradigms such as continual learning,\nmeta-learning, multi-task learning, and reinforcement learning. Additionally,\nwe summarize theoretical studies aimed at understanding MoE and review its\napplications in computer vision and natural language processing. Finally, we\ndiscuss promising future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has achieved astonishing successes in many\ndomains, especially with the recent breakthroughs in the development of\nfoundational large models. These large models, leveraging their extensive\ntraining data, provide versatile solutions for a wide range of downstream\ntasks. However, as modern datasets become increasingly diverse and complex, the\ndevelopment of large AI models faces two major challenges: (1) the enormous\nconsumption of computational resources and deployment difficulties, and (2) the\ndifficulty in fitting heterogeneous and complex data, which limits the\nusability of the models. Mixture of Experts (MoE) models has recently attracted\nmuch attention in addressing these challenges, by dynamically selecting and\nactivating the most relevant sub-models to process input data. It has been\nshown that MoEs can significantly improve model performance and efficiency with\nfewer resources, particularly excelling in handling large-scale, multimodal\ndata. Given the tremendous potential MoE has demonstrated across various\ndomains, it is urgent to provide a comprehensive summary of recent advancements\nof MoEs in many important fields. Existing surveys on MoE have their\nlimitations, e.g., being outdated or lacking discussion on certain key areas,\nand we aim to address these gaps. In this paper, we first introduce the basic\ndesign of MoE, including gating functions, expert networks, routing mechanisms,\ntraining strategies, and system design. We then explore the algorithm design of\nMoE in important machine learning paradigms such as continual learning,\nmeta-learning, multi-task learning, and reinforcement learning. Additionally,\nwe summarize theoretical studies aimed at understanding MoE and review its\napplications in computer vision and natural language processing. Finally, we\ndiscuss promising future research directions."
                },
                "authors": [
                    {
                        "name": "Siyuan Mu"
                    },
                    {
                        "name": "Sen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Sen Lin"
                },
                "author": "Sen Lin",
                "arxiv_comment": "28 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03946v2",
                "updated": "2025-03-10T09:52:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    52,
                    25,
                    0,
                    69,
                    0
                ],
                "published": "2024-09-06T00:02:09Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    2,
                    9,
                    4,
                    250,
                    0
                ],
                "title": "On The Role of Prompt Construction In Enhancing Efficacy and Efficiency\n  of LLM-Based Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Role of Prompt Construction In Enhancing Efficacy and Efficiency\n  of LLM-Based Tabular Data Generation"
                },
                "summary": "LLM-based data generation for real-world tabular data can be challenged by\nthe lack of sufficient semantic context in feature names used to describe\ncolumns. We hypothesize that enriching prompts with domain-specific insights\ncan improve both the quality and efficiency of data generation. To test this\nhypothesis, we explore three prompt construction protocols: Expert-guided,\nLLM-guided, and Novel-Mapping. Through empirical studies with the recently\nproposed GReaT framework, we find that context-enriched prompts lead to\nsignificantly improved data generation quality and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based data generation for real-world tabular data can be challenged by\nthe lack of sufficient semantic context in feature names used to describe\ncolumns. We hypothesize that enriching prompts with domain-specific insights\ncan improve both the quality and efficiency of data generation. To test this\nhypothesis, we explore three prompt construction protocols: Expert-guided,\nLLM-guided, and Novel-Mapping. Through empirical studies with the recently\nproposed GReaT framework, we find that context-enriched prompts lead to\nsignificantly improved data generation quality and training efficiency."
                },
                "authors": [
                    {
                        "name": "Banooqa Banday"
                    },
                    {
                        "name": "Kowshik Thopalli"
                    },
                    {
                        "name": "Tanzima Z. Islam"
                    },
                    {
                        "name": "Jayaraman J. Thiagarajan"
                    }
                ],
                "author_detail": {
                    "name": "Jayaraman J. Thiagarajan"
                },
                "author": "Jayaraman J. Thiagarajan",
                "arxiv_comment": "Accepted to IEEE ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12827v3",
                "updated": "2025-03-10T09:51:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    51,
                    28,
                    0,
                    69,
                    0
                ],
                "published": "2024-04-19T12:04:32Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    4,
                    32,
                    4,
                    110,
                    0
                ],
                "title": "An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical\n  Trial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical\n  Trial Results"
                },
                "summary": "Adverse drug events (ADEs) are a major safety issue in clinical trials. Thus,\npredicting ADEs is key to developing safer medications and enhancing patient\noutcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel\nADE prediction in monopharmacy treatments. CT-ADE encompasses 2,497 drugs and\n168,984 drug-ADE pairs from clinical trial results, annotated using the MedDRA\nontology. Unlike existing resources, CT-ADE integrates treatment and target\npopulation data, enabling comparative analyses under varying conditions, such\nas dosage, administration route, and demographics. In addition, CT-ADE\nsystematically collects all ADEs in the study population, including positive\nand negative cases. To provide a baseline for ADE prediction performance using\nthe CT-ADE dataset, we conducted analyses using large language models (LLMs).\nThe best LLM achieved an F1-score of 56%, with models incorporating treatment\nand patient information outperforming by 21%-38% those relying solely on the\nchemical structure. These findings underscore the importance of contextual\ninformation in ADE prediction and establish CT-ADE as a robust resource for\nsafety risk assessment in pharmaceutical research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adverse drug events (ADEs) are a major safety issue in clinical trials. Thus,\npredicting ADEs is key to developing safer medications and enhancing patient\noutcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel\nADE prediction in monopharmacy treatments. CT-ADE encompasses 2,497 drugs and\n168,984 drug-ADE pairs from clinical trial results, annotated using the MedDRA\nontology. Unlike existing resources, CT-ADE integrates treatment and target\npopulation data, enabling comparative analyses under varying conditions, such\nas dosage, administration route, and demographics. In addition, CT-ADE\nsystematically collects all ADEs in the study population, including positive\nand negative cases. To provide a baseline for ADE prediction performance using\nthe CT-ADE dataset, we conducted analyses using large language models (LLMs).\nThe best LLM achieved an F1-score of 56%, with models incorporating treatment\nand patient information outperforming by 21%-38% those relying solely on the\nchemical structure. These findings underscore the importance of contextual\ninformation in ADE prediction and establish CT-ADE as a robust resource for\nsafety risk assessment in pharmaceutical research and development."
                },
                "authors": [
                    {
                        "name": "Anthony Yazdani"
                    },
                    {
                        "name": "Alban Bornet"
                    },
                    {
                        "name": "Philipp Khlebnikov"
                    },
                    {
                        "name": "Boya Zhang"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Poorya Amini"
                    },
                    {
                        "name": "Douglas Teodoro"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Teodoro"
                },
                "author": "Douglas Teodoro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03135v2",
                "updated": "2025-03-10T09:51:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    51,
                    5,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-05T03:15:38Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    3,
                    15,
                    38,
                    2,
                    64,
                    0
                ],
                "title": "Bridging Molecular Graphs and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Molecular Graphs and Large Language Models"
                },
                "summary": "While Large Language Models (LLMs) have shown exceptional generalization\ncapabilities, their ability to process graph data, such as molecular\nstructures, remains limited. To bridge this gap, this paper proposes\nGraph2Token, an efficient solution that aligns graph tokens to LLM tokens. The\nkey idea is to represent a graph token with the LLM token vocabulary, without\nfine-tuning the LLM backbone. To achieve this goal, we first construct a\nmolecule-text paired dataset from multisources, including CHEBI and HMDB, to\ntrain a graph structure encoder, which reduces the distance between graphs and\ntexts representations in the feature space. Then, we propose a novel alignment\nstrategy that associates a graph token with LLM tokens. To further unleash the\npotential of LLMs, we collect molecular IUPAC name identifiers, which are\nincorporated into the LLM prompts. By aligning molecular graphs as special\ntokens, we can activate LLM generalization ability to molecular few-shot\nlearning. Extensive experiments on molecular classification and regression\ntasks demonstrate the effectiveness of our proposed Graph2Token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown exceptional generalization\ncapabilities, their ability to process graph data, such as molecular\nstructures, remains limited. To bridge this gap, this paper proposes\nGraph2Token, an efficient solution that aligns graph tokens to LLM tokens. The\nkey idea is to represent a graph token with the LLM token vocabulary, without\nfine-tuning the LLM backbone. To achieve this goal, we first construct a\nmolecule-text paired dataset from multisources, including CHEBI and HMDB, to\ntrain a graph structure encoder, which reduces the distance between graphs and\ntexts representations in the feature space. Then, we propose a novel alignment\nstrategy that associates a graph token with LLM tokens. To further unleash the\npotential of LLMs, we collect molecular IUPAC name identifiers, which are\nincorporated into the LLM prompts. By aligning molecular graphs as special\ntokens, we can activate LLM generalization ability to molecular few-shot\nlearning. Extensive experiments on molecular classification and regression\ntasks demonstrate the effectiveness of our proposed Graph2Token."
                },
                "authors": [
                    {
                        "name": "Runze Wang"
                    },
                    {
                        "name": "Mingqi Yang"
                    },
                    {
                        "name": "Yanming Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yanming Shen"
                },
                "author": "Yanming Shen",
                "arxiv_comment": "AAAI 2025 camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14362v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14362v5",
                "updated": "2025-03-10T09:35:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    35,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2024-03-21T12:45:01Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    12,
                    45,
                    1,
                    3,
                    81,
                    0
                ],
                "title": "Enabling Generalized Zero-shot Learning Towards Unseen Domains by\n  Intrinsic Learning from Redundant LLM Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Generalized Zero-shot Learning Towards Unseen Domains by\n  Intrinsic Learning from Redundant LLM Semantics"
                },
                "summary": "Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we study cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods,\nCDGZSL constructs a common feature space across domains and acquires the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class similarity alignment, which eliminates the\nnon-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and unseen-class meta generation, which\npreserves intrinsic semantics to maintain connectivity between seen and unseen\nclasses by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\ntwo datasets, Office-Home and Mini-DomainNet, and we have shared the LLM-based\nsemantics for these datasets as a benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we study cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods,\nCDGZSL constructs a common feature space across domains and acquires the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class similarity alignment, which eliminates the\nnon-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and unseen-class meta generation, which\npreserves intrinsic semantics to maintain connectivity between seen and unseen\nclasses by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\ntwo datasets, Office-Home and Mini-DomainNet, and we have shared the LLM-based\nsemantics for these datasets as a benchmark."
                },
                "authors": [
                    {
                        "name": "Jiaqi Yue"
                    },
                    {
                        "name": "Chunhui Zhao"
                    },
                    {
                        "name": "Jiancheng Zhao"
                    },
                    {
                        "name": "Biao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Biao Huang"
                },
                "author": "Biao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14362v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14362v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17477v2",
                "updated": "2025-03-10T09:32:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    32,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2024-01-30T22:22:55Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    22,
                    22,
                    55,
                    1,
                    30,
                    0
                ],
                "title": "Detecting mental disorder on social media: a ChatGPT-augmented\n  explainable approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting mental disorder on social media: a ChatGPT-augmented\n  explainable approach"
                },
                "summary": "In the digital era, the prevalence of depressive symptoms expressed on social\nmedia has raised serious concerns, necessitating advanced methodologies for\ntimely detection. This paper addresses the challenge of interpretable\ndepression detection by proposing a novel methodology that effectively combines\nLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\nconversational agents like ChatGPT. In our methodology, explanations are\nachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\nnovel self-explanatory model, namely BERT-XDD, capable of providing both\nclassification and explanations via masked attention. The interpretability is\nfurther enhanced using ChatGPT to transform technical explanations into\nhuman-readable commentaries. By introducing an effective and modular approach\nfor interpretable depression detection, our methodology can contribute to the\ndevelopment of socially responsible digital platforms, fostering early\nintervention and support for mental health challenges under the guidance of\nqualified healthcare professionals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital era, the prevalence of depressive symptoms expressed on social\nmedia has raised serious concerns, necessitating advanced methodologies for\ntimely detection. This paper addresses the challenge of interpretable\ndepression detection by proposing a novel methodology that effectively combines\nLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\nconversational agents like ChatGPT. In our methodology, explanations are\nachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\nnovel self-explanatory model, namely BERT-XDD, capable of providing both\nclassification and explanations via masked attention. The interpretability is\nfurther enhanced using ChatGPT to transform technical explanations into\nhuman-readable commentaries. By introducing an effective and modular approach\nfor interpretable depression detection, our methodology can contribute to the\ndevelopment of socially responsible digital platforms, fostering early\nintervention and support for mental health challenges under the guidance of\nqualified healthcare professionals."
                },
                "authors": [
                    {
                        "name": "Loris Belcastro"
                    },
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Fabrizio Marozzo"
                    },
                    {
                        "name": "Domenico Talia"
                    },
                    {
                        "name": "Paolo Trunfio"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Trunfio"
                },
                "author": "Paolo Trunfio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04475v2",
                "updated": "2025-03-10T09:27:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    27,
                    3,
                    0,
                    69,
                    0
                ],
                "published": "2024-04-06T02:29:02Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    2,
                    29,
                    2,
                    5,
                    97,
                    0
                ],
                "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic\n  Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic\n  Evaluators"
                },
                "summary": "LLM-based auto-annotators have become a key component of the LLM development\nprocess due to their cost-effectiveness and scalability compared to human-based\nevaluation. However, these auto-annotators can introduce biases that are hard\nto remove. Even simple, known confounders such as preference for longer outputs\nremain in existing automated evaluation metrics. We propose a simple regression\nanalysis approach for controlling biases in auto-evaluations. As a real case\nstudy, we focus on reducing the length bias of AlpacaEval, a fast and\naffordable benchmark for instruction-tuned LLMs that uses LLMs to estimate\nresponse quality. Despite being highly correlated with human preferences,\nAlpacaEval is known to favor models that generate longer outputs. We introduce\na length-controlled AlpacaEval that aims to answer the counterfactual question:\n\"What would the preference be if the model's and baseline's output had the same\nlength?\" To achieve this, we first fit a generalized linear model to predict\nthe biased auto-annotator's preferences based on the mediators we want to\ncontrol for (length difference) and other relevant features. We then obtain\nlength-controlled preferences by predicting preferences while conditioning the\nGLM with a zero difference in lengths. Length-controlling not only improves the\nrobustness of the metric to manipulations in model verbosity, but we also find\nthat it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94\nto 0.98.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based auto-annotators have become a key component of the LLM development\nprocess due to their cost-effectiveness and scalability compared to human-based\nevaluation. However, these auto-annotators can introduce biases that are hard\nto remove. Even simple, known confounders such as preference for longer outputs\nremain in existing automated evaluation metrics. We propose a simple regression\nanalysis approach for controlling biases in auto-evaluations. As a real case\nstudy, we focus on reducing the length bias of AlpacaEval, a fast and\naffordable benchmark for instruction-tuned LLMs that uses LLMs to estimate\nresponse quality. Despite being highly correlated with human preferences,\nAlpacaEval is known to favor models that generate longer outputs. We introduce\na length-controlled AlpacaEval that aims to answer the counterfactual question:\n\"What would the preference be if the model's and baseline's output had the same\nlength?\" To achieve this, we first fit a generalized linear model to predict\nthe biased auto-annotator's preferences based on the mediators we want to\ncontrol for (length difference) and other relevant features. We then obtain\nlength-controlled preferences by predicting preferences while conditioning the\nGLM with a zero difference in lengths. Length-controlling not only improves the\nrobustness of the metric to manipulations in model verbosity, but we also find\nthat it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94\nto 0.98."
                },
                "authors": [
                    {
                        "name": "Yann Dubois"
                    },
                    {
                        "name": "Balázs Galambosi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori B. Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori B. Hashimoto"
                },
                "author": "Tatsunori B. Hashimoto",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07103v1",
                "updated": "2025-03-10T09:26:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    26,
                    8,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:26:08Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    26,
                    8,
                    0,
                    69,
                    0
                ],
                "title": "Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication"
                },
                "summary": "Large Language Models (LLMs) have shown an impressive capability in code\ngeneration and, specifically, to automatically implement requirements described\nin natural language. The LLM effectiveness generally increases with its size:\nThe higher the number of LLM's trainable parameters the better its ability to\nimplement code. However, when it comes to deploying LLM-based code generators,\nlarger LLMs pose significant challenges related to their memory (and,\nconsequently, carbon) footprint. A previous work by Wei et al. proposed to\nleverage quantization techniques to reduce the memory footprint of LLM-based\ncode generators without substantially degrading their effectiveness. In short,\nthey studied LLMs featuring up to 16B parameters, quantizing their precision\nfrom floating point 32 bits down to int 8 bits and showing their limited impact\non code generation performance. Given the fast pace at which LLM capabilities\nand quantization techniques are evolving, in this work we present a\ndifferentiated replication of the work by Wei et al. in which we consider (i)\non the one side, more recent and larger code-related LLMs, of up to 34B\nparameters; (ii) the latest advancements in model quantization techniques,\nwhich allow pushing the compression to the extreme quantization level of 2 bits\nper model parameter and; (iii) different types of calibration datasets to guide\nthe quantization process, including code-specific ones. Our empirical\nevaluation reveals that the new frontier for LLM quantization is 4-bit\nprecision, resulting in an average memory footprint reduction of 70% compared\nto the original model without observing any significant decrease in\nperformance. Additionally, when the quantization becomes even more extreme (3\nand 2 bits), a code-specific calibration dataset helps to limit the loss of\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown an impressive capability in code\ngeneration and, specifically, to automatically implement requirements described\nin natural language. The LLM effectiveness generally increases with its size:\nThe higher the number of LLM's trainable parameters the better its ability to\nimplement code. However, when it comes to deploying LLM-based code generators,\nlarger LLMs pose significant challenges related to their memory (and,\nconsequently, carbon) footprint. A previous work by Wei et al. proposed to\nleverage quantization techniques to reduce the memory footprint of LLM-based\ncode generators without substantially degrading their effectiveness. In short,\nthey studied LLMs featuring up to 16B parameters, quantizing their precision\nfrom floating point 32 bits down to int 8 bits and showing their limited impact\non code generation performance. Given the fast pace at which LLM capabilities\nand quantization techniques are evolving, in this work we present a\ndifferentiated replication of the work by Wei et al. in which we consider (i)\non the one side, more recent and larger code-related LLMs, of up to 34B\nparameters; (ii) the latest advancements in model quantization techniques,\nwhich allow pushing the compression to the extreme quantization level of 2 bits\nper model parameter and; (iii) different types of calibration datasets to guide\nthe quantization process, including code-specific ones. Our empirical\nevaluation reveals that the new frontier for LLM quantization is 4-bit\nprecision, resulting in an average memory footprint reduction of 70% compared\nto the original model without observing any significant decrease in\nperformance. Additionally, when the quantization becomes even more extreme (3\nand 2 bits), a code-specific calibration dataset helps to limit the loss of\nperformance."
                },
                "authors": [
                    {
                        "name": "Alessandro Giagnorio"
                    },
                    {
                        "name": "Antonio Mastropaolo"
                    },
                    {
                        "name": "Saima Afrin"
                    },
                    {
                        "name": "Massimiliano Di Penta"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07094v1",
                "updated": "2025-03-10T09:19:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    19,
                    55,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:19:55Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    19,
                    55,
                    0,
                    69,
                    0
                ],
                "title": "A Novel Ophthalmic Benchmark for Evaluating Multimodal Large Language\n  Models with Fundus Photographs and OCT Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Ophthalmic Benchmark for Evaluating Multimodal Large Language\n  Models with Fundus Photographs and OCT Images"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated remarkable\npotential across various medical applications. Building on this foundation,\nmultimodal large language models (MLLMs) integrate LLMs with visual models to\nprocess diverse inputs, including clinical data and medical images. In\nophthalmology, LLMs have been explored for analyzing optical coherence\ntomography (OCT) reports, assisting in disease classification, and even\npredicting treatment outcomes. However, existing MLLM benchmarks often fail to\ncapture the complexities of real-world clinical practice, particularly in the\nanalysis of OCT images. Many suffer from limitations such as small sample\nsizes, a lack of diverse OCT datasets, and insufficient expert validation.\nThese shortcomings hinder the accurate assessment of MLLMs' ability to\ninterpret OCT scans and their broader applicability in ophthalmology. Our\ndataset, curated through rigorous quality control and expert annotation,\nconsists of 439 fundus images and 75 OCT images. Using a standardized API-based\nframework, we assessed seven mainstream MLLMs and observed significant\nvariability in diagnostic accuracy across different diseases. While some models\nperformed well in diagnosing conditions such as diabetic retinopathy and\nage-related macular degeneration, they struggled with others, including\nchoroidal neovascularization and myopia, highlighting inconsistencies in\nperformance and the need for further refinement. Our findings emphasize the\nimportance of developing clinically relevant benchmarks to provide a more\naccurate assessment of MLLMs' capabilities. By refining these models and\nexpanding their scope, we can enhance their potential to transform ophthalmic\ndiagnosis and treatment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated remarkable\npotential across various medical applications. Building on this foundation,\nmultimodal large language models (MLLMs) integrate LLMs with visual models to\nprocess diverse inputs, including clinical data and medical images. In\nophthalmology, LLMs have been explored for analyzing optical coherence\ntomography (OCT) reports, assisting in disease classification, and even\npredicting treatment outcomes. However, existing MLLM benchmarks often fail to\ncapture the complexities of real-world clinical practice, particularly in the\nanalysis of OCT images. Many suffer from limitations such as small sample\nsizes, a lack of diverse OCT datasets, and insufficient expert validation.\nThese shortcomings hinder the accurate assessment of MLLMs' ability to\ninterpret OCT scans and their broader applicability in ophthalmology. Our\ndataset, curated through rigorous quality control and expert annotation,\nconsists of 439 fundus images and 75 OCT images. Using a standardized API-based\nframework, we assessed seven mainstream MLLMs and observed significant\nvariability in diagnostic accuracy across different diseases. While some models\nperformed well in diagnosing conditions such as diabetic retinopathy and\nage-related macular degeneration, they struggled with others, including\nchoroidal neovascularization and myopia, highlighting inconsistencies in\nperformance and the need for further refinement. Our findings emphasize the\nimportance of developing clinically relevant benchmarks to provide a more\naccurate assessment of MLLMs' capabilities. By refining these models and\nexpanding their scope, we can enhance their potential to transform ophthalmic\ndiagnosis and treatment."
                },
                "authors": [
                    {
                        "name": "Xiaoyi Liang"
                    },
                    {
                        "name": "Mouxiao Bian"
                    },
                    {
                        "name": "Moxin Chen"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Lin Li"
                    }
                ],
                "author_detail": {
                    "name": "Lin Li"
                },
                "author": "Lin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07078v1",
                "updated": "2025-03-10T09:00:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    0,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:00:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    0,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Linguistic Knowledge Transfer Learning for Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Knowledge Transfer Learning for Speech Enhancement"
                },
                "summary": "Linguistic knowledge plays a crucial role in spoken language comprehension.\nIt provides essential semantic and syntactic context for speech perception in\nnoisy environments. However, most speech enhancement (SE) methods predominantly\nrely on acoustic features to learn the mapping relationship between noisy and\nclean speech, with limited exploration of linguistic integration. While\ntext-informed SE approaches have been investigated, they often require explicit\nspeech-text alignment or externally provided textual data, constraining their\npracticality in real-world scenarios. Additionally, using text as input poses\nchallenges in aligning linguistic and acoustic representations due to their\ninherent differences. In this study, we propose the Cross-Modality Knowledge\nTransfer (CMKT) learning framework, which leverages pre-trained large language\nmodels (LLMs) to infuse linguistic knowledge into SE models without requiring\ntext input or LLMs during inference. Furthermore, we introduce a misalignment\nstrategy to improve knowledge transfer. This strategy applies controlled\ntemporal shifts, encouraging the model to learn more robust representations.\nExperimental evaluations demonstrate that CMKT consistently outperforms\nbaseline models across various SE architectures and LLM embeddings,\nhighlighting its adaptability to different configurations. Additionally,\nresults on Mandarin and English datasets confirm its effectiveness across\ndiverse linguistic conditions, further validating its robustness. Moreover,\nCMKT remains effective even in scenarios without textual data, underscoring its\npracticality for real-world applications. By bridging the gap between\nlinguistic and acoustic modalities, CMKT offers a scalable and innovative\nsolution for integrating linguistic knowledge into SE models, leading to\nsubstantial improvements in both intelligibility and enhancement performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic knowledge plays a crucial role in spoken language comprehension.\nIt provides essential semantic and syntactic context for speech perception in\nnoisy environments. However, most speech enhancement (SE) methods predominantly\nrely on acoustic features to learn the mapping relationship between noisy and\nclean speech, with limited exploration of linguistic integration. While\ntext-informed SE approaches have been investigated, they often require explicit\nspeech-text alignment or externally provided textual data, constraining their\npracticality in real-world scenarios. Additionally, using text as input poses\nchallenges in aligning linguistic and acoustic representations due to their\ninherent differences. In this study, we propose the Cross-Modality Knowledge\nTransfer (CMKT) learning framework, which leverages pre-trained large language\nmodels (LLMs) to infuse linguistic knowledge into SE models without requiring\ntext input or LLMs during inference. Furthermore, we introduce a misalignment\nstrategy to improve knowledge transfer. This strategy applies controlled\ntemporal shifts, encouraging the model to learn more robust representations.\nExperimental evaluations demonstrate that CMKT consistently outperforms\nbaseline models across various SE architectures and LLM embeddings,\nhighlighting its adaptability to different configurations. Additionally,\nresults on Mandarin and English datasets confirm its effectiveness across\ndiverse linguistic conditions, further validating its robustness. Moreover,\nCMKT remains effective even in scenarios without textual data, underscoring its\npracticality for real-world applications. By bridging the gap between\nlinguistic and acoustic modalities, CMKT offers a scalable and innovative\nsolution for integrating linguistic knowledge into SE models, leading to\nsubstantial improvements in both intelligibility and enhancement performance."
                },
                "authors": [
                    {
                        "name": "Kuo-Hsuan Hung"
                    },
                    {
                        "name": "Xugang Lu"
                    },
                    {
                        "name": "Szu-Wei Fu"
                    },
                    {
                        "name": "Huan-Hsin Tseng"
                    },
                    {
                        "name": "Hsin-Yi Lin"
                    },
                    {
                        "name": "Chii-Wann Lin"
                    },
                    {
                        "name": "Yu Tsao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tsao"
                },
                "author": "Yu Tsao",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07070v1",
                "updated": "2025-03-10T08:53:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    53,
                    11,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:53:11Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    53,
                    11,
                    0,
                    69,
                    0
                ],
                "title": "PIED: Physics-Informed Experimental Design for Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIED: Physics-Informed Experimental Design for Inverse Problems"
                },
                "summary": "In many science and engineering settings, system dynamics are characterized\nby governing PDEs, and a major challenge is to solve inverse problems (IPs)\nwhere unknown PDE parameters are inferred based on observational data gathered\nunder limited budget. Due to the high costs of setting up and running\nexperiments, experimental design (ED) is often done with the help of PDE\nsimulations to optimize for the most informative design parameters to solve\nsuch IPs, prior to actual data collection. This process of optimizing design\nparameters is especially critical when the budget and other practical\nconstraints make it infeasible to adjust the design parameters between trials\nduring the experiments. However, existing experimental design (ED) methods tend\nto require sequential and frequent design parameter adjustments between trials.\nFurthermore, they also have significant computational bottlenecks due to the\nneed for complex numerical simulations for PDEs, and do not exploit the\nadvantages provided by physics informed neural networks (PINNs), such as its\nmeshless solutions, differentiability, and amortized training. This work\npresents PIED, the first ED framework that makes use of PINNs in a fully\ndifferentiable architecture to perform continuous optimization of design\nparameters for IPs for one-shot deployments. PIED overcomes existing methods'\ncomputational bottlenecks through parallelized computation and meta-learning of\nPINN parameter initialization, and proposes novel methods to effectively take\ninto account PINN training dynamics in optimizing the ED parameters. Through\nexperiments based on noisy simulated data and even real world experimental\ndata, we empirically show that given limited observation budget, PIED\nsignificantly outperforms existing ED methods in solving IPs, including\nchallenging settings where the inverse parameters are unknown functions rather\nthan just finite-dimensional.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many science and engineering settings, system dynamics are characterized\nby governing PDEs, and a major challenge is to solve inverse problems (IPs)\nwhere unknown PDE parameters are inferred based on observational data gathered\nunder limited budget. Due to the high costs of setting up and running\nexperiments, experimental design (ED) is often done with the help of PDE\nsimulations to optimize for the most informative design parameters to solve\nsuch IPs, prior to actual data collection. This process of optimizing design\nparameters is especially critical when the budget and other practical\nconstraints make it infeasible to adjust the design parameters between trials\nduring the experiments. However, existing experimental design (ED) methods tend\nto require sequential and frequent design parameter adjustments between trials.\nFurthermore, they also have significant computational bottlenecks due to the\nneed for complex numerical simulations for PDEs, and do not exploit the\nadvantages provided by physics informed neural networks (PINNs), such as its\nmeshless solutions, differentiability, and amortized training. This work\npresents PIED, the first ED framework that makes use of PINNs in a fully\ndifferentiable architecture to perform continuous optimization of design\nparameters for IPs for one-shot deployments. PIED overcomes existing methods'\ncomputational bottlenecks through parallelized computation and meta-learning of\nPINN parameter initialization, and proposes novel methods to effectively take\ninto account PINN training dynamics in optimizing the ED parameters. Through\nexperiments based on noisy simulated data and even real world experimental\ndata, we empirically show that given limited observation budget, PIED\nsignificantly outperforms existing ED methods in solving IPs, including\nchallenging settings where the inverse parameters are unknown functions rather\nthan just finite-dimensional."
                },
                "authors": [
                    {
                        "name": "Apivich Hemachandra"
                    },
                    {
                        "name": "Gregory Kang Ruey Lau"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "Accepted to 13th International Conference on Learning Representations\n  (ICLR 2025), 31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07067v1",
                "updated": "2025-03-10T08:51:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    51,
                    32,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:51:32Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    51,
                    32,
                    0,
                    69,
                    0
                ],
                "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs"
                },
                "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types."
                },
                "authors": [
                    {
                        "name": "Jongwoo Ko"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Tianyu Ding"
                    },
                    {
                        "name": "Luming Liang"
                    },
                    {
                        "name": "Ilya Zharkov"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "The code will be available soon at\n  https://github.com/jongwooko/distillm-2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13432v3",
                "updated": "2025-03-10T08:49:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    49,
                    0,
                    0,
                    69,
                    0
                ],
                "published": "2024-12-18T02:07:21Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    2,
                    7,
                    21,
                    2,
                    353,
                    0
                ],
                "title": "Large Language Model Enhanced Recommender Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enhanced Recommender Systems: A Survey"
                },
                "summary": "Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has transformative potential in various domains,\nincluding recommender systems (RS). There have been a handful of research that\nfocuses on empowering the RS by LLM. However, previous efforts mainly focus on\nLLM as RS, which may face the challenge of intolerant inference costs by LLM.\nRecently, the integration of LLM into RS, known as LLM-Enhanced Recommender\nSystems (LLMERS), has garnered significant interest due to its potential to\naddress latency and memory constraints in real-world applications. This paper\npresents a comprehensive survey of the latest research efforts aimed at\nleveraging LLM to enhance RS capabilities. We identify a critical shift in the\nfield with the move towards incorporating LLM into the online system, notably\nby avoiding their use during inference. Our survey categorizes the existing\nLLMERS approaches into three primary types based on the component of the RS\nmodel being augmented: Knowledge Enhancement, Interaction Enhancement, and\nModel Enhancement. We provide an in-depth analysis of each category, discussing\nthe methodologies, challenges, and contributions of recent studies.\nFurthermore, we highlight several promising research directions that could\nfurther advance the field of LLMERS."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Feng Tian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Tian"
                },
                "author": "Feng Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07065v1",
                "updated": "2025-03-10T08:48:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    48,
                    50,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:48:50Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    48,
                    50,
                    0,
                    69,
                    0
                ],
                "title": "Boosting the Generalization and Reasoning of Vision Language Models with\n  Curriculum Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting the Generalization and Reasoning of Vision Language Models with\n  Curriculum Reinforcement Learning"
                },
                "summary": "While state-of-the-art vision-language models (VLMs) have demonstrated\nremarkable capabilities in complex visual-text tasks, their success heavily\nrelies on massive model scaling, limiting their practical deployment.\nSmall-scale VLMs offer a more practical alternative but face significant\nchallenges when trained with traditional supervised fine-tuning (SFT),\nparticularly in two aspects: out-of-domain (OOD) generalization and reasoning\nabilities, which significantly lags behind the contemporary Large language\nmodels (LLMs). To address these challenges, we propose Curriculum Reinforcement\nFinetuning (Curr-ReFT), a novel post-training paradigm specifically designed\nfor small-scale VLMs. Inspired by the success of reinforcement learning in\nLLMs, Curr-ReFT comprises two sequential stages: (1) Curriculum Reinforcement\nLearning, which ensures steady progression of model capabilities through\ndifficulty-aware reward design, transitioning from basic visual perception to\ncomplex reasoning tasks; and (2) Rejected Sampling-based Self-improvement,\nwhich maintains the fundamental capabilities of VLMs through selective learning\nfrom high-quality multimodal and language examples. Extensive experiments\ndemonstrate that models trained with Curr-ReFT paradigm achieve\nstate-of-the-art performance across various visual tasks in both in-domain and\nout-of-domain settings. Moreover, our Curr-ReFT enhanced 3B model matches the\nperformance of 32B-parameter models, demonstrating that efficient training\nparadigms can effectively bridge the gap between small and large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While state-of-the-art vision-language models (VLMs) have demonstrated\nremarkable capabilities in complex visual-text tasks, their success heavily\nrelies on massive model scaling, limiting their practical deployment.\nSmall-scale VLMs offer a more practical alternative but face significant\nchallenges when trained with traditional supervised fine-tuning (SFT),\nparticularly in two aspects: out-of-domain (OOD) generalization and reasoning\nabilities, which significantly lags behind the contemporary Large language\nmodels (LLMs). To address these challenges, we propose Curriculum Reinforcement\nFinetuning (Curr-ReFT), a novel post-training paradigm specifically designed\nfor small-scale VLMs. Inspired by the success of reinforcement learning in\nLLMs, Curr-ReFT comprises two sequential stages: (1) Curriculum Reinforcement\nLearning, which ensures steady progression of model capabilities through\ndifficulty-aware reward design, transitioning from basic visual perception to\ncomplex reasoning tasks; and (2) Rejected Sampling-based Self-improvement,\nwhich maintains the fundamental capabilities of VLMs through selective learning\nfrom high-quality multimodal and language examples. Extensive experiments\ndemonstrate that models trained with Curr-ReFT paradigm achieve\nstate-of-the-art performance across various visual tasks in both in-domain and\nout-of-domain settings. Moreover, our Curr-ReFT enhanced 3B model matches the\nperformance of 32B-parameter models, demonstrating that efficient training\nparadigms can effectively bridge the gap between small and large models."
                },
                "authors": [
                    {
                        "name": "Huilin Deng"
                    },
                    {
                        "name": "Ding Zou"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Hongchen Luo"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Yu Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Kang"
                },
                "author": "Yu Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04832v2",
                "updated": "2025-03-10T08:47:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    47,
                    3,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-05T10:59:32Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    59,
                    32,
                    2,
                    64,
                    0
                ],
                "title": "RD Efficient FPGA Deployment of Learned Image Compression: Knowledge\n  Distillation and Hybrid Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RD Efficient FPGA Deployment of Learned Image Compression: Knowledge\n  Distillation and Hybrid Quantization"
                },
                "summary": "Learnable Image Compression (LIC) has shown the potential to outperform\nstandardized video codecs in RD efficiency, prompting the research for\nhardware-friendly implementations. Most existing LIC hardware implementations\nprioritize latency to RD-efficiency and through an extensive exploration of the\nhardware design space. We present a novel design paradigm where the burden of\ntuning the design for a specific hardware platform is shifted towards model\ndimensioning and without compromising on RD-efficiency. First, we design a\nframework for distilling a leaner student LIC model from a reference teacher:\nby tuning a single model hyperparameters, we can meet the constraints of\ndifferent hardware platforms without a complex hardware design exploration.\nSecond, we propose a hardware-friendly implementation of the Generalized\nDivisive Normalization (GDN) activation that preserves RD efficiency even post\nparameter quantization. Third, we design a pipelined FPGA configuration which\ntakes full advantage of available FPGA resources by leveraging parallel\nprocessing and optimizing resource allocation. Our experiments with a state of\nthe art LIC model show that we outperform all existing FPGA implementations\nwhile performing very close to the original model in terms of RD efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnable Image Compression (LIC) has shown the potential to outperform\nstandardized video codecs in RD efficiency, prompting the research for\nhardware-friendly implementations. Most existing LIC hardware implementations\nprioritize latency to RD-efficiency and through an extensive exploration of the\nhardware design space. We present a novel design paradigm where the burden of\ntuning the design for a specific hardware platform is shifted towards model\ndimensioning and without compromising on RD-efficiency. First, we design a\nframework for distilling a leaner student LIC model from a reference teacher:\nby tuning a single model hyperparameters, we can meet the constraints of\ndifferent hardware platforms without a complex hardware design exploration.\nSecond, we propose a hardware-friendly implementation of the Generalized\nDivisive Normalization (GDN) activation that preserves RD efficiency even post\nparameter quantization. Third, we design a pipelined FPGA configuration which\ntakes full advantage of available FPGA resources by leveraging parallel\nprocessing and optimizing resource allocation. Our experiments with a state of\nthe art LIC model show that we outperform all existing FPGA implementations\nwhile performing very close to the original model in terms of RD efficiency."
                },
                "authors": [
                    {
                        "name": "Alaa Mazouz"
                    },
                    {
                        "name": "Sumanta Chaudhuri"
                    },
                    {
                        "name": "Marco Cagnanzzo"
                    },
                    {
                        "name": "Mihai Mitrea"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    },
                    {
                        "name": "Attilio Fiandrotti"
                    }
                ],
                "author_detail": {
                    "name": "Attilio Fiandrotti"
                },
                "author": "Attilio Fiandrotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09307v2",
                "updated": "2025-03-10T08:46:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    46,
                    11,
                    0,
                    69,
                    0
                ],
                "published": "2025-01-16T05:40:37Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    40,
                    37,
                    3,
                    16,
                    0
                ],
                "title": "RoboReflect: A Robotic Reflective Reasoning Framework for Grasping\n  Ambiguous-Condition Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboReflect: A Robotic Reflective Reasoning Framework for Grasping\n  Ambiguous-Condition Objects"
                },
                "summary": "As robotic technology rapidly develops, robots are being employed in an\nincreasing number of fields. However, due to the complexity of deployment\nenvironments or the prevalence of ambiguous-condition objects, the practical\napplication of robotics still faces many challenges, leading to frequent\nerrors. Traditional methods and some LLM-based approaches, although improved,\nstill require substantial human intervention and struggle with autonomous error\ncorrection in complex scenarios. In this work, we propose RoboReflect, a novel\nframework leveraging large vision-language models (LVLMs) to enable\nself-reflection and autonomous error correction in robotic grasping tasks.\nRoboReflect allows robots to automatically adjust their strategies based on\nunsuccessful attempts until successful execution is achieved. The corrected\nstrategies are saved in the memory for future task reference. We evaluate\nRoboReflect through extensive testing on eight common objects prone to\nambiguous conditions of three categories. Our results demonstrate that\nRoboReflect not only outperforms existing grasp pose estimation methods like\nAnyGrasp and high-level action planning techniques ReKep with GPT-4V but also\nsignificantly enhances the robot's capability to adapt and correct errors\nindependently. These findings underscore the critical importance of autonomous\nself-reflection in robotic systems while effectively addressing the challenges\nposed by ambiguous-condition environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robotic technology rapidly develops, robots are being employed in an\nincreasing number of fields. However, due to the complexity of deployment\nenvironments or the prevalence of ambiguous-condition objects, the practical\napplication of robotics still faces many challenges, leading to frequent\nerrors. Traditional methods and some LLM-based approaches, although improved,\nstill require substantial human intervention and struggle with autonomous error\ncorrection in complex scenarios. In this work, we propose RoboReflect, a novel\nframework leveraging large vision-language models (LVLMs) to enable\nself-reflection and autonomous error correction in robotic grasping tasks.\nRoboReflect allows robots to automatically adjust their strategies based on\nunsuccessful attempts until successful execution is achieved. The corrected\nstrategies are saved in the memory for future task reference. We evaluate\nRoboReflect through extensive testing on eight common objects prone to\nambiguous conditions of three categories. Our results demonstrate that\nRoboReflect not only outperforms existing grasp pose estimation methods like\nAnyGrasp and high-level action planning techniques ReKep with GPT-4V but also\nsignificantly enhances the robot's capability to adapt and correct errors\nindependently. These findings underscore the critical importance of autonomous\nself-reflection in robotic systems while effectively addressing the challenges\nposed by ambiguous-condition environments."
                },
                "authors": [
                    {
                        "name": "Zhen Luo"
                    },
                    {
                        "name": "Yixuan Yang"
                    },
                    {
                        "name": "Yanfu Zhang"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07058v1",
                "updated": "2025-03-10T08:43:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    43,
                    36,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:43:36Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    43,
                    36,
                    0,
                    69,
                    0
                ],
                "title": "Breaking the Limits of Quantization-Aware Defenses: QADT-R for\n  Robustness Against Patch-Based Adversarial Attacks in QNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Limits of Quantization-Aware Defenses: QADT-R for\n  Robustness Against Patch-Based Adversarial Attacks in QNNs"
                },
                "summary": "Quantized Neural Networks (QNNs) have emerged as a promising solution for\nreducing model size and computational costs, making them well-suited for\ndeployment in edge and resource-constrained environments. While quantization is\nknown to disrupt gradient propagation and enhance robustness against\npixel-level adversarial attacks, its effectiveness against patch-based\nadversarial attacks remains largely unexplored. In this work, we demonstrate\nthat adversarial patches remain highly transferable across quantized models,\nachieving over 70\\% attack success rates (ASR) even at extreme bit-width\nreductions (e.g., 2-bit). This challenges the common assumption that\nquantization inherently mitigates adversarial threats. To address this, we\npropose Quantization-Aware Defense Training with Randomization (QADT-R), a\nnovel defense strategy that integrates Adaptive Quantization-Aware Patch\nGeneration (A-QAPA), Dynamic Bit-Width Training (DBWT), and\nGradient-Inconsistent Regularization (GIR) to enhance resilience against highly\ntransferable patch-based attacks. A-QAPA generates adversarial patches within\nquantized models, ensuring robustness across different bit-widths. DBWT\nintroduces bit-width cycling during training to prevent overfitting to a\nspecific quantization setting, while GIR injects controlled gradient\nperturbations to disrupt adversarial optimization. Extensive evaluations on\nCIFAR-10 and ImageNet show that QADT-R reduces ASR by up to 25\\% compared to\nprior defenses such as PBAT and DWQ. Our findings further reveal that\nPBAT-trained models, while effective against seen patch configurations, fail to\ngeneralize to unseen patches due to quantization shift. Additionally, our\nempirical analysis of gradient alignment, spatial sensitivity, and patch\nvisibility provides insights into the mechanisms that contribute to the high\ntransferability of patch-based attacks in QNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized Neural Networks (QNNs) have emerged as a promising solution for\nreducing model size and computational costs, making them well-suited for\ndeployment in edge and resource-constrained environments. While quantization is\nknown to disrupt gradient propagation and enhance robustness against\npixel-level adversarial attacks, its effectiveness against patch-based\nadversarial attacks remains largely unexplored. In this work, we demonstrate\nthat adversarial patches remain highly transferable across quantized models,\nachieving over 70\\% attack success rates (ASR) even at extreme bit-width\nreductions (e.g., 2-bit). This challenges the common assumption that\nquantization inherently mitigates adversarial threats. To address this, we\npropose Quantization-Aware Defense Training with Randomization (QADT-R), a\nnovel defense strategy that integrates Adaptive Quantization-Aware Patch\nGeneration (A-QAPA), Dynamic Bit-Width Training (DBWT), and\nGradient-Inconsistent Regularization (GIR) to enhance resilience against highly\ntransferable patch-based attacks. A-QAPA generates adversarial patches within\nquantized models, ensuring robustness across different bit-widths. DBWT\nintroduces bit-width cycling during training to prevent overfitting to a\nspecific quantization setting, while GIR injects controlled gradient\nperturbations to disrupt adversarial optimization. Extensive evaluations on\nCIFAR-10 and ImageNet show that QADT-R reduces ASR by up to 25\\% compared to\nprior defenses such as PBAT and DWQ. Our findings further reveal that\nPBAT-trained models, while effective against seen patch configurations, fail to\ngeneralize to unseen patches due to quantization shift. Additionally, our\nempirical analysis of gradient alignment, spatial sensitivity, and patch\nvisibility provides insights into the mechanisms that contribute to the high\ntransferability of patch-based attacks in QNNs."
                },
                "authors": [
                    {
                        "name": "Amira Guesmi"
                    },
                    {
                        "name": "Bassem Ouni"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07044v1",
                "updated": "2025-03-10T08:32:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    32,
                    33,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:32:33Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    32,
                    33,
                    0,
                    69,
                    0
                ],
                "title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data\n  Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data\n  Science"
                },
                "summary": "Data Science tasks are multifaceted, dynamic, and often domain-specific.\nExisting LLM-based approaches largely concentrate on isolated phases,\nneglecting the interdependent nature of many data science tasks and limiting\ntheir capacity for comprehensive end-to-end support. We propose DatawiseAgent,\na notebook-centric LLM agent framework that unifies interactions among user,\nagent and the computational environment through markdown and executable code\ncells, supporting flexible and adaptive automated data science. Built on a\nFinite State Transducer(FST), DatawiseAgent orchestrates four stages, including\nDSF-like planning, incremental execution, self-debugging, and post-filtering.\nSpecifically, the DFS-like planning stage systematically explores the solution\nspace, while incremental execution harnesses real-time feedback and\naccommodates LLM's limited capabilities to progressively complete tasks. The\nself-debugging and post-filtering modules further enhance reliability by\ndiagnosing and correcting errors and pruning extraneous information. Extensive\nexperiments on diverse tasks, including data analysis, visualization, and data\nmodeling, show that DatawiseAgent consistently outperforms or matches\nstate-of-the-art methods across multiple model settings. These results\nhighlight its potential to generalize across data science scenarios and lay the\ngroundwork for more efficient, fully automated workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Science tasks are multifaceted, dynamic, and often domain-specific.\nExisting LLM-based approaches largely concentrate on isolated phases,\nneglecting the interdependent nature of many data science tasks and limiting\ntheir capacity for comprehensive end-to-end support. We propose DatawiseAgent,\na notebook-centric LLM agent framework that unifies interactions among user,\nagent and the computational environment through markdown and executable code\ncells, supporting flexible and adaptive automated data science. Built on a\nFinite State Transducer(FST), DatawiseAgent orchestrates four stages, including\nDSF-like planning, incremental execution, self-debugging, and post-filtering.\nSpecifically, the DFS-like planning stage systematically explores the solution\nspace, while incremental execution harnesses real-time feedback and\naccommodates LLM's limited capabilities to progressively complete tasks. The\nself-debugging and post-filtering modules further enhance reliability by\ndiagnosing and correcting errors and pruning extraneous information. Extensive\nexperiments on diverse tasks, including data analysis, visualization, and data\nmodeling, show that DatawiseAgent consistently outperforms or matches\nstate-of-the-art methods across multiple model settings. These results\nhighlight its potential to generalize across data science scenarios and lay the\ngroundwork for more efficient, fully automated workflows."
                },
                "authors": [
                    {
                        "name": "Ziming You"
                    },
                    {
                        "name": "Yumiao Zhang"
                    },
                    {
                        "name": "Dexuan Xu"
                    },
                    {
                        "name": "Yiwei Lou"
                    },
                    {
                        "name": "Yandong Yan"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Huaming Zhang"
                    },
                    {
                        "name": "Yu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Huang"
                },
                "author": "Yu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09108v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09108v3",
                "updated": "2025-03-10T08:30:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    30,
                    52,
                    0,
                    69,
                    0
                ],
                "published": "2024-08-17T06:23:38Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    6,
                    23,
                    38,
                    5,
                    230,
                    0
                ],
                "title": "Temporal Reversal Regularization for Spiking Neural Networks: Hybrid\n  Spatio-Temporal Invariance for Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Reversal Regularization for Spiking Neural Networks: Hybrid\n  Spatio-Temporal Invariance for Generalization"
                },
                "summary": "Spiking neural networks (SNNs) have received widespread attention as an\nultra-low power computing paradigm. Recent studies have shown that SNNs suffer\nfrom severe overfitting, which limits their generalization performance. In this\npaper, we propose a simple yet effective Temporal Reversal Regularization (TRR)\nto mitigate overfitting during training and facilitate generalization of SNNs.\nWe exploit the inherent temporal properties of SNNs to perform input/feature\ntemporal reversal perturbations, prompting the SNN to produce original-reversed\nconsistent outputs and learn perturbation-invariant representations. To further\nenhance generalization, we utilize the lightweight ``star operation\" (Hadamard\nproduct) for feature hybridization of original and temporally reversed spike\nfiring rates, which expands the implicit dimensionality and acts as a\nspatio-temporal regularizer. We show theoretically that our method is able to\ntighten the upper bound of the generalization error, and extensive experiments\non static/neuromorphic recognition as well as 3D point cloud classification\ntasks demonstrate its effectiveness, versatility, and adversarial robustness.\nIn particular, our regularization significantly improves the recognition\naccuracy of low-latency SNN for neuromorphic objects, contributing to the\nreal-world deployment of neuromorphic computational software-hardware\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks (SNNs) have received widespread attention as an\nultra-low power computing paradigm. Recent studies have shown that SNNs suffer\nfrom severe overfitting, which limits their generalization performance. In this\npaper, we propose a simple yet effective Temporal Reversal Regularization (TRR)\nto mitigate overfitting during training and facilitate generalization of SNNs.\nWe exploit the inherent temporal properties of SNNs to perform input/feature\ntemporal reversal perturbations, prompting the SNN to produce original-reversed\nconsistent outputs and learn perturbation-invariant representations. To further\nenhance generalization, we utilize the lightweight ``star operation\" (Hadamard\nproduct) for feature hybridization of original and temporally reversed spike\nfiring rates, which expands the implicit dimensionality and acts as a\nspatio-temporal regularizer. We show theoretically that our method is able to\ntighten the upper bound of the generalization error, and extensive experiments\non static/neuromorphic recognition as well as 3D point cloud classification\ntasks demonstrate its effectiveness, versatility, and adversarial robustness.\nIn particular, our regularization significantly improves the recognition\naccuracy of low-latency SNN for neuromorphic objects, contributing to the\nreal-world deployment of neuromorphic computational software-hardware\nintegration."
                },
                "authors": [
                    {
                        "name": "Lin Zuo"
                    },
                    {
                        "name": "Yongqi Ding"
                    },
                    {
                        "name": "Wenwei Luo"
                    },
                    {
                        "name": "Mengmeng Jing"
                    },
                    {
                        "name": "Kunshan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kunshan Yang"
                },
                "author": "Kunshan Yang",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09108v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07041v1",
                "updated": "2025-03-10T08:29:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    29,
                    15,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:29:15Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    29,
                    15,
                    0,
                    69,
                    0
                ],
                "title": "TCM-3CEval: A Triaxial Benchmark for Assessing Responses from Large\n  Language Models in Traditional Chinese Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TCM-3CEval: A Triaxial Benchmark for Assessing Responses from Large\n  Language Models in Traditional Chinese Medicine"
                },
                "summary": "Large language models (LLMs) excel in various NLP tasks and modern medicine,\nbut their evaluation in traditional Chinese medicine (TCM) is underexplored. To\naddress this, we introduce TCM3CEval, a benchmark assessing LLMs in TCM across\nthree dimensions: core knowledge mastery, classical text understanding, and\nclinical decision-making. We evaluate diverse models, including international\n(e.g., GPT-4o), Chinese (e.g., InternLM), and medical-specific (e.g., PLUSE).\nResults show a performance hierarchy: all models have limitations in\nspecialized subdomains like Meridian & Acupoint theory and Various TCM Schools,\nrevealing gaps between current capabilities and clinical needs. Models with\nChinese linguistic and cultural priors perform better in classical text\ninterpretation and clinical reasoning. TCM-3CEval sets a standard for AI\nevaluation in TCM, offering insights for optimizing LLMs in culturally grounded\nmedical domains. The benchmark is available on Medbench's TCM track, aiming to\nassess LLMs' TCM capabilities in basic knowledge, classic texts, and clinical\ndecision-making through multidimensional questions and real cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in various NLP tasks and modern medicine,\nbut their evaluation in traditional Chinese medicine (TCM) is underexplored. To\naddress this, we introduce TCM3CEval, a benchmark assessing LLMs in TCM across\nthree dimensions: core knowledge mastery, classical text understanding, and\nclinical decision-making. We evaluate diverse models, including international\n(e.g., GPT-4o), Chinese (e.g., InternLM), and medical-specific (e.g., PLUSE).\nResults show a performance hierarchy: all models have limitations in\nspecialized subdomains like Meridian & Acupoint theory and Various TCM Schools,\nrevealing gaps between current capabilities and clinical needs. Models with\nChinese linguistic and cultural priors perform better in classical text\ninterpretation and clinical reasoning. TCM-3CEval sets a standard for AI\nevaluation in TCM, offering insights for optimizing LLMs in culturally grounded\nmedical domains. The benchmark is available on Medbench's TCM track, aiming to\nassess LLMs' TCM capabilities in basic knowledge, classic texts, and clinical\ndecision-making through multidimensional questions and real cases."
                },
                "authors": [
                    {
                        "name": "Tianai Huang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Jiayuan Chen"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Yuping Zhao"
                    },
                    {
                        "name": "Wenchao Tang"
                    },
                    {
                        "name": "Jie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Xu"
                },
                "author": "Jie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07036v1",
                "updated": "2025-03-10T08:21:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    21,
                    36,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:21:36Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    21,
                    36,
                    0,
                    69,
                    0
                ],
                "title": "Bot Wars Evolved: Orchestrating Competing LLMs in a Counterstrike\n  Against Phone Scams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bot Wars Evolved: Orchestrating Competing LLMs in a Counterstrike\n  Against Phone Scams"
                },
                "summary": "We present \"Bot Wars,\" a framework using Large Language Models (LLMs)\nscam-baiters to counter phone scams through simulated adversarial dialogues.\nOur key contribution is a formal foundation for strategy emergence through\nchain-of-thought reasoning without explicit optimization. Through a novel\ntwo-layer prompt architecture, our framework enables LLMs to craft\ndemographically authentic victim personas while maintaining strategic\ncoherence. We evaluate our approach using a dataset of 3,200 scam dialogues\nvalidated against 179 hours of human scam-baiting interactions, demonstrating\nits effectiveness in capturing complex adversarial dynamics. Our systematic\nevaluation through cognitive, quantitative, and content-specific metrics shows\nthat GPT-4 excels in dialogue naturalness and persona authenticity, while\nDeepseek demonstrates superior engagement sustainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Bot Wars,\" a framework using Large Language Models (LLMs)\nscam-baiters to counter phone scams through simulated adversarial dialogues.\nOur key contribution is a formal foundation for strategy emergence through\nchain-of-thought reasoning without explicit optimization. Through a novel\ntwo-layer prompt architecture, our framework enables LLMs to craft\ndemographically authentic victim personas while maintaining strategic\ncoherence. We evaluate our approach using a dataset of 3,200 scam dialogues\nvalidated against 179 hours of human scam-baiting interactions, demonstrating\nits effectiveness in capturing complex adversarial dynamics. Our systematic\nevaluation through cognitive, quantitative, and content-specific metrics shows\nthat GPT-4 excels in dialogue naturalness and persona authenticity, while\nDeepseek demonstrates superior engagement sustainability."
                },
                "authors": [
                    {
                        "name": "Nardine Basta"
                    },
                    {
                        "name": "Conor Atkins"
                    },
                    {
                        "name": "Dali Kaafar"
                    }
                ],
                "author_detail": {
                    "name": "Dali Kaafar"
                },
                "author": "Dali Kaafar",
                "arxiv_journal_ref": "Pacific-Asia Conference on Knowledge Discovery and Data Mining,\n  PAKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07032v1",
                "updated": "2025-03-10T08:16:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    16,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:16:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    16,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Multimodal Human-AI Synergy for Medical Imaging Quality Control: A\n  Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Human-AI Synergy for Medical Imaging Quality Control: A\n  Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop\n  Evaluation"
                },
                "summary": "Medical imaging quality control (QC) is essential for accurate diagnosis, yet\ntraditional QC methods remain labor-intensive and subjective. To address this\nchallenge, in this study, we establish a standardized dataset and evaluation\nframework for medical imaging QC, systematically assessing large language\nmodels (LLMs) in image quality assessment and report standardization.\nSpecifically, we first constructed and anonymized a dataset of 161 chest X-ray\n(CXR) radiographs and 219 CT reports for evaluation. Then, multiple LLMs,\nincluding Gemini 2.0-Flash, GPT-4o, and DeepSeek-R1, were evaluated based on\nrecall, precision, and F1 score to detect technical errors and inconsistencies.\nExperimental results show that Gemini 2.0-Flash achieved a Macro F1 score of 90\nin CXR tasks, demonstrating strong generalization but limited fine-grained\nperformance. DeepSeek-R1 excelled in CT report auditing with a 62.23\\% recall\nrate, outperforming other models. However, its distilled variants performed\npoorly, while InternLM2.5-7B-chat exhibited the highest additional discovery\nrate, indicating broader but less precise error detection. These findings\nhighlight the potential of LLMs in medical imaging QC, with DeepSeek-R1 and\nGemini 2.0-Flash demonstrating superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical imaging quality control (QC) is essential for accurate diagnosis, yet\ntraditional QC methods remain labor-intensive and subjective. To address this\nchallenge, in this study, we establish a standardized dataset and evaluation\nframework for medical imaging QC, systematically assessing large language\nmodels (LLMs) in image quality assessment and report standardization.\nSpecifically, we first constructed and anonymized a dataset of 161 chest X-ray\n(CXR) radiographs and 219 CT reports for evaluation. Then, multiple LLMs,\nincluding Gemini 2.0-Flash, GPT-4o, and DeepSeek-R1, were evaluated based on\nrecall, precision, and F1 score to detect technical errors and inconsistencies.\nExperimental results show that Gemini 2.0-Flash achieved a Macro F1 score of 90\nin CXR tasks, demonstrating strong generalization but limited fine-grained\nperformance. DeepSeek-R1 excelled in CT report auditing with a 62.23\\% recall\nrate, outperforming other models. However, its distilled variants performed\npoorly, while InternLM2.5-7B-chat exhibited the highest additional discovery\nrate, indicating broader but less precise error detection. These findings\nhighlight the potential of LLMs in medical imaging QC, with DeepSeek-R1 and\nGemini 2.0-Flash demonstrating superior performance."
                },
                "authors": [
                    {
                        "name": "Zhi Qin"
                    },
                    {
                        "name": "Qianhui Gui"
                    },
                    {
                        "name": "Mouxiao Bian"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Hong Ge"
                    },
                    {
                        "name": "Dandan Yao"
                    },
                    {
                        "name": "Ziying Sun"
                    },
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Hui Shi"
                    },
                    {
                        "name": "Dongdong Wang"
                    },
                    {
                        "name": "Chenxin Song"
                    },
                    {
                        "name": "Shenghong Ju"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Yuan-Cheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Cheng Wang"
                },
                "author": "Yuan-Cheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07020v1",
                "updated": "2025-03-10T08:01:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    1,
                    41,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:01:41Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    1,
                    41,
                    0,
                    69,
                    0
                ],
                "title": "Combating Partial Perception Deficit in Autonomous Driving with\n  Multimodal LLM Commonsense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating Partial Perception Deficit in Autonomous Driving with\n  Multimodal LLM Commonsense"
                },
                "summary": "Partial perception deficits can compromise autonomous vehicle safety by\ndisrupting environmental understanding. Current protocols typically respond\nwith immediate stops or minimal-risk maneuvers, worsening traffic flow and\nlacking flexibility for rare driving scenarios. In this paper, we propose\nLLM-RCO, a framework leveraging large language models to integrate human-like\ndriving commonsense into autonomous systems facing perception deficits. LLM-RCO\nfeatures four key modules: hazard inference, short-term motion planner, action\ncondition verifier, and safety constraint generator. These modules interact\nwith the dynamic driving environment, enabling proactive and context-aware\ncontrol actions to override the original control policy of autonomous agents.\nTo improve safety in such challenging conditions, we construct DriveLM-Deficit,\na dataset of 53,895 video clips featuring deficits of safety-critical objects,\ncomplete with annotations for LLM-based hazard inference and motion planning\nfine-tuning. Extensive experiments in adverse driving conditions with the CARLA\nsimulator demonstrate that systems equipped with LLM-RCO significantly improve\ndriving performance, highlighting its potential for enhancing autonomous\ndriving resilience against adverse perception deficits. Our results also show\nthat LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements\ninstead of conservative stops in the context of perception deficits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial perception deficits can compromise autonomous vehicle safety by\ndisrupting environmental understanding. Current protocols typically respond\nwith immediate stops or minimal-risk maneuvers, worsening traffic flow and\nlacking flexibility for rare driving scenarios. In this paper, we propose\nLLM-RCO, a framework leveraging large language models to integrate human-like\ndriving commonsense into autonomous systems facing perception deficits. LLM-RCO\nfeatures four key modules: hazard inference, short-term motion planner, action\ncondition verifier, and safety constraint generator. These modules interact\nwith the dynamic driving environment, enabling proactive and context-aware\ncontrol actions to override the original control policy of autonomous agents.\nTo improve safety in such challenging conditions, we construct DriveLM-Deficit,\na dataset of 53,895 video clips featuring deficits of safety-critical objects,\ncomplete with annotations for LLM-based hazard inference and motion planning\nfine-tuning. Extensive experiments in adverse driving conditions with the CARLA\nsimulator demonstrate that systems equipped with LLM-RCO significantly improve\ndriving performance, highlighting its potential for enhancing autonomous\ndriving resilience against adverse perception deficits. Our results also show\nthat LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements\ninstead of conservative stops in the context of perception deficits."
                },
                "authors": [
                    {
                        "name": "Yuting Hu"
                    },
                    {
                        "name": "Chenhui Xu"
                    },
                    {
                        "name": "Ruiyang Qin"
                    },
                    {
                        "name": "Dancheng Liu"
                    },
                    {
                        "name": "Amir Nassereldine"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Jinjun Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Jinjun Xiong"
                },
                "author": "Jinjun Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07018v1",
                "updated": "2025-03-10T07:59:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    59,
                    41,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T07:59:41Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    59,
                    41,
                    0,
                    69,
                    0
                ],
                "title": "Toward Multi-Session Personalized Conversation: A Large-Scale Dataset\n  and Hierarchical Tree Framework for Implicit Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Multi-Session Personalized Conversation: A Large-Scale Dataset\n  and Hierarchical Tree Framework for Implicit Reasoning"
                },
                "summary": "There has been a surge in the use of large language models (LLM)\nconversational agents to generate responses based on long-term history from\nmultiple sessions. However, existing long-term open-domain dialogue datasets\nlack complex, real-world personalization and fail to capture implicit\nreasoning-where relevant information is embedded in subtle, syntactic, or\nsemantically distant connections rather than explicit statements. In such\ncases, traditional retrieval methods fail to capture relevant context, and\nlong-context modeling also becomes inefficient due to numerous complicated\npersona-related details. To address this gap, we introduce ImplexConv, a\nlarge-scale long-term dataset with 2,500 examples, each containing\napproximately 100 conversation sessions, designed to study implicit reasoning\nin personalized dialogues. Additionally, we propose TaciTree, a novel\nhierarchical tree framework that structures conversation history into multiple\nlevels of summarization. Instead of brute-force searching all data, TaciTree\nenables an efficient, level-based retrieval process where models refine their\nsearch by progressively selecting relevant details. Our experiments demonstrate\nthat TaciTree significantly improves the ability of LLMs to reason over\nlong-term conversations with implicit contextual dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a surge in the use of large language models (LLM)\nconversational agents to generate responses based on long-term history from\nmultiple sessions. However, existing long-term open-domain dialogue datasets\nlack complex, real-world personalization and fail to capture implicit\nreasoning-where relevant information is embedded in subtle, syntactic, or\nsemantically distant connections rather than explicit statements. In such\ncases, traditional retrieval methods fail to capture relevant context, and\nlong-context modeling also becomes inefficient due to numerous complicated\npersona-related details. To address this gap, we introduce ImplexConv, a\nlarge-scale long-term dataset with 2,500 examples, each containing\napproximately 100 conversation sessions, designed to study implicit reasoning\nin personalized dialogues. Additionally, we propose TaciTree, a novel\nhierarchical tree framework that structures conversation history into multiple\nlevels of summarization. Instead of brute-force searching all data, TaciTree\nenables an efficient, level-based retrieval process where models refine their\nsearch by progressively selecting relevant details. Our experiments demonstrate\nthat TaciTree significantly improves the ability of LLMs to reason over\nlong-term conversations with implicit contextual dependencies."
                },
                "authors": [
                    {
                        "name": "Xintong Li"
                    },
                    {
                        "name": "Jalend Bantupalli"
                    },
                    {
                        "name": "Ria Dharmani"
                    },
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Jingbo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Shang"
                },
                "author": "Jingbo Shang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07010v1",
                "updated": "2025-03-10T07:47:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    47,
                    27,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T07:47:27Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    47,
                    27,
                    0,
                    69,
                    0
                ],
                "title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on\n  Project-Level Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on\n  Project-Level Code Generation"
                },
                "summary": "Recently, LLM agents have made rapid progress in improving their programming\ncapabilities. However, existing benchmarks lack the ability to automatically\nevaluate from users' perspective, and also lack the explainability of the\nresults of LLM agents' code generation capabilities. Thus, we introduce\nProjectEval, a new benchmark for LLM agents project-level code generation's\nautomated evaluation by simulating user interaction. ProjectEval is constructed\nby LLM with human reviewing. It has three different level inputs of natural\nlanguages or code skeletons. ProjectEval can evaluate the generated projects by\nuser interaction simulation for execution, and by code similarity through\nexisting objective indicators. Through ProjectEval, we find that systematic\nengineering project code, overall understanding of the project and\ncomprehensive analysis capability are the keys for LLM agents to achieve\npractical projects. Our findings and benchmark provide valuable insights for\ndeveloping more effective programming agents that can be deployed in future\nreal-world production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LLM agents have made rapid progress in improving their programming\ncapabilities. However, existing benchmarks lack the ability to automatically\nevaluate from users' perspective, and also lack the explainability of the\nresults of LLM agents' code generation capabilities. Thus, we introduce\nProjectEval, a new benchmark for LLM agents project-level code generation's\nautomated evaluation by simulating user interaction. ProjectEval is constructed\nby LLM with human reviewing. It has three different level inputs of natural\nlanguages or code skeletons. ProjectEval can evaluate the generated projects by\nuser interaction simulation for execution, and by code similarity through\nexisting objective indicators. Through ProjectEval, we find that systematic\nengineering project code, overall understanding of the project and\ncomprehensive analysis capability are the keys for LLM agents to achieve\npractical projects. Our findings and benchmark provide valuable insights for\ndeveloping more effective programming agents that can be deployed in future\nreal-world production."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Liu"
                    },
                    {
                        "name": "Youcheng Pan"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Daojing He"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Yexing Du"
                    },
                    {
                        "name": "Tianrun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Tianrun Gao"
                },
                "author": "Tianrun Gao",
                "arxiv_comment": "17 pages (9 Appendix pages), 4 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07006v1",
                "updated": "2025-03-10T07:40:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    40,
                    1,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T07:40:01Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    40,
                    1,
                    0,
                    69,
                    0
                ],
                "title": "HELM: Human-Preferred Exploration with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Human-Preferred Exploration with Language Models"
                },
                "summary": "In autonomous exploration tasks, robots are required to explore and map\nunknown environments while efficiently planning in dynamic and uncertain\nconditions. Given the significant variability of environments, human operators\noften have specific preference requirements for exploration, such as\nprioritizing certain areas or optimizing for different aspects of efficiency.\nHowever, existing methods struggle to accommodate these human preferences\nadaptively, often requiring extensive parameter tuning or network retraining.\nWith the recent advancements in Large Language Models (LLMs), which have been\nwidely applied to text-based planning and complex reasoning, their potential\nfor enhancing autonomous exploration is becoming increasingly promising.\nMotivated by this, we propose an LLM-based human-preferred exploration\nframework that seamlessly integrates a mobile robot system with LLMs. By\nleveraging the reasoning and adaptability of LLMs, our approach enables\nintuitive and flexible preference control through natural language while\nmaintaining a task success rate comparable to state-of-the-art traditional\nmethods. Experimental results demonstrate that our framework effectively\nbridges the gap between human intent and policy preference in autonomous\nexploration, offering a more user-friendly and adaptable solution for\nreal-world robotic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In autonomous exploration tasks, robots are required to explore and map\nunknown environments while efficiently planning in dynamic and uncertain\nconditions. Given the significant variability of environments, human operators\noften have specific preference requirements for exploration, such as\nprioritizing certain areas or optimizing for different aspects of efficiency.\nHowever, existing methods struggle to accommodate these human preferences\nadaptively, often requiring extensive parameter tuning or network retraining.\nWith the recent advancements in Large Language Models (LLMs), which have been\nwidely applied to text-based planning and complex reasoning, their potential\nfor enhancing autonomous exploration is becoming increasingly promising.\nMotivated by this, we propose an LLM-based human-preferred exploration\nframework that seamlessly integrates a mobile robot system with LLMs. By\nleveraging the reasoning and adaptability of LLMs, our approach enables\nintuitive and flexible preference control through natural language while\nmaintaining a task success rate comparable to state-of-the-art traditional\nmethods. Experimental results demonstrate that our framework effectively\nbridges the gap between human intent and policy preference in autonomous\nexploration, offering a more user-friendly and adaptable solution for\nreal-world robotic applications."
                },
                "authors": [
                    {
                        "name": "Shuhao Liao"
                    },
                    {
                        "name": "Xuxin Lv"
                    },
                    {
                        "name": "Yuhong Cao"
                    },
                    {
                        "name": "Jeric Lew"
                    },
                    {
                        "name": "Wenjun Wu"
                    },
                    {
                        "name": "Guillaume Sartoretti"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Sartoretti"
                },
                "author": "Guillaume Sartoretti",
                "arxiv_comment": "submitted to IROS'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07004v1",
                "updated": "2025-03-10T07:38:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    38,
                    46,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T07:38:46Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    38,
                    46,
                    0,
                    69,
                    0
                ],
                "title": "NukesFormers: Unpaired Hyperspectral Image Generation with Non-Uniform\n  Domain Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NukesFormers: Unpaired Hyperspectral Image Generation with Non-Uniform\n  Domain Alignment"
                },
                "summary": "The inherent difficulty in acquiring accurately co-registered\nRGB-hyperspectral image (HSI) pairs has significantly impeded the practical\ndeployment of current data-driven Hyperspectral Image Generation (HIG) networks\nin engineering applications. Gleichzeitig, the ill-posed nature of the aligning\nconstraints, compounded with the complexities of mining cross-domain features,\nalso hinders the advancement of unpaired HIG (UnHIG) tasks. In this paper, we\nconquer these challenges by modeling the UnHIG to range space interaction and\ncompensations of null space through Range-Null Space Decomposition (RND)\nmethodology. Specifically, the introduced contrastive learning effectively\naligns the geometric and spectral distributions of unpaired data by building\nthe interaction of range space, considering the consistent feature in\ndegradation process. Following this, we map the frequency representations of\ndual-domain input and thoroughly mining the null space, like degraded and\nhigh-frequency components, through the proposed Non-uniform Kolmogorov-Arnold\nNetworks. Extensive comparative experiments demonstrate that it establishes a\nnew benchmark in UnHIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent difficulty in acquiring accurately co-registered\nRGB-hyperspectral image (HSI) pairs has significantly impeded the practical\ndeployment of current data-driven Hyperspectral Image Generation (HIG) networks\nin engineering applications. Gleichzeitig, the ill-posed nature of the aligning\nconstraints, compounded with the complexities of mining cross-domain features,\nalso hinders the advancement of unpaired HIG (UnHIG) tasks. In this paper, we\nconquer these challenges by modeling the UnHIG to range space interaction and\ncompensations of null space through Range-Null Space Decomposition (RND)\nmethodology. Specifically, the introduced contrastive learning effectively\naligns the geometric and spectral distributions of unpaired data by building\nthe interaction of range space, considering the consistent feature in\ndegradation process. Following this, we map the frequency representations of\ndual-domain input and thoroughly mining the null space, like degraded and\nhigh-frequency components, through the proposed Non-uniform Kolmogorov-Arnold\nNetworks. Extensive comparative experiments demonstrate that it establishes a\nnew benchmark in UnHIG."
                },
                "authors": [
                    {
                        "name": "Jiaojiao Li"
                    },
                    {
                        "name": "Shiyao Duan"
                    },
                    {
                        "name": "Haitao XU"
                    },
                    {
                        "name": "Rui Song"
                    }
                ],
                "author_detail": {
                    "name": "Rui Song"
                },
                "author": "Rui Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03592v2",
                "updated": "2025-03-10T07:36:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    36,
                    46,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-05T15:26:59Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    15,
                    26,
                    59,
                    2,
                    64,
                    0
                ],
                "title": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance"
                },
                "summary": "For consumer usage of locally deployed LLMs, the GGUF format and\nk\\_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to yielded\nnon-significant results indicating that current quantization practices do not\ndisproportionately harm multilingual performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For consumer usage of locally deployed LLMs, the GGUF format and\nk\\_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to yielded\nnon-significant results indicating that current quantization practices do not\ndisproportionately harm multilingual performance."
                },
                "authors": [
                    {
                        "name": "Karl Audun Borgersen"
                    }
                ],
                "author_detail": {
                    "name": "Karl Audun Borgersen"
                },
                "author": "Karl Audun Borgersen",
                "arxiv_comment": "8 pages, 6 figures, v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07003v1",
                "updated": "2025-03-10T07:34:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    34,
                    54,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T07:34:54Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    34,
                    54,
                    0,
                    69,
                    0
                ],
                "title": "Large Language Models Often Say One Thing and Do Another",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Often Say One Thing and Do Another"
                },
                "summary": "As large language models (LLMs) increasingly become central to various\napplications and interact with diverse user populations, ensuring their\nreliable and consistent performance is becoming more important. This paper\nexplores a critical issue in assessing the reliability of LLMs: the consistency\nbetween their words and deeds. To quantitatively explore this consistency, we\ndeveloped a novel evaluation benchmark called the Words and Deeds Consistency\nTest (WDCT). The benchmark establishes a strict correspondence between\nword-based and deed-based questions across different domains, including opinion\nvs. action, non-ethical value vs. action, ethical value vs. action, and theory\nvs. application. The evaluation results reveal a widespread inconsistency\nbetween words and deeds across different LLMs and domains. Subsequently, we\nconducted experiments with either word alignment or deed alignment to observe\ntheir impact on the other aspect. The experimental results indicate that\nalignment only on words or deeds poorly and unpredictably influences the other\naspect. This supports our hypothesis that the underlying knowledge guiding\nLLMs' word or deed choices is not contained within a unified space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly become central to various\napplications and interact with diverse user populations, ensuring their\nreliable and consistent performance is becoming more important. This paper\nexplores a critical issue in assessing the reliability of LLMs: the consistency\nbetween their words and deeds. To quantitatively explore this consistency, we\ndeveloped a novel evaluation benchmark called the Words and Deeds Consistency\nTest (WDCT). The benchmark establishes a strict correspondence between\nword-based and deed-based questions across different domains, including opinion\nvs. action, non-ethical value vs. action, ethical value vs. action, and theory\nvs. application. The evaluation results reveal a widespread inconsistency\nbetween words and deeds across different LLMs and domains. Subsequently, we\nconducted experiments with either word alignment or deed alignment to observe\ntheir impact on the other aspect. The experimental results indicate that\nalignment only on words or deeds poorly and unpredictably influences the other\naspect. This supports our hypothesis that the underlying knowledge guiding\nLLMs' word or deed choices is not contained within a unified space."
                },
                "authors": [
                    {
                        "name": "Ruoxi Xu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Weixiang Zhou"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Yingfei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yingfei Sun"
                },
                "author": "Yingfei Sun",
                "arxiv_comment": "Published on ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11882v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11882v4",
                "updated": "2025-03-10T07:25:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    25,
                    31,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-17T15:09:45Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    15,
                    9,
                    45,
                    0,
                    48,
                    0
                ],
                "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration"
                },
                "summary": "Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. DPT-Agent can effectively\nhelp LLMs convert correct slow thinking and reasoning into executable actions,\nthereby improving performance. To the best of our knowledge, DPT-Agent is the\nfirst language agent framework that achieves successful real-time simultaneous\nhuman-AI collaboration autonomously. Code of DPT-Agent can be found in\nhttps://github.com/sjtu-marl/DPT-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. DPT-Agent can effectively\nhelp LLMs convert correct slow thinking and reasoning into executable actions,\nthereby improving performance. To the best of our knowledge, DPT-Agent is the\nfirst language agent framework that achieves successful real-time simultaneous\nhuman-AI collaboration autonomously. Code of DPT-Agent can be found in\nhttps://github.com/sjtu-marl/DPT-Agent."
                },
                "authors": [
                    {
                        "name": "Shao Zhang"
                    },
                    {
                        "name": "Xihuai Wang"
                    },
                    {
                        "name": "Wenhao Zhang"
                    },
                    {
                        "name": "Chaoran Li"
                    },
                    {
                        "name": "Junru Song"
                    },
                    {
                        "name": "Tingyu Li"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Wen Yao"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "arxiv_comment": "Preprint under review. Update the experimental results of the\n  DeepSeek-R1 series models, QwQ-32b, o3-mini-high and o3-mini-medium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11882v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11882v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06989v1",
                "updated": "2025-03-10T07:10:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    10,
                    38,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T07:10:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    10,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal contents. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on inputs to maximize jailbreak probability. To counteract\nattacks, we also propose two defensive methods: Jailbreak-Probability-based\nFinetuning (JPF) and Jailbreak-Probability-based Defensive Noise (JPDN), which\nminimizes jailbreak probability in the MLLM parameters and input space,\nrespectively. Extensive experiments show that (1) JPA yields improvements (up\nto 28.38\\%) under both white and black box settings compared to previous\nmethods with small perturbation bounds and few iterations. (2) JPF and JPDN\nsignificantly reduce jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal contents. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on inputs to maximize jailbreak probability. To counteract\nattacks, we also propose two defensive methods: Jailbreak-Probability-based\nFinetuning (JPF) and Jailbreak-Probability-based Defensive Noise (JPDN), which\nminimizes jailbreak probability in the MLLM parameters and input space,\nrespectively. Extensive experiments show that (1) JPA yields improvements (up\nto 28.38\\%) under both white and black box settings compared to previous\nmethods with small perturbation bounds and few iterations. (2) JPF and JPDN\nsignificantly reduce jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities."
                },
                "authors": [
                    {
                        "name": "Wenzhuo Xu"
                    },
                    {
                        "name": "Zhipeng Wei"
                    },
                    {
                        "name": "Xiongtao Sun"
                    },
                    {
                        "name": "Deyue Zhang"
                    },
                    {
                        "name": "Dongdong Yang"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Xiangzheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzheng Zhang"
                },
                "author": "Xiangzheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06987v1",
                "updated": "2025-03-10T07:06:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    6,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T07:06:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    7,
                    6,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Social Bias Benchmark for Generation: A Comparison of Generation and\n  QA-Based Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Bias Benchmark for Generation: A Comparison of Generation and\n  QA-Based Evaluations"
                },
                "summary": "Measuring social bias in large language models (LLMs) is crucial, but\nexisting bias evaluation methods struggle to assess bias in long-form\ngeneration. We propose a Bias Benchmark for Generation (BBG), an adaptation of\nthe Bias Benchmark for QA (BBQ), designed to evaluate social bias in long-form\ngeneration by having LLMs generate continuations of story prompts. Building our\nbenchmark in English and Korean, we measure the probability of neutral and\nbiased generations across ten LLMs. We also compare our long-form story\ngeneration evaluation results with multiple-choice BBQ evaluation, showing that\nthe two approaches produce inconsistent results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring social bias in large language models (LLMs) is crucial, but\nexisting bias evaluation methods struggle to assess bias in long-form\ngeneration. We propose a Bias Benchmark for Generation (BBG), an adaptation of\nthe Bias Benchmark for QA (BBQ), designed to evaluate social bias in long-form\ngeneration by having LLMs generate continuations of story prompts. Building our\nbenchmark in English and Korean, we measure the probability of neutral and\nbiased generations across ten LLMs. We also compare our long-form story\ngeneration evaluation results with multiple-choice BBQ evaluation, showing that\nthe two approaches produce inconsistent results."
                },
                "authors": [
                    {
                        "name": "Jiho Jin"
                    },
                    {
                        "name": "Woosung Kang"
                    },
                    {
                        "name": "Junho Myung"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12214v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12214v7",
                "updated": "2025-03-10T06:52:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    52,
                    58,
                    0,
                    69,
                    0
                ],
                "published": "2023-10-18T18:00:11Z",
                "published_parsed": [
                    2023,
                    10,
                    18,
                    18,
                    0,
                    11,
                    2,
                    291,
                    0
                ],
                "title": "InferDPT: Privacy-Preserving Inference for Black-box Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferDPT: Privacy-Preserving Inference for Black-box Large Language\n  Model"
                },
                "summary": "Large language models (LLMs), like ChatGPT, have greatly simplified text\ngeneration tasks. However, they have also raised concerns about privacy risks\nsuch as data leakage and unauthorized data collection. Existing solutions for\nprivacy-preserving inference face practical challenges related to computation\ntime and communication costs. In this paper, we propose InferDPT, the first\npractical framework for the privacy-preserving Inference of black-box LLMs,\nimplementing Differential Privacy in Text generation. InferDPT comprises two\nkey modules: the \"perturbation module\" utilizes the exponential mechanism to\ngenerate a perturbed prompt, facilitating privacy-preserving inference with\nblack-box LLMs, and the \"extraction module\", inspired by knowledge distillation\nand retrieval-augmented generation, extracts coherent and consistent text from\nthe perturbed generation result, ensuring successful text generation\ncompletion. To address privacy concerns related to previous exponential\nmechanisms' susceptibility to embedding revision attacks, we introduce RANTEXT,\na novel differential privacy mechanism integrated into the perturbation module\nof InferDPT, which introduces the concept of \"RANdom adjacency\" for TEXT\nperturbation within the prompt. Experimental results across three datasets\ndemonstrate that the text generation quality of InferDPT is comparable to that\nof non-private GPT-4, and RANTEXT surpasses existing state-of-the-art\nmechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy and\nutility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achieves\nan average privacy protection rate exceeding 90% against embedding revision\nattacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higher\nthan that of CUSTEXT+.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), like ChatGPT, have greatly simplified text\ngeneration tasks. However, they have also raised concerns about privacy risks\nsuch as data leakage and unauthorized data collection. Existing solutions for\nprivacy-preserving inference face practical challenges related to computation\ntime and communication costs. In this paper, we propose InferDPT, the first\npractical framework for the privacy-preserving Inference of black-box LLMs,\nimplementing Differential Privacy in Text generation. InferDPT comprises two\nkey modules: the \"perturbation module\" utilizes the exponential mechanism to\ngenerate a perturbed prompt, facilitating privacy-preserving inference with\nblack-box LLMs, and the \"extraction module\", inspired by knowledge distillation\nand retrieval-augmented generation, extracts coherent and consistent text from\nthe perturbed generation result, ensuring successful text generation\ncompletion. To address privacy concerns related to previous exponential\nmechanisms' susceptibility to embedding revision attacks, we introduce RANTEXT,\na novel differential privacy mechanism integrated into the perturbation module\nof InferDPT, which introduces the concept of \"RANdom adjacency\" for TEXT\nperturbation within the prompt. Experimental results across three datasets\ndemonstrate that the text generation quality of InferDPT is comparable to that\nof non-private GPT-4, and RANTEXT surpasses existing state-of-the-art\nmechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy and\nutility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achieves\nan average privacy protection rate exceeding 90% against embedding revision\nattacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higher\nthan that of CUSTEXT+."
                },
                "authors": [
                    {
                        "name": "Meng Tong"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Yuang Qi"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Zhikun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhikun Zhang"
                },
                "author": "Zhikun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12214v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12214v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06980v1",
                "updated": "2025-03-10T06:52:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    52,
                    35,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T06:52:35Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    52,
                    35,
                    0,
                    69,
                    0
                ],
                "title": "Exploring Multimodal Perception in Large Language Models Through\n  Perceptual Strength Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Multimodal Perception in Large Language Models Through\n  Perceptual Strength Ratings"
                },
                "summary": "This study investigated the multimodal perception of large language models\n(LLMs), focusing on their ability to capture human-like perceptual strength\nratings across sensory modalities. Utilizing perceptual strength ratings as a\nbenchmark, the research compared GPT-3.5, GPT-4, GPT-4o, and GPT-4o-mini,\nhighlighting the influence of multimodal inputs on grounding and linguistic\nreasoning. While GPT-4 and GPT-4o demonstrated strong alignment with human\nevaluations and significant advancements over smaller models, qualitative\nanalyses revealed distinct differences in processing patterns, such as\nmultisensory overrating and reliance on loose semantic associations. Despite\nintegrating multimodal capabilities, GPT-4o did not exhibit superior grounding\ncompared to GPT-4, raising questions about their role in improving human-like\ngrounding. These findings underscore how LLMs' reliance on linguistic patterns\ncan both approximate and diverge from human embodied cognition, revealing\nlimitations in replicating sensory experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigated the multimodal perception of large language models\n(LLMs), focusing on their ability to capture human-like perceptual strength\nratings across sensory modalities. Utilizing perceptual strength ratings as a\nbenchmark, the research compared GPT-3.5, GPT-4, GPT-4o, and GPT-4o-mini,\nhighlighting the influence of multimodal inputs on grounding and linguistic\nreasoning. While GPT-4 and GPT-4o demonstrated strong alignment with human\nevaluations and significant advancements over smaller models, qualitative\nanalyses revealed distinct differences in processing patterns, such as\nmultisensory overrating and reliance on loose semantic associations. Despite\nintegrating multimodal capabilities, GPT-4o did not exhibit superior grounding\ncompared to GPT-4, raising questions about their role in improving human-like\ngrounding. These findings underscore how LLMs' reliance on linguistic patterns\ncan both approximate and diverge from human embodied cognition, revealing\nlimitations in replicating sensory experiences."
                },
                "authors": [
                    {
                        "name": "Jonghyun Lee"
                    },
                    {
                        "name": "Dojun Park"
                    },
                    {
                        "name": "Jiwoo Lee"
                    },
                    {
                        "name": "Hoekeon Choi"
                    },
                    {
                        "name": "Sung-Eun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sung-Eun Lee"
                },
                "author": "Sung-Eun Lee",
                "arxiv_comment": "under review, 15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07818v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07818v6",
                "updated": "2025-03-10T06:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    52,
                    3,
                    0,
                    69,
                    0
                ],
                "published": "2024-02-12T17:24:15Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    17,
                    24,
                    15,
                    0,
                    43,
                    0
                ],
                "title": "Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning"
                },
                "summary": "Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks)."
                },
                "authors": [
                    {
                        "name": "Z Liu"
                    },
                    {
                        "name": "J Lou"
                    },
                    {
                        "name": "W Bao"
                    },
                    {
                        "name": "Y Hu"
                    },
                    {
                        "name": "B Li"
                    },
                    {
                        "name": "Z Qin"
                    },
                    {
                        "name": "K Ren"
                    }
                ],
                "author_detail": {
                    "name": "K Ren"
                },
                "author": "K Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07818v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07818v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04809v2",
                "updated": "2025-03-10T06:49:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    49,
                    1,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-04T07:40:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    7,
                    40,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "PanguIR Technical Report for NTCIR-18 AEOLLM Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PanguIR Technical Report for NTCIR-18 AEOLLM Task"
                },
                "summary": "As large language models (LLMs) gain widespread attention in both academia\nand industry, it becomes increasingly critical and challenging to effectively\nevaluate their capabilities. Existing evaluation methods can be broadly\ncategorized into two types: manual evaluation and automatic evaluation. Manual\nevaluation, while comprehensive, is often costly and resource-intensive.\nConversely, automatic evaluation offers greater scalability but is constrained\nby the limitations of its evaluation criteria (dominated by reference-based\nanswers). To address these challenges, NTCIR-18 introduced the AEOLLM\n(Automatic Evaluation of LLMs) task, aiming to encourage reference-free\nevaluation methods that can overcome the limitations of existing approaches. In\nthis paper, to enhance the evaluation performance of the AEOLLM task, we\npropose three key methods to improve the reference-free evaluation: 1)\nMulti-model Collaboration: Leveraging multiple LLMs to approximate human\nratings across various subtasks; 2) Prompt Auto-optimization: Utilizing LLMs to\niteratively refine the initial task prompts based on evaluation feedback from\ntraining samples; and 3) In-context Learning (ICL) Optimization: Based on the\nmulti-task evaluation feedback, we train a specialized in-context example\nretrieval model, combined with a semantic relevance retrieval model, to jointly\nidentify the most effective in-context learning examples. Experiments conducted\non the final dataset demonstrate that our approach achieves superior\nperformance on the AEOLLM task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) gain widespread attention in both academia\nand industry, it becomes increasingly critical and challenging to effectively\nevaluate their capabilities. Existing evaluation methods can be broadly\ncategorized into two types: manual evaluation and automatic evaluation. Manual\nevaluation, while comprehensive, is often costly and resource-intensive.\nConversely, automatic evaluation offers greater scalability but is constrained\nby the limitations of its evaluation criteria (dominated by reference-based\nanswers). To address these challenges, NTCIR-18 introduced the AEOLLM\n(Automatic Evaluation of LLMs) task, aiming to encourage reference-free\nevaluation methods that can overcome the limitations of existing approaches. In\nthis paper, to enhance the evaluation performance of the AEOLLM task, we\npropose three key methods to improve the reference-free evaluation: 1)\nMulti-model Collaboration: Leveraging multiple LLMs to approximate human\nratings across various subtasks; 2) Prompt Auto-optimization: Utilizing LLMs to\niteratively refine the initial task prompts based on evaluation feedback from\ntraining samples; and 3) In-context Learning (ICL) Optimization: Based on the\nmulti-task evaluation feedback, we train a specialized in-context example\nretrieval model, combined with a semantic relevance retrieval model, to jointly\nidentify the most effective in-context learning examples. Experiments conducted\non the final dataset demonstrate that our approach achieves superior\nperformance on the AEOLLM task."
                },
                "authors": [
                    {
                        "name": "Lang Mei"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06978v1",
                "updated": "2025-03-10T06:47:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    47,
                    38,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T06:47:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    47,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "Lightweight Multimodal Artificial Intelligence Framework for Maritime\n  Multi-Scene Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Multimodal Artificial Intelligence Framework for Maritime\n  Multi-Scene Recognition"
                },
                "summary": "Maritime Multi-Scene Recognition is crucial for enhancing the capabilities of\nintelligent marine robotics, particularly in applications such as marine\nconservation, environmental monitoring, and disaster response. However, this\ntask presents significant challenges due to environmental interference, where\nmarine conditions degrade image quality, and the complexity of maritime scenes,\nwhich requires deeper reasoning for accurate recognition. Pure vision models\nalone are insufficient to address these issues. To overcome these limitations,\nwe propose a novel multimodal Artificial Intelligence (AI) framework that\nintegrates image data, textual descriptions and classification vectors\ngenerated by a Multimodal Large Language Model (MLLM), to provide richer\nsemantic understanding and improve recognition accuracy. Our framework employs\nan efficient multimodal fusion mechanism to further enhance model robustness\nand adaptability in complex maritime environments. Experimental results show\nthat our model achieves 98$\\%$ accuracy, surpassing previous SOTA models by\n3.5$\\%$. To optimize deployment on resource-constrained platforms, we adopt\nactivation-aware weight quantization (AWQ) as a lightweight technique, reducing\nthe model size to 68.75MB with only a 0.5$\\%$ accuracy drop while significantly\nlowering computational overhead. This work provides a high-performance solution\nfor real-time maritime scene recognition, enabling Autonomous Surface Vehicles\n(ASVs) to support environmental monitoring and disaster response in\nresource-limited settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maritime Multi-Scene Recognition is crucial for enhancing the capabilities of\nintelligent marine robotics, particularly in applications such as marine\nconservation, environmental monitoring, and disaster response. However, this\ntask presents significant challenges due to environmental interference, where\nmarine conditions degrade image quality, and the complexity of maritime scenes,\nwhich requires deeper reasoning for accurate recognition. Pure vision models\nalone are insufficient to address these issues. To overcome these limitations,\nwe propose a novel multimodal Artificial Intelligence (AI) framework that\nintegrates image data, textual descriptions and classification vectors\ngenerated by a Multimodal Large Language Model (MLLM), to provide richer\nsemantic understanding and improve recognition accuracy. Our framework employs\nan efficient multimodal fusion mechanism to further enhance model robustness\nand adaptability in complex maritime environments. Experimental results show\nthat our model achieves 98$\\%$ accuracy, surpassing previous SOTA models by\n3.5$\\%$. To optimize deployment on resource-constrained platforms, we adopt\nactivation-aware weight quantization (AWQ) as a lightweight technique, reducing\nthe model size to 68.75MB with only a 0.5$\\%$ accuracy drop while significantly\nlowering computational overhead. This work provides a high-performance solution\nfor real-time maritime scene recognition, enabling Autonomous Surface Vehicles\n(ASVs) to support environmental monitoring and disaster response in\nresource-limited settings."
                },
                "authors": [
                    {
                        "name": "Xinyu Xi"
                    },
                    {
                        "name": "Hua Yang"
                    },
                    {
                        "name": "Shentai Zhang"
                    },
                    {
                        "name": "Yijie Liu"
                    },
                    {
                        "name": "Sijin Sun"
                    },
                    {
                        "name": "Xiuju Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiuju Fu"
                },
                "author": "Xiuju Fu",
                "arxiv_comment": "19 pages, 4 figures, submitted to Engineering Applications of\n  Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14855v2",
                "updated": "2025-03-10T06:44:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    44,
                    48,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-20T18:58:07Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    7,
                    3,
                    51,
                    0
                ],
                "title": "Prompt-to-Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-to-Leaderboard"
                },
                "summary": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the #1 spot on the Chatbot\nArena leaderboard. Our code is available on GitHub at\nhttps://github.com/lmarena/p2l.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the #1 spot on the Chatbot\nArena leaderboard. Our code is available on GitHub at\nhttps://github.com/lmarena/p2l."
                },
                "authors": [
                    {
                        "name": "Evan Frick"
                    },
                    {
                        "name": "Connor Chen"
                    },
                    {
                        "name": "Joseph Tennyson"
                    },
                    {
                        "name": "Tianle Li"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02519v2",
                "updated": "2025-03-10T06:22:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    22,
                    25,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-04T11:31:05Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    31,
                    5,
                    1,
                    63,
                    0
                ],
                "title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model\n  Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generator-Assistant Stepwise Rollback Framework for Large Language Model\n  Agent"
                },
                "summary": "Large language model (LLM) agents typically adopt a step-by-step reasoning\nframework, in which they interleave the processes of thinking and acting to\naccomplish the given task. However, this paradigm faces a deep-rooted one-pass\nissue whereby each generated intermediate thought is plugged into the\ntrajectory regardless of its correctness, which can cause irreversible error\npropagation. To address the issue, this paper proposes a novel framework called\nGenerator-Assistant Stepwise Rollback (GA-Rollback) to induce better\ndecision-making for LLM agents. Particularly, GA-Rollback utilizes a generator\nto interact with the environment and an assistant to examine each action\nproduced by the generator, where the assistant triggers a rollback operation\nupon detection of incorrect actions. Moreover, we introduce two additional\nstrategies tailored for the rollback scenario to further improve its\neffectiveness. Extensive experiments show that GA-Rollback achieves significant\nimprovements over several strong baselines on three widely used benchmarks. Our\nanalysis further reveals that GA-Rollback can function as a robust\nplug-and-play module, integrating seamlessly with other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents typically adopt a step-by-step reasoning\nframework, in which they interleave the processes of thinking and acting to\naccomplish the given task. However, this paradigm faces a deep-rooted one-pass\nissue whereby each generated intermediate thought is plugged into the\ntrajectory regardless of its correctness, which can cause irreversible error\npropagation. To address the issue, this paper proposes a novel framework called\nGenerator-Assistant Stepwise Rollback (GA-Rollback) to induce better\ndecision-making for LLM agents. Particularly, GA-Rollback utilizes a generator\nto interact with the environment and an assistant to examine each action\nproduced by the generator, where the assistant triggers a rollback operation\nupon detection of incorrect actions. Moreover, we introduce two additional\nstrategies tailored for the rollback scenario to further improve its\neffectiveness. Extensive experiments show that GA-Rollback achieves significant\nimprovements over several strong baselines on three widely used benchmarks. Our\nanalysis further reveals that GA-Rollback can function as a robust\nplug-and-play module, integrating seamlessly with other methods."
                },
                "authors": [
                    {
                        "name": "Xingzuo Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11934v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11934v3",
                "updated": "2025-03-10T06:22:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    22,
                    15,
                    0,
                    69,
                    0
                ],
                "published": "2024-12-16T16:20:41Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    20,
                    41,
                    0,
                    351,
                    0
                ],
                "title": "Stepwise Reasoning Error Disruption Attack of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepwise Reasoning Error Disruption Attack of LLMs"
                },
                "summary": "Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications."
                },
                "authors": [
                    {
                        "name": "Jingyu Peng"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Ruocheng Guo"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11934v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11934v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17417v2",
                "updated": "2025-03-10T06:18:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    18,
                    24,
                    0,
                    69,
                    0
                ],
                "published": "2024-07-24T16:53:09Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    16,
                    53,
                    9,
                    2,
                    206,
                    0
                ],
                "title": "Can Watermarking Large Language Models Prevent Copyrighted Text\n  Generation and Hide Training Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Watermarking Large Language Models Prevent Copyrighted Text\n  Generation and Hide Training Data?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating diverse and contextually rich text. However, concerns regarding\ncopyright infringement arise as LLMs may inadvertently produce copyrighted\nmaterial. In this paper, we first investigate the effectiveness of watermarking\nLLMs as a deterrent against the generation of copyrighted texts. Through\ntheoretical analysis and empirical evaluation, we demonstrate that\nincorporating watermarks into LLMs significantly reduces the likelihood of\ngenerating copyrighted content, thereby addressing a critical concern in the\ndeployment of LLMs. However, we also find that watermarking can have unintended\nconsequences on Membership Inference Attacks (MIAs), which aim to discern\nwhether a sample was part of the pretraining dataset and may be used to detect\ncopyright violations. Surprisingly, we find that watermarking adversely affects\nthe success rate of MIAs, complicating the task of detecting copyrighted text\nin the pretraining dataset. These results reveal the complex interplay between\ndifferent regulatory measures, which may impact each other in unforeseen ways.\nFinally, we propose an adaptive technique to improve the success rate of a\nrecent MIA under watermarking. Our findings underscore the importance of\ndeveloping adaptive methods to study critical problems in LLMs with potential\nlegal implications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating diverse and contextually rich text. However, concerns regarding\ncopyright infringement arise as LLMs may inadvertently produce copyrighted\nmaterial. In this paper, we first investigate the effectiveness of watermarking\nLLMs as a deterrent against the generation of copyrighted texts. Through\ntheoretical analysis and empirical evaluation, we demonstrate that\nincorporating watermarks into LLMs significantly reduces the likelihood of\ngenerating copyrighted content, thereby addressing a critical concern in the\ndeployment of LLMs. However, we also find that watermarking can have unintended\nconsequences on Membership Inference Attacks (MIAs), which aim to discern\nwhether a sample was part of the pretraining dataset and may be used to detect\ncopyright violations. Surprisingly, we find that watermarking adversely affects\nthe success rate of MIAs, complicating the task of detecting copyrighted text\nin the pretraining dataset. These results reveal the complex interplay between\ndifferent regulatory measures, which may impact each other in unforeseen ways.\nFinally, we propose an adaptive technique to improve the success rate of a\nrecent MIA under watermarking. Our findings underscore the importance of\ndeveloping adaptive methods to study critical problems in LLMs with potential\nlegal implications."
                },
                "authors": [
                    {
                        "name": "Michael-Andrei Panaitescu-Liess"
                    },
                    {
                        "name": "Zora Che"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Yuancheng Xu"
                    },
                    {
                        "name": "Pankayaraj Pathmanathan"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Sicheng Zhu"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "19 pages, 7 figures. Published at AAAI 2025. Code will be available\n  at https://github.com/michael-panaitescu/watermark_copyright_aaai25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04773v2",
                "updated": "2025-03-10T06:15:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    15,
                    16,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-17T09:52:17Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    9,
                    52,
                    17,
                    0,
                    48,
                    0
                ],
                "title": "Invisible Walls in Cities: Leveraging Large Language Models to Predict\n  Urban Segregation Experience with Social Media Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Walls in Cities: Leveraging Large Language Models to Predict\n  Urban Segregation Experience with Social Media Content"
                },
                "summary": "Understanding experienced segregation in urban daily life is crucial for\naddressing societal inequalities and fostering inclusivity. The abundance of\nuser-generated reviews on social media encapsulates nuanced perceptions and\nfeelings associated with different places, offering rich insights into\nsegregation. However, leveraging this data poses significant challenges due to\nits vast volume, ambiguity, and confluence of diverse perspectives. To tackle\nthese challenges, we propose using Large Language Models (LLMs) to automate\nonline review mining for segregation prediction. We design a Reflective LLM\nCoder to digest social media content into insights consistent with real-world\nfeedback, and eventually produce a codebook capturing key dimensions that\nsignal segregation experience, such as cultural resonance and appeal,\naccessibility and convenience, and community engagement and local involvement.\nGuided by the codebook, LLMs can generate both informative review summaries and\nratings for segregation prediction. Moreover, we design a\nREasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and\nembedding capabilities of language models to integrate multi-channel features\nfor segregation prediction. Experiments on real-world data demonstrate that our\nframework greatly improves prediction accuracy, with a 22.79% elevation in R2\nand a 9.33% reduction in MSE. The derived codebook is generalizable across\nthree different cities, consistently improving prediction accuracy. Moreover,\nour user study confirms that the codebook-guided summaries provide cognitive\ngains for human participants in perceiving POIs' social inclusiveness. Our\nstudy marks an important step toward understanding implicit social barriers and\ninequalities, demonstrating the great potential of promoting social\ninclusiveness with AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding experienced segregation in urban daily life is crucial for\naddressing societal inequalities and fostering inclusivity. The abundance of\nuser-generated reviews on social media encapsulates nuanced perceptions and\nfeelings associated with different places, offering rich insights into\nsegregation. However, leveraging this data poses significant challenges due to\nits vast volume, ambiguity, and confluence of diverse perspectives. To tackle\nthese challenges, we propose using Large Language Models (LLMs) to automate\nonline review mining for segregation prediction. We design a Reflective LLM\nCoder to digest social media content into insights consistent with real-world\nfeedback, and eventually produce a codebook capturing key dimensions that\nsignal segregation experience, such as cultural resonance and appeal,\naccessibility and convenience, and community engagement and local involvement.\nGuided by the codebook, LLMs can generate both informative review summaries and\nratings for segregation prediction. Moreover, we design a\nREasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and\nembedding capabilities of language models to integrate multi-channel features\nfor segregation prediction. Experiments on real-world data demonstrate that our\nframework greatly improves prediction accuracy, with a 22.79% elevation in R2\nand a 9.33% reduction in MSE. The derived codebook is generalizable across\nthree different cities, consistently improving prediction accuracy. Moreover,\nour user study confirms that the codebook-guided summaries provide cognitive\ngains for human participants in perceiving POIs' social inclusiveness. Our\nstudy marks an important step toward understanding implicit social barriers and\ninequalities, demonstrating the great potential of promoting social\ninclusiveness with AI."
                },
                "authors": [
                    {
                        "name": "Bingbing Fan"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Songwei Li"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Pan Hui"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20548v2",
                "updated": "2025-03-10T06:00:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    0,
                    8,
                    0,
                    69,
                    0
                ],
                "published": "2024-09-30T17:49:09Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    17,
                    49,
                    9,
                    0,
                    274,
                    0
                ],
                "title": "Robi Butler: Multimodal Remote Interaction with a Household Robot\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robi Butler: Multimodal Remote Interaction with a Household Robot\n  Assistant"
                },
                "summary": "Imagine a future when we can Zoom-call a robot to manage household chores\nremotely. This work takes one step in this direction. Robi Butler is a new\nhousehold robot assistant that enables seamless multimodal remote interaction.\nIt allows the human user to monitor its environment from a first-person view,\nissue voice or text commands, and specify target objects through hand-pointing\ngestures. At its core, a high-level behavior module, powered by Large Language\nModels (LLMs), interprets multimodal instructions to generate multistep action\nplans. Each plan consists of open-vocabulary primitives supported by\nvision-language models, enabling the robot to process both textual and gestural\ninputs. Zoom provides a convenient interface to implement remote interactions\nbetween the human and the robot. The integration of these components allows\nRobi Butler to ground remote multimodal instructions in real-world home\nenvironments in a zero-shot manner. We evaluated the system on various\nhousehold tasks, demonstrating its ability to execute complex user commands\nwith multimodal inputs. We also conducted a user study to examine how\nmultimodal interaction influences user experiences in remote human-robot\ninteraction. These results suggest that with the advances in robot foundation\nmodels, we are moving closer to the reality of remote household robot\nassistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine a future when we can Zoom-call a robot to manage household chores\nremotely. This work takes one step in this direction. Robi Butler is a new\nhousehold robot assistant that enables seamless multimodal remote interaction.\nIt allows the human user to monitor its environment from a first-person view,\nissue voice or text commands, and specify target objects through hand-pointing\ngestures. At its core, a high-level behavior module, powered by Large Language\nModels (LLMs), interprets multimodal instructions to generate multistep action\nplans. Each plan consists of open-vocabulary primitives supported by\nvision-language models, enabling the robot to process both textual and gestural\ninputs. Zoom provides a convenient interface to implement remote interactions\nbetween the human and the robot. The integration of these components allows\nRobi Butler to ground remote multimodal instructions in real-world home\nenvironments in a zero-shot manner. We evaluated the system on various\nhousehold tasks, demonstrating its ability to execute complex user commands\nwith multimodal inputs. We also conducted a user study to examine how\nmultimodal interaction influences user experiences in remote human-robot\ninteraction. These results suggest that with the advances in robot foundation\nmodels, we are moving closer to the reality of remote household robot\nassistants."
                },
                "authors": [
                    {
                        "name": "Anxing Xiao"
                    },
                    {
                        "name": "Nuwan Janaka"
                    },
                    {
                        "name": "Tianrun Hu"
                    },
                    {
                        "name": "Anshul Gupta"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Cunjun Yu"
                    },
                    {
                        "name": "David Hsu"
                    }
                ],
                "author_detail": {
                    "name": "David Hsu"
                },
                "author": "David Hsu",
                "arxiv_comment": "Accepted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06951v1",
                "updated": "2025-03-10T05:56:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    56,
                    46,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:56:46Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    56,
                    46,
                    0,
                    69,
                    0
                ],
                "title": "ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced\n  Multi-Hop QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced\n  Multi-Hop QA"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly improved\nmulti-hop question answering (QA) through direct Chain-of-Thought (CoT)\nreasoning. However, the irreversible nature of CoT leads to error accumulation,\nmaking it challenging to correct mistakes in multi-hop reasoning. This paper\nintroduces ReAgent: a Reversible multi-Agent collaborative framework augmented\nwith explicit backtracking mechanisms, enabling reversible multi-hop reasoning.\nBy incorporating text-based retrieval, information aggregation and validation,\nour system can detect and correct errors mid-reasoning, leading to more robust\nand interpretable QA outcomes. The framework and experiments serve as a\nfoundation for future work on error-tolerant QA systems. Empirical evaluations\nacross three benchmarks indicate ReAgent's efficacy, yielding average about 6\\%\nimprovements against baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly improved\nmulti-hop question answering (QA) through direct Chain-of-Thought (CoT)\nreasoning. However, the irreversible nature of CoT leads to error accumulation,\nmaking it challenging to correct mistakes in multi-hop reasoning. This paper\nintroduces ReAgent: a Reversible multi-Agent collaborative framework augmented\nwith explicit backtracking mechanisms, enabling reversible multi-hop reasoning.\nBy incorporating text-based retrieval, information aggregation and validation,\nour system can detect and correct errors mid-reasoning, leading to more robust\nand interpretable QA outcomes. The framework and experiments serve as a\nfoundation for future work on error-tolerant QA systems. Empirical evaluations\nacross three benchmarks indicate ReAgent's efficacy, yielding average about 6\\%\nimprovements against baseline models."
                },
                "authors": [
                    {
                        "name": "Zhao Xinjie"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Yuyang Wang"
                    },
                    {
                        "name": "Ying Zhu"
                    },
                    {
                        "name": "Jiacheng Tang"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "25pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06950v1",
                "updated": "2025-03-10T05:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    55,
                    15,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:55:15Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    55,
                    15,
                    0,
                    69,
                    0
                ],
                "title": "CtrlRAG: Black-box Adversarial Attacks Based on Masked Language Models\n  in Retrieval-Augmented Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CtrlRAG: Black-box Adversarial Attacks Based on Masked Language Models\n  in Retrieval-Augmented Language Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by integrating external knowledge bases. However, this integration\nintroduces a new security threat: adversaries can exploit the retrieval\nmechanism to inject malicious content into the knowledge base, thereby\ninfluencing the generated responses. Based on this attack vector, we propose\nCtrlRAG, a novel attack method designed for RAG system in the black-box\nsetting, which aligns with real-world scenarios. Unlike existing attack\nmethods, CtrlRAG introduces a perturbation mechanism using Masked Language\nModel (MLM) to dynamically optimize malicious content in response to changes in\nthe retrieved context. Experimental results demonstrate that CtrlRAG\noutperforms three baseline methods in both Emotional Manipulation and\nHallucination Amplification objectives. Furthermore, we evaluate three existing\ndefense mechanisms, revealing their limited effectiveness against CtrlRAG and\nunderscoring the urgent need for more robust defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by integrating external knowledge bases. However, this integration\nintroduces a new security threat: adversaries can exploit the retrieval\nmechanism to inject malicious content into the knowledge base, thereby\ninfluencing the generated responses. Based on this attack vector, we propose\nCtrlRAG, a novel attack method designed for RAG system in the black-box\nsetting, which aligns with real-world scenarios. Unlike existing attack\nmethods, CtrlRAG introduces a perturbation mechanism using Masked Language\nModel (MLM) to dynamically optimize malicious content in response to changes in\nthe retrieved context. Experimental results demonstrate that CtrlRAG\noutperforms three baseline methods in both Emotional Manipulation and\nHallucination Amplification objectives. Furthermore, we evaluate three existing\ndefense mechanisms, revealing their limited effectiveness against CtrlRAG and\nunderscoring the urgent need for more robust defenses."
                },
                "authors": [
                    {
                        "name": "Runqi Sui"
                    }
                ],
                "author_detail": {
                    "name": "Runqi Sui"
                },
                "author": "Runqi Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06949v2",
                "updated": "2025-03-11T04:58:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    4,
                    58,
                    27,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-10T05:54:23Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    54,
                    23,
                    0,
                    69,
                    0
                ],
                "title": "LexPro-1.0 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexPro-1.0 Technical Report"
                },
                "summary": "In this report, we introduce our first-generation reasoning model,\nLexPro-1.0, a large language model designed for the highly specialized Chinese\nlegal domain, offering comprehensive capabilities to meet diverse realistic\nneeds. Existing legal LLMs face two primary challenges. Firstly, their design\nand evaluation are predominantly driven by computer science perspectives,\nleading to insufficient incorporation of legal expertise and logic, which is\ncrucial for high-precision legal applications, such as handling complex\nprosecutorial tasks. Secondly, these models often underperform due to a lack of\ncomprehensive training data from the legal domain, limiting their ability to\neffectively address real-world legal scenarios. To address this, we first\ncompile millions of legal documents covering over 20 types of crimes from 31\nprovinces in China for model training. From the extensive dataset, we further\nselect high-quality for supervised fine-tuning, ensuring enhanced relevance and\nprecision. The model further undergoes large-scale reinforcement learning\nwithout additional supervision, emphasizing the enhancement of its reasoning\ncapabilities and explainability. To validate its effectiveness in complex legal\napplications, we also conduct human evaluations with legal experts. We develop\nfine-tuned models based on DeepSeek-R1-Distilled versions, available in three\ndense configurations: 14B, 32B, and 70B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we introduce our first-generation reasoning model,\nLexPro-1.0, a large language model designed for the highly specialized Chinese\nlegal domain, offering comprehensive capabilities to meet diverse realistic\nneeds. Existing legal LLMs face two primary challenges. Firstly, their design\nand evaluation are predominantly driven by computer science perspectives,\nleading to insufficient incorporation of legal expertise and logic, which is\ncrucial for high-precision legal applications, such as handling complex\nprosecutorial tasks. Secondly, these models often underperform due to a lack of\ncomprehensive training data from the legal domain, limiting their ability to\neffectively address real-world legal scenarios. To address this, we first\ncompile millions of legal documents covering over 20 types of crimes from 31\nprovinces in China for model training. From the extensive dataset, we further\nselect high-quality for supervised fine-tuning, ensuring enhanced relevance and\nprecision. The model further undergoes large-scale reinforcement learning\nwithout additional supervision, emphasizing the enhancement of its reasoning\ncapabilities and explainability. To validate its effectiveness in complex legal\napplications, we also conduct human evaluations with legal experts. We develop\nfine-tuned models based on DeepSeek-R1-Distilled versions, available in three\ndense configurations: 14B, 32B, and 70B."
                },
                "authors": [
                    {
                        "name": "Haotian Chen"
                    },
                    {
                        "name": "Yanyu Xu"
                    },
                    {
                        "name": "Boyan Wang"
                    },
                    {
                        "name": "Chaoyue Zhao"
                    },
                    {
                        "name": "Xiaoyu Han"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Lizhen Cui"
                    },
                    {
                        "name": "Yonghui Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yonghui Xu"
                },
                "author": "Yonghui Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06948v1",
                "updated": "2025-03-10T05:53:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    53,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:53:30Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    53,
                    30,
                    0,
                    69,
                    0
                ],
                "title": "Large Language Model Guided Progressive Feature Alignment for Multimodal\n  UAV Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Guided Progressive Feature Alignment for Multimodal\n  UAV Object Detection"
                },
                "summary": "Existing multimodal UAV object detection methods often overlook the impact of\nsemantic gaps between modalities, which makes it difficult to achieve accurate\nsemantic and spatial alignments, limiting detection performance. To address\nthis problem, we propose a Large Language Model (LLM) guided Progressive\nfeature Alignment Network called LPANet, which leverages the semantic features\nextracted from a large language model to guide the progressive semantic and\nspatial alignment between modalities for multimodal UAV object detection. To\nemploy the powerful semantic representation of LLM, we generate the\nfine-grained text descriptions of each object category by ChatGPT and then\nextract the semantic features using the large language model MPNet. Based on\nthe semantic features, we guide the semantic and spatial alignments in a\nprogressive manner as follows. First, we design the Semantic Alignment Module\n(SAM) to pull the semantic features and multimodal visual features of each\nobject closer, alleviating the semantic differences of objects between\nmodalities. Second, we design the Explicit Spatial alignment Module (ESM) by\nintegrating the semantic relations into the estimation of feature-level\noffsets, alleviating the coarse spatial misalignment between modalities.\nFinally, we design the Implicit Spatial alignment Module (ISM), which leverages\nthe cross-modal correlations to aggregate key features from neighboring regions\nto achieve implicit spatial alignment. Comprehensive experiments on two public\nmultimodal UAV object detection datasets demonstrate that our approach\noutperforms state-of-the-art multimodal UAV object detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing multimodal UAV object detection methods often overlook the impact of\nsemantic gaps between modalities, which makes it difficult to achieve accurate\nsemantic and spatial alignments, limiting detection performance. To address\nthis problem, we propose a Large Language Model (LLM) guided Progressive\nfeature Alignment Network called LPANet, which leverages the semantic features\nextracted from a large language model to guide the progressive semantic and\nspatial alignment between modalities for multimodal UAV object detection. To\nemploy the powerful semantic representation of LLM, we generate the\nfine-grained text descriptions of each object category by ChatGPT and then\nextract the semantic features using the large language model MPNet. Based on\nthe semantic features, we guide the semantic and spatial alignments in a\nprogressive manner as follows. First, we design the Semantic Alignment Module\n(SAM) to pull the semantic features and multimodal visual features of each\nobject closer, alleviating the semantic differences of objects between\nmodalities. Second, we design the Explicit Spatial alignment Module (ESM) by\nintegrating the semantic relations into the estimation of feature-level\noffsets, alleviating the coarse spatial misalignment between modalities.\nFinally, we design the Implicit Spatial alignment Module (ISM), which leverages\nthe cross-modal correlations to aggregate key features from neighboring regions\nto achieve implicit spatial alignment. Comprehensive experiments on two public\nmultimodal UAV object detection datasets demonstrate that our approach\noutperforms state-of-the-art multimodal UAV object detectors."
                },
                "authors": [
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Chenglong Li"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Bin Luo"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]