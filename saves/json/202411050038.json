[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v2",
                "updated": "2024-11-01T13:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    13,
                    31,
                    39,
                    4,
                    306,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Optimizing Cache Content Placement in Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Cache Content Placement in Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v3",
                "updated": "2024-10-29T10:52:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    52,
                    35,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v1",
                "updated": "2024-10-16T08:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2403.04182v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04182v3",
                "updated": "2024-11-01T17:57:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    57,
                    1,
                    4,
                    306,
                    0
                ],
                "published": "2024-03-07T03:24:34Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    3,
                    24,
                    34,
                    3,
                    67,
                    0
                ],
                "title": "Regression-aware Inference with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression-aware Inference with LLMs"
                },
                "summary": "Large language models (LLMs) have shown strong results on a range of\napplications, including regression and scoring tasks. Typically, one obtains\noutputs from an LLM via autoregressive sampling from the model's output\ndistribution. We show that this inference strategy can be sub-optimal for\ncommon regression and scoring evaluation metrics. As a remedy, we build on\nprior work on Minimum Bayes Risk decoding, and propose alternate inference\nstrategies that estimate the Bayes-optimal solution for regression and scoring\nmetrics in closed-form from sampled responses. We show that our proposal\nsignificantly improves over baselines across datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong results on a range of\napplications, including regression and scoring tasks. Typically, one obtains\noutputs from an LLM via autoregressive sampling from the model's output\ndistribution. We show that this inference strategy can be sub-optimal for\ncommon regression and scoring evaluation metrics. As a remedy, we build on\nprior work on Minimum Bayes Risk decoding, and propose alternate inference\nstrategies that estimate the Bayes-optimal solution for regression and scoring\nmetrics in closed-form from sampled responses. We show that our proposal\nsignificantly improves over baselines across datasets and models."
                },
                "authors": [
                    {
                        "name": "Michal Lukasik"
                    },
                    {
                        "name": "Harikrishna Narasimhan"
                    },
                    {
                        "name": "Aditya Krishna Menon"
                    },
                    {
                        "name": "Felix Yu"
                    },
                    {
                        "name": "Sanjiv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiv Kumar"
                },
                "author": "Sanjiv Kumar",
                "arxiv_comment": "EMNLP Findings 2024",
                "arxiv_journal_ref": "EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04182v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04182v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09583v3",
                "updated": "2024-11-01T17:55:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    55,
                    11,
                    4,
                    306,
                    0
                ],
                "published": "2024-02-14T21:18:11Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    21,
                    18,
                    11,
                    2,
                    45,
                    0
                ],
                "title": "Pochhammer Priors for Sparse Count Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pochhammer Priors for Sparse Count Models"
                },
                "summary": "Bayesian hierarchical models are commonly employed for inference in count\ndatasets, as they account for multiple levels of variation by incorporating\nprior distributions for parameters at different levels. Examples include\nBeta-Binomial, Negative-Binomial (NB), Dirichlet-Multinomial (DM)\ndistributions. In this paper, we address two crucial challenges that arise in\nvarious Bayesian count models: inference for the concentration parameter in the\nratio of Gamma functions and the inability of these models to effectively\nhandle excessive zeros and small nonzero counts. We propose a novel class of\nprior distributions that facilitates conjugate updating of the concentration\nparameter in Gamma ratios, enabling full Bayesian inference for the\naforementioned count distributions. We use DM models as our running examples.\nOur methodology leverages fast residue computation and admits closed-form\nposterior moments. Additionally, we recommend a default horseshoe type prior\nwhich has a heavy tail and substantial mass around zero. It admits continuous\nshrinkage, making the posterior highly adaptable to sparsity or quasi-sparsity\nin the data. Furthermore, we offer insights and potential generalizations to\nother count models facing the two challenges. We demonstrate the usefulness of\nour approach on both simulated examples and on real-world applications.\nFinally, we conclude with directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian hierarchical models are commonly employed for inference in count\ndatasets, as they account for multiple levels of variation by incorporating\nprior distributions for parameters at different levels. Examples include\nBeta-Binomial, Negative-Binomial (NB), Dirichlet-Multinomial (DM)\ndistributions. In this paper, we address two crucial challenges that arise in\nvarious Bayesian count models: inference for the concentration parameter in the\nratio of Gamma functions and the inability of these models to effectively\nhandle excessive zeros and small nonzero counts. We propose a novel class of\nprior distributions that facilitates conjugate updating of the concentration\nparameter in Gamma ratios, enabling full Bayesian inference for the\naforementioned count distributions. We use DM models as our running examples.\nOur methodology leverages fast residue computation and admits closed-form\nposterior moments. Additionally, we recommend a default horseshoe type prior\nwhich has a heavy tail and substantial mass around zero. It admits continuous\nshrinkage, making the posterior highly adaptable to sparsity or quasi-sparsity\nin the data. Furthermore, we offer insights and potential generalizations to\nother count models facing the two challenges. We demonstrate the usefulness of\nour approach on both simulated examples and on real-world applications.\nFinally, we conclude with directions for future research."
                },
                "authors": [
                    {
                        "name": "Yuexi Wang"
                    },
                    {
                        "name": "Nicholas G. Polson"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas G. Polson"
                },
                "author": "Nicholas G. Polson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16016v2",
                "updated": "2024-11-01T17:44:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    44,
                    34,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-24T12:19:31Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    19,
                    31,
                    1,
                    268,
                    0
                ],
                "title": "VascX Models: Model Ensembles for Retinal Vascular Analysis from Color\n  Fundus Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VascX Models: Model Ensembles for Retinal Vascular Analysis from Color\n  Fundus Images"
                },
                "summary": "We introduce VascX models, a comprehensive set of model ensembles for\nanalyzing retinal vasculature from color fundus images (CFIs). Annotated CFIs\nwere aggregated from public datasets . Additional CFIs, mainly from the\npopulation-based Rotterdam Study were annotated by graders for arteries and\nveins at pixel level, resulting in a dataset diverse in patient demographics\nand imaging conditions. VascX models demonstrated superior segmentation\nperformance across datasets, image quality levels, and anatomic regions when\ncompared to existing, publicly available models, likely due to the increased\nsize and variety of our training set. Important improvements were observed in\nartery-vein and disc segmentation performance, particularly in segmentations of\nthese structures on CFIs of intermediate quality, common in large cohorts and\nclinical datasets. Importantly, these improvements translated into\nsignificantly more accurate vascular features when we compared features\nextracted from VascX segmentation masks with features extracted from\nsegmentation masks generated by previous models. With VascX models we provide a\nrobust, ready-to-use set of model ensembles and inference code aimed at\nsimplifying the implementation and enhancing the quality of automated retinal\nvasculature analyses. The precise vessel parameters generated by the model can\nserve as starting points for the identification of disease patterns in and\noutside of the eye.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce VascX models, a comprehensive set of model ensembles for\nanalyzing retinal vasculature from color fundus images (CFIs). Annotated CFIs\nwere aggregated from public datasets . Additional CFIs, mainly from the\npopulation-based Rotterdam Study were annotated by graders for arteries and\nveins at pixel level, resulting in a dataset diverse in patient demographics\nand imaging conditions. VascX models demonstrated superior segmentation\nperformance across datasets, image quality levels, and anatomic regions when\ncompared to existing, publicly available models, likely due to the increased\nsize and variety of our training set. Important improvements were observed in\nartery-vein and disc segmentation performance, particularly in segmentations of\nthese structures on CFIs of intermediate quality, common in large cohorts and\nclinical datasets. Importantly, these improvements translated into\nsignificantly more accurate vascular features when we compared features\nextracted from VascX segmentation masks with features extracted from\nsegmentation masks generated by previous models. With VascX models we provide a\nrobust, ready-to-use set of model ensembles and inference code aimed at\nsimplifying the implementation and enhancing the quality of automated retinal\nvasculature analyses. The precise vessel parameters generated by the model can\nserve as starting points for the identification of disease patterns in and\noutside of the eye."
                },
                "authors": [
                    {
                        "name": "Jose Vargas Quiros"
                    },
                    {
                        "name": "Bart Liefers"
                    },
                    {
                        "name": "Karin van Garderen"
                    },
                    {
                        "name": "Jeroen Vermeulen"
                    },
                    {
                        "name": "Eyened Reading Center"
                    },
                    {
                        "name": "Sinergia Consortium"
                    },
                    {
                        "name": "Caroline Klaver"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Klaver"
                },
                "author": "Caroline Klaver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01721v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01721v3",
                "updated": "2024-11-01T17:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    12,
                    53,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-03T18:27:44Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    27,
                    44,
                    0,
                    155,
                    0
                ],
                "title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs"
                },
                "summary": "Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address Normal\nOutliers, which are activations across all tokens with relatively large\nmagnitudes. However, these methods struggle with smoothing Massive Outliers\nthat display significantly larger values, which leads to significant\nperformance degradation in low-bit quantization. In this paper, we introduce\nDuQuant, a novel approach that utilizes rotation and permutation\ntransformations to more effectively mitigate both massive and normal outliers.\nFirst, DuQuant starts by constructing the rotation matrix, using specific\noutlier dimensions as prior knowledge, to redistribute outliers to adjacent\nchannels by block-wise rotation. Second, We further employ a zigzag permutation\nto balance the distribution of outliers across blocks, thereby reducing\nblock-wise variance. A subsequent rotation further smooths the activation\nlandscape, enhancing model performance. DuQuant simplifies the quantization\nprocess and excels in managing outliers, outperforming the state-of-the-art\nbaselines across various sizes and types of LLMs on multiple tasks, even with\n4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address Normal\nOutliers, which are activations across all tokens with relatively large\nmagnitudes. However, these methods struggle with smoothing Massive Outliers\nthat display significantly larger values, which leads to significant\nperformance degradation in low-bit quantization. In this paper, we introduce\nDuQuant, a novel approach that utilizes rotation and permutation\ntransformations to more effectively mitigate both massive and normal outliers.\nFirst, DuQuant starts by constructing the rotation matrix, using specific\noutlier dimensions as prior knowledge, to redistribute outliers to adjacent\nchannels by block-wise rotation. Second, We further employ a zigzag permutation\nto balance the distribution of outliers across blocks, thereby reducing\nblock-wise variance. A subsequent rotation further smooths the activation\nlandscape, enhancing model performance. DuQuant simplifies the quantization\nprocess and excels in managing outliers, outperforming the state-of-the-art\nbaselines across various sizes and types of LLMs on multiple tasks, even with\n4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant."
                },
                "authors": [
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Haobo Xu"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Jingzhi Cui"
                    },
                    {
                        "name": "Yingtao Zhang"
                    },
                    {
                        "name": "Linzhan Mou"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Zhenan Sun"
                    },
                    {
                        "name": "Ying Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wei"
                },
                "author": "Ying Wei",
                "arxiv_comment": "NeurIPS 2024 Oral, Website at https://duquant.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01721v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01721v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12032v2",
                "updated": "2024-11-01T16:46:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    46,
                    49,
                    4,
                    306,
                    0
                ],
                "published": "2023-09-21T12:53:45Z",
                "published_parsed": [
                    2023,
                    9,
                    21,
                    12,
                    53,
                    45,
                    3,
                    264,
                    0
                ],
                "title": "Human-in-the-Loop Causal Discovery under Latent Confounding using\n  Ancestral GFlowNets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-in-the-Loop Causal Discovery under Latent Confounding using\n  Ancestral GFlowNets"
                },
                "summary": "Structure learning is the crux of causal inference. Notably, causal discovery\n(CD) algorithms are brittle when data is scarce, possibly inferring imprecise\ncausal relations that contradict expert knowledge -- especially when\nconsidering latent confounders. To aggravate the issue, most CD methods do not\nprovide uncertainty estimates, making it hard for users to interpret results\nand improve the inference process. Surprisingly, while CD is a human-centered\naffair, no works have focused on building methods that both 1) output\nuncertainty estimates that can be verified by experts and 2) interact with\nthose experts to iteratively refine CD. To solve these issues, we start by\nproposing to sample (causal) ancestral graphs proportionally to a belief\ndistribution based on a score function, such as the Bayesian information\ncriterion (BIC), using generative flow networks. Then, we leverage the\ndiversity in candidate graphs and introduce an optimal experimental design to\niteratively probe the expert about the relations among variables, effectively\nreducing the uncertainty of our belief over ancestral graphs. Finally, we\nupdate our samples to incorporate human feedback via importance sampling.\nImportantly, our method does not require causal sufficiency (i.e., unobserved\nconfounders may exist). Experiments with synthetic observational data show that\nour method can accurately sample from distributions over ancestral graphs and\nthat we can greatly improve inference quality with human aid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure learning is the crux of causal inference. Notably, causal discovery\n(CD) algorithms are brittle when data is scarce, possibly inferring imprecise\ncausal relations that contradict expert knowledge -- especially when\nconsidering latent confounders. To aggravate the issue, most CD methods do not\nprovide uncertainty estimates, making it hard for users to interpret results\nand improve the inference process. Surprisingly, while CD is a human-centered\naffair, no works have focused on building methods that both 1) output\nuncertainty estimates that can be verified by experts and 2) interact with\nthose experts to iteratively refine CD. To solve these issues, we start by\nproposing to sample (causal) ancestral graphs proportionally to a belief\ndistribution based on a score function, such as the Bayesian information\ncriterion (BIC), using generative flow networks. Then, we leverage the\ndiversity in candidate graphs and introduce an optimal experimental design to\niteratively probe the expert about the relations among variables, effectively\nreducing the uncertainty of our belief over ancestral graphs. Finally, we\nupdate our samples to incorporate human feedback via importance sampling.\nImportantly, our method does not require causal sufficiency (i.e., unobserved\nconfounders may exist). Experiments with synthetic observational data show that\nour method can accurately sample from distributions over ancestral graphs and\nthat we can greatly improve inference quality with human aid."
                },
                "authors": [
                    {
                        "name": "Tiago da Silva"
                    },
                    {
                        "name": "Eliezer Silva"
                    },
                    {
                        "name": "António Góis"
                    },
                    {
                        "name": "Dominik Heider"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Diego Mesquita"
                    },
                    {
                        "name": "Adèle Ribeiro"
                    }
                ],
                "author_detail": {
                    "name": "Adèle Ribeiro"
                },
                "author": "Adèle Ribeiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.12032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19499v2",
                "updated": "2024-11-01T16:45:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    45,
                    29,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-25T11:58:12Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    58,
                    12,
                    4,
                    299,
                    0
                ],
                "title": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization"
                },
                "summary": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs."
                },
                "authors": [
                    {
                        "name": "Anthony Cui"
                    },
                    {
                        "name": "Pranav Nandyalam"
                    },
                    {
                        "name": "Ethan Cheung"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15589v3",
                "updated": "2024-11-01T16:39:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    39,
                    36,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-24T14:20:09Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    14,
                    20,
                    9,
                    4,
                    145,
                    0
                ],
                "title": "Efficient Adversarial Training in LLMs with Continuous Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Adversarial Training in LLMs with Continuous Attacks"
                },
                "summary": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nbypass their safety guardrails. In many domains, adversarial training has\nproven to be one of the most promising methods to reliably improve robustness\nagainst such attacks. Yet, in the context of LLMs, current methods for\nadversarial training are hindered by the high computational costs required to\nperform discrete adversarial attacks at each training iteration. We address\nthis problem by instead calculating adversarial attacks in the continuous\nembedding space of the LLM, which is orders of magnitudes more efficient. We\npropose a fast adversarial training algorithm (C-AdvUL) composed of two losses:\nthe first makes the model robust on continuous embedding attacks computed on an\nadversarial behaviour dataset; the second ensures the usefulness of the final\nmodel by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an\nadversarial variant of IPO that does not require utility data for adversarially\nrobust alignment. Our empirical evaluation on five models from different\nfamilies (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B,\n3.8B, 7B) shows that both algorithms substantially enhance LLM robustness\nagainst discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our\nresults demonstrate that robustness to continuous perturbations can extrapolate\nto discrete threat models. Thereby, we present a path toward scalable\nadversarial training algorithms for robustly aligning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nbypass their safety guardrails. In many domains, adversarial training has\nproven to be one of the most promising methods to reliably improve robustness\nagainst such attacks. Yet, in the context of LLMs, current methods for\nadversarial training are hindered by the high computational costs required to\nperform discrete adversarial attacks at each training iteration. We address\nthis problem by instead calculating adversarial attacks in the continuous\nembedding space of the LLM, which is orders of magnitudes more efficient. We\npropose a fast adversarial training algorithm (C-AdvUL) composed of two losses:\nthe first makes the model robust on continuous embedding attacks computed on an\nadversarial behaviour dataset; the second ensures the usefulness of the final\nmodel by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an\nadversarial variant of IPO that does not require utility data for adversarially\nrobust alignment. Our empirical evaluation on five models from different\nfamilies (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B,\n3.8B, 7B) shows that both algorithms substantially enhance LLM robustness\nagainst discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our\nresults demonstrate that robustness to continuous perturbations can extrapolate\nto discrete threat models. Thereby, we present a path toward scalable\nadversarial training algorithms for robustly aligning LLMs."
                },
                "authors": [
                    {
                        "name": "Sophie Xhonneux"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16121v2",
                "updated": "2024-11-01T16:30:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    30,
                    0,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-23T14:24:14Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    14,
                    24,
                    14,
                    6,
                    175,
                    0
                ],
                "title": "Diffusion Spectral Representation for Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Spectral Representation for Reinforcement Learning"
                },
                "summary": "Diffusion-based models have achieved notable empirical successes in\nreinforcement learning (RL) due to their expressiveness in modeling complex\ndistributions. Despite existing methods being promising, the key challenge of\nextending existing methods for broader real-world applications lies in the\ncomputational cost at inference time, i.e., sampling from a diffusion model is\nconsiderably slow as it often requires tens to hundreds of iterations to\ngenerate even one sample. To circumvent this issue, we propose to leverage the\nflexibility of diffusion models for RL from a representation learning\nperspective. In particular, by exploiting the connection between diffusion\nmodels and energy-based models, we develop Diffusion Spectral Representation\n(Diff-SR), a coherent algorithm framework that enables extracting sufficient\nrepresentations for value functions in Markov decision processes (MDP) and\npartially observable Markov decision processes (POMDP). We further demonstrate\nhow Diff-SR facilitates efficient policy optimization and practical algorithms\nwhile explicitly bypassing the difficulty and inference cost of sampling from\nthe diffusion model. Finally, we provide comprehensive empirical studies to\nverify the benefits of Diff-SR in delivering robust and advantageous\nperformance across various benchmarks with both fully and partially observable\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based models have achieved notable empirical successes in\nreinforcement learning (RL) due to their expressiveness in modeling complex\ndistributions. Despite existing methods being promising, the key challenge of\nextending existing methods for broader real-world applications lies in the\ncomputational cost at inference time, i.e., sampling from a diffusion model is\nconsiderably slow as it often requires tens to hundreds of iterations to\ngenerate even one sample. To circumvent this issue, we propose to leverage the\nflexibility of diffusion models for RL from a representation learning\nperspective. In particular, by exploiting the connection between diffusion\nmodels and energy-based models, we develop Diffusion Spectral Representation\n(Diff-SR), a coherent algorithm framework that enables extracting sufficient\nrepresentations for value functions in Markov decision processes (MDP) and\npartially observable Markov decision processes (POMDP). We further demonstrate\nhow Diff-SR facilitates efficient policy optimization and practical algorithms\nwhile explicitly bypassing the difficulty and inference cost of sampling from\nthe diffusion model. Finally, we provide comprehensive empirical studies to\nverify the benefits of Diff-SR in delivering robust and advantageous\nperformance across various benchmarks with both fully and partially observable\nsettings."
                },
                "authors": [
                    {
                        "name": "Dmitry Shribak"
                    },
                    {
                        "name": "Chen-Xiao Gao"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Chenjun Xiao"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04559v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04559v4",
                "updated": "2024-11-01T16:10:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    10,
                    41,
                    4,
                    306,
                    0
                ],
                "published": "2024-02-07T03:37:19Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    3,
                    37,
                    19,
                    2,
                    38,
                    0
                ],
                "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Model Agents Simulate Human Trust Behavior?"
                },
                "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount."
                },
                "authors": [
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Feiran Jia"
                    },
                    {
                        "name": "Ziyu Ye"
                    },
                    {
                        "name": "Shiyang Lai"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Adel Bibi"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "David Jurgens"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Guohao Li"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Li"
                },
                "author": "Guohao Li",
                "arxiv_comment": "Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04559v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04559v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24198v2",
                "updated": "2024-11-01T16:06:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    6,
                    10,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-31T17:55:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "SelfCodeAlign: Self-Alignment for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfCodeAlign: Self-Alignment for Code Generation"
                },
                "summary": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Yifeng Ding"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Zachary Mueller"
                    },
                    {
                        "name": "Harm de Vries"
                    },
                    {
                        "name": "Leandro von Werra"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19869v2",
                "updated": "2024-11-01T16:02:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    2,
                    47,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-24T00:12:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    0,
                    12,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "Comparing YOLO11 and YOLOv8 for instance segmentation of occluded and\n  non-occluded immature green fruits in complex orchard environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing YOLO11 and YOLOv8 for instance segmentation of occluded and\n  non-occluded immature green fruits in complex orchard environment"
                },
                "summary": "This study conducted a comprehensive performance evaluation on YOLO11 and\nYOLOv8, the latest in the \"You Only Look Once\" (YOLO) series, focusing on their\ninstance segmentation capabilities for immature green apples in orchard\nenvironments. YOLO11n-seg achieved the highest mask precision across all\ncategories with a notable score of 0.831, highlighting its effectiveness in\nfruit detection. YOLO11m-seg and YOLO11l-seg excelled in non-occluded and\noccluded fruitlet segmentation with scores of 0.851 and 0.829, respectively.\nAdditionally, YOLO11x-seg led in mask recall for all categories, achieving a\nscore of 0.815, with YOLO11m-seg performing best for non-occluded immature\ngreen fruitlets at 0.858 and YOLOv8x-seg leading the occluded category with\n0.800. In terms of mean average precision at a 50\\% intersection over union\n(mAP@50), YOLO11m-seg consistently outperformed, registering the highest scores\nfor both box and mask segmentation, at 0.876 and 0.860 for the \"All\" class and\n0.908 and 0.909 for non-occluded immature fruitlets, respectively. YOLO11l-seg\nand YOLOv8l-seg shared the top box mAP@50 for occluded immature fruitlets at\n0.847, while YOLO11m-seg achieved the highest mask mAP@50 of 0.810. Despite the\nadvancements in YOLO11, YOLOv8n surpassed its counterparts in image processing\nspeed, with an impressive inference speed of 3.3 milliseconds, compared to the\nfastest YOLO11 series model at 4.8 milliseconds, underscoring its suitability\nfor real-time agricultural applications related to complex green fruit\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study conducted a comprehensive performance evaluation on YOLO11 and\nYOLOv8, the latest in the \"You Only Look Once\" (YOLO) series, focusing on their\ninstance segmentation capabilities for immature green apples in orchard\nenvironments. YOLO11n-seg achieved the highest mask precision across all\ncategories with a notable score of 0.831, highlighting its effectiveness in\nfruit detection. YOLO11m-seg and YOLO11l-seg excelled in non-occluded and\noccluded fruitlet segmentation with scores of 0.851 and 0.829, respectively.\nAdditionally, YOLO11x-seg led in mask recall for all categories, achieving a\nscore of 0.815, with YOLO11m-seg performing best for non-occluded immature\ngreen fruitlets at 0.858 and YOLOv8x-seg leading the occluded category with\n0.800. In terms of mean average precision at a 50\\% intersection over union\n(mAP@50), YOLO11m-seg consistently outperformed, registering the highest scores\nfor both box and mask segmentation, at 0.876 and 0.860 for the \"All\" class and\n0.908 and 0.909 for non-occluded immature fruitlets, respectively. YOLO11l-seg\nand YOLOv8l-seg shared the top box mAP@50 for occluded immature fruitlets at\n0.847, while YOLO11m-seg achieved the highest mask mAP@50 of 0.810. Despite the\nadvancements in YOLO11, YOLOv8n surpassed its counterparts in image processing\nspeed, with an impressive inference speed of 3.3 milliseconds, compared to the\nfastest YOLO11 series model at 4.8 milliseconds, underscoring its suitability\nfor real-time agricultural applications related to complex green fruit\nenvironments."
                },
                "authors": [
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Manoj Karkee"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Karkee"
                },
                "author": "Manoj Karkee",
                "arxiv_comment": "16 Pages, 10 Figures, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08964v2",
                "updated": "2024-11-01T15:53:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    53,
                    8,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-11T16:32:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    32,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Imbalance Driven Rewarding for Multilingual Self-improving"
                },
                "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11225v2",
                "updated": "2024-11-01T15:51:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    51,
                    56,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-15T03:09:52Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    3,
                    9,
                    52,
                    1,
                    289,
                    0
                ],
                "title": "Statistical Inference in Tensor Completion: Optimal Uncertainty\n  Quantification and Statistical-to-Computational Gaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference in Tensor Completion: Optimal Uncertainty\n  Quantification and Statistical-to-Computational Gaps"
                },
                "summary": "This paper presents a simple yet efficient method for statistical inference\nof tensor linear forms using incomplete and noisy observations. Under the\nTucker low-rank tensor model and the missing-at-random assumption, we utilize\nan appropriate initial estimate along with a debiasing technique followed by a\none-step power iteration to construct an asymptotically normal test statistic.\nThis method is suitable for various statistical inference tasks, including\nconstructing confidence intervals, inference under heteroskedastic and\nsub-exponential noise, and simultaneous testing. We demonstrate that the\nestimator achieves the Cram\\'er-Rao lower bound on Riemannian manifolds,\nindicating its optimality in uncertainty quantification. We comprehensively\nexamine the statistical-to-computational gaps and investigate the impact of\ninitialization on the minimal conditions regarding sample size and\nsignal-to-noise ratio required for accurate inference. Our findings show that\nwith independent initialization, statistically optimal sample sizes and\nsignal-to-noise ratios are sufficient for accurate inference. Conversely, if\nonly dependent initialization is available, computationally optimal sample\nsizes and signal-to-noise ratio conditions still guarantee asymptotic normality\nwithout the need for data-splitting. We present the phase transition between\ncomputational and statistical limits. Numerical simulation results align with\nthe theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a simple yet efficient method for statistical inference\nof tensor linear forms using incomplete and noisy observations. Under the\nTucker low-rank tensor model and the missing-at-random assumption, we utilize\nan appropriate initial estimate along with a debiasing technique followed by a\none-step power iteration to construct an asymptotically normal test statistic.\nThis method is suitable for various statistical inference tasks, including\nconstructing confidence intervals, inference under heteroskedastic and\nsub-exponential noise, and simultaneous testing. We demonstrate that the\nestimator achieves the Cram\\'er-Rao lower bound on Riemannian manifolds,\nindicating its optimality in uncertainty quantification. We comprehensively\nexamine the statistical-to-computational gaps and investigate the impact of\ninitialization on the minimal conditions regarding sample size and\nsignal-to-noise ratio required for accurate inference. Our findings show that\nwith independent initialization, statistically optimal sample sizes and\nsignal-to-noise ratios are sufficient for accurate inference. Conversely, if\nonly dependent initialization is available, computationally optimal sample\nsizes and signal-to-noise ratio conditions still guarantee asymptotic normality\nwithout the need for data-splitting. We present the phase transition between\ncomputational and statistical limits. Numerical simulation results align with\nthe theoretical findings."
                },
                "authors": [
                    {
                        "name": "Wanteng Ma"
                    },
                    {
                        "name": "Dong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Dong Xia"
                },
                "author": "Dong Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18050v2",
                "updated": "2024-11-01T15:36:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    36,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-23T17:24:58Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    24,
                    58,
                    2,
                    297,
                    0
                ],
                "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering"
                },
                "summary": "Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG."
                },
                "authors": [
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Yukuo Cen"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Shicheng Tan"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "EMNLP 2024 Main, Final",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19381v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19381v3",
                "updated": "2024-11-01T14:51:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    51,
                    38,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-28T15:12:55Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    12,
                    55,
                    5,
                    272,
                    0
                ],
                "title": "INC-Math: Integrating Natural Language and Code for Enhanced\n  Mathematical Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INC-Math: Integrating Natural Language and Code for Enhanced\n  Mathematical Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are commonly used to generate solutions for\nmathematical reasoning problems in the following formats: natural language,\ncode, or a combination of both. In this paper, we explore fundamental questions\nrelated to solving mathematical reasoning problems using natural language and\ncode with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.\nOur findings show that LLMs are better at reasoning in natural language\ncompared to code. Additionally, although natural language and code serve as\ncomplementary forms of reasoning, they can affect each other in a negative way\nin certain scenarios. These insights motivate our development of a new\nprompting method, INC-Math, which leverages an LLM to dynamically select the\nmost appropriate reasoning form, resulting in improved performance over\ncomparable baselines with GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are commonly used to generate solutions for\nmathematical reasoning problems in the following formats: natural language,\ncode, or a combination of both. In this paper, we explore fundamental questions\nrelated to solving mathematical reasoning problems using natural language and\ncode with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.\nOur findings show that LLMs are better at reasoning in natural language\ncompared to code. Additionally, although natural language and code serve as\ncomplementary forms of reasoning, they can affect each other in a negative way\nin certain scenarios. These insights motivate our development of a new\nprompting method, INC-Math, which leverages an LLM to dynamically select the\nmost appropriate reasoning form, resulting in improved performance over\ncomparable baselines with GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Xuyuan Xiong"
                    },
                    {
                        "name": "Simeng Han"
                    },
                    {
                        "name": "Ziyue Zhou"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19381v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19381v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.19003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.19003v3",
                "updated": "2024-11-01T14:46:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    46,
                    51,
                    4,
                    306,
                    0
                ],
                "published": "2023-05-30T12:59:29Z",
                "published_parsed": [
                    2023,
                    5,
                    30,
                    12,
                    59,
                    29,
                    1,
                    150,
                    0
                ],
                "title": "Rapid identification of time-frequency domain gravitational wave signals\n  from binary black holes using deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid identification of time-frequency domain gravitational wave signals\n  from binary black holes using deep learning"
                },
                "summary": "Recent developments in deep learning techniques have offered an alternative\nand complementary approach to traditional matched filtering methods for the\nidentification of gravitational wave (GW) signals. The rapid and accurate\nidentification of GW signals is crucial for the progress of GW physics and\nmulti-messenger astronomy, particularly in light of the upcoming fourth and\nfifth observing runs of LIGO-Virgo-KAGRA. In this work, we use the 2D U-Net\nalgorithm to identify the time-frequency domain GW signals from stellar-mass\nbinary black hole (BBH) mergers. We simulate BBH mergers with component masses\nfrom 5 to 80 $M_{\\odot}$ and account for the LIGO detector noise. We find that\nthe GW events in the first and second observation runs could all be clearly and\nrapidly identified. For the third observing run, about $80\\%$ GW events could\nbe identified. In particular, GW190814, currently unknown, is a special case\nthat can be identified by the network, while other binary neutron star mergers\nand neutron star-black hole mergers can not be identified. Compared to the\ntraditional convolutional neural network, the U-Net algorithm can output the\ntime-frequency domain signal images rather than probabilities, providing a more\nintuitive investigation. Moreover, some of the results through U-Net can\nprovide preliminary inference on the chirp mass information. In conclusion, the\nU-Net algorithm can rapidly identify the time-frequency domain GW signals from\nBBH mergers and potentially be helpful for future parameter inferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in deep learning techniques have offered an alternative\nand complementary approach to traditional matched filtering methods for the\nidentification of gravitational wave (GW) signals. The rapid and accurate\nidentification of GW signals is crucial for the progress of GW physics and\nmulti-messenger astronomy, particularly in light of the upcoming fourth and\nfifth observing runs of LIGO-Virgo-KAGRA. In this work, we use the 2D U-Net\nalgorithm to identify the time-frequency domain GW signals from stellar-mass\nbinary black hole (BBH) mergers. We simulate BBH mergers with component masses\nfrom 5 to 80 $M_{\\odot}$ and account for the LIGO detector noise. We find that\nthe GW events in the first and second observation runs could all be clearly and\nrapidly identified. For the third observing run, about $80\\%$ GW events could\nbe identified. In particular, GW190814, currently unknown, is a special case\nthat can be identified by the network, while other binary neutron star mergers\nand neutron star-black hole mergers can not be identified. Compared to the\ntraditional convolutional neural network, the U-Net algorithm can output the\ntime-frequency domain signal images rather than probabilities, providing a more\nintuitive investigation. Moreover, some of the results through U-Net can\nprovide preliminary inference on the chirp mass information. In conclusion, the\nU-Net algorithm can rapidly identify the time-frequency domain GW signals from\nBBH mergers and potentially be helpful for future parameter inferences."
                },
                "authors": [
                    {
                        "name": "Yu-Xin Wang"
                    },
                    {
                        "name": "Shang-Jie Jin"
                    },
                    {
                        "name": "Tian-Yang Sun"
                    },
                    {
                        "name": "Jing-Fei Zhang"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_doi": "10.1088/1674-1137/ad73ac",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1674-1137/ad73ac",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.19003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.19003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 11 figures",
                "arxiv_journal_ref": "Chin. Phys. C 48, no. 12, 125107 (2024)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18760v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18760v4",
                "updated": "2024-11-01T14:37:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    37,
                    37,
                    4,
                    306,
                    0
                ],
                "published": "2023-11-30T18:02:44Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    18,
                    2,
                    44,
                    3,
                    334,
                    0
                ],
                "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaskBench: Benchmarking Large Language Models for Task Automation"
                },
                "summary": "In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents."
                },
                "authors": [
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18760v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18760v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.00352v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.00352v7",
                "updated": "2024-11-01T14:36:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    36,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2023-08-01T07:49:10Z",
                "published_parsed": [
                    2023,
                    8,
                    1,
                    7,
                    49,
                    10,
                    1,
                    213,
                    0
                ],
                "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework"
                },
                "summary": "Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT"
                },
                "authors": [
                    {
                        "name": "Sirui Hong"
                    },
                    {
                        "name": "Mingchen Zhuge"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Yuheng Cheng"
                    },
                    {
                        "name": "Ceyao Zhang"
                    },
                    {
                        "name": "Jinlin Wang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Steven Ka Shing Yau"
                    },
                    {
                        "name": "Zijuan Lin"
                    },
                    {
                        "name": "Liyang Zhou"
                    },
                    {
                        "name": "Chenyu Ran"
                    },
                    {
                        "name": "Lingfeng Xiao"
                    },
                    {
                        "name": "Chenglin Wu"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.00352v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.00352v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00132v2",
                "updated": "2024-11-01T14:36:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    36,
                    49,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-31T18:47:30Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    18,
                    47,
                    30,
                    4,
                    152,
                    0
                ],
                "title": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed\n  Tensor Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed\n  Tensor Adaptation"
                },
                "summary": "We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,\neasy-to-implement, fine-tuning method with no inference overhead for\nlarge-scale pre-trained language models. By leveraging quantum-inspired methods\nderived from quantum circuit structures, QuanTA enables efficient high-rank\nfine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank\napproximation may fail for complicated downstream tasks. Our approach is\ntheoretically supported by the universality theorem and the rank representation\ntheorem to achieve efficient high-rank adaptations. Experiments demonstrate\nthat QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,\nand scalability compared to traditional methods. Furthermore, QuanTA shows\nsuperior performance with fewer trainable parameters compared to other\napproaches and can be designed to integrate with existing fine-tuning\nalgorithms for further improvement, providing a scalable and efficient solution\nfor fine-tuning large language models and advancing state-of-the-art in natural\nlanguage processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,\neasy-to-implement, fine-tuning method with no inference overhead for\nlarge-scale pre-trained language models. By leveraging quantum-inspired methods\nderived from quantum circuit structures, QuanTA enables efficient high-rank\nfine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank\napproximation may fail for complicated downstream tasks. Our approach is\ntheoretically supported by the universality theorem and the rank representation\ntheorem to achieve efficient high-rank adaptations. Experiments demonstrate\nthat QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,\nand scalability compared to traditional methods. Furthermore, QuanTA shows\nsuperior performance with fewer trainable parameters compared to other\napproaches and can be designed to integrate with existing fine-tuning\nalgorithms for further improvement, providing a scalable and efficient solution\nfor fine-tuning large language models and advancing state-of-the-art in natural\nlanguage processing."
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Rumen Dangovski"
                    },
                    {
                        "name": "Charlotte Loh"
                    },
                    {
                        "name": "Owen Dugan"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "Marin Soljačić"
                    }
                ],
                "author_detail": {
                    "name": "Marin Soljačić"
                },
                "author": "Marin Soljačić",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23528v2",
                "updated": "2024-11-01T14:27:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    27,
                    42,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-31T00:29:52Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    29,
                    52,
                    3,
                    305,
                    0
                ],
                "title": "Large Language Models for Patient Comments Multi-Label Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Patient Comments Multi-Label Classification"
                },
                "summary": "Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4 Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4 Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4 Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4 Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses."
                },
                "authors": [
                    {
                        "name": "Hajar Sakai"
                    },
                    {
                        "name": "Sarah S. Lam"
                    },
                    {
                        "name": "Mohammadsadegh Mikaeili"
                    },
                    {
                        "name": "Joshua Bosire"
                    },
                    {
                        "name": "Franziska Jovin"
                    }
                ],
                "author_detail": {
                    "name": "Franziska Jovin"
                },
                "author": "Franziska Jovin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19127v2",
                "updated": "2024-11-01T14:14:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    14,
                    26,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-24T19:56:26Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    56,
                    26,
                    3,
                    298,
                    0
                ],
                "title": "Growth of Large Area WSe$_{2-x}$ and Observation of Photogenerated\n  Inversion Layer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growth of Large Area WSe$_{2-x}$ and Observation of Photogenerated\n  Inversion Layer"
                },
                "summary": "Here, we report the full-fledged journey towards the material synthesis and\ncharacterization of few-layered/thin WSe$_2$ using sputtered W-films on\nSiO$_2$/Si substrates followed by electrical studies under dark and\nillumination conditions. Growth temperature 500oC and gas pressure 55 sccm are\nfound to be the optimized parameters for formation of thermodynamically stable\nWSe$_{2-x}$ with dominant Raman peak at 265 cm-1. XRD and HR-TEM measurement\nclarify the formation of high crystallinity along the c-axis and\nquasi-crystallinity along a and b axes respectively. Lower intensities from\nRaman-measurement and PL-peak at 768 nm (with 532 nm excitation wavelength)\ninfers the thin nature of the grown film, along with strong second harmonic\nemission with excitation wavelength varying from 350nm to 450 nm. This work\nalso retracks the controlled etching by reactive ions to achieve large area\nbi/tri-layer films to fabricate advanced devices. We also have fabricated an\nadvanced MOS structure on SiO$_2$/p-Si substrate which shows tremendous\nperformance by means of photo-capacitance under illumination condition where\nphoto-carriers can survive the higher probe frequencies (> 1MHz). Under\nillumination condition, HfO$_2$/WSe$_2$ embedded MOS shows its dominance\nshowing a huge electron-inversion region over HfO$_2$/ SiO$_2$/p-Si and\nSiO$_2$/p-Si MOS devices even at high frequencies (1-10 MHz). Thereby, this\nwork also reveals a possible route for capacitance based highly sensitive\nphotodetection using conventional Si-technology with integration of such\nWSe$_2$/W as an active material.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here, we report the full-fledged journey towards the material synthesis and\ncharacterization of few-layered/thin WSe$_2$ using sputtered W-films on\nSiO$_2$/Si substrates followed by electrical studies under dark and\nillumination conditions. Growth temperature 500oC and gas pressure 55 sccm are\nfound to be the optimized parameters for formation of thermodynamically stable\nWSe$_{2-x}$ with dominant Raman peak at 265 cm-1. XRD and HR-TEM measurement\nclarify the formation of high crystallinity along the c-axis and\nquasi-crystallinity along a and b axes respectively. Lower intensities from\nRaman-measurement and PL-peak at 768 nm (with 532 nm excitation wavelength)\ninfers the thin nature of the grown film, along with strong second harmonic\nemission with excitation wavelength varying from 350nm to 450 nm. This work\nalso retracks the controlled etching by reactive ions to achieve large area\nbi/tri-layer films to fabricate advanced devices. We also have fabricated an\nadvanced MOS structure on SiO$_2$/p-Si substrate which shows tremendous\nperformance by means of photo-capacitance under illumination condition where\nphoto-carriers can survive the higher probe frequencies (> 1MHz). Under\nillumination condition, HfO$_2$/WSe$_2$ embedded MOS shows its dominance\nshowing a huge electron-inversion region over HfO$_2$/ SiO$_2$/p-Si and\nSiO$_2$/p-Si MOS devices even at high frequencies (1-10 MHz). Thereby, this\nwork also reveals a possible route for capacitance based highly sensitive\nphotodetection using conventional Si-technology with integration of such\nWSe$_2$/W as an active material."
                },
                "authors": [
                    {
                        "name": "Kajal Sharma"
                    },
                    {
                        "name": "Abir Mukherjee"
                    },
                    {
                        "name": "Kritika Bhattacharya"
                    },
                    {
                        "name": "Dhiman Mallick"
                    },
                    {
                        "name": "Samaresh Das"
                    }
                ],
                "author_detail": {
                    "name": "Samaresh Das"
                },
                "author": "Samaresh Das",
                "arxiv_comment": "23, 7",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10691v2",
                "updated": "2024-11-01T14:08:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    8,
                    31,
                    4,
                    306,
                    0
                ],
                "published": "2024-07-15T13:04:09Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    13,
                    4,
                    9,
                    0,
                    197,
                    0
                ],
                "title": "$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity"
                },
                "summary": "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based\nretrievers, respectively, averaged on queries containing multiple subqueries\nfrom five scientific retrieval datasets. Moreover, the efficacy of two\ndownstream scientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The\ncode and experimental datasets are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based\nretrievers, respectively, averaged on queries containing multiple subqueries\nfrom five scientific retrieval datasets. Moreover, the efficacy of two\ndownstream scientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The\ncode and experimental datasets are available."
                },
                "authors": [
                    {
                        "name": "Fengyu Cai"
                    },
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Sihao Chen"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Heinz Koeppl"
                    }
                ],
                "author_detail": {
                    "name": "Heinz Koeppl"
                },
                "author": "Heinz Koeppl",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16020v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16020v3",
                "updated": "2024-11-01T13:59:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    13,
                    59,
                    5,
                    4,
                    306,
                    0
                ],
                "published": "2023-10-24T17:30:26Z",
                "published_parsed": [
                    2023,
                    10,
                    24,
                    17,
                    30,
                    26,
                    1,
                    297,
                    0
                ],
                "title": "ConvBKI: Real-Time Probabilistic Semantic Mapping Network with\n  Quantifiable Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConvBKI: Real-Time Probabilistic Semantic Mapping Network with\n  Quantifiable Uncertainty"
                },
                "summary": "In this paper, we develop a modular neural network for real-time\n{\\color{black}(> 10 Hz)} semantic mapping in uncertain environments, which\nexplicitly updates per-voxel probabilistic distributions within a neural\nnetwork layer. Our approach combines the reliability of classical probabilistic\nalgorithms with the performance and efficiency of modern neural networks.\nAlthough robotic perception is often divided between modern differentiable\nmethods and classical explicit methods, a union of both is necessary for\nreal-time and trustworthy performance. We introduce a novel Convolutional\nBayesian Kernel Inference (ConvBKI) layer which incorporates semantic\nsegmentation predictions online into a 3D map through a depthwise convolution\nlayer by leveraging conjugate priors. We compare ConvBKI against\nstate-of-the-art deep learning approaches and probabilistic algorithms for\nmapping to evaluate reliability and performance. We also create a Robot\nOperating System (ROS) package of ConvBKI and test it on real-world\nperceptually challenging off-road driving data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we develop a modular neural network for real-time\n{\\color{black}(> 10 Hz)} semantic mapping in uncertain environments, which\nexplicitly updates per-voxel probabilistic distributions within a neural\nnetwork layer. Our approach combines the reliability of classical probabilistic\nalgorithms with the performance and efficiency of modern neural networks.\nAlthough robotic perception is often divided between modern differentiable\nmethods and classical explicit methods, a union of both is necessary for\nreal-time and trustworthy performance. We introduce a novel Convolutional\nBayesian Kernel Inference (ConvBKI) layer which incorporates semantic\nsegmentation predictions online into a 3D map through a depthwise convolution\nlayer by leveraging conjugate priors. We compare ConvBKI against\nstate-of-the-art deep learning approaches and probabilistic algorithms for\nmapping to evaluate reliability and performance. We also create a Robot\nOperating System (ROS) package of ConvBKI and test it on real-world\nperceptually challenging off-road driving data."
                },
                "authors": [
                    {
                        "name": "Joey Wilson"
                    },
                    {
                        "name": "Yuewei Fu"
                    },
                    {
                        "name": "Joshua Friesen"
                    },
                    {
                        "name": "Parker Ewen"
                    },
                    {
                        "name": "Andrew Capodieci"
                    },
                    {
                        "name": "Paramsothy Jayakumar"
                    },
                    {
                        "name": "Kira Barton"
                    },
                    {
                        "name": "Maani Ghaffari"
                    }
                ],
                "author_detail": {
                    "name": "Maani Ghaffari"
                },
                "author": "Maani Ghaffari",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2209.10663",
                "arxiv_journal_ref": "10.1109/TRO.2024.3453771",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16020v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16020v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19047v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19047v3",
                "updated": "2024-11-01T13:28:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    13,
                    28,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-02-29T11:20:16Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    11,
                    20,
                    16,
                    3,
                    60,
                    0
                ],
                "title": "Theoretical Foundations of Deep Selective State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Foundations of Deep Selective State-Space Models"
                },
                "summary": "Structured state-space models (SSMs) such as S4, stemming from the seminal\nwork of Gu et al., are gaining popularity as effective approaches for modeling\nsequential data. Deep SSMs demonstrate outstanding performance across a diverse\nset of domains, at a reduced training and inference cost compared to\nattention-based transformers. Recent developments show that if the linear\nrecurrence powering SSMs allows for multiplicative interactions between inputs\nand hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture\ncan surpass in both in accuracy and efficiency attention-powered foundation\nmodels trained on text, at scales of billion parameters. In this paper, we give\ntheoretical grounding to this recent finding using tools from Rough Path\nTheory: we show that when random linear recurrences are equipped with simple\ninput-controlled transitions (selectivity mechanism), then the hidden state is\nprovably a low-dimensional projection of a powerful mathematical object called\nthe signature of the input -- capturing non-linear interactions between tokens\nat distinct timescales. Our theory not only motivates the success of modern\nselective state-space models such as Mamba but also provides a solid framework\nto understand the expressive power of future SSM variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured state-space models (SSMs) such as S4, stemming from the seminal\nwork of Gu et al., are gaining popularity as effective approaches for modeling\nsequential data. Deep SSMs demonstrate outstanding performance across a diverse\nset of domains, at a reduced training and inference cost compared to\nattention-based transformers. Recent developments show that if the linear\nrecurrence powering SSMs allows for multiplicative interactions between inputs\nand hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture\ncan surpass in both in accuracy and efficiency attention-powered foundation\nmodels trained on text, at scales of billion parameters. In this paper, we give\ntheoretical grounding to this recent finding using tools from Rough Path\nTheory: we show that when random linear recurrences are equipped with simple\ninput-controlled transitions (selectivity mechanism), then the hidden state is\nprovably a low-dimensional projection of a powerful mathematical object called\nthe signature of the input -- capturing non-linear interactions between tokens\nat distinct timescales. Our theory not only motivates the success of modern\nselective state-space models such as Mamba but also provides a solid framework\nto understand the expressive power of future SSM variants."
                },
                "authors": [
                    {
                        "name": "Nicola Muca Cirone"
                    },
                    {
                        "name": "Antonio Orvieto"
                    },
                    {
                        "name": "Benjamin Walker"
                    },
                    {
                        "name": "Cristopher Salvi"
                    },
                    {
                        "name": "Terry Lyons"
                    }
                ],
                "author_detail": {
                    "name": "Terry Lyons"
                },
                "author": "Terry Lyons",
                "arxiv_comment": "NeurIPS Version w/ minor edits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19047v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19047v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13845v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13845v3",
                "updated": "2024-11-01T13:25:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    13,
                    25,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-22T17:13:49Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    17,
                    13,
                    49,
                    2,
                    143,
                    0
                ],
                "title": "Semantic Density: Uncertainty Quantification for Large Language Models\n  through Confidence Measurement in Semantic Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Density: Uncertainty Quantification for Large Language Models\n  through Confidence Measurement in Semantic Space"
                },
                "summary": "With the widespread application of Large Language Models (LLMs) to various\ndomains, concerns regarding the trustworthiness of LLMs in safety-critical\nscenarios have been raised, due to their unpredictable tendency to hallucinate\nand generate misinformation. Existing LLMs do not have an inherent\nfunctionality to provide the users with an uncertainty/confidence metric for\neach response it generates, making it difficult to evaluate trustworthiness.\nAlthough several studies aim to develop uncertainty quantification methods for\nLLMs, they have fundamental limitations, such as being restricted to\nclassification tasks, requiring additional training and data, considering only\nlexical instead of semantic information, and being prompt-wise but not\nresponse-wise. A new framework is proposed in this paper to address these\nissues. Semantic density extracts uncertainty/confidence information for each\nresponse from a probability distribution perspective in semantic space. It has\nno restriction on task types and is \"off-the-shelf\" for new models and tasks.\nExperiments on seven state-of-the-art LLMs, including the latest Llama 3 and\nMixtral-8x22B models, on four free-form question-answering benchmarks\ndemonstrate the superior performance and robustness of semantic density\ncompared to prior approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Large Language Models (LLMs) to various\ndomains, concerns regarding the trustworthiness of LLMs in safety-critical\nscenarios have been raised, due to their unpredictable tendency to hallucinate\nand generate misinformation. Existing LLMs do not have an inherent\nfunctionality to provide the users with an uncertainty/confidence metric for\neach response it generates, making it difficult to evaluate trustworthiness.\nAlthough several studies aim to develop uncertainty quantification methods for\nLLMs, they have fundamental limitations, such as being restricted to\nclassification tasks, requiring additional training and data, considering only\nlexical instead of semantic information, and being prompt-wise but not\nresponse-wise. A new framework is proposed in this paper to address these\nissues. Semantic density extracts uncertainty/confidence information for each\nresponse from a probability distribution perspective in semantic space. It has\nno restriction on task types and is \"off-the-shelf\" for new models and tasks.\nExperiments on seven state-of-the-art LLMs, including the latest Llama 3 and\nMixtral-8x22B models, on four free-form question-answering benchmarks\ndemonstrate the superior performance and robustness of semantic density\ncompared to prior approaches."
                },
                "authors": [
                    {
                        "name": "Xin Qiu"
                    },
                    {
                        "name": "Risto Miikkulainen"
                    }
                ],
                "author_detail": {
                    "name": "Risto Miikkulainen"
                },
                "author": "Risto Miikkulainen",
                "arxiv_comment": "Accepted to Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13845v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13845v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10979v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10979v4",
                "updated": "2024-11-01T13:24:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    13,
                    24,
                    50,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-17T01:24:08Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    1,
                    24,
                    8,
                    2,
                    108,
                    0
                ],
                "title": "Scales of Stability and Turbulence in the Molecular ISM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scales of Stability and Turbulence in the Molecular ISM"
                },
                "summary": "We re-analyze the data of the BU-FCRAO $^{13}{\\rm CO}$ Galactic Ring Survey\n(GRS) to understand the dynamics of the turbulent molecular interstellar\nmedium. We define molecular clouds by their spatial half-power contours of\n$^{13}{\\rm CO}$ integrated intensity, independent of a boundary based on\nthresholding or tiling. We find properties of hydrostatic equilibrium (HE) and\nvirial equilibrium (VE), the former independent and the latter dependent on\ntime and spatial scales. We suggest that HE is a stationary property of the\nturbulence and that molecular clouds are high-density regions of a fluctuating\ncomponent. The gravitational and turbulent kinetic energies within clouds are\ncontinuously evolving toward a time-dependent VE with the fluctuating,\nexternal, turbulent pressure energy (PE) that can be treated parametrically\nowing to the shorter time scale for virialization. The average PE is comparable\nto the pressure of the multiphase ISM at the Galactic mid-plane. Larson's\nscaling relations analyzed by different statistical methods are not\nsignificant. The non-dimensional variances of size, line width, and column\ndensity are of comparable magnitude, ruling out the inference of constant\ncolumn density. Previously unrecognized autocorrelations may have contributed\nto the apparent validity of the inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We re-analyze the data of the BU-FCRAO $^{13}{\\rm CO}$ Galactic Ring Survey\n(GRS) to understand the dynamics of the turbulent molecular interstellar\nmedium. We define molecular clouds by their spatial half-power contours of\n$^{13}{\\rm CO}$ integrated intensity, independent of a boundary based on\nthresholding or tiling. We find properties of hydrostatic equilibrium (HE) and\nvirial equilibrium (VE), the former independent and the latter dependent on\ntime and spatial scales. We suggest that HE is a stationary property of the\nturbulence and that molecular clouds are high-density regions of a fluctuating\ncomponent. The gravitational and turbulent kinetic energies within clouds are\ncontinuously evolving toward a time-dependent VE with the fluctuating,\nexternal, turbulent pressure energy (PE) that can be treated parametrically\nowing to the shorter time scale for virialization. The average PE is comparable\nto the pressure of the multiphase ISM at the Galactic mid-plane. Larson's\nscaling relations analyzed by different statistical methods are not\nsignificant. The non-dimensional variances of size, line width, and column\ndensity are of comparable magnitude, ruling out the inference of constant\ncolumn density. Previously unrecognized autocorrelations may have contributed\nto the apparent validity of the inference."
                },
                "authors": [
                    {
                        "name": "Eric Keto"
                    }
                ],
                "author_detail": {
                    "name": "Eric Keto"
                },
                "author": "Eric Keto",
                "arxiv_doi": "10.1002/asna.20240044",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/asna.20240044",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.10979v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10979v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by Astronomische Nachrichten",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15782v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15782v3",
                "updated": "2024-11-01T12:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    53,
                    4,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-24T10:04:53Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    10,
                    4,
                    53,
                    2,
                    115,
                    0
                ],
                "title": "Risk or Chance? Large Language Models and Reproducibility in HCI\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk or Chance? Large Language Models and Reproducibility in HCI\n  Research"
                },
                "summary": "Reproducibility is a major concern across scientific fields. Human-Computer\nInteraction (HCI), in particular, is subject to diverse reproducibility\nchallenges due to the wide range of research methodologies employed. In this\narticle, we explore how the increasing adoption of Large Language Models (LLMs)\nacross all user experience (UX) design and research activities impacts\nreproducibility in HCI. In particular, we review upcoming reproducibility\nchallenges through the lenses of analogies from past to future (mis)practices\nlike p-hacking and prompt-hacking, general bias, support in data analysis,\ndocumentation and education requirements, and possible pressure on the\ncommunity. We discuss the risks and chances for each of these lenses with the\nexpectation that a more comprehensive discussion will help shape best practices\nand contribute to valid and reproducible practices around using LLMs in HCI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducibility is a major concern across scientific fields. Human-Computer\nInteraction (HCI), in particular, is subject to diverse reproducibility\nchallenges due to the wide range of research methodologies employed. In this\narticle, we explore how the increasing adoption of Large Language Models (LLMs)\nacross all user experience (UX) design and research activities impacts\nreproducibility in HCI. In particular, we review upcoming reproducibility\nchallenges through the lenses of analogies from past to future (mis)practices\nlike p-hacking and prompt-hacking, general bias, support in data analysis,\ndocumentation and education requirements, and possible pressure on the\ncommunity. We discuss the risks and chances for each of these lenses with the\nexpectation that a more comprehensive discussion will help shape best practices\nand contribute to valid and reproducible practices around using LLMs in HCI\nresearch."
                },
                "authors": [
                    {
                        "name": "Thomas Kosch"
                    },
                    {
                        "name": "Sebastian Feger"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Feger"
                },
                "author": "Sebastian Feger",
                "arxiv_doi": "10.1145/3695765",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695765",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.15782v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15782v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in ACM Interactions",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15246v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15246v3",
                "updated": "2024-11-01T12:49:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    49,
                    19,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-23T17:42:05Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    42,
                    5,
                    0,
                    267,
                    0
                ],
                "title": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks"
                },
                "summary": "Earth Observation (EO) systems are crucial for cartography, disaster\nsurveillance, and resource administration. Nonetheless, they encounter\nconsiderable obstacles in the processing and transmission of extensive data,\nespecially in specialized domains such as precision agriculture and real-time\ndisaster response. Earth observation satellites, outfitted with remote sensing\ntechnology, gather data from onboard sensors and IoT-enabled terrestrial\nobjects, delivering important information remotely. Domain-adapted Large\nLanguage Models (LLMs) provide a solution by enabling the integration of raw\nand processed EO data. Through domain adaptation, LLMs improve the assimilation\nand analysis of many data sources, tackling the intricacies of specialized\ndatasets in agriculture and disaster response. This data synthesis, directed by\nLLMs, enhances the precision and pertinence of conveyed information. This study\nprovides a thorough examination of using semantic inference and deep learning\nfor sophisticated EO systems. It presents an innovative architecture for\nsemantic communication in EO satellite networks, designed to improve data\ntransmission efficiency using semantic processing methodologies. Recent\nadvancements in onboard processing technologies enable dependable, adaptable,\nand energy-efficient data management in orbit. These improvements guarantee\nreliable performance in adverse space circumstances using radiation-hardened\nand reconfigurable technology. Collectively, these advancements enable\nnext-generation satellite missions with improved processing capabilities,\ncrucial for operational flexibility and real-time decision-making in 6G\nsatellite communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth Observation (EO) systems are crucial for cartography, disaster\nsurveillance, and resource administration. Nonetheless, they encounter\nconsiderable obstacles in the processing and transmission of extensive data,\nespecially in specialized domains such as precision agriculture and real-time\ndisaster response. Earth observation satellites, outfitted with remote sensing\ntechnology, gather data from onboard sensors and IoT-enabled terrestrial\nobjects, delivering important information remotely. Domain-adapted Large\nLanguage Models (LLMs) provide a solution by enabling the integration of raw\nand processed EO data. Through domain adaptation, LLMs improve the assimilation\nand analysis of many data sources, tackling the intricacies of specialized\ndatasets in agriculture and disaster response. This data synthesis, directed by\nLLMs, enhances the precision and pertinence of conveyed information. This study\nprovides a thorough examination of using semantic inference and deep learning\nfor sophisticated EO systems. It presents an innovative architecture for\nsemantic communication in EO satellite networks, designed to improve data\ntransmission efficiency using semantic processing methodologies. Recent\nadvancements in onboard processing technologies enable dependable, adaptable,\nand energy-efficient data management in orbit. These improvements guarantee\nreliable performance in adverse space circumstances using radiation-hardened\nand reconfigurable technology. Collectively, these advancements enable\nnext-generation satellite missions with improved processing capabilities,\ncrucial for operational flexibility and real-time decision-making in 6G\nsatellite communication."
                },
                "authors": [
                    {
                        "name": "Hong-fu Chou"
                    },
                    {
                        "name": "Vu Nguyen Ha"
                    },
                    {
                        "name": "Prabhu Thiruvasagam"
                    },
                    {
                        "name": "Thanh-Dung Le"
                    },
                    {
                        "name": "Geoffrey Eappen"
                    },
                    {
                        "name": "Ti Ti Nguyen"
                    },
                    {
                        "name": "Luis M. Garces-Socarras"
                    },
                    {
                        "name": "Jorge L. Gonzalez-Rios"
                    },
                    {
                        "name": "Juan Carlos Merlano-Duncan"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas",
                "arxiv_comment": "17 pages, 7 figures, Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15246v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15246v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22932v2",
                "updated": "2024-11-01T12:37:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    37,
                    10,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-30T11:38:13Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    38,
                    13,
                    2,
                    304,
                    0
                ],
                "title": "Multi-Agent Large Language Models for Conversational Task-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Large Language Models for Conversational Task-Solving"
                },
                "summary": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs."
                },
                "authors": [
                    {
                        "name": "Jonas Becker"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Becker"
                },
                "author": "Jonas Becker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16405v2",
                "updated": "2024-11-01T12:15:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    15,
                    36,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-26T02:12:02Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    12,
                    2,
                    6,
                    147,
                    0
                ],
                "title": "Intruding with Words: Towards Understanding Graph Injection Attacks at\n  the Text Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intruding with Words: Towards Understanding Graph Injection Attacks at\n  the Text Level"
                },
                "summary": "Graph Neural Networks (GNNs) excel across various applications but remain\nvulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs),\nwhich inject malicious nodes into the original graph and pose realistic\nthreats. Text-attributed graphs (TAGs), where nodes are associated with textual\nfeatures, are crucial due to their prevalence in real-world applications and\nare commonly used to evaluate these vulnerabilities. However, existing research\nonly focuses on embedding-level GIAs, which inject node embeddings rather than\nactual textual content, limiting their applicability and simplifying detection.\nIn this paper, we pioneer the exploration of GIAs at the text level, presenting\nthree novel attack designs that inject textual content into the graph. Through\ntheoretical and empirical analysis, we demonstrate that text interpretability,\na factor previously overlooked at the embedding level, plays a crucial role in\nattack strength. Among the designs we investigate, the Word-frequency-based\nText-level GIA (WTGIA) is particularly notable for its balance between\nperformance and interpretability. Despite the success of WTGIA, we discover\nthat defenders can easily enhance their defenses with customized text embedding\nmethods or large language model (LLM)--based predictors. These insights\nunderscore the necessity for further research into the potential and practical\nsignificance of text-level GIAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) excel across various applications but remain\nvulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs),\nwhich inject malicious nodes into the original graph and pose realistic\nthreats. Text-attributed graphs (TAGs), where nodes are associated with textual\nfeatures, are crucial due to their prevalence in real-world applications and\nare commonly used to evaluate these vulnerabilities. However, existing research\nonly focuses on embedding-level GIAs, which inject node embeddings rather than\nactual textual content, limiting their applicability and simplifying detection.\nIn this paper, we pioneer the exploration of GIAs at the text level, presenting\nthree novel attack designs that inject textual content into the graph. Through\ntheoretical and empirical analysis, we demonstrate that text interpretability,\na factor previously overlooked at the embedding level, plays a crucial role in\nattack strength. Among the designs we investigate, the Word-frequency-based\nText-level GIA (WTGIA) is particularly notable for its balance between\nperformance and interpretability. Despite the success of WTGIA, we discover\nthat defenders can easily enhance their defenses with customized text embedding\nmethods or large language model (LLM)--based predictors. These insights\nunderscore the necessity for further research into the potential and practical\nsignificance of text-level GIAs."
                },
                "authors": [
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Yuchen Ren"
                    },
                    {
                        "name": "Zhewei Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhewei Wei"
                },
                "author": "Zhewei Wei",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10188v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10188v5",
                "updated": "2024-11-01T10:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    10,
                    57,
                    37,
                    4,
                    306,
                    0
                ],
                "published": "2024-08-19T17:48:08Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    48,
                    8,
                    0,
                    232,
                    0
                ],
                "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos"
                },
                "summary": "Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long video supervised fine-tuning. However, training on long\nvideo is computationally and memory intensive. We introduce the long-context\nMulti-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes\nlong video training and inference, enabling 2M context length training on 256\nGPUs without any gradient checkpointing. LongVILA efficiently extends the\nnumber of video frames of VILA from 8 to 2048, improving the long video\ncaptioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy in\n6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%\nwith subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence\nparallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long video supervised fine-tuning. However, training on long\nvideo is computationally and memory intensive. We introduce the long-context\nMulti-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes\nlong video training and inference, enabling 2M context length training on 256\nGPUs without any gradient checkpointing. LongVILA efficiently extends the\nnumber of video frames of VILA from 8 to 2048, improving the long video\ncaptioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy in\n6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%\nwith subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence\nparallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers."
                },
                "authors": [
                    {
                        "name": "Fuzhao Xue"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Ethan He"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Linxi Fan"
                    },
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at\n  https://github.com/NVlabs/VILA/blob/main/LongVILA.md",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10188v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10188v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.08385v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.08385v4",
                "updated": "2024-11-01T10:28:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    10,
                    28,
                    12,
                    4,
                    306,
                    0
                ],
                "published": "2023-11-14T18:48:27Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    18,
                    48,
                    27,
                    1,
                    318,
                    0
                ],
                "title": "Aligning Large Language Models with Human Opinions through Persona\n  Selection and Value--Belief--Norm Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models with Human Opinions through Persona\n  Selection and Value--Belief--Norm Reasoning"
                },
                "summary": "Reasoning and predicting human opinions with large language models (LLMs) is\nessential yet challenging. Current methods employ role-playing with personae\nbut face two major issues: LLMs are sensitive to even a single irrelevant\npersona, skewing predictions by up to 30%, and LLMs fail to reason\nstrategically over personae. We propose Chain-of-Opinion (COO), a simple\nfour-step solution modeling which and how to reason with personae, inspired by\nthe Value--Belief--Norm (VBN) theory. COO differentiates between explicit\npersonae (demographics and ideology) and implicit personae (historical\nopinions), involves: (1) filtering irrelevant attributes from explicit\npersonae, (2) ranking implicit personae into a preferential list for selecting\ntop-k, (3) applying novel VBN reasoning to extract user environmental and\npersonal value, belief, and norm variables for accurate and reliable\npredictions, and (4) iterating VBN reasoning with progressively larger lists of\nimplicit personae to handle potential persona insufficiency. COO efficiently\nachieves new state-of-the-art opinion prediction via prompting with only 5\ninference calls, improving prior techniques by up to 4%. Notably, fine-tuning\nLMs with COO data results in significantly better opinion-aligned models, by up\nto 23%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and predicting human opinions with large language models (LLMs) is\nessential yet challenging. Current methods employ role-playing with personae\nbut face two major issues: LLMs are sensitive to even a single irrelevant\npersona, skewing predictions by up to 30%, and LLMs fail to reason\nstrategically over personae. We propose Chain-of-Opinion (COO), a simple\nfour-step solution modeling which and how to reason with personae, inspired by\nthe Value--Belief--Norm (VBN) theory. COO differentiates between explicit\npersonae (demographics and ideology) and implicit personae (historical\nopinions), involves: (1) filtering irrelevant attributes from explicit\npersonae, (2) ranking implicit personae into a preferential list for selecting\ntop-k, (3) applying novel VBN reasoning to extract user environmental and\npersonal value, belief, and norm variables for accurate and reliable\npredictions, and (4) iterating VBN reasoning with progressively larger lists of\nimplicit personae to handle potential persona insufficiency. COO efficiently\nachieves new state-of-the-art opinion prediction via prompting with only 5\ninference calls, improving prior techniques by up to 4%. Notably, fine-tuning\nLMs with COO data results in significantly better opinion-aligned models, by up\nto 23%."
                },
                "authors": [
                    {
                        "name": "Do Xuan Long"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Min-Yen Kan"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.08385v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.08385v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20778v2",
                "updated": "2024-11-01T09:53:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    53,
                    53,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-28T06:10:12Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    6,
                    10,
                    12,
                    1,
                    149,
                    0
                ],
                "title": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs"
                },
                "summary": "Adversarial prompts generated using gradient-based methods exhibit\noutstanding performance in performing automatic jailbreak attacks against\nsafety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the\ninput gradient of LLMs struggles to precisely reflect the magnitude of loss\nchange that results from token replacements in the prompt, leading to limited\nattack success rates against safety-aligned LLMs, even in the white-box\nsetting. In this paper, we explore a new perspective on this problem,\nsuggesting that it can be alleviated by leveraging innovations inspired in\ntransfer-based attacks that were originally proposed for attacking black-box\nimage classification models. For the first time, we appropriate the ideologies\nof effective methods among these transfer-based attacks, i.e., Skip Gradient\nMethod and Intermediate Level Attack, into gradient-based adversarial prompt\ngeneration and achieve significant performance gains without introducing\nobvious computational cost. Meanwhile, by discussing mechanisms behind the\ngains, new insights are drawn, and proper combinations of these methods are\nalso developed. Our empirical results show that 87% of the query-specific\nadversarial suffixes generated by the developed combination can induce\nLlama-2-7B-Chat to produce the output that exactly matches the target string on\nAdvBench. This match rate is 33% higher than that of a very strong baseline\nknown as GCG, demonstrating advanced discrete optimization for adversarial\nprompt generation against LLMs. In addition, without introducing obvious cost,\nthe combination achieves >30% absolute increase in attack success rates\ncompared with GCG when generating both query-specific (38% -> 68%) and\nuniversal adversarial prompts (26.68% -> 60.32%) for attacking the\nLlama-2-7B-Chat model on AdvBench. Code at:\nhttps://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial prompts generated using gradient-based methods exhibit\noutstanding performance in performing automatic jailbreak attacks against\nsafety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the\ninput gradient of LLMs struggles to precisely reflect the magnitude of loss\nchange that results from token replacements in the prompt, leading to limited\nattack success rates against safety-aligned LLMs, even in the white-box\nsetting. In this paper, we explore a new perspective on this problem,\nsuggesting that it can be alleviated by leveraging innovations inspired in\ntransfer-based attacks that were originally proposed for attacking black-box\nimage classification models. For the first time, we appropriate the ideologies\nof effective methods among these transfer-based attacks, i.e., Skip Gradient\nMethod and Intermediate Level Attack, into gradient-based adversarial prompt\ngeneration and achieve significant performance gains without introducing\nobvious computational cost. Meanwhile, by discussing mechanisms behind the\ngains, new insights are drawn, and proper combinations of these methods are\nalso developed. Our empirical results show that 87% of the query-specific\nadversarial suffixes generated by the developed combination can induce\nLlama-2-7B-Chat to produce the output that exactly matches the target string on\nAdvBench. This match rate is 33% higher than that of a very strong baseline\nknown as GCG, demonstrating advanced discrete optimization for adversarial\nprompt generation against LLMs. In addition, without introducing obvious cost,\nthe combination achieves >30% absolute increase in attack success rates\ncompared with GCG when generating both query-specific (38% -> 68%) and\nuniversal adversarial prompts (26.68% -> 60.32%) for attacking the\nLlama-2-7B-Chat model on AdvBench. Code at:\nhttps://github.com/qizhangli/Gradient-based-Jailbreak-Attacks."
                },
                "authors": [
                    {
                        "name": "Qizhang Li"
                    },
                    {
                        "name": "Yiwen Guo"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14716v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14716v3",
                "updated": "2024-11-01T09:38:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    38,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-11T13:17:19Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    17,
                    19,
                    4,
                    285,
                    0
                ],
                "title": "A Systematic Survey on Large Language Models for Algorithm Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Survey on Large Language Models for Algorithm Design"
                },
                "summary": "Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has seen substantial progress, with applications spanning\noptimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid advancements and expanding scope of this field, a\nsystematic review is both timely and necessary. This paper provides a\nsystematic review of LLM4AD. First, we offer an overview and summary of\nexisting studies. Then, we introduce a taxonomy and review the literature\nacross four dimensions: the roles of LLMs, search methods, prompt methods, and\napplication domains with a discussion of potential and achievements of LLMs in\nAD. Finally, we identify current challenges and highlight several promising\ndirections for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has seen substantial progress, with applications spanning\noptimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid advancements and expanding scope of this field, a\nsystematic review is both timely and necessary. This paper provides a\nsystematic review of LLM4AD. First, we offer an overview and summary of\nexisting studies. Then, we introduce a taxonomy and review the literature\nacross four dimensions: the roles of LLMs, search methods, prompt methods, and\napplication domains with a discussion of potential and achievements of LLMs in\nAD. Finally, we identify current challenges and highlight several promising\ndirections for future research."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yiming Yao"
                    },
                    {
                        "name": "Ping Guo"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14716v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14716v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20358v2",
                "updated": "2024-11-01T09:20:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    20,
                    53,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-27T07:19:39Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    7,
                    19,
                    39,
                    6,
                    301,
                    0
                ],
                "title": "RopeTP: Global Human Motion Recovery via Integrating Robust Pose\n  Estimation with Diffusion Trajectory Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RopeTP: Global Human Motion Recovery via Integrating Robust Pose\n  Estimation with Diffusion Trajectory Prior"
                },
                "summary": "We present RopeTP, a novel framework that combines Robust pose estimation\nwith a diffusion Trajectory Prior to reconstruct global human motion from\nvideos. At the heart of RopeTP is a hierarchical attention mechanism that\nsignificantly improves context awareness, which is essential for accurately\ninferring the posture of occluded body parts. This is achieved by exploiting\nthe relationships with visible anatomical structures, enhancing the accuracy of\nlocal pose estimations. The improved robustness of these local estimations\nallows for the reconstruction of precise and stable global trajectories.\nAdditionally, RopeTP incorporates a diffusion trajectory model that predicts\nrealistic human motion from local pose sequences. This model ensures that the\ngenerated trajectories are not only consistent with observed local actions but\nalso unfold naturally over time, thereby improving the realism and stability of\n3D human motion reconstruction. Extensive experimental validation shows that\nRopeTP surpasses current methods on two benchmark datasets, particularly\nexcelling in scenarios with occlusions. It also outperforms methods that rely\non SLAM for initial camera estimates and extensive optimization, delivering\nmore accurate and realistic trajectories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RopeTP, a novel framework that combines Robust pose estimation\nwith a diffusion Trajectory Prior to reconstruct global human motion from\nvideos. At the heart of RopeTP is a hierarchical attention mechanism that\nsignificantly improves context awareness, which is essential for accurately\ninferring the posture of occluded body parts. This is achieved by exploiting\nthe relationships with visible anatomical structures, enhancing the accuracy of\nlocal pose estimations. The improved robustness of these local estimations\nallows for the reconstruction of precise and stable global trajectories.\nAdditionally, RopeTP incorporates a diffusion trajectory model that predicts\nrealistic human motion from local pose sequences. This model ensures that the\ngenerated trajectories are not only consistent with observed local actions but\nalso unfold naturally over time, thereby improving the realism and stability of\n3D human motion reconstruction. Extensive experimental validation shows that\nRopeTP surpasses current methods on two benchmark datasets, particularly\nexcelling in scenarios with occlusions. It also outperforms methods that rely\non SLAM for initial camera estimates and extensive optimization, delivering\nmore accurate and realistic trajectories."
                },
                "authors": [
                    {
                        "name": "Mingjiang Liang"
                    },
                    {
                        "name": "Yongkang Cheng"
                    },
                    {
                        "name": "Hualin Liang"
                    },
                    {
                        "name": "Shaoli Huang"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "arxiv_comment": "Accepted by WACV 2025 (Round 1)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05019v2",
                "updated": "2024-11-01T08:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    55,
                    43,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-07T17:17:23Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    17,
                    17,
                    23,
                    6,
                    98,
                    0
                ],
                "title": "Shortcut-connected Expert Parallelism for Accelerating\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shortcut-connected Expert Parallelism for Accelerating\n  Mixture-of-Experts"
                },
                "summary": "Expert parallelism has been introduced as a strategy to distribute the\ncomputational workload of sparsely-gated mixture-of-experts (MoE) models across\nmultiple computing devices, facilitating the execution of these increasingly\nlarge-scale models. However, the All-to-All communication intrinsic to expert\nparallelism constitutes a significant overhead, diminishing the MoE models'\nefficiency. Current optimization approaches offer some relief, yet they are\nconstrained by the sequential interdependence of communication and computation\noperations. To address this limitation, we present a novel shortcut-connected\nMoE (ScMoE) architecture with an overlapping parallel strategy, which\neffectively decouples communication from its conventional sequence, allowing\nfor a substantial overlap of 70% to 100% with computation. When compared with\nthe prevalent top-2 MoE architecture, ScMoE demonstrates training speed\nimprovements of 30% and 11%, and inference improvements of 40% and 15%, in our\ndistributed environments with PCIe and NVLink hardware, respectively, where\ncommunication constitutes 60% and 15% of the total MoE time consumption.\nBuilding on the ScMoE architecture, we further implement an expert offloading\nstrategy to facilitate memory-limited inference, optimizing latency through the\noverlap of expert migration. Additionally, extensive experiments and\ntheoretical analyses indicate that ScMoE not only achieves comparable but in\nsome instances surpasses the model quality of existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert parallelism has been introduced as a strategy to distribute the\ncomputational workload of sparsely-gated mixture-of-experts (MoE) models across\nmultiple computing devices, facilitating the execution of these increasingly\nlarge-scale models. However, the All-to-All communication intrinsic to expert\nparallelism constitutes a significant overhead, diminishing the MoE models'\nefficiency. Current optimization approaches offer some relief, yet they are\nconstrained by the sequential interdependence of communication and computation\noperations. To address this limitation, we present a novel shortcut-connected\nMoE (ScMoE) architecture with an overlapping parallel strategy, which\neffectively decouples communication from its conventional sequence, allowing\nfor a substantial overlap of 70% to 100% with computation. When compared with\nthe prevalent top-2 MoE architecture, ScMoE demonstrates training speed\nimprovements of 30% and 11%, and inference improvements of 40% and 15%, in our\ndistributed environments with PCIe and NVLink hardware, respectively, where\ncommunication constitutes 60% and 15% of the total MoE time consumption.\nBuilding on the ScMoE architecture, we further implement an expert offloading\nstrategy to facilitate memory-limited inference, optimizing latency through the\noverlap of expert migration. Additionally, extensive experiments and\ntheoretical analyses indicate that ScMoE not only achieves comparable but in\nsome instances surpasses the model quality of existing approaches."
                },
                "authors": [
                    {
                        "name": "Weilin Cai"
                    },
                    {
                        "name": "Juyong Jiang"
                    },
                    {
                        "name": "Le Qin"
                    },
                    {
                        "name": "Junwei Cui"
                    },
                    {
                        "name": "Sunghun Kim"
                    },
                    {
                        "name": "Jiayi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Huang"
                },
                "author": "Jiayi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.08648v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.08648v4",
                "updated": "2024-11-01T08:06:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    6,
                    25,
                    4,
                    306,
                    0
                ],
                "published": "2023-09-15T13:15:54Z",
                "published_parsed": [
                    2023,
                    9,
                    15,
                    13,
                    15,
                    54,
                    4,
                    258,
                    0
                ],
                "title": "MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings"
                },
                "summary": "In recent years, predicting mobile app usage has become increasingly\nimportant for areas like app recommendation, user behaviour analysis, and\nmobile resource management. Existing models, however, struggle with the\nheterogeneous nature of contextual data and the user cold start problem. This\nstudy introduces a novel prediction model, Mobile App Prediction Leveraging\nLarge Language Model Embeddings (MAPLE), which employs Large Language Models\n(LLMs) and installed app similarity to overcome these challenges. MAPLE\nutilises the power of LLMs to process contextual data and discern intricate\nrelationships within it effectively. Additionally, we explore the use of\ninstalled app similarity to address the cold start problem, facilitating the\nmodelling of user preferences and habits, even for new users with limited\nhistorical data. In essence, our research presents MAPLE as a novel, potent,\nand practical approach to app usage prediction, making significant strides in\nresolving issues faced by existing models. MAPLE stands out as a comprehensive\nand effective solution, setting a new benchmark for more precise and\npersonalised app usage predictions. In tests on two real-world datasets, MAPLE\nsurpasses contemporary models in both standard and cold start scenarios. These\noutcomes validate MAPLE's capacity for precise app usage predictions and its\nresilience against the cold start problem. This enhanced performance stems from\nthe model's proficiency in capturing complex temporal patterns and leveraging\ncontextual information. As a result, MAPLE can potentially improve personalised\nmobile app usage predictions and user experiences markedly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, predicting mobile app usage has become increasingly\nimportant for areas like app recommendation, user behaviour analysis, and\nmobile resource management. Existing models, however, struggle with the\nheterogeneous nature of contextual data and the user cold start problem. This\nstudy introduces a novel prediction model, Mobile App Prediction Leveraging\nLarge Language Model Embeddings (MAPLE), which employs Large Language Models\n(LLMs) and installed app similarity to overcome these challenges. MAPLE\nutilises the power of LLMs to process contextual data and discern intricate\nrelationships within it effectively. Additionally, we explore the use of\ninstalled app similarity to address the cold start problem, facilitating the\nmodelling of user preferences and habits, even for new users with limited\nhistorical data. In essence, our research presents MAPLE as a novel, potent,\nand practical approach to app usage prediction, making significant strides in\nresolving issues faced by existing models. MAPLE stands out as a comprehensive\nand effective solution, setting a new benchmark for more precise and\npersonalised app usage predictions. In tests on two real-world datasets, MAPLE\nsurpasses contemporary models in both standard and cold start scenarios. These\noutcomes validate MAPLE's capacity for precise app usage predictions and its\nresilience against the cold start problem. This enhanced performance stems from\nthe model's proficiency in capturing complex temporal patterns and leveraging\ncontextual information. As a result, MAPLE can potentially improve personalised\nmobile app usage predictions and user experiences markedly."
                },
                "authors": [
                    {
                        "name": "Yonchanok Khaokaew"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_doi": "10.1145/3643514",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3643514",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.08648v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.08648v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13752v3",
                "updated": "2024-11-01T07:51:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    51,
                    36,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-21T19:24:15Z",
                "published_parsed": [
                    2024,
                    4,
                    21,
                    19,
                    24,
                    15,
                    6,
                    112,
                    0
                ],
                "title": "Adversarial Representation Engineering: A General Model Editing\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Representation Engineering: A General Model Editing\n  Framework for Large Language Models"
                },
                "summary": "Since the rapid development of Large Language Models (LLMs) has achieved\nremarkable success, understanding and rectifying their internal complex\nmechanisms has become an urgent issue. Recent research has attempted to\ninterpret their behaviors through the lens of inner representation. However,\ndeveloping practical and efficient methods for applying these representations\nfor general and flexible model editing remains challenging. In this work, we\nexplore how to leverage insights from representation engineering to guide the\nediting of LLMs by deploying a representation sensor as an editing oracle. We\nfirst identify the importance of a robust and reliable sensor during editing,\nthen propose an Adversarial Representation Engineering (ARE) framework to\nprovide a unified and interpretable approach for conceptual model editing\nwithout compromising baseline performance. Experiments on multiple tasks\ndemonstrate the effectiveness of ARE in various model editing scenarios. Our\ncode and data are available at\nhttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the rapid development of Large Language Models (LLMs) has achieved\nremarkable success, understanding and rectifying their internal complex\nmechanisms has become an urgent issue. Recent research has attempted to\ninterpret their behaviors through the lens of inner representation. However,\ndeveloping practical and efficient methods for applying these representations\nfor general and flexible model editing remains challenging. In this work, we\nexplore how to leverage insights from representation engineering to guide the\nediting of LLMs by deploying a representation sensor as an editing oracle. We\nfirst identify the importance of a robust and reliable sensor during editing,\nthen propose an Adversarial Representation Engineering (ARE) framework to\nprovide a unified and interpretable approach for conceptual model editing\nwithout compromising baseline performance. Experiments on multiple tasks\ndemonstrate the effectiveness of ARE in various model editing scenarios. Our\ncode and data are available at\nhttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering."
                },
                "authors": [
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Meng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Meng Sun"
                },
                "author": "Meng Sun",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v4",
                "updated": "2024-11-01T07:41:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    41,
                    4,
                    4,
                    306,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder)."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01475v2",
                "updated": "2024-11-01T07:05:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    5,
                    33,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-01T20:56:25Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    20,
                    56,
                    25,
                    0,
                    92,
                    0
                ],
                "title": "Are large language models superhuman chemists?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are large language models superhuman chemists?"
                },
                "summary": "Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained.\n  However, we possess only a limited systematic understanding of the chemical\ncapabilities of LLMs, which would be required to improve models and mitigate\npotential harm. Here, we introduce \"ChemBench,\" an automated framework for\nevaluating the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of chemists.\n  We curated more than 2,700 question-answer pairs, evaluated leading open- and\nclosed-source LLMs, and found that the best models outperformed the best human\nchemists in our study on average. However, the models struggle with some basic\ntasks and provide overconfident predictions.\n  These findings reveal LLMs' impressive chemical capabilities while\nemphasizing the need for further research to improve their safety and\nusefulness. They also suggest adapting chemistry education and show the value\nof benchmarking frameworks for evaluating LLMs in specific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained.\n  However, we possess only a limited systematic understanding of the chemical\ncapabilities of LLMs, which would be required to improve models and mitigate\npotential harm. Here, we introduce \"ChemBench,\" an automated framework for\nevaluating the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of chemists.\n  We curated more than 2,700 question-answer pairs, evaluated leading open- and\nclosed-source LLMs, and found that the best models outperformed the best human\nchemists in our study on average. However, the models struggle with some basic\ntasks and provide overconfident predictions.\n  These findings reveal LLMs' impressive chemical capabilities while\nemphasizing the need for further research to improve their safety and\nusefulness. They also suggest adapting chemistry education and show the value\nof benchmarking frameworks for evaluating LLMs in specific domains."
                },
                "authors": [
                    {
                        "name": "Adrian Mirza"
                    },
                    {
                        "name": "Nawaf Alampara"
                    },
                    {
                        "name": "Sreekanth Kunchapu"
                    },
                    {
                        "name": "Martiño Ríos-García"
                    },
                    {
                        "name": "Benedict Emoekabu"
                    },
                    {
                        "name": "Aswanth Krishnan"
                    },
                    {
                        "name": "Tanya Gupta"
                    },
                    {
                        "name": "Mara Schilling-Wilhelmi"
                    },
                    {
                        "name": "Macjonathan Okereke"
                    },
                    {
                        "name": "Anagha Aneesh"
                    },
                    {
                        "name": "Amir Mohammad Elahi"
                    },
                    {
                        "name": "Mehrdad Asgari"
                    },
                    {
                        "name": "Juliane Eberhardt"
                    },
                    {
                        "name": "Hani M. Elbeheiry"
                    },
                    {
                        "name": "María Victoria Gil"
                    },
                    {
                        "name": "Maximilian Greiner"
                    },
                    {
                        "name": "Caroline T. Holick"
                    },
                    {
                        "name": "Christina Glaubitz"
                    },
                    {
                        "name": "Tim Hoffmann"
                    },
                    {
                        "name": "Abdelrahman Ibrahim"
                    },
                    {
                        "name": "Lea C. Klepsch"
                    },
                    {
                        "name": "Yannik Köster"
                    },
                    {
                        "name": "Fabian Alexander Kreth"
                    },
                    {
                        "name": "Jakob Meyer"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Jan Matthias Peschel"
                    },
                    {
                        "name": "Michael Ringleb"
                    },
                    {
                        "name": "Nicole Roesner"
                    },
                    {
                        "name": "Johanna Schreiber"
                    },
                    {
                        "name": "Ulrich S. Schubert"
                    },
                    {
                        "name": "Leanne M. Stafast"
                    },
                    {
                        "name": "Dinga Wonanke"
                    },
                    {
                        "name": "Michael Pieler"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Kevin Maik Jablonka"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Maik Jablonka"
                },
                "author": "Kevin Maik Jablonka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15665v2",
                "updated": "2024-11-01T06:57:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    57,
                    43,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-21T06:09:30Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    6,
                    9,
                    30,
                    0,
                    295,
                    0
                ],
                "title": "Long Term Memory: The Foundation of AI Self-Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Term Memory: The Foundation of AI Self-Evolution"
                },
                "summary": "Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications."
                },
                "authors": [
                    {
                        "name": "Xun Jiang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Jiaying Wang"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shu Zhang"
                    },
                    {
                        "name": "Weiling Chen"
                    },
                    {
                        "name": "Xavier Tang"
                    },
                    {
                        "name": "Yize Chen"
                    },
                    {
                        "name": "Mengyue Wu"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Tianqiao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqiao Chen"
                },
                "author": "Tianqiao Chen",
                "arxiv_comment": "56 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11406v3",
                "updated": "2024-11-01T06:25:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    25,
                    15,
                    4,
                    306,
                    0
                ],
                "published": "2024-07-16T05:48:24Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    5,
                    48,
                    24,
                    1,
                    198,
                    0
                ],
                "title": "Revisiting the Impact of Pursuing Modularity for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Impact of Pursuing Modularity for Code Generation"
                },
                "summary": "Modular programming, which aims to construct the final program by integrating\nsmaller, independent building blocks, has been regarded as a desirable practice\nin software development. However, with the rise of recent code generation\nagents built upon large language models (LLMs), a question emerges: is this\ntraditional practice equally effective for these new tools? In this work, we\nassess the impact of modularity in code generation by introducing a novel\nmetric for its quantitative measurement. Surprisingly, unlike conventional\nwisdom on the topic, we find that modularity is not a core factor for improving\nthe performance of code generation models. We also explore potential\nexplanations for why LLMs do not exhibit a preference for modular code compared\nto non-modular code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular programming, which aims to construct the final program by integrating\nsmaller, independent building blocks, has been regarded as a desirable practice\nin software development. However, with the rise of recent code generation\nagents built upon large language models (LLMs), a question emerges: is this\ntraditional practice equally effective for these new tools? In this work, we\nassess the impact of modularity in code generation by introducing a novel\nmetric for its quantitative measurement. Surprisingly, unlike conventional\nwisdom on the topic, we find that modularity is not a core factor for improving\nthe performance of code generation models. We also explore potential\nexplanations for why LLMs do not exhibit a preference for modular code compared\nto non-modular code."
                },
                "authors": [
                    {
                        "name": "Deokyeong Kang"
                    },
                    {
                        "name": "Ki Jung Seo"
                    },
                    {
                        "name": "Taeuk Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taeuk Kim"
                },
                "author": "Taeuk Kim",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20468v2",
                "updated": "2024-11-01T06:21:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    21,
                    26,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-27T15:08:54Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    15,
                    8,
                    54,
                    6,
                    301,
                    0
                ],
                "title": "Understanding Communication Preferences of Information Workers in\n  Engagement with Text-Based Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Communication Preferences of Information Workers in\n  Engagement with Text-Based Conversational Agents"
                },
                "summary": "Communication traits in text-based human-AI conversations play pivotal roles\nin shaping user experiences and perceptions of systems. With the advancement of\nlarge language models (LLMs), it is now feasible to analyze these traits at a\nmore granular level. In this study, we explore the preferences of information\nworkers regarding chatbot communication traits across seven applications.\nParticipants were invited to participate in an interactive survey, which\nfeatured adjustable sliders, allowing them to adjust and express their\npreferences for five key communication traits: formality, personification,\nempathy, sociability, and humor. Our findings reveal distinct communication\npreferences across different applications; for instance, there was a preference\nfor relatively high empathy in wellbeing contexts and relatively low\npersonification in coding. Similarities in preferences were also noted between\napplications such as chatbots for customer service and scheduling. These\ninsights offer crucial design guidelines for future chatbots, emphasizing the\nneed for nuanced trait adjustments for each application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication traits in text-based human-AI conversations play pivotal roles\nin shaping user experiences and perceptions of systems. With the advancement of\nlarge language models (LLMs), it is now feasible to analyze these traits at a\nmore granular level. In this study, we explore the preferences of information\nworkers regarding chatbot communication traits across seven applications.\nParticipants were invited to participate in an interactive survey, which\nfeatured adjustable sliders, allowing them to adjust and express their\npreferences for five key communication traits: formality, personification,\nempathy, sociability, and humor. Our findings reveal distinct communication\npreferences across different applications; for instance, there was a preference\nfor relatively high empathy in wellbeing contexts and relatively low\npersonification in coding. Similarities in preferences were also noted between\napplications such as chatbots for customer service and scheduling. These\ninsights offer crucial design guidelines for future chatbots, emphasizing the\nneed for nuanced trait adjustments for each application."
                },
                "authors": [
                    {
                        "name": "Ananya Bhattacharjee"
                    },
                    {
                        "name": "Jina Suh"
                    },
                    {
                        "name": "Mahsa Ershadi"
                    },
                    {
                        "name": "Shamsi T. Iqbal"
                    },
                    {
                        "name": "Andrew D. Wilson"
                    },
                    {
                        "name": "Javier Hernandez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Hernandez"
                },
                "author": "Javier Hernandez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14155v2",
                "updated": "2024-11-01T06:19:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    19,
                    47,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-18T03:45:42Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    45,
                    42,
                    4,
                    292,
                    0
                ],
                "title": "Towards Faithful Natural Language Explanations: A Study Using Activation\n  Patching in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Faithful Natural Language Explanations: A Study Using Activation\n  Patching in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}"
                },
                "authors": [
                    {
                        "name": "Wei Jie Yeo"
                    },
                    {
                        "name": "Ranjan Satapathy"
                    },
                    {
                        "name": "Erik Cambria"
                    }
                ],
                "author_detail": {
                    "name": "Erik Cambria"
                },
                "author": "Erik Cambria",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15677v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15677v3",
                "updated": "2024-11-01T06:19:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    19,
                    24,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-24T16:17:35Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    16,
                    17,
                    35,
                    4,
                    145,
                    0
                ],
                "title": "SMART: Scalable Multi-agent Real-time Motion Generation via Next-token\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMART: Scalable Multi-agent Real-time Motion Generation via Next-token\n  Prediction"
                },
                "summary": "Data-driven autonomous driving motion generation tasks are frequently\nimpacted by the limitations of dataset size and the domain gap between\ndatasets, which precludes their extensive application in real-world scenarios.\nTo address this issue, we introduce SMART, a novel autonomous driving motion\ngeneration paradigm that models vectorized map and agent trajectory data into\ndiscrete sequence tokens. These tokens are then processed through a\ndecoder-only transformer architecture to train for the next token prediction\ntask across spatial-temporal series. This GPT-style method allows the model to\nlearn the motion distribution in real driving scenarios. SMART achieves\nstate-of-the-art performance across most of the metrics on the generative Sim\nAgents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset\n(WOMD), demonstrating remarkable inference speed. Moreover, SMART represents\nthe generative model in the autonomous driving motion domain, exhibiting\nzero-shot generalization capabilities: Using only the NuPlan dataset for\ntraining and WOMD for validation, SMART achieved a competitive score of 0.72 on\nthe Sim Agents challenge. Lastly, we have collected over 1 billion motion\ntokens from multiple datasets, validating the model's scalability. These\nresults suggest that SMART has initially emulated two important properties:\nscalability and zero-shot generalization, and preliminarily meets the needs of\nlarge-scale real-time simulation applications. We have released all the code to\npromote the exploration of models for motion generation in the autonomous\ndriving field. The source code is available at\nhttps://github.com/rainmaker22/SMART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven autonomous driving motion generation tasks are frequently\nimpacted by the limitations of dataset size and the domain gap between\ndatasets, which precludes their extensive application in real-world scenarios.\nTo address this issue, we introduce SMART, a novel autonomous driving motion\ngeneration paradigm that models vectorized map and agent trajectory data into\ndiscrete sequence tokens. These tokens are then processed through a\ndecoder-only transformer architecture to train for the next token prediction\ntask across spatial-temporal series. This GPT-style method allows the model to\nlearn the motion distribution in real driving scenarios. SMART achieves\nstate-of-the-art performance across most of the metrics on the generative Sim\nAgents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset\n(WOMD), demonstrating remarkable inference speed. Moreover, SMART represents\nthe generative model in the autonomous driving motion domain, exhibiting\nzero-shot generalization capabilities: Using only the NuPlan dataset for\ntraining and WOMD for validation, SMART achieved a competitive score of 0.72 on\nthe Sim Agents challenge. Lastly, we have collected over 1 billion motion\ntokens from multiple datasets, validating the model's scalability. These\nresults suggest that SMART has initially emulated two important properties:\nscalability and zero-shot generalization, and preliminarily meets the needs of\nlarge-scale real-time simulation applications. We have released all the code to\npromote the exploration of models for motion generation in the autonomous\ndriving field. The source code is available at\nhttps://github.com/rainmaker22/SMART."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Xiaoxin Feng"
                    },
                    {
                        "name": "Ziyan Gao"
                    },
                    {
                        "name": "Yuheng Kan"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Kan"
                },
                "author": "Yuheng Kan",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15677v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15677v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16247v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16247v3",
                "updated": "2024-11-01T06:13:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    13,
                    12,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-25T14:11:44Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    14,
                    11,
                    44,
                    5,
                    146,
                    0
                ],
                "title": "AutoManual: Generating Instruction Manuals by LLM Agents via Interactive\n  Environmental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoManual: Generating Instruction Manuals by LLM Agents via Interactive\n  Environmental Learning"
                },
                "summary": "Large Language Models (LLM) based agents have shown promise in autonomously\ncompleting tasks across various domains, e.g., robotics, games, and web\nnavigation. However, these agents typically require elaborate design and expert\nprompts to solve tasks in specific domains, which limits their adaptability. We\nintroduce AutoManual, a framework enabling LLM agents to autonomously build\ntheir understanding through interaction and adapt to new environments.\nAutoManual categorizes environmental knowledge into diverse rules and optimizes\nthem in an online fashion by two agents: 1) The Planner codes actionable plans\nbased on current rules for interacting with the environment. 2) The Builder\nupdates the rules through a well-structured rule system that facilitates online\nrule management and essential detail retention. To mitigate hallucinations in\nmanaging rules, we introduce a *case-conditioned prompting* strategy for the\nBuilder. Finally, the Formulator agent compiles these rules into a\ncomprehensive manual. The self-generated manual can not only improve the\nadaptability but also guide the planning of smaller LLMs while being\nhuman-readable. Given only one simple demonstration, AutoManual significantly\nimproves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with\nGPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at\nhttps://github.com/minghchen/automanual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) based agents have shown promise in autonomously\ncompleting tasks across various domains, e.g., robotics, games, and web\nnavigation. However, these agents typically require elaborate design and expert\nprompts to solve tasks in specific domains, which limits their adaptability. We\nintroduce AutoManual, a framework enabling LLM agents to autonomously build\ntheir understanding through interaction and adapt to new environments.\nAutoManual categorizes environmental knowledge into diverse rules and optimizes\nthem in an online fashion by two agents: 1) The Planner codes actionable plans\nbased on current rules for interacting with the environment. 2) The Builder\nupdates the rules through a well-structured rule system that facilitates online\nrule management and essential detail retention. To mitigate hallucinations in\nmanaging rules, we introduce a *case-conditioned prompting* strategy for the\nBuilder. Finally, the Formulator agent compiles these rules into a\ncomprehensive manual. The self-generated manual can not only improve the\nadaptability but also guide the planning of smaller LLMs while being\nhuman-readable. Given only one simple demonstration, AutoManual significantly\nimproves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with\nGPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at\nhttps://github.com/minghchen/automanual."
                },
                "authors": [
                    {
                        "name": "Minghao Chen"
                    },
                    {
                        "name": "Yihang Li"
                    },
                    {
                        "name": "Yanting Yang"
                    },
                    {
                        "name": "Shiyu Yu"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Xiaofei He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofei He"
                },
                "author": "Xiaofei He",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16247v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01548v2",
                "updated": "2024-11-01T06:12:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    12,
                    33,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-02T13:37:54Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    37,
                    54,
                    2,
                    276,
                    0
                ],
                "title": "In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks"
                },
                "summary": "In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Xiao Xu"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03059v2",
                "updated": "2024-11-01T05:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    5,
                    50,
                    3,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-03T20:56:33Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    20,
                    56,
                    33,
                    2,
                    94,
                    0
                ],
                "title": "Asymptotically-exact selective inference for quantile regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically-exact selective inference for quantile regression"
                },
                "summary": "In modern data analysis, it is common to select a model before performing\nstatistical inference. Selective inference tools make adjustments for the model\nselection process in order to ensure reliable inference post selection. In this\npaper, we introduce an asymptotic pivot to infer about the effects of selected\nvariables on conditional quantile functions. Utilizing estimators from smoothed\nquantile regression, our proposed pivot is easy to compute and yields\nasymptotically-exact selective inference without making strict distributional\nassumptions about the response variable. At the core of our pivot is the use of\nexternal randomization variables, which allows us to utilize all available\nsamples for both selection and inference without partitioning the data into\nindependent subsets or discarding any samples at any step. From simulation\nstudies, we find that: (i) the asymptotic confidence intervals based on our\npivot achieve the desired coverage rates, even in cases where sample splitting\nfails due to insufficient sample size for inference; (ii) our intervals are\nconsistently shorter than those produced by sample splitting across various\nmodels and signal settings. We report similar findings when we apply our\napproach to study risk factors for low birth weights in a publicly accessible\ndataset of US birth records from 2022.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern data analysis, it is common to select a model before performing\nstatistical inference. Selective inference tools make adjustments for the model\nselection process in order to ensure reliable inference post selection. In this\npaper, we introduce an asymptotic pivot to infer about the effects of selected\nvariables on conditional quantile functions. Utilizing estimators from smoothed\nquantile regression, our proposed pivot is easy to compute and yields\nasymptotically-exact selective inference without making strict distributional\nassumptions about the response variable. At the core of our pivot is the use of\nexternal randomization variables, which allows us to utilize all available\nsamples for both selection and inference without partitioning the data into\nindependent subsets or discarding any samples at any step. From simulation\nstudies, we find that: (i) the asymptotic confidence intervals based on our\npivot achieve the desired coverage rates, even in cases where sample splitting\nfails due to insufficient sample size for inference; (ii) our intervals are\nconsistently shorter than those produced by sample splitting across various\nmodels and signal settings. We report similar findings when we apply our\napproach to study risk factors for low birth weights in a publicly accessible\ndataset of US birth records from 2022."
                },
                "authors": [
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Snigdha Panigrahi"
                    },
                    {
                        "name": "Xuming He"
                    }
                ],
                "author_detail": {
                    "name": "Xuming He"
                },
                "author": "Xuming He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04744v2",
                "updated": "2024-11-01T05:30:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    5,
                    30,
                    17,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-07T08:43:07Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    8,
                    43,
                    7,
                    4,
                    159,
                    0
                ],
                "title": "CRAG -- Comprehensive RAG Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRAG -- Comprehensive RAG Benchmark"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution to alleviate Large Language Model (LLM)'s deficiency in lack of\nknowledge. Existing RAG datasets, however, do not adequately represent the\ndiverse and dynamic nature of real-world Question Answering (QA) tasks. To\nbridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual\nquestion answering benchmark of 4,409 question-answer pairs and mock APIs to\nsimulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a\ndiverse array of questions across five domains and eight question categories,\nreflecting varied entity popularity from popular to long-tail, and temporal\ndynamisms ranging from years to seconds. Our evaluation of this benchmark\nhighlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve\n<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the\naccuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%\nof questions without any hallucination. CRAG also reveals much lower accuracy\nin answering questions regarding facts with higher dynamism, lower popularity,\nor higher complexity, suggesting future research directions. The CRAG benchmark\nlaid the groundwork for a KDD Cup 2024 challenge and attracted thousands of\nparticipants and submissions. We commit to maintaining CRAG to serve research\ncommunities in advancing RAG solutions and general QA solutions. CRAG is\navailable at https://github.com/facebookresearch/CRAG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution to alleviate Large Language Model (LLM)'s deficiency in lack of\nknowledge. Existing RAG datasets, however, do not adequately represent the\ndiverse and dynamic nature of real-world Question Answering (QA) tasks. To\nbridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual\nquestion answering benchmark of 4,409 question-answer pairs and mock APIs to\nsimulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a\ndiverse array of questions across five domains and eight question categories,\nreflecting varied entity popularity from popular to long-tail, and temporal\ndynamisms ranging from years to seconds. Our evaluation of this benchmark\nhighlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve\n<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the\naccuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%\nof questions without any hallucination. CRAG also reveals much lower accuracy\nin answering questions regarding facts with higher dynamism, lower popularity,\nor higher complexity, suggesting future research directions. The CRAG benchmark\nlaid the groundwork for a KDD Cup 2024 challenge and attracted thousands of\nparticipants and submissions. We commit to maintaining CRAG to serve research\ncommunities in advancing RAG solutions and general QA solutions. CRAG is\navailable at https://github.com/facebookresearch/CRAG/."
                },
                "authors": [
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Hao Xin"
                    },
                    {
                        "name": "Yushi Sun"
                    },
                    {
                        "name": "Nikita Bhalla"
                    },
                    {
                        "name": "Xiangsen Chen"
                    },
                    {
                        "name": "Sajal Choudhary"
                    },
                    {
                        "name": "Rongze Daniel Gui"
                    },
                    {
                        "name": "Ziran Will Jiang"
                    },
                    {
                        "name": "Ziyu Jiang"
                    },
                    {
                        "name": "Lingkun Kong"
                    },
                    {
                        "name": "Brian Moran"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yifan Ethan Xu"
                    },
                    {
                        "name": "An Yan"
                    },
                    {
                        "name": "Chenyu Yang"
                    },
                    {
                        "name": "Eting Yuan"
                    },
                    {
                        "name": "Hanwen Zha"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Nicolas Scheffer"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Nirav Shah"
                    },
                    {
                        "name": "Rakesh Wanga"
                    },
                    {
                        "name": "Anuj Kumar"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Xin Luna Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xin Luna Dong"
                },
                "author": "Xin Luna Dong",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11724v2",
                "updated": "2024-11-01T04:19:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    4,
                    19,
                    21,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-18T06:19:59Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    19,
                    59,
                    2,
                    262,
                    0
                ],
                "title": "TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning"
                },
                "summary": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lu"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13220v2",
                "updated": "2024-11-01T04:04:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    4,
                    4,
                    44,
                    4,
                    306,
                    0
                ],
                "published": "2023-10-20T01:55:34Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    1,
                    55,
                    34,
                    4,
                    293,
                    0
                ],
                "title": "Towards Understanding How Transformers Learn In-context Through a\n  Representation Learning Lens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding How Transformers Learn In-context Through a\n  Representation Learning Lens"
                },
                "summary": "Pre-trained large language models based on Transformers have demonstrated\nremarkable in-context learning (ICL) abilities. With just a few demonstration\nexamples, the models can implement new tasks without any parameter updates.\nHowever, it is still an open question to understand the mechanism of ICL. In\nthis paper, we attempt to explore the ICL process in Transformers through a\nlens of representation learning. Initially, leveraging kernel methods, we\nfigure out a dual model for one softmax attention layer. The ICL inference\nprocess of the attention layer aligns with the training procedure of its dual\nmodel, generating token representation predictions that are equivalent to the\ndual model's test outputs. We delve into the training process of this dual\nmodel from a representation learning standpoint and further derive a\ngeneralization error bound related to the quantity of demonstration tokens.\nSubsequently, we extend our theoretical conclusions to more complicated\nscenarios, including one Transformer layer and multiple attention layers.\nFurthermore, drawing inspiration from existing representation learning methods\nespecially contrastive learning, we propose potential modifications for the\nattention layer. Finally, experiments are designed to support our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained large language models based on Transformers have demonstrated\nremarkable in-context learning (ICL) abilities. With just a few demonstration\nexamples, the models can implement new tasks without any parameter updates.\nHowever, it is still an open question to understand the mechanism of ICL. In\nthis paper, we attempt to explore the ICL process in Transformers through a\nlens of representation learning. Initially, leveraging kernel methods, we\nfigure out a dual model for one softmax attention layer. The ICL inference\nprocess of the attention layer aligns with the training procedure of its dual\nmodel, generating token representation predictions that are equivalent to the\ndual model's test outputs. We delve into the training process of this dual\nmodel from a representation learning standpoint and further derive a\ngeneralization error bound related to the quantity of demonstration tokens.\nSubsequently, we extend our theoretical conclusions to more complicated\nscenarios, including one Transformer layer and multiple attention layers.\nFurthermore, drawing inspiration from existing representation learning methods\nespecially contrastive learning, we propose potential modifications for the\nattention layer. Finally, experiments are designed to support our findings."
                },
                "authors": [
                    {
                        "name": "Ruifeng Ren"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.13220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01763v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01763v3",
                "updated": "2024-11-01T03:49:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    49,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-01-30T23:35:28Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    23,
                    35,
                    28,
                    1,
                    30,
                    0
                ],
                "title": "When Large Language Models Meet Vector Databases: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Large Language Models Meet Vector Databases: A Survey"
                },
                "summary": "This survey explores the synergistic potential of Large Language Models\n(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving\nresearch area. With the proliferation of LLMs comes a host of challenges,\nincluding hallucinations, outdated knowledge, prohibitive commercial\napplication costs, and memory issues. VecDBs emerge as a compelling solution to\nthese issues by offering an efficient means to store, retrieve, and manage the\nhigh-dimensional vector representations intrinsic to LLM operations. Through\nthis nuanced review, we delineate the foundational principles of LLMs and\nVecDBs and critically analyze their integration's impact on enhancing LLM\nfunctionalities. This discourse extends into a discussion on the speculative\nfuture developments in this domain, aiming to catalyze further research into\noptimizing the confluence of LLMs and VecDBs for advanced data handling and\nknowledge extraction capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the synergistic potential of Large Language Models\n(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving\nresearch area. With the proliferation of LLMs comes a host of challenges,\nincluding hallucinations, outdated knowledge, prohibitive commercial\napplication costs, and memory issues. VecDBs emerge as a compelling solution to\nthese issues by offering an efficient means to store, retrieve, and manage the\nhigh-dimensional vector representations intrinsic to LLM operations. Through\nthis nuanced review, we delineate the foundational principles of LLMs and\nVecDBs and critically analyze their integration's impact on enhancing LLM\nfunctionalities. This discourse extends into a discussion on the speculative\nfuture developments in this domain, aiming to catalyze further research into\noptimizing the confluence of LLMs and VecDBs for advanced data handling and\nknowledge extraction capabilities."
                },
                "authors": [
                    {
                        "name": "Zhi Jing"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Yikun Han"
                    }
                ],
                "author_detail": {
                    "name": "Yikun Han"
                },
                "author": "Yikun Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01763v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01763v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10159v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10159v3",
                "updated": "2024-11-01T03:47:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    47,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-08-19T17:09:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    9,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10159v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10159v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02428v3",
                "updated": "2024-11-01T03:47:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    47,
                    51,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-04T04:15:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    15,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models as Efficient Reward Function Searchers for\n  Custom-Environment Multi-Objective Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Efficient Reward Function Searchers for\n  Custom-Environment Multi-Objective Reinforcement Learning"
                },
                "summary": "Achieving the effective design and improvement of reward functions in\nreinforcement learning (RL) tasks with complex custom environments and multiple\nrequirements presents considerable challenges. In this paper, we propose ERFSL,\nan efficient reward function searcher using LLMs, which enables LLMs to be\neffective white-box searchers and highlights their advanced semantic\nunderstanding capabilities. Specifically, we generate reward components for\neach numerically explicit user requirement and employ a reward critic to\nidentify the correct code form. Then, LLMs assign weights to the reward\ncomponents to balance their values and iteratively adjust the weights without\nambiguity and redundant adjustments by flexibly adopting directional mutation\nand crossover strategies, similar to genetic algorithms, based on the context\nprovided by the training log analyzer. We applied the framework to an\nunderwater data collection RL task without direct human feedback or reward\nexamples (zero-shot learning). The reward critic successfully corrects the\nreward code with only one feedback instance for each requirement, effectively\npreventing unrectifiable errors. The initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout the need for weight search. Even in cases where a weight is 500 times\noff, on average, only 5.2 iterations are needed to meet user requirements. The\nERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose\nthe weight searching process to reduce the requirement for numerical and\nlong-context understanding capabilities",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving the effective design and improvement of reward functions in\nreinforcement learning (RL) tasks with complex custom environments and multiple\nrequirements presents considerable challenges. In this paper, we propose ERFSL,\nan efficient reward function searcher using LLMs, which enables LLMs to be\neffective white-box searchers and highlights their advanced semantic\nunderstanding capabilities. Specifically, we generate reward components for\neach numerically explicit user requirement and employ a reward critic to\nidentify the correct code form. Then, LLMs assign weights to the reward\ncomponents to balance their values and iteratively adjust the weights without\nambiguity and redundant adjustments by flexibly adopting directional mutation\nand crossover strategies, similar to genetic algorithms, based on the context\nprovided by the training log analyzer. We applied the framework to an\nunderwater data collection RL task without direct human feedback or reward\nexamples (zero-shot learning). The reward critic successfully corrects the\nreward code with only one feedback instance for each requirement, effectively\npreventing unrectifiable errors. The initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout the need for weight search. Even in cases where a weight is 500 times\noff, on average, only 5.2 iterations are needed to meet user requirements. The\nERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose\nthe weight searching process to reduce the requirement for numerical and\nlong-context understanding capabilities"
                },
                "authors": [
                    {
                        "name": "Guanwen Xie"
                    },
                    {
                        "name": "Jingzehua Xu"
                    },
                    {
                        "name": "Yiyuan Yang"
                    },
                    {
                        "name": "Yimian Ding"
                    },
                    {
                        "name": "Shuai Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Zhang"
                },
                "author": "Shuai Zhang",
                "arxiv_journal_ref": "AAAI (Student) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04501v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04501v3",
                "updated": "2024-11-01T03:42:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    42,
                    37,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-06T14:45:01Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    14,
                    45,
                    1,
                    6,
                    280,
                    0
                ],
                "title": "Leveraging Large Language Models for Suicide Detection on Social Media\n  with Limited Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Suicide Detection on Social Media\n  with Limited Labels"
                },
                "summary": "The increasing frequency of suicidal thoughts highlights the importance of\nearly detection and intervention. Social media platforms, where users often\nshare personal experiences and seek help, could be utilized to identify\nindividuals at risk. However, the large volume of daily posts makes manual\nreview impractical. This paper explores the use of Large Language Models (LLMs)\nto automatically detect suicidal content in text-based social media posts. We\npropose a novel method for generating pseudo-labels for unlabeled data by\nprompting LLMs, along with traditional classification fine-tuning techniques to\nenhance label accuracy. To create a strong suicide detection model, we develop\nan ensemble approach involving prompting with Qwen2-72B-Instruct, and using\nfine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate\nour approach on the dataset of the Suicide Ideation Detection on Social Media\nChallenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we\nconduct a comprehensive analysis to assess the impact of different models and\nfine-tuning strategies on detection performance. Experimental results show that\nthe ensemble model significantly improves the detection accuracy, by 5% points\ncompared with the individual models. It achieves a weight F1 score of 0.770 on\nthe public test set, and 0.731 on the private test set, providing a promising\nsolution for identifying suicidal content in social media. Our analysis shows\nthat the choice of LLMs affects the prompting performance, with larger models\nproviding better accuracy. Our code and checkpoints are publicly available at\nhttps://github.com/khanhvynguyen/Suicide_Detection_LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing frequency of suicidal thoughts highlights the importance of\nearly detection and intervention. Social media platforms, where users often\nshare personal experiences and seek help, could be utilized to identify\nindividuals at risk. However, the large volume of daily posts makes manual\nreview impractical. This paper explores the use of Large Language Models (LLMs)\nto automatically detect suicidal content in text-based social media posts. We\npropose a novel method for generating pseudo-labels for unlabeled data by\nprompting LLMs, along with traditional classification fine-tuning techniques to\nenhance label accuracy. To create a strong suicide detection model, we develop\nan ensemble approach involving prompting with Qwen2-72B-Instruct, and using\nfine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate\nour approach on the dataset of the Suicide Ideation Detection on Social Media\nChallenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we\nconduct a comprehensive analysis to assess the impact of different models and\nfine-tuning strategies on detection performance. Experimental results show that\nthe ensemble model significantly improves the detection accuracy, by 5% points\ncompared with the individual models. It achieves a weight F1 score of 0.770 on\nthe public test set, and 0.731 on the private test set, providing a promising\nsolution for identifying suicidal content in social media. Our analysis shows\nthat the choice of LLMs affects the prompting performance, with larger models\nproviding better accuracy. Our code and checkpoints are publicly available at\nhttps://github.com/khanhvynguyen/Suicide_Detection_LLMs."
                },
                "authors": [
                    {
                        "name": "Vy Nguyen"
                    },
                    {
                        "name": "Chau Pham"
                    }
                ],
                "author_detail": {
                    "name": "Chau Pham"
                },
                "author": "Chau Pham",
                "arxiv_comment": "Accepted at IEEE International Conference on Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04501v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16978v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16978v3",
                "updated": "2024-11-01T03:40:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    40,
                    24,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-27T09:21:40Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    9,
                    21,
                    40,
                    0,
                    148,
                    0
                ],
                "title": "OSLO: One-Shot Label-Only Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSLO: One-Shot Label-Only Membership Inference Attacks"
                },
                "summary": "We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs),\nwhich accurately infer a given sample's membership in a target model's training\nset with high precision using just \\emph{a single query}, where the target\nmodel only returns the predicted hard label. This is in contrast to\nstate-of-the-art label-only attacks which require $\\sim6000$ queries, yet get\nattack precisions lower than OSLO's. OSLO leverages transfer-based black-box\nadversarial attacks. The core idea is that a member sample exhibits more\nresistance to adversarial perturbations than a non-member. We compare OSLO\nagainst state-of-the-art label-only attacks and demonstrate that, despite\nrequiring only one query, our method significantly outperforms previous attacks\nin terms of precision and true positive rate (TPR) under the same false\npositive rates (FPR). For example, compared to previous label-only MIAs, OSLO\nachieves a TPR that is at least 7$\\times$ higher under a 1\\% FPR and at least\n22$\\times$ higher under a 0.1\\% FPR on CIFAR100 for a ResNet18 model. We\nevaluated multiple defense mechanisms against OSLO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs),\nwhich accurately infer a given sample's membership in a target model's training\nset with high precision using just \\emph{a single query}, where the target\nmodel only returns the predicted hard label. This is in contrast to\nstate-of-the-art label-only attacks which require $\\sim6000$ queries, yet get\nattack precisions lower than OSLO's. OSLO leverages transfer-based black-box\nadversarial attacks. The core idea is that a member sample exhibits more\nresistance to adversarial perturbations than a non-member. We compare OSLO\nagainst state-of-the-art label-only attacks and demonstrate that, despite\nrequiring only one query, our method significantly outperforms previous attacks\nin terms of precision and true positive rate (TPR) under the same false\npositive rates (FPR). For example, compared to previous label-only MIAs, OSLO\nachieves a TPR that is at least 7$\\times$ higher under a 1\\% FPR and at least\n22$\\times$ higher under a 0.1\\% FPR on CIFAR100 for a ResNet18 model. We\nevaluated multiple defense mechanisms against OSLO."
                },
                "authors": [
                    {
                        "name": "Yuefeng Peng"
                    },
                    {
                        "name": "Jaechul Roh"
                    },
                    {
                        "name": "Subhransu Maji"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "arxiv_comment": "To appear at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16978v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16978v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13056v2",
                "updated": "2024-11-01T03:16:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    16,
                    30,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-16T21:34:41Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    21,
                    34,
                    41,
                    2,
                    290,
                    0
                ],
                "title": "Channel-Wise Mixed-Precision Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel-Wise Mixed-Precision Quantization for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of language tasks, but their deployment on edge devices remains\nchallenging due to the substantial memory requirements imposed by their large\nparameter sizes. Weight-only quantization presents a promising solution to\nreduce the memory footprint of LLMs. However, existing approaches primarily\nfocus on integer-bit quantization, limiting their adaptability to\nfractional-bit quantization tasks and preventing the full utilization of\navailable storage space on devices. In this paper, we introduce Channel-Wise\nMixed-Precision Quantization (CMPQ), a novel mixed-precision quantization\nmethod that allocates quantization precision in a channel-wise pattern based on\nactivation distributions. By assigning different precision levels to different\nweight channels, CMPQ can adapt to any bit-width constraint. CMPQ employs a\nnon-uniform quantization strategy and incorporates two outlier extraction\ntechniques that collaboratively preserve the critical information, thereby\nminimizing the quantization loss. Experiments on different sizes of LLMs\ndemonstrate that CMPQ not only enhances performance in integer-bit quantization\ntasks but also achieves significant performance gains with a modest increase in\nmemory usage. CMPQ thus represents an adaptive and effective approach to LLM\nquantization, offering substantial benefits across diverse device capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of language tasks, but their deployment on edge devices remains\nchallenging due to the substantial memory requirements imposed by their large\nparameter sizes. Weight-only quantization presents a promising solution to\nreduce the memory footprint of LLMs. However, existing approaches primarily\nfocus on integer-bit quantization, limiting their adaptability to\nfractional-bit quantization tasks and preventing the full utilization of\navailable storage space on devices. In this paper, we introduce Channel-Wise\nMixed-Precision Quantization (CMPQ), a novel mixed-precision quantization\nmethod that allocates quantization precision in a channel-wise pattern based on\nactivation distributions. By assigning different precision levels to different\nweight channels, CMPQ can adapt to any bit-width constraint. CMPQ employs a\nnon-uniform quantization strategy and incorporates two outlier extraction\ntechniques that collaboratively preserve the critical information, thereby\nminimizing the quantization loss. Experiments on different sizes of LLMs\ndemonstrate that CMPQ not only enhances performance in integer-bit quantization\ntasks but also achieves significant performance gains with a modest increase in\nmemory usage. CMPQ thus represents an adaptive and effective approach to LLM\nquantization, offering substantial benefits across diverse device capabilities."
                },
                "authors": [
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Bike Xie"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Cong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Cong Shen"
                },
                "author": "Cong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20646v2",
                "updated": "2024-11-01T03:12:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    12,
                    44,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-31T07:24:42Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    7,
                    24,
                    42,
                    4,
                    152,
                    0
                ],
                "title": "LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential\n  Recommendation"
                },
                "summary": "Sequential recommender systems (SRS) aim to predict users' subsequent choices\nbased on their historical interactions and have found applications in diverse\nfields such as e-commerce and social media. However, in real-world systems,\nmost users interact with only a handful of items, while the majority of items\nare seldom consumed. These two issues, known as the long-tail user and\nlong-tail item challenges, often pose difficulties for existing SRS. These\nchallenges can adversely affect user experience and seller benefits, making\nthem crucial to address. Though a few works have addressed the challenges, they\nstill struggle with the seesaw or noisy issues due to the intrinsic scarcity of\ninteractions. The advancements in large language models (LLMs) present a\npromising solution to these problems from a semantic perspective. As one of the\npioneers in this field, we propose the Large Language Models Enhancement\nframework for Sequential Recommendation (LLM-ESR). This framework utilizes\nsemantic embeddings derived from LLMs to enhance SRS without adding extra\ninference load from LLMs. To address the long-tail item challenge, we design a\ndual-view modeling framework that combines semantics from LLMs and\ncollaborative signals from conventional SRS. For the long-tail user challenge,\nwe propose a retrieval augmented self-distillation method to enhance user\npreference representation using more informative interactions from similar\nusers. To verify the effectiveness and versatility of our proposed enhancement\nframework, we conduct extensive experiments on three real-world datasets using\nthree popular SRS models. The results show that our method surpasses existing\nbaselines consistently, and benefits long-tail users and items especially. The\nimplementation code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/LLM-ESR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommender systems (SRS) aim to predict users' subsequent choices\nbased on their historical interactions and have found applications in diverse\nfields such as e-commerce and social media. However, in real-world systems,\nmost users interact with only a handful of items, while the majority of items\nare seldom consumed. These two issues, known as the long-tail user and\nlong-tail item challenges, often pose difficulties for existing SRS. These\nchallenges can adversely affect user experience and seller benefits, making\nthem crucial to address. Though a few works have addressed the challenges, they\nstill struggle with the seesaw or noisy issues due to the intrinsic scarcity of\ninteractions. The advancements in large language models (LLMs) present a\npromising solution to these problems from a semantic perspective. As one of the\npioneers in this field, we propose the Large Language Models Enhancement\nframework for Sequential Recommendation (LLM-ESR). This framework utilizes\nsemantic embeddings derived from LLMs to enhance SRS without adding extra\ninference load from LLMs. To address the long-tail item challenge, we design a\ndual-view modeling framework that combines semantics from LLMs and\ncollaborative signals from conventional SRS. For the long-tail user challenge,\nwe propose a retrieval augmented self-distillation method to enhance user\npreference representation using more informative interactions from similar\nusers. To verify the effectiveness and versatility of our proposed enhancement\nframework, we conduct extensive experiments on three real-world datasets using\nthree popular SRS models. The results show that our method surpasses existing\nbaselines consistently, and benefits long-tail users and items especially. The\nimplementation code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/LLM-ESR."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "accepted by NeruIPS'24 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21358v2",
                "updated": "2024-11-01T02:50:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    50,
                    16,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-28T17:35:59Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    35,
                    59,
                    0,
                    302,
                    0
                ],
                "title": "\"We do use it, but not how hearing people think\": How the Deaf and Hard\n  of Hearing Community Uses Large Language Model Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"We do use it, but not how hearing people think\": How the Deaf and Hard\n  of Hearing Community Uses Large Language Model Tools"
                },
                "summary": "Generative AI tools, particularly those utilizing large language models\n(LLMs), have become increasingly prevalent in both professional and personal\ncontexts, offering powerful capabilities for text generation and communication\nsupport. While these tools are widely used to enhance productivity and\naccessibility, there has been limited exploration of how Deaf and Hard of\nHearing (DHH) individuals engage with text-based generative AI tools, as well\nas the challenges they may encounter. This paper presents a mixed-method survey\nstudy investigating how the DHH community uses Text AI tools, such as ChatGPT,\nto reduce communication barriers, bridge Deaf and hearing cultures, and improve\naccess to information. Through a survey of 80 DHH participants and separate\ninterviews with 11 other participants, we found that while these tools provide\nsignificant benefits, including enhanced communication and mental health\nsupport, they also introduce barriers, such as a lack of American Sign Language\n(ASL) support and understanding of Deaf cultural nuances. Our findings\nhighlight unique usage patterns within the DHH community and underscore the\nneed for inclusive design improvements. We conclude by offering practical\nrecommendations to enhance the accessibility of Text AI for the DHH community\nand suggest directions for future research in AI and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI tools, particularly those utilizing large language models\n(LLMs), have become increasingly prevalent in both professional and personal\ncontexts, offering powerful capabilities for text generation and communication\nsupport. While these tools are widely used to enhance productivity and\naccessibility, there has been limited exploration of how Deaf and Hard of\nHearing (DHH) individuals engage with text-based generative AI tools, as well\nas the challenges they may encounter. This paper presents a mixed-method survey\nstudy investigating how the DHH community uses Text AI tools, such as ChatGPT,\nto reduce communication barriers, bridge Deaf and hearing cultures, and improve\naccess to information. Through a survey of 80 DHH participants and separate\ninterviews with 11 other participants, we found that while these tools provide\nsignificant benefits, including enhanced communication and mental health\nsupport, they also introduce barriers, such as a lack of American Sign Language\n(ASL) support and understanding of Deaf cultural nuances. Our findings\nhighlight unique usage patterns within the DHH community and underscore the\nneed for inclusive design improvements. We conclude by offering practical\nrecommendations to enhance the accessibility of Text AI for the DHH community\nand suggest directions for future research in AI and accessibility."
                },
                "authors": [
                    {
                        "name": "Shuxu Huffman"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Kelly Avery Mack"
                    },
                    {
                        "name": "Haotian Su"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Raja Kushalnagar"
                    }
                ],
                "author_detail": {
                    "name": "Raja Kushalnagar"
                },
                "author": "Raja Kushalnagar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15880v2",
                "updated": "2024-11-01T02:46:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    46,
                    3,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-24T18:45:51Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    18,
                    45,
                    51,
                    4,
                    145,
                    0
                ],
                "title": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis"
                },
                "summary": "Many structured prediction and reasoning tasks can be framed as program\nsynthesis problems, where the goal is to generate a program in a\ndomain-specific language (DSL) that transforms input data into the desired\noutput. Unfortunately, purely neural approaches, such as large language models\n(LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while\npurely symbolic methods based on combinatorial search scale poorly to complex\nproblems. Motivated by these limitations, we introduce a hybrid approach, where\nLLM completions for a given task are used to learn a task-specific,\ncontext-free surrogate model, which is then used to guide program synthesis. We\nevaluate this hybrid approach on three domains, and show that it outperforms\nboth unguided search and direct sampling from LLMs, as well as existing program\nsynthesizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many structured prediction and reasoning tasks can be framed as program\nsynthesis problems, where the goal is to generate a program in a\ndomain-specific language (DSL) that transforms input data into the desired\noutput. Unfortunately, purely neural approaches, such as large language models\n(LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while\npurely symbolic methods based on combinatorial search scale poorly to complex\nproblems. Motivated by these limitations, we introduce a hybrid approach, where\nLLM completions for a given task are used to learn a task-specific,\ncontext-free surrogate model, which is then used to guide program synthesis. We\nevaluate this hybrid approach on three domains, and show that it outperforms\nboth unguided search and direct sampling from LLMs, as well as existing program\nsynthesizers."
                },
                "authors": [
                    {
                        "name": "Shraddha Barke"
                    },
                    {
                        "name": "Emmanuel Anaya Gonzalez"
                    },
                    {
                        "name": "Saketh Ram Kasibatla"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Nadia Polikarpova"
                    }
                ],
                "author_detail": {
                    "name": "Nadia Polikarpova"
                },
                "author": "Nadia Polikarpova",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09131v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09131v4",
                "updated": "2024-11-01T02:43:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    43,
                    34,
                    4,
                    306,
                    0
                ],
                "published": "2024-03-14T06:49:16Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    6,
                    49,
                    16,
                    3,
                    74,
                    0
                ],
                "title": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between\n  Professional and Non-Professional Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between\n  Professional and Non-Professional Answers"
                },
                "summary": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\ninstruction tuning remain underexplored. This study concentrates on the\nstyle-switching abilities of LLMs and introduces a novel approach, named\nProSwitch, which enables a language model to switch between professional and\nnon-professional answers, by tuning and evaluating through the guidance of\ndomain and style knowledge. ProSwitch unfolds across three phases:\nLLM-augmented preparation to collect domain knowledge and QA pairs, instruction\ntuning to optimize LLMs with multiple levels of knowledge, and comprehensive\nevaluation to assess both style discrimination and reference-based quality of\ngenerated text. Comparative analysis of ProSwitch against general and\nspecialized LLMs reveals that our approach outperforms baselines in switching\nbetween professional and non-professional answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\ninstruction tuning remain underexplored. This study concentrates on the\nstyle-switching abilities of LLMs and introduces a novel approach, named\nProSwitch, which enables a language model to switch between professional and\nnon-professional answers, by tuning and evaluating through the guidance of\ndomain and style knowledge. ProSwitch unfolds across three phases:\nLLM-augmented preparation to collect domain knowledge and QA pairs, instruction\ntuning to optimize LLMs with multiple levels of knowledge, and comprehensive\nevaluation to assess both style discrimination and reference-based quality of\ngenerated text. Comparative analysis of ProSwitch against general and\nspecialized LLMs reveals that our approach outperforms baselines in switching\nbetween professional and non-professional answers."
                },
                "authors": [
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Yuyan Chen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "8 pages main body, 16 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09131v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09131v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13623v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13623v3",
                "updated": "2024-11-01T02:41:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    41,
                    36,
                    4,
                    306,
                    0
                ],
                "published": "2024-07-18T15:58:54Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    15,
                    58,
                    54,
                    3,
                    200,
                    0
                ],
                "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies"
                },
                "summary": "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe conclusion that the optimal vocabulary size depends on the compute budget,\nwith larger models requiring larger vocabularies. Most LLMs, however, use\ninsufficient vocabulary sizes. For example, we predict that the optimal\nvocabulary size of Llama2-70B should have been at least 216K, 7 times larger\nthan its vocabulary of 32K. We validate our predictions empirically by training\nmodels with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly\nconsidering tokenization and model scaling for efficient pre-training. The code\nand demo are available at https://github.com/sail-sg/scaling-with-vocab and\nhttps://hf.co/spaces/sail/scaling-with-vocab-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe conclusion that the optimal vocabulary size depends on the compute budget,\nwith larger models requiring larger vocabularies. Most LLMs, however, use\ninsufficient vocabulary sizes. For example, we predict that the optimal\nvocabulary size of Llama2-70B should have been at least 216K, 7 times larger\nthan its vocabulary of 32K. We validate our predictions empirically by training\nmodels with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly\nconsidering tokenization and model scaling for efficient pre-training. The code\nand demo are available at https://github.com/sail-sg/scaling-with-vocab and\nhttps://hf.co/spaces/sail/scaling-with-vocab-demo."
                },
                "authors": [
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13623v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13623v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17508v2",
                "updated": "2024-11-01T02:38:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    38,
                    53,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-26T03:33:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    33,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task\n  Learning Via Connector-MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task\n  Learning Via Connector-MoE"
                },
                "summary": "Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization in MLLMs, recent advances primarily focus on improving\nthe LLM components, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector in MLLMs. Extensive ablation\nexperiments validate the effectiveness of introducing CMoE under any\nconfiguration, with up to an average 8% performance gains. We further provide\ninterpretation analysis of the tug-of-war problem from the perspective of\ngradient optimization and parameter statistics. Compared to previous\nstate-of-the-art medical MLLMs, Uni-Med achieves competitive or superior\nevaluation metrics on diverse tasks. Code and resources are available at\nhttps://github.com/tsinghua-msiip/Uni-Med.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization in MLLMs, recent advances primarily focus on improving\nthe LLM components, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector in MLLMs. Extensive ablation\nexperiments validate the effectiveness of introducing CMoE under any\nconfiguration, with up to an average 8% performance gains. We further provide\ninterpretation analysis of the tug-of-war problem from the perspective of\ngradient optimization and parameter statistics. Compared to previous\nstate-of-the-art medical MLLMs, Uni-Med achieves competitive or superior\nevaluation metrics on diverse tasks. Code and resources are available at\nhttps://github.com/tsinghua-msiip/Uni-Med."
                },
                "authors": [
                    {
                        "name": "Xun Zhu"
                    },
                    {
                        "name": "Ying Hu"
                    },
                    {
                        "name": "Fanbin Mo"
                    },
                    {
                        "name": "Miao Li"
                    },
                    {
                        "name": "Ji Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ji Wu"
                },
                "author": "Ji Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14909v2",
                "updated": "2024-11-01T02:26:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    26,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-21T06:58:37Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    6,
                    58,
                    37,
                    4,
                    173,
                    0
                ],
                "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoA: Mixture of Sparse Attention for Automatic Large Language Model\n  Compression"
                },
                "summary": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by $3.9\\times$ with the same\naverage attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the\nuniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$\nacross two long-context understanding benchmarks. MoA achieves a\n$1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by\n$6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with\nminimal impact on performance. Our code is available at\n\\url{https://github.com/thu-nics/MoA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by $3.9\\times$ with the same\naverage attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the\nuniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$\nacross two long-context understanding benchmarks. MoA achieves a\n$1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by\n$6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with\nminimal impact on performance. Our code is available at\n\\url{https://github.com/thu-nics/MoA}."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Genghan Zhang"
                    },
                    {
                        "name": "Boju Chen"
                    },
                    {
                        "name": "Tianqi Wu"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Zixiao Huang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02834v3",
                "updated": "2024-11-01T02:21:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    21,
                    13,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-04T16:00:21Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    0,
                    21,
                    2,
                    248,
                    0
                ],
                "title": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models"
                },
                "summary": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets."
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Ji Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17213v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17213v4",
                "updated": "2024-11-01T02:08:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    8,
                    3,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-25T17:38:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    38,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles"
                },
                "summary": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by democratic deliberation theory, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by democratic deliberation theory, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Emily Fry"
                    },
                    {
                        "name": "Narendra Edara"
                    },
                    {
                        "name": "Eric Gilbert"
                    },
                    {
                        "name": "Ceren Budak"
                    }
                ],
                "author_detail": {
                    "name": "Ceren Budak"
                },
                "author": "Ceren Budak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17213v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17213v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15235v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15235v3",
                "updated": "2024-11-01T02:00:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    0,
                    49,
                    4,
                    306,
                    0
                ],
                "published": "2024-02-23T09:57:20Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    57,
                    20,
                    4,
                    54,
                    0
                ],
                "title": "MACRec: a Multi-Agent Collaboration Framework for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACRec: a Multi-Agent Collaboration Framework for Recommendation"
                },
                "summary": "LLM-based agents have gained considerable attention for their decision-making\nskills and ability to handle complex tasks. Recognizing the current gap in\nleveraging agent capabilities for multi-agent collaboration in recommendation\nsystems, we introduce MACRec, a novel framework designed to enhance\nrecommendation systems through multi-agent collaboration. Unlike existing work\non using agents for user/item simulation, we aim to deploy multi-agents to\ntackle recommendation tasks directly. In our framework, recommendation tasks\nare addressed through the collaborative efforts of various specialized agents,\nincluding Manager, User/Item Analyst, Reflector, Searcher, and Task\nInterpreter, with different working flows. Furthermore, we provide application\nexamples of how developers can easily use MACRec on various recommendation\ntasks, including rating prediction, sequential recommendation, conversational\nrecommendation, and explanation generation of recommendation results. The\nframework and demonstration video are publicly available at\nhttps://github.com/wzf2000/MACRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have gained considerable attention for their decision-making\nskills and ability to handle complex tasks. Recognizing the current gap in\nleveraging agent capabilities for multi-agent collaboration in recommendation\nsystems, we introduce MACRec, a novel framework designed to enhance\nrecommendation systems through multi-agent collaboration. Unlike existing work\non using agents for user/item simulation, we aim to deploy multi-agents to\ntackle recommendation tasks directly. In our framework, recommendation tasks\nare addressed through the collaborative efforts of various specialized agents,\nincluding Manager, User/Item Analyst, Reflector, Searcher, and Task\nInterpreter, with different working flows. Furthermore, we provide application\nexamples of how developers can easily use MACRec on various recommendation\ntasks, including rating prediction, sequential recommendation, conversational\nrecommendation, and explanation generation of recommendation results. The\nframework and demonstration video are publicly available at\nhttps://github.com/wzf2000/MACRec."
                },
                "authors": [
                    {
                        "name": "Zhefan Wang"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Wendi Zheng"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_doi": "10.1145/3626772.3657669",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626772.3657669",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.15235v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15235v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by SIGIR2024",
                "arxiv_journal_ref": "ACM SIGIR Conference on Research and Development in Information\n  Retrieval 47 (2024) 2760-2764",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08357v2",
                "updated": "2024-11-01T01:45:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    1,
                    45,
                    2,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-12T18:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    18,
                    50,
                    13,
                    3,
                    256,
                    0
                ],
                "title": "An Experimental Study of Competitive Market Behavior Through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Experimental Study of Competitive Market Behavior Through LLMs"
                },
                "summary": "This study explores the potential of large language models (LLMs) to conduct\nmarket experiments, aiming to understand their capability to comprehend\ncompetitive market dynamics. We model the behavior of market agents in a\ncontrolled experimental setting, assessing their ability to converge toward\ncompetitive equilibria. The results reveal the challenges current LLMs face in\nreplicating the dynamic decision-making processes characteristic of human\ntrading behavior. Unlike humans, LLMs lacked the capacity to achieve market\nequilibrium. The research demonstrates that while LLMs provide a valuable tool\nfor scalable and reproducible market simulations, their current limitations\nnecessitate further advancements to fully capture the complexities of market\nbehavior. Future work that enhances dynamic learning capabilities and\nincorporates elements of behavioral economics could improve the effectiveness\nof LLMs in the economic domain, providing new insights into market dynamics and\naiding in the refinement of economic policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the potential of large language models (LLMs) to conduct\nmarket experiments, aiming to understand their capability to comprehend\ncompetitive market dynamics. We model the behavior of market agents in a\ncontrolled experimental setting, assessing their ability to converge toward\ncompetitive equilibria. The results reveal the challenges current LLMs face in\nreplicating the dynamic decision-making processes characteristic of human\ntrading behavior. Unlike humans, LLMs lacked the capacity to achieve market\nequilibrium. The research demonstrates that while LLMs provide a valuable tool\nfor scalable and reproducible market simulations, their current limitations\nnecessitate further advancements to fully capture the complexities of market\nbehavior. Future work that enhances dynamic learning capabilities and\nincorporates elements of behavioral economics could improve the effectiveness\nof LLMs in the economic domain, providing new insights into market dynamics and\naiding in the refinement of economic policies."
                },
                "authors": [
                    {
                        "name": "Jingru Jia"
                    },
                    {
                        "name": "Zehua Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zehua Yuan"
                },
                "author": "Zehua Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14556v2",
                "updated": "2024-11-01T01:15:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    1,
                    15,
                    51,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-22T18:39:27Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    18,
                    39,
                    27,
                    6,
                    266,
                    0
                ],
                "title": "RACOON: An LLM-based Framework for Retrieval-Augmented Column Type\n  Annotation with a Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RACOON: An LLM-based Framework for Retrieval-Augmented Column Type\n  Annotation with a Knowledge Graph"
                },
                "summary": "As an important component of data exploration and integration, Column Type\nAnnotation (CTA) aims to label columns of a table with one or more semantic\ntypes. With the recent development of Large Language Models (LLMs), researchers\nhave started to explore the possibility of using LLMs for CTA, leveraging their\nstrong zero-shot capabilities. In this paper, we build on this promising work\nand improve on LLM-based methods for CTA by showing how to use a Knowledge\nGraph (KG) to augment the context information provided to the LLM. Our\napproach, called RACOON, combines both pre-trained parametric and\nnon-parametric knowledge during generation to improve LLMs' performance on CTA.\nOur experiments show that RACOON achieves up to a 0.21 micro F-1 improvement\ncompared against vanilla LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an important component of data exploration and integration, Column Type\nAnnotation (CTA) aims to label columns of a table with one or more semantic\ntypes. With the recent development of Large Language Models (LLMs), researchers\nhave started to explore the possibility of using LLMs for CTA, leveraging their\nstrong zero-shot capabilities. In this paper, we build on this promising work\nand improve on LLM-based methods for CTA by showing how to use a Knowledge\nGraph (KG) to augment the context information provided to the LLM. Our\napproach, called RACOON, combines both pre-trained parametric and\nnon-parametric knowledge during generation to improve LLMs' performance on CTA.\nOur experiments show that RACOON achieves up to a 0.21 micro F-1 improvement\ncompared against vanilla LLM inference."
                },
                "authors": [
                    {
                        "name": "Lindsey Linxi Wei"
                    },
                    {
                        "name": "Guorui Xiao"
                    },
                    {
                        "name": "Magdalena Balazinska"
                    }
                ],
                "author_detail": {
                    "name": "Magdalena Balazinska"
                },
                "author": "Magdalena Balazinska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05972v2",
                "updated": "2024-11-01T00:50:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    0,
                    50,
                    56,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-10T02:14:19Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    2,
                    14,
                    19,
                    0,
                    162,
                    0
                ],
                "title": "Decision-Making Behavior Evaluation Framework for LLMs under Uncertain\n  Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Making Behavior Evaluation Framework for LLMs under Uncertain\n  Context"
                },
                "summary": "When making decisions under uncertainty, individuals often deviate from\nrational behavior, which can be evaluated across three dimensions: risk\npreference, probability weighting, and loss aversion. Given the widespread use\nof large language models (LLMs) in decision-making processes, it is crucial to\nassess whether their behavior aligns with human norms and ethical expectations\nor exhibits potential biases. Several empirical studies have investigated the\nrationality and social behavior performance of LLMs, yet their internal\ndecision-making tendencies and capabilities remain inadequately understood.\nThis paper proposes a framework, grounded in behavioral economics, to evaluate\nthe decision-making behaviors of LLMs. Through a multiple-choice-list\nexperiment, we estimate the degree of risk preference, probability weighting,\nand loss aversion in a context-free setting for three commercial LLMs:\nChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that\nLLMs generally exhibit patterns similar to humans, such as risk aversion and\nloss aversion, with a tendency to overweight small probabilities. However,\nthere are significant variations in the degree to which these behaviors are\nexpressed across different LLMs. We also explore their behavior when embedded\nwith socio-demographic features, uncovering significant disparities. For\ninstance, when modeled with attributes of sexual minority groups or physical\ndisabilities, Claude-3-Opus displays increased risk aversion, leading to more\nconservative choices. These findings underscore the need for careful\nconsideration of the ethical implications and potential biases in deploying\nLLMs in decision-making scenarios. Therefore, this study advocates for\ndeveloping standards and guidelines to ensure that LLMs operate within ethical\nboundaries while enhancing their utility in complex decision-making\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When making decisions under uncertainty, individuals often deviate from\nrational behavior, which can be evaluated across three dimensions: risk\npreference, probability weighting, and loss aversion. Given the widespread use\nof large language models (LLMs) in decision-making processes, it is crucial to\nassess whether their behavior aligns with human norms and ethical expectations\nor exhibits potential biases. Several empirical studies have investigated the\nrationality and social behavior performance of LLMs, yet their internal\ndecision-making tendencies and capabilities remain inadequately understood.\nThis paper proposes a framework, grounded in behavioral economics, to evaluate\nthe decision-making behaviors of LLMs. Through a multiple-choice-list\nexperiment, we estimate the degree of risk preference, probability weighting,\nand loss aversion in a context-free setting for three commercial LLMs:\nChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that\nLLMs generally exhibit patterns similar to humans, such as risk aversion and\nloss aversion, with a tendency to overweight small probabilities. However,\nthere are significant variations in the degree to which these behaviors are\nexpressed across different LLMs. We also explore their behavior when embedded\nwith socio-demographic features, uncovering significant disparities. For\ninstance, when modeled with attributes of sexual minority groups or physical\ndisabilities, Claude-3-Opus displays increased risk aversion, leading to more\nconservative choices. These findings underscore the need for careful\nconsideration of the ethical implications and potential biases in deploying\nLLMs in decision-making scenarios. Therefore, this study advocates for\ndeveloping standards and guidelines to ensure that LLMs operate within ethical\nboundaries while enhancing their utility in complex decision-making\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jingru Jia"
                    },
                    {
                        "name": "Zehua Yuan"
                    },
                    {
                        "name": "Junhao Pan"
                    },
                    {
                        "name": "Paul E. McNamara"
                    },
                    {
                        "name": "Deming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Deming Chen"
                },
                "author": "Deming Chen",
                "arxiv_comment": "Jingru Jia and Zehua Yuan have equal contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00966v3",
                "updated": "2024-11-01T00:18:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    0,
                    18,
                    10,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-03T03:39:07Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    3,
                    39,
                    7,
                    0,
                    155,
                    0
                ],
                "title": "Guaranteeing Data Privacy in Federated Unlearning with Dynamic User\n  Participation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guaranteeing Data Privacy in Federated Unlearning with Dynamic User\n  Participation"
                },
                "summary": "Federated Unlearning (FU) is gaining prominence for its capability to\neliminate influences of Federated Learning (FL) users' data from trained global\nFL models. A straightforward FU method involves removing the unlearned users\nand subsequently retraining a new global FL model from scratch with all\nremaining users, a process that leads to considerable overhead. To enhance\nunlearning efficiency, a widely adopted strategy employs clustering, dividing\nFL users into clusters, with each cluster maintaining its own FL model. The\nfinal inference is then determined by aggregating the majority vote from the\ninferences of these sub-models. This method confines unlearning processes to\nindividual clusters for removing a user, thereby enhancing unlearning\nefficiency by eliminating the need for participation from all remaining users.\nHowever, current clustering-based FU schemes mainly concentrate on refining\nclustering to boost unlearning efficiency but overlook the potential\ninformation leakage from FL users' gradients, a privacy concern that has been\nextensively studied. Typically, integrating secure aggregation (SecAgg) schemes\nwithin each cluster can facilitate a privacy-preserving FU. Nevertheless,\ncrafting a clustering methodology that seamlessly incorporates SecAgg schemes\nis challenging, particularly in scenarios involving adversarial users and\ndynamic users. In this connection, we systematically explore the integration of\nSecAgg protocols within the most widely used federated unlearning scheme, which\nis based on clustering, to establish a privacy-preserving FU framework, aimed\nat ensuring privacy while effectively managing dynamic user participation.\nComprehensive theoretical assessments and experimental results show that our\nproposed scheme achieves comparable unlearning effectiveness, alongside\noffering improved privacy protection and resilience in the face of varying user\nparticipation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Unlearning (FU) is gaining prominence for its capability to\neliminate influences of Federated Learning (FL) users' data from trained global\nFL models. A straightforward FU method involves removing the unlearned users\nand subsequently retraining a new global FL model from scratch with all\nremaining users, a process that leads to considerable overhead. To enhance\nunlearning efficiency, a widely adopted strategy employs clustering, dividing\nFL users into clusters, with each cluster maintaining its own FL model. The\nfinal inference is then determined by aggregating the majority vote from the\ninferences of these sub-models. This method confines unlearning processes to\nindividual clusters for removing a user, thereby enhancing unlearning\nefficiency by eliminating the need for participation from all remaining users.\nHowever, current clustering-based FU schemes mainly concentrate on refining\nclustering to boost unlearning efficiency but overlook the potential\ninformation leakage from FL users' gradients, a privacy concern that has been\nextensively studied. Typically, integrating secure aggregation (SecAgg) schemes\nwithin each cluster can facilitate a privacy-preserving FU. Nevertheless,\ncrafting a clustering methodology that seamlessly incorporates SecAgg schemes\nis challenging, particularly in scenarios involving adversarial users and\ndynamic users. In this connection, we systematically explore the integration of\nSecAgg protocols within the most widely used federated unlearning scheme, which\nis based on clustering, to establish a privacy-preserving FU framework, aimed\nat ensuring privacy while effectively managing dynamic user participation.\nComprehensive theoretical assessments and experimental results show that our\nproposed scheme achieves comparable unlearning effectiveness, alongside\noffering improved privacy protection and resilience in the face of varying user\nparticipation."
                },
                "authors": [
                    {
                        "name": "Ziyao Liu"
                    },
                    {
                        "name": "Yu Jiang"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Jiale Guo"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok-Yan Lam"
                },
                "author": "Kwok-Yan Lam",
                "arxiv_comment": "Accepted by IEEE Transactions on Dependable and Secure Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15596v2",
                "updated": "2024-11-01T00:15:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    0,
                    15,
                    37,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-21T02:43:07Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    43,
                    7,
                    0,
                    295,
                    0
                ],
                "title": "Assessing mediation in cross-sectional stepped wedge cluster randomized\n  trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing mediation in cross-sectional stepped wedge cluster randomized\n  trials"
                },
                "summary": "Mediation analysis has been comprehensively studied for independent data but\nrelatively little work has been done for correlated data, especially for the\nincreasingly adopted stepped wedge cluster randomized trials (SW-CRTs).\nMotivated by challenges in underlying the effect mechanisms in pragmatic and\nimplementation science clinical trials, we develop new methods for mediation\nanalysis in SW-CRTs. Specifically, based on a linear and generalized linear\nmixed models, we demonstrate how to estimate the natural indirect effect and\nmediation proportion in typical SW-CRTs with four data types, including both\ncontinuous and binary mediators and outcomes. Furthermore, to address the\nemerging challenges in exposure-time treatment effect heterogeneity, we derive\nthe mediation expressions in SW-CRTs when the total effect varies as a function\nof the exposure time. The cluster jackknife approach is considered for\ninference across all data types and treatment effect structures. We conduct\nextensive simulations to evaluate the finite-sample performances of proposed\nmediation estimators and demonstrate the proposed approach in a real data\nexample. A user-friendly R package mediateSWCRT has been developed to\nfacilitate the practical implementation of the estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mediation analysis has been comprehensively studied for independent data but\nrelatively little work has been done for correlated data, especially for the\nincreasingly adopted stepped wedge cluster randomized trials (SW-CRTs).\nMotivated by challenges in underlying the effect mechanisms in pragmatic and\nimplementation science clinical trials, we develop new methods for mediation\nanalysis in SW-CRTs. Specifically, based on a linear and generalized linear\nmixed models, we demonstrate how to estimate the natural indirect effect and\nmediation proportion in typical SW-CRTs with four data types, including both\ncontinuous and binary mediators and outcomes. Furthermore, to address the\nemerging challenges in exposure-time treatment effect heterogeneity, we derive\nthe mediation expressions in SW-CRTs when the total effect varies as a function\nof the exposure time. The cluster jackknife approach is considered for\ninference across all data types and treatment effect structures. We conduct\nextensive simulations to evaluate the finite-sample performances of proposed\nmediation estimators and demonstrate the proposed approach in a real data\nexample. A user-friendly R package mediateSWCRT has been developed to\nfacilitate the practical implementation of the estimators."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Cao"
                    },
                    {
                        "name": "Fan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fan Li"
                },
                "author": "Fan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16218v2",
                "updated": "2024-11-01T00:01:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    0,
                    1,
                    1,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-23T21:05:31Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    21,
                    5,
                    31,
                    6,
                    175,
                    0
                ],
                "title": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback,\n  Execution Traces, and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback,\n  Execution Traces, and LLMs"
                },
                "summary": "We study a class of optimization problems motivated by automating the design\nand update of AI systems like coding assistants, robots, and copilots. AutoDiff\nframeworks, like PyTorch, enable efficient end-to-end optimization of\ndifferentiable systems. However, general computational workflows can be\nnon-differentiable and involve rich feedback (e.g. console output or user's\nresponses), heterogeneous parameters (e.g. prompts, codes), and intricate\nobjectives (beyond maximizing a score). We investigate end-to-end generative\noptimization -- using generative models such as LLMs within the optimizer for\nautomatic updating of general computational workflows. We discover that\nworkflow execution traces are akin to back-propagated gradients in AutoDiff and\ncan provide key information to interpret feedback for efficient optimization.\nFormally, we frame a new mathematical setup, Optimization with Trace Oracle\n(OPTO). In OPTO, an optimizer receives an execution trace along with feedback\non the computed output and updates parameters iteratively. We provide a Python\nlibrary, Trace, that efficiently converts a workflow optimization problem into\nan OPTO instance using PyTorch-like syntax. Using Trace, we develop a general\nLLM-based generative optimizer called OptoPrime. In empirical studies, we find\nthat OptoPrime is capable of first-order numerical optimization, prompt\noptimization, hyper-parameter tuning, robot controller design, code debugging,\netc., and is often competitive with specialized optimizers for each domain. We\nenvision Trace as an open research platform for devising novel generative\noptimizers and developing the next generation of interactive learning agents.\nWebsite: https://microsoft.github.io/Trace/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study a class of optimization problems motivated by automating the design\nand update of AI systems like coding assistants, robots, and copilots. AutoDiff\nframeworks, like PyTorch, enable efficient end-to-end optimization of\ndifferentiable systems. However, general computational workflows can be\nnon-differentiable and involve rich feedback (e.g. console output or user's\nresponses), heterogeneous parameters (e.g. prompts, codes), and intricate\nobjectives (beyond maximizing a score). We investigate end-to-end generative\noptimization -- using generative models such as LLMs within the optimizer for\nautomatic updating of general computational workflows. We discover that\nworkflow execution traces are akin to back-propagated gradients in AutoDiff and\ncan provide key information to interpret feedback for efficient optimization.\nFormally, we frame a new mathematical setup, Optimization with Trace Oracle\n(OPTO). In OPTO, an optimizer receives an execution trace along with feedback\non the computed output and updates parameters iteratively. We provide a Python\nlibrary, Trace, that efficiently converts a workflow optimization problem into\nan OPTO instance using PyTorch-like syntax. Using Trace, we develop a general\nLLM-based generative optimizer called OptoPrime. In empirical studies, we find\nthat OptoPrime is capable of first-order numerical optimization, prompt\noptimization, hyper-parameter tuning, robot controller design, code debugging,\netc., and is often competitive with specialized optimizers for each domain. We\nenvision Trace as an open research platform for devising novel generative\noptimizers and developing the next generation of interactive learning agents.\nWebsite: https://microsoft.github.io/Trace/."
                },
                "authors": [
                    {
                        "name": "Ching-An Cheng"
                    },
                    {
                        "name": "Allen Nie"
                    },
                    {
                        "name": "Adith Swaminathan"
                    }
                ],
                "author_detail": {
                    "name": "Adith Swaminathan"
                },
                "author": "Adith Swaminathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01006v2",
                "updated": "2024-10-31T23:44:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    23,
                    44,
                    31,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-03T05:36:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    5,
                    36,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "SemCoder: Training Code Language Models with Comprehensive Semantics\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemCoder: Training Code Language Models with Comprehensive Semantics\n  Reasoning"
                },
                "summary": "Code Large Language Models (Code LLMs) have excelled at tasks like code\ncompletion but often miss deeper semantics such as execution effects and\ndynamic states. This paper aims to bridge the gap between Code LLMs' reliance\non static text data and the need for semantic understanding for complex tasks\nlike debugging and program repair. We introduce a novel strategy, monologue\nreasoning, to train Code LLMs to reason comprehensive semantics, encompassing\nhigh-level functional descriptions, local execution effects of individual\nstatements, and overall input/output behavior, thereby linking static code text\nwith dynamic execution states. We begin by collecting PyX, a clean Python\ncorpus of fully executable code samples with functional descriptions and test\ncases. We propose training Code LLMs not only to write code but also to\nunderstand code semantics by reasoning about key properties, constraints, and\nexecution behaviors using natural language, mimicking human verbal debugging,\ni.e., rubber-duck debugging. This approach led to the development of SemCoder,\na Code LLM with only 6.7B parameters, which shows competitive performance with\nGPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder\nachieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I\n(GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also\nstudy the effectiveness of SemCoder's monologue-style execution reasoning\ncompared to concrete scratchpad reasoning, showing that our approach integrates\nsemantics from multiple dimensions more smoothly. Finally, we demonstrate the\npotential of applying learned semantics to improve Code LLMs' debugging and\nself-refining capabilities. Our data, code, and models are available at:\nhttps://github.com/ARiSE-Lab/SemCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (Code LLMs) have excelled at tasks like code\ncompletion but often miss deeper semantics such as execution effects and\ndynamic states. This paper aims to bridge the gap between Code LLMs' reliance\non static text data and the need for semantic understanding for complex tasks\nlike debugging and program repair. We introduce a novel strategy, monologue\nreasoning, to train Code LLMs to reason comprehensive semantics, encompassing\nhigh-level functional descriptions, local execution effects of individual\nstatements, and overall input/output behavior, thereby linking static code text\nwith dynamic execution states. We begin by collecting PyX, a clean Python\ncorpus of fully executable code samples with functional descriptions and test\ncases. We propose training Code LLMs not only to write code but also to\nunderstand code semantics by reasoning about key properties, constraints, and\nexecution behaviors using natural language, mimicking human verbal debugging,\ni.e., rubber-duck debugging. This approach led to the development of SemCoder,\na Code LLM with only 6.7B parameters, which shows competitive performance with\nGPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder\nachieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I\n(GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also\nstudy the effectiveness of SemCoder's monologue-style execution reasoning\ncompared to concrete scratchpad reasoning, showing that our approach integrates\nsemantics from multiple dimensions more smoothly. Finally, we demonstrate the\npotential of applying learned semantics to improve Code LLMs' debugging and\nself-refining capabilities. Our data, code, and models are available at:\nhttps://github.com/ARiSE-Lab/SemCoder."
                },
                "authors": [
                    {
                        "name": "Yangruibo Ding"
                    },
                    {
                        "name": "Jinjun Peng"
                    },
                    {
                        "name": "Marcus J. Min"
                    },
                    {
                        "name": "Gail Kaiser"
                    },
                    {
                        "name": "Junfeng Yang"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "arxiv_comment": "NeurIPS 2024 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07791v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07791v6",
                "updated": "2024-10-31T23:10:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    23,
                    10,
                    55,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-12T01:12:28Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    1,
                    12,
                    28,
                    2,
                    164,
                    0
                ],
                "title": "Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs"
                },
                "summary": "LLM-as-a-Judge presents a promising alternative to human evaluators across\nvarious tasks, but inherent biases, especially position bias - a tendency to\nfavor solutions based on their position in the prompt - have compromised its\neffectiveness. Our study introduces a systematic framework to examine position\nbias in pairwise comparisons, focusing on repetition stability, position\nconsistency, and preference fairness. This research significantly contributes\nto the field by introducing new concepts for understanding position bias and\nproviding a multi-dimensional framework for evaluations. We conducted\nexperiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks\nand approximately 40 solution-generating models - candidates, resulting in over\n100,000 evaluation instances. Our findings confirm that position bias in\ncapable LLM judges is not due to random chances, along with notable variations\nobserved across judges and tasks. Moreover, position bias is weakly influenced\nby the length of prompt components but significantly impacted by the quality\ngap between solutions. These insights can help optimize judge model selections,\nimprove benchmark design, and inform future research on debiasing strategies,\nultimately enhancing the reliability of LLM judges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge presents a promising alternative to human evaluators across\nvarious tasks, but inherent biases, especially position bias - a tendency to\nfavor solutions based on their position in the prompt - have compromised its\neffectiveness. Our study introduces a systematic framework to examine position\nbias in pairwise comparisons, focusing on repetition stability, position\nconsistency, and preference fairness. This research significantly contributes\nto the field by introducing new concepts for understanding position bias and\nproviding a multi-dimensional framework for evaluations. We conducted\nexperiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks\nand approximately 40 solution-generating models - candidates, resulting in over\n100,000 evaluation instances. Our findings confirm that position bias in\ncapable LLM judges is not due to random chances, along with notable variations\nobserved across judges and tasks. Moreover, position bias is weakly influenced\nby the length of prompt components but significantly impacted by the quality\ngap between solutions. These insights can help optimize judge model selections,\nimprove benchmark design, and inform future research on debiasing strategies,\nultimately enhancing the reliability of LLM judges."
                },
                "authors": [
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Chiyu Ma"
                    },
                    {
                        "name": "Wenhua Liang"
                    },
                    {
                        "name": "Weicheng Ma"
                    },
                    {
                        "name": "Soroush Vosoughi"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Vosoughi"
                },
                "author": "Soroush Vosoughi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07791v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07791v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10491v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10491v3",
                "updated": "2024-10-31T22:34:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    22,
                    34,
                    20,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-15T04:07:05Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    4,
                    7,
                    5,
                    5,
                    167,
                    0
                ],
                "title": "FuseMax: Leveraging Extended Einsums to Optimize Attention Accelerator\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuseMax: Leveraging Extended Einsums to Optimize Attention Accelerator\n  Design"
                },
                "summary": "Attention for transformers is a critical workload that has recently received\nsignificant \"attention\" as a target for custom acceleration. Yet, while prior\nwork succeeds in reducing attention's memory-bandwidth requirements, it creates\nload imbalance between operators that comprise the attention computation\n(resulting in severe compute under-utilization) and requires on-chip memory\nthat scales with sequence length (which is expected to grow over time).\n  This paper ameliorates these issues, enabling attention with nearly 100%\ncompute utilization, no off-chip memory traffic bottlenecks, and on-chip buffer\nsize requirements that are independent of sequence length. The main conceptual\ncontribution is to use a recently proposed abstraction -- the cascade of\nEinsums -- to describe, formalize, and taxonomize the space of attention\nalgorithms that appear in the literature. In particular, we show how Einsum\ncascades can be used to infer non-trivial lower bounds on the number of passes\na kernel must take through its input data, which has implications for either\nrequired on-chip buffer capacity or memory traffic. We show how this notion can\nbe used to meaningfully divide the space of attention algorithms into several\ncategories and use these categories to inform our design process.\n  Based on the above characterization, we propose FuseMax -- a novel mapping\nand binding of attention onto a spatial array-style architecture. On attention,\nin an iso-area comparison, FuseMax achieves an average 6.7x speedup over the\nprior state-of-the-art, FLAT, while using 79\\% of the energy. Similarly, on\nfull end-to-end transformer inference, FuseMax achieves an average 5.3x speedup\nover FLAT using 83 of the energy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention for transformers is a critical workload that has recently received\nsignificant \"attention\" as a target for custom acceleration. Yet, while prior\nwork succeeds in reducing attention's memory-bandwidth requirements, it creates\nload imbalance between operators that comprise the attention computation\n(resulting in severe compute under-utilization) and requires on-chip memory\nthat scales with sequence length (which is expected to grow over time).\n  This paper ameliorates these issues, enabling attention with nearly 100%\ncompute utilization, no off-chip memory traffic bottlenecks, and on-chip buffer\nsize requirements that are independent of sequence length. The main conceptual\ncontribution is to use a recently proposed abstraction -- the cascade of\nEinsums -- to describe, formalize, and taxonomize the space of attention\nalgorithms that appear in the literature. In particular, we show how Einsum\ncascades can be used to infer non-trivial lower bounds on the number of passes\na kernel must take through its input data, which has implications for either\nrequired on-chip buffer capacity or memory traffic. We show how this notion can\nbe used to meaningfully divide the space of attention algorithms into several\ncategories and use these categories to inform our design process.\n  Based on the above characterization, we propose FuseMax -- a novel mapping\nand binding of attention onto a spatial array-style architecture. On attention,\nin an iso-area comparison, FuseMax achieves an average 6.7x speedup over the\nprior state-of-the-art, FLAT, while using 79\\% of the energy. Similarly, on\nfull end-to-end transformer inference, FuseMax achieves an average 5.3x speedup\nover FLAT using 83 of the energy."
                },
                "authors": [
                    {
                        "name": "Nandeeka Nayak"
                    },
                    {
                        "name": "Xinrui Wu"
                    },
                    {
                        "name": "Toluwanimi O. Odemuyiwa"
                    },
                    {
                        "name": "Michael Pellauer"
                    },
                    {
                        "name": "Joel S. Emer"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    }
                ],
                "author_detail": {
                    "name": "Christopher W. Fletcher"
                },
                "author": "Christopher W. Fletcher",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10491v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10491v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01318v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01318v5",
                "updated": "2024-10-31T22:26:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    22,
                    26,
                    40,
                    3,
                    305,
                    0
                ],
                "published": "2024-03-28T02:44:02Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    2,
                    44,
                    2,
                    3,
                    88,
                    0
                ],
                "title": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large\n  Language Models"
                },
                "summary": "Jailbreak attacks cause large language models (LLMs) to generate harmful,\nunethical, or otherwise objectionable content. Evaluating these attacks\npresents a number of challenges, which the current collection of benchmarks and\nevaluation techniques do not adequately address. First, there is no clear\nstandard of practice regarding jailbreaking evaluation. Second, existing works\ncompute costs and success rates in incomparable ways. And third, numerous works\nare not reproducible, as they withhold adversarial prompts, involve\nclosed-source code, or rely on evolving proprietary APIs. To address these\nchallenges, we introduce JailbreakBench, an open-sourced benchmark with the\nfollowing components: (1) an evolving repository of state-of-the-art\nadversarial prompts, which we refer to as jailbreak artifacts; (2) a\njailbreaking dataset comprising 100 behaviors -- both original and sourced from\nprior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with\nOpenAI's usage policies; (3) a standardized evaluation framework at\nhttps://github.com/JailbreakBench/jailbreakbench that includes a clearly\ndefined threat model, system prompts, chat templates, and scoring functions;\nand (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the\nperformance of attacks and defenses for various LLMs. We have carefully\nconsidered the potential ethical implications of releasing this benchmark, and\nbelieve that it will be a net positive for the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks cause large language models (LLMs) to generate harmful,\nunethical, or otherwise objectionable content. Evaluating these attacks\npresents a number of challenges, which the current collection of benchmarks and\nevaluation techniques do not adequately address. First, there is no clear\nstandard of practice regarding jailbreaking evaluation. Second, existing works\ncompute costs and success rates in incomparable ways. And third, numerous works\nare not reproducible, as they withhold adversarial prompts, involve\nclosed-source code, or rely on evolving proprietary APIs. To address these\nchallenges, we introduce JailbreakBench, an open-sourced benchmark with the\nfollowing components: (1) an evolving repository of state-of-the-art\nadversarial prompts, which we refer to as jailbreak artifacts; (2) a\njailbreaking dataset comprising 100 behaviors -- both original and sourced from\nprior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with\nOpenAI's usage policies; (3) a standardized evaluation framework at\nhttps://github.com/JailbreakBench/jailbreakbench that includes a clearly\ndefined threat model, system prompts, chat templates, and scoring functions;\nand (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the\nperformance of attacks and defenses for various LLMs. We have carefully\nconsidered the potential ethical implications of releasing this benchmark, and\nbelieve that it will be a net positive for the community."
                },
                "authors": [
                    {
                        "name": "Patrick Chao"
                    },
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Alexander Robey"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Francesco Croce"
                    },
                    {
                        "name": "Vikash Sehwag"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Florian Tramer"
                    },
                    {
                        "name": "Hamed Hassani"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "arxiv_comment": "The camera-ready version of JailbreakBench v1.0 (accepted at NeurIPS\n  2024 Datasets and Benchmarks Track): more attack artifacts, more test-time\n  defenses, a more accurate jailbreak judge (Llama-3-70B with a custom prompt),\n  a larger dataset of human preferences for selecting a jailbreak judge (300\n  examples), an over-refusal evaluation dataset, a semantic refusal judge based\n  on Llama-3-8B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01318v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01318v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22296v2",
                "updated": "2024-10-31T21:46:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    46,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-29T17:45:57Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    45,
                    57,
                    1,
                    303,
                    0
                ],
                "title": "LLMs are Highly-Constrained Biophysical Sequence Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Highly-Constrained Biophysical Sequence Optimizers"
                },
                "summary": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Samuel D. Stanton"
                    },
                    {
                        "name": "Robert G. Alberstein"
                    },
                    {
                        "name": "Andrew M. Watkins"
                    },
                    {
                        "name": "Richard Bonneau"
                    },
                    {
                        "name": "Vladimir Gligorijevi"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Nathan C. Frey"
                    }
                ],
                "author_detail": {
                    "name": "Nathan C. Frey"
                },
                "author": "Nathan C. Frey",
                "arxiv_comment": "Supercedes arXiv:2407.00236v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14755v3",
                "updated": "2024-10-31T21:33:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    33,
                    37,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-23T16:21:57Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    21,
                    57,
                    3,
                    144,
                    0
                ],
                "title": "Large language models can be zero-shot anomaly detectors for time\n  series?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be zero-shot anomaly detectors for time\n  series?"
                },
                "summary": "Recent studies have shown the ability of large language models to perform a\nvariety of tasks, including time series forecasting. The flexible nature of\nthese models allows them to be used for many applications. In this paper, we\npresent a novel study of large language models used for the challenging task of\ntime series anomaly detection. This problem entails two aspects novel for LLMs:\nthe need for the model to identify part of the input sequence (or multiple\nparts) as anomalous; and the need for it to work with time series data rather\nthan the traditional text input. We introduce sigllm, a framework for time\nseries anomaly detection using large language models. Our framework includes a\ntime-series-to-text conversion module, as well as end-to-end pipelines that\nprompt language models to perform time series anomaly detection. We investigate\ntwo paradigms for testing the abilities of large language models to perform the\ndetection task. First, we present a prompt-based detection method that directly\nasks a language model to indicate which elements of the input are anomalies.\nSecond, we leverage the forecasting capability of a large language model to\nguide the anomaly detection process. We evaluated our framework on 11 datasets\nspanning various sources and 10 pipelines. We show that the forecasting method\nsignificantly outperformed the prompting method in all 11 datasets with respect\nto the F1 score. Moreover, while large language models are capable of finding\nanomalies, state-of-the-art deep learning models are still superior in\nperformance, achieving results 30% better than large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown the ability of large language models to perform a\nvariety of tasks, including time series forecasting. The flexible nature of\nthese models allows them to be used for many applications. In this paper, we\npresent a novel study of large language models used for the challenging task of\ntime series anomaly detection. This problem entails two aspects novel for LLMs:\nthe need for the model to identify part of the input sequence (or multiple\nparts) as anomalous; and the need for it to work with time series data rather\nthan the traditional text input. We introduce sigllm, a framework for time\nseries anomaly detection using large language models. Our framework includes a\ntime-series-to-text conversion module, as well as end-to-end pipelines that\nprompt language models to perform time series anomaly detection. We investigate\ntwo paradigms for testing the abilities of large language models to perform the\ndetection task. First, we present a prompt-based detection method that directly\nasks a language model to indicate which elements of the input are anomalies.\nSecond, we leverage the forecasting capability of a large language model to\nguide the anomaly detection process. We evaluated our framework on 11 datasets\nspanning various sources and 10 pipelines. We show that the forecasting method\nsignificantly outperformed the prompting method in all 11 datasets with respect\nto the F1 score. Moreover, while large language models are capable of finding\nanomalies, state-of-the-art deep learning models are still superior in\nperformance, achieving results 30% better than large language models."
                },
                "authors": [
                    {
                        "name": "Sarah Alnegheimish"
                    },
                    {
                        "name": "Linh Nguyen"
                    },
                    {
                        "name": "Laure Berti-Equille"
                    },
                    {
                        "name": "Kalyan Veeramachaneni"
                    }
                ],
                "author_detail": {
                    "name": "Kalyan Veeramachaneni"
                },
                "author": "Kalyan Veeramachaneni",
                "arxiv_comment": "This work is accepted by IEEE International Conference on Data\n  Science and Advanced Analytics (DSAA 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02982v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02982v4",
                "updated": "2024-10-31T21:30:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    30,
                    45,
                    3,
                    305,
                    0
                ],
                "published": "2024-04-03T18:07:02Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    18,
                    7,
                    2,
                    2,
                    94,
                    0
                ],
                "title": "Spatio-temporal count autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal count autoregression"
                },
                "summary": "We study the problem of modeling and inference for spatio-temporal count\nprocesses. Our approach uses parsimonious parameterisations of multivariate\nautoregressive count time series models, including possible regression on\ncovariates. We control the number of parameters by specifying spatial\nneighbourhood structures for possibly huge matrices that take into account\nspatio-temporal dependencies. This work is motivated by real data applications\nwhich call for suitable models. Extensive simulation studies show that our\napproach yields reliable estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of modeling and inference for spatio-temporal count\nprocesses. Our approach uses parsimonious parameterisations of multivariate\nautoregressive count time series models, including possible regression on\ncovariates. We control the number of parameters by specifying spatial\nneighbourhood structures for possibly huge matrices that take into account\nspatio-temporal dependencies. This work is motivated by real data applications\nwhich call for suitable models. Extensive simulation studies show that our\napproach yields reliable estimators."
                },
                "authors": [
                    {
                        "name": "Steffen Maletz"
                    },
                    {
                        "name": "Konstantinos Fokianos"
                    },
                    {
                        "name": "Roland Fried"
                    }
                ],
                "author_detail": {
                    "name": "Roland Fried"
                },
                "author": "Roland Fried",
                "arxiv_comment": "24 pages, 16 figures and 22 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02982v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02982v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18381v2",
                "updated": "2024-10-31T20:29:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    20,
                    29,
                    28,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-24T02:45:13Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    2,
                    45,
                    13,
                    3,
                    298,
                    0
                ],
                "title": "Inference on High Dimensional Selective Labeling Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on High Dimensional Selective Labeling Models"
                },
                "summary": "A class of simultaneous equation models arise in the many domains where\nobserved binary outcomes are themselves a consequence of the existing choices\nof of one of the agents in the model. These models are gaining increasing\ninterest in the computer science and machine learning literatures where they\nrefer the potentially endogenous sample selection as the {\\em selective labels}\nproblem. Empirical settings for such models arise in fields as diverse as\ncriminal justice, health care, and insurance. For important recent work in this\narea, see for example Lakkaruju et al. (2017), Kleinberg et al. (2018), and\nCoston et al.(2021) where the authors focus on judicial bail decisions, and\nwhere one observes the outcome of whether a defendant filed to return for their\ncourt appearance only if the judge in the case decides to release the defendant\non bail. Identifying and estimating such models can be computationally\nchallenging for two reasons. One is the nonconcavity of the bivariate\nlikelihood function, and the other is the large number of covariates in each\nequation. Despite these challenges, in this paper we propose a novel\ndistribution free estimation procedure that is computationally friendly in many\ncovariates settings. The new method combines the semiparametric batched\ngradient descent algorithm introduced in Khan et al.(2023) with a novel sorting\nalgorithms incorporated to control for selection bias. Asymptotic properties of\nthe new procedure are established under increasing dimension conditions in both\nequations, and its finite sample properties are explored through a simulation\nstudy and an application using judicial bail data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A class of simultaneous equation models arise in the many domains where\nobserved binary outcomes are themselves a consequence of the existing choices\nof of one of the agents in the model. These models are gaining increasing\ninterest in the computer science and machine learning literatures where they\nrefer the potentially endogenous sample selection as the {\\em selective labels}\nproblem. Empirical settings for such models arise in fields as diverse as\ncriminal justice, health care, and insurance. For important recent work in this\narea, see for example Lakkaruju et al. (2017), Kleinberg et al. (2018), and\nCoston et al.(2021) where the authors focus on judicial bail decisions, and\nwhere one observes the outcome of whether a defendant filed to return for their\ncourt appearance only if the judge in the case decides to release the defendant\non bail. Identifying and estimating such models can be computationally\nchallenging for two reasons. One is the nonconcavity of the bivariate\nlikelihood function, and the other is the large number of covariates in each\nequation. Despite these challenges, in this paper we propose a novel\ndistribution free estimation procedure that is computationally friendly in many\ncovariates settings. The new method combines the semiparametric batched\ngradient descent algorithm introduced in Khan et al.(2023) with a novel sorting\nalgorithms incorporated to control for selection bias. Asymptotic properties of\nthe new procedure are established under increasing dimension conditions in both\nequations, and its finite sample properties are explored through a simulation\nstudy and an application using judicial bail data."
                },
                "authors": [
                    {
                        "name": "Shakeeb Khan"
                    },
                    {
                        "name": "Elie Tamer"
                    },
                    {
                        "name": "Qingsong Yao"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Yao"
                },
                "author": "Qingsong Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18923v2",
                "updated": "2024-10-31T19:44:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    44,
                    5,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-24T17:11:52Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    11,
                    52,
                    3,
                    298,
                    0
                ],
                "title": "SegLLM: Multi-round Reasoning Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SegLLM: Multi-round Reasoning Segmentation"
                },
                "summary": "We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization."
                },
                "authors": [
                    {
                        "name": "XuDong Wang"
                    },
                    {
                        "name": "Shaolun Zhang"
                    },
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Trevor Darrell"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Darrell"
                },
                "author": "Trevor Darrell",
                "arxiv_comment": "22 pages, 10 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01178v2",
                "updated": "2024-10-31T19:43:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    43,
                    33,
                    3,
                    305,
                    0
                ],
                "published": "2024-03-02T11:12:13Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    11,
                    12,
                    13,
                    5,
                    62,
                    0
                ],
                "title": "RAIDER: Rapid, anatomy-independent, deep learning-based PDFF and R2*\n  estimation using magnitude-only signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIDER: Rapid, anatomy-independent, deep learning-based PDFF and R2*\n  estimation using magnitude-only signals"
                },
                "summary": "Purpose: There has been substantial recent interest in magnitude-based\nfitting methods for estimating proton density fat fraction (PDFF) and R2* from\nchemical shift-encoded MRI data, since these methods can still be used when\ncomplex-based methods fail or when phase data are inaccessible or unreliable,\nand may also be used as a final processing step with complex-based methods.\nHowever, conventional fitting techniques are computationally expensive. Deep\nlearning (DL)-based methods promise to accelerate parameter estimation, but\nprevious attempts have used convolutional neural networks (CNNs), which are\nlimited by training requirements and poor generalizability\n(anatomy-dependence). To address these limitations, we propose RAIDER, a\nvoxelwise method for rapid, anatomy-independent deep learning-based PDFF and\nR2* estimation using multi-echo magnitude-data. Theory and Methods: RAIDER uses\ntwo multilayer perceptrons, each trained separately with simulated single-voxel\nmulti-echo magnitude signals, to estimate PDFF and R2*. The use of two\nnetworks, each with restricted training distribution, solves the problem of\ndegeneracy during training. During inference, the solution from one of the two\nnetworks is chosen based on likelihood. Performance and speed are investigated\nin a series of simulation experiments, in phantoms and in vivo. Results: RAIDER\nis 285-1450 times faster than conventional magnitude fitting, taking 1.4-2.3s\nper slice rather than 8-56minutes, and offers performance similar to\nconventional fitting. It produces accurate PDFF measurements in phantoms and in\nvivo images with different anatomies, despite having been trained only on\nsimulation data. Conclusion: RAIDER can substantially accelerate\nmagnitude-based PDFF and R2* estimation, whilst avoiding intrinsic limitations\nof CNN-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: There has been substantial recent interest in magnitude-based\nfitting methods for estimating proton density fat fraction (PDFF) and R2* from\nchemical shift-encoded MRI data, since these methods can still be used when\ncomplex-based methods fail or when phase data are inaccessible or unreliable,\nand may also be used as a final processing step with complex-based methods.\nHowever, conventional fitting techniques are computationally expensive. Deep\nlearning (DL)-based methods promise to accelerate parameter estimation, but\nprevious attempts have used convolutional neural networks (CNNs), which are\nlimited by training requirements and poor generalizability\n(anatomy-dependence). To address these limitations, we propose RAIDER, a\nvoxelwise method for rapid, anatomy-independent deep learning-based PDFF and\nR2* estimation using multi-echo magnitude-data. Theory and Methods: RAIDER uses\ntwo multilayer perceptrons, each trained separately with simulated single-voxel\nmulti-echo magnitude signals, to estimate PDFF and R2*. The use of two\nnetworks, each with restricted training distribution, solves the problem of\ndegeneracy during training. During inference, the solution from one of the two\nnetworks is chosen based on likelihood. Performance and speed are investigated\nin a series of simulation experiments, in phantoms and in vivo. Results: RAIDER\nis 285-1450 times faster than conventional magnitude fitting, taking 1.4-2.3s\nper slice rather than 8-56minutes, and offers performance similar to\nconventional fitting. It produces accurate PDFF measurements in phantoms and in\nvivo images with different anatomies, despite having been trained only on\nsimulation data. Conclusion: RAIDER can substantially accelerate\nmagnitude-based PDFF and R2* estimation, whilst avoiding intrinsic limitations\nof CNN-based methods."
                },
                "authors": [
                    {
                        "name": "T. J. P. Bray"
                    },
                    {
                        "name": "G. V. Minore"
                    },
                    {
                        "name": "A. Bainbridge"
                    },
                    {
                        "name": "L. Dwyer-Hemmings"
                    },
                    {
                        "name": "S. A. Taylor"
                    },
                    {
                        "name": "M. A. Hall-Craggs"
                    },
                    {
                        "name": "H. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "H. Zhang"
                },
                "author": "H. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17946v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17946v4",
                "updated": "2024-10-31T19:38:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    38,
                    15,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-28T00:09:07Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    0,
                    9,
                    7,
                    2,
                    59,
                    0
                ],
                "title": "SparseLLM: Towards Global Pruning for Pre-trained Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseLLM: Towards Global Pruning for Pre-trained Language Models"
                },
                "summary": "The transformative impact of large language models (LLMs) like LLaMA and GPT\non natural language processing is countered by their prohibitive computational\ndemands. Pruning has emerged as a pivotal compression strategy, introducing\nsparsity to enhance both memory and computational efficiency. Yet, traditional\nglobal pruning is impractical for LLMs due to scalability issues, while local\npruning, despite its efficiency, leads to suboptimal solutions. Addressing\nthese challenges, we propose SparseLLM, a novel framework that redefines the\nglobal pruning process into manageable, coordinated subproblems, allowing for\nresource-efficient optimization with global optimality. SparseLLM's approach,\nwhich conceptualizes LLMs as a chain of modular functions and leverages\nauxiliary variables for problem decomposition, not only facilitates a pragmatic\napplication on LLMs but also demonstrates significant performance improvements,\nparticularly in high-sparsity regimes where it surpasses current\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformative impact of large language models (LLMs) like LLaMA and GPT\non natural language processing is countered by their prohibitive computational\ndemands. Pruning has emerged as a pivotal compression strategy, introducing\nsparsity to enhance both memory and computational efficiency. Yet, traditional\nglobal pruning is impractical for LLMs due to scalability issues, while local\npruning, despite its efficiency, leads to suboptimal solutions. Addressing\nthese challenges, we propose SparseLLM, a novel framework that redefines the\nglobal pruning process into manageable, coordinated subproblems, allowing for\nresource-efficient optimization with global optimality. SparseLLM's approach,\nwhich conceptualizes LLMs as a chain of modular functions and leverages\nauxiliary variables for problem decomposition, not only facilitates a pragmatic\napplication on LLMs but also demonstrates significant performance improvements,\nparticularly in high-sparsity regimes where it surpasses current\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Guangji Bai"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Chen Ling"
                    },
                    {
                        "name": "Kibaek Kim"
                    },
                    {
                        "name": "Liang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhao"
                },
                "author": "Liang Zhao",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17946v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17946v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19302v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19302v3",
                "updated": "2024-10-31T19:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    37,
                    7,
                    3,
                    305,
                    0
                ],
                "published": "2024-03-28T10:40:22Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    10,
                    40,
                    22,
                    3,
                    88,
                    0
                ],
                "title": "Generating Multi-Aspect Queries for Conversational Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Multi-Aspect Queries for Conversational Search"
                },
                "summary": "Conversational information seeking (CIS) systems aim to model the user's\ninformation need within the conversational context and retrieve the relevant\ninformation. One major approach to modeling the conversational context aims to\nrewrite the user utterance in the conversation to represent the information\nneed independently. Recent work has shown the benefit of expanding the\nrewritten utterance with relevant terms. In this work, we hypothesize that\nbreaking down the information of an utterance into multi-aspect rewritten\nqueries can lead to more effective retrieval performance. This is more evident\nin more complex utterances that require gathering evidence from various\ninformation sources, where a single query rewrite or query representation\ncannot capture the complexity of the utterance. To test this hypothesis, we\nconduct extensive experiments on five widely used CIS datasets where we\nleverage LLMs to generate multi-aspect queries to represent the information\nneed for each utterance in multiple query rewrites. We show that, for most of\nthe utterances, the same retrieval model would perform better with more than\none rewritten query by 85% in terms of nDCG@3. We further propose a\nmulti-aspect query generation and retrieval framework, called MQ4CS. Our\nextensive experiments show that MQ4CS outperforms the state-of-the-art query\nrewriting methods. We make our code and our new dataset of generated\nmulti-aspect queries publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational information seeking (CIS) systems aim to model the user's\ninformation need within the conversational context and retrieve the relevant\ninformation. One major approach to modeling the conversational context aims to\nrewrite the user utterance in the conversation to represent the information\nneed independently. Recent work has shown the benefit of expanding the\nrewritten utterance with relevant terms. In this work, we hypothesize that\nbreaking down the information of an utterance into multi-aspect rewritten\nqueries can lead to more effective retrieval performance. This is more evident\nin more complex utterances that require gathering evidence from various\ninformation sources, where a single query rewrite or query representation\ncannot capture the complexity of the utterance. To test this hypothesis, we\nconduct extensive experiments on five widely used CIS datasets where we\nleverage LLMs to generate multi-aspect queries to represent the information\nneed for each utterance in multiple query rewrites. We show that, for most of\nthe utterances, the same retrieval model would perform better with more than\none rewritten query by 85% in terms of nDCG@3. We further propose a\nmulti-aspect query generation and retrieval framework, called MQ4CS. Our\nextensive experiments show that MQ4CS outperforms the state-of-the-art query\nrewriting methods. We make our code and our new dataset of generated\nmulti-aspect queries publicly available."
                },
                "authors": [
                    {
                        "name": "Zahra Abbasiantaeb"
                    },
                    {
                        "name": "Simon Lupart"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Aliannejadi"
                },
                "author": "Mohammad Aliannejadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19302v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19302v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10648v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10648v3",
                "updated": "2024-10-31T19:26:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    26,
                    43,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-14T15:59:16Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    59,
                    16,
                    0,
                    288,
                    0
                ],
                "title": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers"
                },
                "summary": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences."
                },
                "authors": [
                    {
                        "name": "Alex Stein"
                    },
                    {
                        "name": "Samuel Sharpe"
                    },
                    {
                        "name": "Doron Bergman"
                    },
                    {
                        "name": "Senthil Kumar"
                    },
                    {
                        "name": "C. Bayan Bruss"
                    },
                    {
                        "name": "John Dickerson"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum",
                "arxiv_comment": "10 pages, 6 pages of references+appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10648v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10648v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09359v2",
                "updated": "2024-10-31T19:02:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    2,
                    17,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-14T08:17:30Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    8,
                    17,
                    30,
                    5,
                    258,
                    0
                ],
                "title": "Symbolic Regression with a Learned Concept Library",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Regression with a Learned Concept Library"
                },
                "summary": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs."
                },
                "authors": [
                    {
                        "name": "Arya Grayeli"
                    },
                    {
                        "name": "Atharva Sehgal"
                    },
                    {
                        "name": "Omar Costilla-Reyes"
                    },
                    {
                        "name": "Miles Cranmer"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Swarat Chaudhuri"
                },
                "author": "Swarat Chaudhuri",
                "arxiv_comment": "NeurIPS version; 10 pages; no checklist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06423v3",
                "updated": "2024-10-31T18:43:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    43,
                    1,
                    3,
                    305,
                    0
                ],
                "published": "2024-08-12T18:01:50Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    1,
                    50,
                    0,
                    225,
                    0
                ],
                "title": "Evaluating LLMs on Entity Disambiguation in Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs on Entity Disambiguation in Tables"
                },
                "summary": "Tables are crucial containers of information, but understanding their meaning\nmay be challenging. Over the years, there has been a surge in interest in\ndata-driven approaches based on deep learning that have increasingly been\ncombined with heuristic-based ones. In the last period, the advent of\n\\acf{llms} has led to a new category of approaches for table annotation.\nHowever, these approaches have not been consistently evaluated on a common\nground, making evaluation and comparison difficult. This work proposes an\nextensive evaluation of four STI SOTA approaches: Alligator (formerly s-elbat),\nDagobah, TURL, and TableLlama; the first two belong to the family of\nheuristic-based algorithms, while the others are respectively encoder-only and\ndecoder-only Large Language Models (LLMs). We also include in the evaluation\nboth GPT-4o and GPT-4o-mini, since they excel in various public benchmarks. The\nprimary objective is to measure the ability of these approaches to solve the\nentity disambiguation task with respect to both the performance achieved on a\ncommon-ground evaluation setting and the computational and cost requirements\ninvolved, with the ultimate aim of charting new research paths in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables are crucial containers of information, but understanding their meaning\nmay be challenging. Over the years, there has been a surge in interest in\ndata-driven approaches based on deep learning that have increasingly been\ncombined with heuristic-based ones. In the last period, the advent of\n\\acf{llms} has led to a new category of approaches for table annotation.\nHowever, these approaches have not been consistently evaluated on a common\nground, making evaluation and comparison difficult. This work proposes an\nextensive evaluation of four STI SOTA approaches: Alligator (formerly s-elbat),\nDagobah, TURL, and TableLlama; the first two belong to the family of\nheuristic-based algorithms, while the others are respectively encoder-only and\ndecoder-only Large Language Models (LLMs). We also include in the evaluation\nboth GPT-4o and GPT-4o-mini, since they excel in various public benchmarks. The\nprimary objective is to measure the ability of these approaches to solve the\nentity disambiguation task with respect to both the performance achieved on a\ncommon-ground evaluation setting and the computational and cost requirements\ninvolved, with the ultimate aim of charting new research paths in the field."
                },
                "authors": [
                    {
                        "name": "Federico Belotti"
                    },
                    {
                        "name": "Fabio Dadda"
                    },
                    {
                        "name": "Marco Cremaschi"
                    },
                    {
                        "name": "Roberto Avogadro"
                    },
                    {
                        "name": "Matteo Palmonari"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Palmonari"
                },
                "author": "Matteo Palmonari",
                "arxiv_comment": "13 pages, 6 figures; fixed avg. accuracy-over-price plot for GPT\n  families, fixed typos in table referencing, added evaluation and inference\n  subsubsection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13959v2",
                "updated": "2024-10-31T18:38:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    38,
                    37,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-17T18:34:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    18,
                    34,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven\n  Question Answering Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven\n  Question Answering Pipeline"
                },
                "summary": "Financial decision-making hinges on the analysis of relevant information\nembedded in the enormous volume of documents in the financial domain. To\naddress this challenge, we developed FinQAPT, an end-to-end pipeline that\nstreamlines the identification of relevant financial reports based on a query,\nextracts pertinent context, and leverages Large Language Models (LLMs) to\nperform downstream tasks. To evaluate the pipeline, we experimented with\nvarious techniques to optimize the performance of each module using the FinQA\ndataset. We introduced a novel clustering-based negative sampling technique to\nenhance context extraction and a novel prompting method called Dynamic N-shot\nPrompting to boost the numerical question-answering capabilities of LLMs. At\nthe module level, we achieved state-of-the-art accuracy on FinQA, attaining an\naccuracy of 80.6%. However, at the pipeline level, we observed decreased\nperformance due to challenges in extracting relevant context from financial\nreports. We conducted a detailed error analysis of each module and the\nend-to-end pipeline, pinpointing specific challenges that must be addressed to\ndevelop a robust solution for handling complex financial tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial decision-making hinges on the analysis of relevant information\nembedded in the enormous volume of documents in the financial domain. To\naddress this challenge, we developed FinQAPT, an end-to-end pipeline that\nstreamlines the identification of relevant financial reports based on a query,\nextracts pertinent context, and leverages Large Language Models (LLMs) to\nperform downstream tasks. To evaluate the pipeline, we experimented with\nvarious techniques to optimize the performance of each module using the FinQA\ndataset. We introduced a novel clustering-based negative sampling technique to\nenhance context extraction and a novel prompting method called Dynamic N-shot\nPrompting to boost the numerical question-answering capabilities of LLMs. At\nthe module level, we achieved state-of-the-art accuracy on FinQA, attaining an\naccuracy of 80.6%. However, at the pipeline level, we observed decreased\nperformance due to challenges in extracting relevant context from financial\nreports. We conducted a detailed error analysis of each module and the\nend-to-end pipeline, pinpointing specific challenges that must be addressed to\ndevelop a robust solution for handling complex financial tasks."
                },
                "authors": [
                    {
                        "name": "Kuldeep Singh"
                    },
                    {
                        "name": "Simerjot Kaur"
                    },
                    {
                        "name": "Charese Smiley"
                    }
                ],
                "author_detail": {
                    "name": "Charese Smiley"
                },
                "author": "Charese Smiley",
                "arxiv_doi": "10.1145/3677052.3698682",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3677052.3698682",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.13959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in ICAIF 2024, 8 pages, 5 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.2.6; I.5.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20290v2",
                "updated": "2024-10-31T18:27:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    27,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-26T23:20:48Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    23,
                    20,
                    48,
                    5,
                    300,
                    0
                ],
                "title": "Fast Best-of-N Decoding via Speculative Rejection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Best-of-N Decoding via Speculative Rejection"
                },
                "summary": "The safe and effective deployment of Large Language Models (LLMs) involves a\ncritical step called alignment, which ensures that the model's responses are in\naccordance with human preferences. Prevalent alignment techniques, such as DPO,\nPPO and their variants, align LLMs by changing the pre-trained model weights\nduring a phase called post-training. While predominant, these post-training\nmethods add substantial complexity before LLMs can be deployed. Inference-time\nalignment methods avoid the complex post-training step and instead bias the\ngeneration towards responses that are aligned with human preferences. The\nbest-known inference-time alignment method, called Best-of-N, is as effective\nas the state-of-the-art post-training procedures. Unfortunately, Best-of-N\nrequires vastly more resources at inference time than standard decoding\nstrategies, which makes it computationally not viable. In this work, we\nintroduce Speculative Rejection, a computationally-viable inference-time\nalignment algorithm. It generates high-scoring responses according to a given\nreward model, like Best-of-N does, while being between 16 to 32 times more\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safe and effective deployment of Large Language Models (LLMs) involves a\ncritical step called alignment, which ensures that the model's responses are in\naccordance with human preferences. Prevalent alignment techniques, such as DPO,\nPPO and their variants, align LLMs by changing the pre-trained model weights\nduring a phase called post-training. While predominant, these post-training\nmethods add substantial complexity before LLMs can be deployed. Inference-time\nalignment methods avoid the complex post-training step and instead bias the\ngeneration towards responses that are aligned with human preferences. The\nbest-known inference-time alignment method, called Best-of-N, is as effective\nas the state-of-the-art post-training procedures. Unfortunately, Best-of-N\nrequires vastly more resources at inference time than standard decoding\nstrategies, which makes it computationally not viable. In this work, we\nintroduce Speculative Rejection, a computationally-viable inference-time\nalignment algorithm. It generates high-scoring responses according to a given\nreward model, like Best-of-N does, while being between 16 to 32 times more\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Momin Haider"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Huitao Yang"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Andrea Zanette"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanette"
                },
                "author": "Andrea Zanette",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05817v2",
                "updated": "2024-10-31T18:20:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    20,
                    32,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-08T08:47:11Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    47,
                    11,
                    1,
                    282,
                    0
                ],
                "title": "Probing Language Models on Their Knowledge Source",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Language Models on Their Knowledge Source"
                },
                "summary": "Large Language Models (LLMs) often encounter conflicts between their learned,\ninternal (parametric knowledge, PK) and external knowledge provided during\ninference (contextual knowledge, CK). Understanding how LLMs models prioritize\none knowledge source over the other remains a challenge. In this paper, we\npropose a novel probing framework to explore the mechanisms governing the\nselection between PK and CK in LLMs. Using controlled prompts designed to\ncontradict the model's PK, we demonstrate that specific model activations are\nindicative of the knowledge source employed. We evaluate this framework on\nvarious LLMs of different sizes and demonstrate that mid-layer activations,\nparticularly those related to relations in the input, are crucial in predicting\nknowledge source selection, paving the way for more reliable models capable of\nhandling knowledge conflicts effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often encounter conflicts between their learned,\ninternal (parametric knowledge, PK) and external knowledge provided during\ninference (contextual knowledge, CK). Understanding how LLMs models prioritize\none knowledge source over the other remains a challenge. In this paper, we\npropose a novel probing framework to explore the mechanisms governing the\nselection between PK and CK in LLMs. Using controlled prompts designed to\ncontradict the model's PK, we demonstrate that specific model activations are\nindicative of the knowledge source employed. We evaluate this framework on\nvarious LLMs of different sizes and demonstrate that mid-layer activations,\nparticularly those related to relations in the input, are crucial in predicting\nknowledge source selection, paving the way for more reliable models capable of\nhandling knowledge conflicts effectively."
                },
                "authors": [
                    {
                        "name": "Zineddine Tighidet"
                    },
                    {
                        "name": "Andrea Mogini"
                    },
                    {
                        "name": "Jiali Mei"
                    },
                    {
                        "name": "Benjamin Piwowarski"
                    },
                    {
                        "name": "Patrick Gallinari"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Gallinari"
                },
                "author": "Patrick Gallinari",
                "arxiv_comment": "Accepted at BlackBoxNLP@EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19690v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19690v3",
                "updated": "2024-10-31T18:09:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    9,
                    38,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-30T05:04:33Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    5,
                    4,
                    33,
                    3,
                    151,
                    0
                ],
                "title": "Diffusion Policies creating a Trust Region for Offline Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies creating a Trust Region for Offline Reinforcement\n  Learning"
                },
                "summary": "Offline reinforcement learning (RL) leverages pre-collected datasets to train\noptimal policies. Diffusion Q-Learning (DQL), introducing diffusion models as a\npowerful and expressive policy class, significantly boosts the performance of\noffline RL. However, its reliance on iterative denoising sampling to generate\nactions slows down both training and inference. While several recent attempts\nhave tried to accelerate diffusion-QL, the improvement in training and/or\ninference speed often results in degraded performance. In this paper, we\nintroduce a dual policy approach, Diffusion Trusted Q-Learning (DTQL), which\ncomprises a diffusion policy for pure behavior cloning and a practical one-step\npolicy. We bridge the two polices by a newly introduced diffusion trust region\nloss. The diffusion policy maintains expressiveness, while the trust region\nloss directs the one-step policy to explore freely and seek modes within the\nregion defined by the diffusion policy. DTQL eliminates the need for iterative\ndenoising sampling during both training and inference, making it remarkably\ncomputationally efficient. We evaluate its effectiveness and algorithmic\ncharacteristics against popular Kullback--Leibler divergence-based distillation\nmethods in 2D bandit scenarios and gym tasks. We then show that DTQL could not\nonly outperform other methods on the majority of the D4RL benchmark tasks but\nalso demonstrate efficiency in training and inference speeds. The PyTorch\nimplementation is available at\nhttps://github.com/TianyuCodings/Diffusion_Trusted_Q_Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline reinforcement learning (RL) leverages pre-collected datasets to train\noptimal policies. Diffusion Q-Learning (DQL), introducing diffusion models as a\npowerful and expressive policy class, significantly boosts the performance of\noffline RL. However, its reliance on iterative denoising sampling to generate\nactions slows down both training and inference. While several recent attempts\nhave tried to accelerate diffusion-QL, the improvement in training and/or\ninference speed often results in degraded performance. In this paper, we\nintroduce a dual policy approach, Diffusion Trusted Q-Learning (DTQL), which\ncomprises a diffusion policy for pure behavior cloning and a practical one-step\npolicy. We bridge the two polices by a newly introduced diffusion trust region\nloss. The diffusion policy maintains expressiveness, while the trust region\nloss directs the one-step policy to explore freely and seek modes within the\nregion defined by the diffusion policy. DTQL eliminates the need for iterative\ndenoising sampling during both training and inference, making it remarkably\ncomputationally efficient. We evaluate its effectiveness and algorithmic\ncharacteristics against popular Kullback--Leibler divergence-based distillation\nmethods in 2D bandit scenarios and gym tasks. We then show that DTQL could not\nonly outperform other methods on the majority of the D4RL benchmark tasks but\nalso demonstrate efficiency in training and inference speeds. The PyTorch\nimplementation is available at\nhttps://github.com/TianyuCodings/Diffusion_Trusted_Q_Learning."
                },
                "authors": [
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Zhendong Wang"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Mingyuan Zhou"
                },
                "author": "Mingyuan Zhou",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19690v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19690v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23277v2",
                "updated": "2024-10-31T18:03:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    3,
                    51,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T17:55:52Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    55,
                    52,
                    2,
                    304,
                    0
                ],
                "title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video\n  Generation"
                },
                "summary": "Human beings are endowed with a complementary learning system, which bridges\nthe slow learning of general world dynamics with fast storage of episodic\nmemory from a new experience. Previous video generation models, however,\nprimarily focus on slow learning by pre-training on vast amounts of data,\noverlooking the fast learning phase crucial for episodic memory storage. This\noversight leads to inconsistencies across temporally distant frames when\ngenerating longer videos, as these frames fall beyond the model's context\nwindow. To this end, we introduce SlowFast-VGen, a novel dual-speed learning\nsystem for action-driven long video generation. Our approach incorporates a\nmasked conditional video diffusion model for the slow learning of world\ndynamics, alongside an inference-time fast learning strategy based on a\ntemporal LoRA module. Specifically, the fast learning process updates its\ntemporal LoRA parameters based on local inputs and outputs, thereby efficiently\nstoring episodic memory in its parameters. We further propose a slow-fast\nlearning loop algorithm that seamlessly integrates the inner fast learning loop\ninto the outer slow learning loop, enabling the recall of prior multi-episode\nexperiences for context-aware skill learning. To facilitate the slow learning\nof an approximate world model, we collect a large-scale dataset of 200k videos\nwith language action annotations, covering a wide range of scenarios. Extensive\nexperiments show that SlowFast-VGen outperforms baselines across various\nmetrics for action-driven video generation, achieving an FVD score of 514\ncompared to 782, and maintaining consistency in longer videos, with an average\nof 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm\nsignificantly enhances performances on long-horizon planning tasks as well.\nProject Website: https://slowfast-vgen.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human beings are endowed with a complementary learning system, which bridges\nthe slow learning of general world dynamics with fast storage of episodic\nmemory from a new experience. Previous video generation models, however,\nprimarily focus on slow learning by pre-training on vast amounts of data,\noverlooking the fast learning phase crucial for episodic memory storage. This\noversight leads to inconsistencies across temporally distant frames when\ngenerating longer videos, as these frames fall beyond the model's context\nwindow. To this end, we introduce SlowFast-VGen, a novel dual-speed learning\nsystem for action-driven long video generation. Our approach incorporates a\nmasked conditional video diffusion model for the slow learning of world\ndynamics, alongside an inference-time fast learning strategy based on a\ntemporal LoRA module. Specifically, the fast learning process updates its\ntemporal LoRA parameters based on local inputs and outputs, thereby efficiently\nstoring episodic memory in its parameters. We further propose a slow-fast\nlearning loop algorithm that seamlessly integrates the inner fast learning loop\ninto the outer slow learning loop, enabling the recall of prior multi-episode\nexperiences for context-aware skill learning. To facilitate the slow learning\nof an approximate world model, we collect a large-scale dataset of 200k videos\nwith language action annotations, covering a wide range of scenarios. Extensive\nexperiments show that SlowFast-VGen outperforms baselines across various\nmetrics for action-driven video generation, achieving an FVD score of 514\ncompared to 782, and maintaining consistency in longer videos, with an average\nof 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm\nsignificantly enhances performances on long-horizon planning tasks as well.\nProject Website: https://slowfast-vgen.github.io"
                },
                "authors": [
                    {
                        "name": "Yining Hong"
                    },
                    {
                        "name": "Beide Liu"
                    },
                    {
                        "name": "Maxine Wu"
                    },
                    {
                        "name": "Yuanhao Zhai"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Chung-Ching Lin"
                    },
                    {
                        "name": "Jianfeng Wang"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Yingnian Wu"
                    },
                    {
                        "name": "Lijuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijuan Wang"
                },
                "author": "Lijuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24222v1",
                "updated": "2024-10-31T17:59:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    56,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:59:56Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    56,
                    3,
                    305,
                    0
                ],
                "title": "Robust Gaussian Processes via Relevance Pursuit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Gaussian Processes via Relevance Pursuit"
                },
                "summary": "Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range."
                },
                "authors": [
                    {
                        "name": "Sebastian Ament"
                    },
                    {
                        "name": "Elizabeth Santorella"
                    },
                    {
                        "name": "David Eriksson"
                    },
                    {
                        "name": "Ben Letham"
                    },
                    {
                        "name": "Maximilian Balandat"
                    },
                    {
                        "name": "Eytan Bakshy"
                    }
                ],
                "author_detail": {
                    "name": "Eytan Bakshy"
                },
                "author": "Eytan Bakshy",
                "arxiv_comment": "NeurIPS 2024 Article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24223v1",
                "updated": "2024-10-31T17:59:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    56,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:59:56Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    56,
                    3,
                    305,
                    0
                ],
                "title": "URAvatar: Universal Relightable Gaussian Codec Avatars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URAvatar: Universal Relightable Gaussian Codec Avatars"
                },
                "summary": "We present a new approach to creating photorealistic and relightable head\navatars from a phone scan with unknown illumination. The reconstructed avatars\ncan be animated and relit in real time with the global illumination of diverse\nenvironments. Unlike existing approaches that estimate parametric reflectance\nparameters via inverse rendering, our approach directly models learnable\nradiance transfer that incorporates global light transport in an efficient\nmanner for real-time rendering. However, learning such a complex light\ntransport that can generalize across identities is non-trivial. A phone scan in\na single environment lacks sufficient information to infer how the head would\nappear in general environments. To address this, we build a universal\nrelightable avatar model represented by 3D Gaussians. We train on hundreds of\nhigh-quality multi-view human scans with controllable point lights.\nHigh-resolution geometric guidance further enhances the reconstruction accuracy\nand generalization. Once trained, we finetune the pretrained model on a phone\nscan using inverse rendering to obtain a personalized relightable avatar. Our\nexperiments establish the efficacy of our design, outperforming existing\napproaches while retaining real-time rendering capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new approach to creating photorealistic and relightable head\navatars from a phone scan with unknown illumination. The reconstructed avatars\ncan be animated and relit in real time with the global illumination of diverse\nenvironments. Unlike existing approaches that estimate parametric reflectance\nparameters via inverse rendering, our approach directly models learnable\nradiance transfer that incorporates global light transport in an efficient\nmanner for real-time rendering. However, learning such a complex light\ntransport that can generalize across identities is non-trivial. A phone scan in\na single environment lacks sufficient information to infer how the head would\nappear in general environments. To address this, we build a universal\nrelightable avatar model represented by 3D Gaussians. We train on hundreds of\nhigh-quality multi-view human scans with controllable point lights.\nHigh-resolution geometric guidance further enhances the reconstruction accuracy\nand generalization. Once trained, we finetune the pretrained model on a phone\nscan using inverse rendering to obtain a personalized relightable avatar. Our\nexperiments establish the efficacy of our design, outperforming existing\napproaches while retaining real-time rendering capability."
                },
                "authors": [
                    {
                        "name": "Junxuan Li"
                    },
                    {
                        "name": "Chen Cao"
                    },
                    {
                        "name": "Gabriel Schwartz"
                    },
                    {
                        "name": "Rawal Khirodkar"
                    },
                    {
                        "name": "Christian Richardt"
                    },
                    {
                        "name": "Tomas Simon"
                    },
                    {
                        "name": "Yaser Sheikh"
                    },
                    {
                        "name": "Shunsuke Saito"
                    }
                ],
                "author_detail": {
                    "name": "Shunsuke Saito"
                },
                "author": "Shunsuke Saito",
                "arxiv_comment": "SIGGRAPH Asia 2024. Website:\n  https://junxuan-li.github.io/urgca-website/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24218v1",
                "updated": "2024-10-31T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    52,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    52,
                    3,
                    305,
                    0
                ],
                "title": "Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use"
                },
                "summary": "In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL"
                },
                "authors": [
                    {
                        "name": "Jiajun Xi"
                    },
                    {
                        "name": "Yinong He"
                    },
                    {
                        "name": "Jianing Yang"
                    },
                    {
                        "name": "Yinpei Dai"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24207v1",
                "updated": "2024-10-31T17:58:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    58,
                    22,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:58:22Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    58,
                    22,
                    3,
                    305,
                    0
                ],
                "title": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse\n  Unposed Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse\n  Unposed Images"
                },
                "summary": "We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D\nscenes parameterized by 3D Gaussians from \\textit{unposed} sparse multi-view\nimages. Our model, trained exclusively with photometric loss, achieves\nreal-time 3D Gaussian reconstruction during inference. To eliminate the need\nfor accurate pose input during reconstruction, we anchor one input view's local\ncamera coordinates as the canonical space and train the network to predict\nGaussian primitives for all views within this space. This approach obviates the\nneed to transform Gaussian primitives from local coordinates into a global\ncoordinate system, thus avoiding errors associated with per-frame Gaussians and\npose estimation. To resolve scale ambiguity, we design and compare various\nintrinsic embedding methods, ultimately opting to convert camera intrinsics\ninto a token embedding and concatenate it with image tokens as input to the\nmodel, enabling accurate scene scale prediction. We utilize the reconstructed\n3D Gaussians for novel view synthesis and pose estimation tasks and propose a\ntwo-stage coarse-to-fine pipeline for accurate pose estimation. Experimental\nresults demonstrate that our pose-free approach can achieve superior novel view\nsynthesis quality compared to pose-required methods, particularly in scenarios\nwith limited input image overlap. For pose estimation, our method, trained\nwithout ground truth depth or explicit matching loss, significantly outperforms\nthe state-of-the-art methods with substantial improvements. This work makes\nsignificant advances in pose-free generalizable 3D reconstruction and\ndemonstrates its applicability to real-world scenarios. Code and trained models\nare available at https://noposplat.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D\nscenes parameterized by 3D Gaussians from \\textit{unposed} sparse multi-view\nimages. Our model, trained exclusively with photometric loss, achieves\nreal-time 3D Gaussian reconstruction during inference. To eliminate the need\nfor accurate pose input during reconstruction, we anchor one input view's local\ncamera coordinates as the canonical space and train the network to predict\nGaussian primitives for all views within this space. This approach obviates the\nneed to transform Gaussian primitives from local coordinates into a global\ncoordinate system, thus avoiding errors associated with per-frame Gaussians and\npose estimation. To resolve scale ambiguity, we design and compare various\nintrinsic embedding methods, ultimately opting to convert camera intrinsics\ninto a token embedding and concatenate it with image tokens as input to the\nmodel, enabling accurate scene scale prediction. We utilize the reconstructed\n3D Gaussians for novel view synthesis and pose estimation tasks and propose a\ntwo-stage coarse-to-fine pipeline for accurate pose estimation. Experimental\nresults demonstrate that our pose-free approach can achieve superior novel view\nsynthesis quality compared to pose-required methods, particularly in scenarios\nwith limited input image overlap. For pose estimation, our method, trained\nwithout ground truth depth or explicit matching loss, significantly outperforms\nthe state-of-the-art methods with substantial improvements. This work makes\nsignificant advances in pose-free generalizable 3D reconstruction and\ndemonstrates its applicability to real-world scenarios. Code and trained models\nare available at https://noposplat.github.io/."
                },
                "authors": [
                    {
                        "name": "Botao Ye"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Haofei Xu"
                    },
                    {
                        "name": "Xueting Li"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Songyou Peng"
                    }
                ],
                "author_detail": {
                    "name": "Songyou Peng"
                },
                "author": "Songyou Peng",
                "arxiv_comment": "Project page: https://noposplat.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24199v1",
                "updated": "2024-10-31T17:55:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:55:27Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    27,
                    3,
                    305,
                    0
                ],
                "title": "Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation"
                },
                "summary": "We present a novel approach to paraphrase generation that enables precise\ncontrol and fine-tuning of 40 linguistic attributes for English. Our model is\nan encoder-decoder architecture that takes as input a source sentence and\ndesired linguistic attributes, and produces paraphrases of the source that\nsatisfy the desired attributes. To guarantee high-quality outputs at inference\ntime, our method is equipped with a quality control mechanism that gradually\nadjusts the embedding of linguistic attributes to find the nearest and most\nattainable configuration of desired attributes for paraphrase generation. We\nevaluate the effectiveness of our method by comparing it to recent controllable\ngeneration models. Experimental results demonstrate that the proposed model\noutperforms baselines in generating paraphrases that satisfy desired linguistic\nattributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to paraphrase generation that enables precise\ncontrol and fine-tuning of 40 linguistic attributes for English. Our model is\nan encoder-decoder architecture that takes as input a source sentence and\ndesired linguistic attributes, and produces paraphrases of the source that\nsatisfy the desired attributes. To guarantee high-quality outputs at inference\ntime, our method is equipped with a quality control mechanism that gradually\nadjusts the embedding of linguistic attributes to find the nearest and most\nattainable configuration of desired attributes for paraphrase generation. We\nevaluate the effectiveness of our method by comparing it to recent controllable\ngeneration models. Experimental results demonstrate that the proposed model\noutperforms baselines in generating paraphrases that satisfy desired linguistic\nattributes."
                },
                "authors": [
                    {
                        "name": "Mohamed Elgaar"
                    },
                    {
                        "name": "Hadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Hadi Amiri"
                },
                "author": "Hadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2403.04182v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04182v3",
                "updated": "2024-11-01T17:57:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    57,
                    1,
                    4,
                    306,
                    0
                ],
                "published": "2024-03-07T03:24:34Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    3,
                    24,
                    34,
                    3,
                    67,
                    0
                ],
                "title": "Regression-aware Inference with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression-aware Inference with LLMs"
                },
                "summary": "Large language models (LLMs) have shown strong results on a range of\napplications, including regression and scoring tasks. Typically, one obtains\noutputs from an LLM via autoregressive sampling from the model's output\ndistribution. We show that this inference strategy can be sub-optimal for\ncommon regression and scoring evaluation metrics. As a remedy, we build on\nprior work on Minimum Bayes Risk decoding, and propose alternate inference\nstrategies that estimate the Bayes-optimal solution for regression and scoring\nmetrics in closed-form from sampled responses. We show that our proposal\nsignificantly improves over baselines across datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong results on a range of\napplications, including regression and scoring tasks. Typically, one obtains\noutputs from an LLM via autoregressive sampling from the model's output\ndistribution. We show that this inference strategy can be sub-optimal for\ncommon regression and scoring evaluation metrics. As a remedy, we build on\nprior work on Minimum Bayes Risk decoding, and propose alternate inference\nstrategies that estimate the Bayes-optimal solution for regression and scoring\nmetrics in closed-form from sampled responses. We show that our proposal\nsignificantly improves over baselines across datasets and models."
                },
                "authors": [
                    {
                        "name": "Michal Lukasik"
                    },
                    {
                        "name": "Harikrishna Narasimhan"
                    },
                    {
                        "name": "Aditya Krishna Menon"
                    },
                    {
                        "name": "Felix Yu"
                    },
                    {
                        "name": "Sanjiv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiv Kumar"
                },
                "author": "Sanjiv Kumar",
                "arxiv_comment": "EMNLP Findings 2024",
                "arxiv_journal_ref": "EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04182v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04182v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01721v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01721v3",
                "updated": "2024-11-01T17:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    12,
                    53,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-03T18:27:44Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    27,
                    44,
                    0,
                    155,
                    0
                ],
                "title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs"
                },
                "summary": "Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address Normal\nOutliers, which are activations across all tokens with relatively large\nmagnitudes. However, these methods struggle with smoothing Massive Outliers\nthat display significantly larger values, which leads to significant\nperformance degradation in low-bit quantization. In this paper, we introduce\nDuQuant, a novel approach that utilizes rotation and permutation\ntransformations to more effectively mitigate both massive and normal outliers.\nFirst, DuQuant starts by constructing the rotation matrix, using specific\noutlier dimensions as prior knowledge, to redistribute outliers to adjacent\nchannels by block-wise rotation. Second, We further employ a zigzag permutation\nto balance the distribution of outliers across blocks, thereby reducing\nblock-wise variance. A subsequent rotation further smooths the activation\nlandscape, enhancing model performance. DuQuant simplifies the quantization\nprocess and excels in managing outliers, outperforming the state-of-the-art\nbaselines across various sizes and types of LLMs on multiple tasks, even with\n4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address Normal\nOutliers, which are activations across all tokens with relatively large\nmagnitudes. However, these methods struggle with smoothing Massive Outliers\nthat display significantly larger values, which leads to significant\nperformance degradation in low-bit quantization. In this paper, we introduce\nDuQuant, a novel approach that utilizes rotation and permutation\ntransformations to more effectively mitigate both massive and normal outliers.\nFirst, DuQuant starts by constructing the rotation matrix, using specific\noutlier dimensions as prior knowledge, to redistribute outliers to adjacent\nchannels by block-wise rotation. Second, We further employ a zigzag permutation\nto balance the distribution of outliers across blocks, thereby reducing\nblock-wise variance. A subsequent rotation further smooths the activation\nlandscape, enhancing model performance. DuQuant simplifies the quantization\nprocess and excels in managing outliers, outperforming the state-of-the-art\nbaselines across various sizes and types of LLMs on multiple tasks, even with\n4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant."
                },
                "authors": [
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Haobo Xu"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Jingzhi Cui"
                    },
                    {
                        "name": "Yingtao Zhang"
                    },
                    {
                        "name": "Linzhan Mou"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Zhenan Sun"
                    },
                    {
                        "name": "Ying Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wei"
                },
                "author": "Ying Wei",
                "arxiv_comment": "NeurIPS 2024 Oral, Website at https://duquant.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01721v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01721v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19499v2",
                "updated": "2024-11-01T16:45:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    45,
                    29,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-25T11:58:12Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    58,
                    12,
                    4,
                    299,
                    0
                ],
                "title": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization"
                },
                "summary": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs."
                },
                "authors": [
                    {
                        "name": "Anthony Cui"
                    },
                    {
                        "name": "Pranav Nandyalam"
                    },
                    {
                        "name": "Ethan Cheung"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15589v3",
                "updated": "2024-11-01T16:39:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    39,
                    36,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-24T14:20:09Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    14,
                    20,
                    9,
                    4,
                    145,
                    0
                ],
                "title": "Efficient Adversarial Training in LLMs with Continuous Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Adversarial Training in LLMs with Continuous Attacks"
                },
                "summary": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nbypass their safety guardrails. In many domains, adversarial training has\nproven to be one of the most promising methods to reliably improve robustness\nagainst such attacks. Yet, in the context of LLMs, current methods for\nadversarial training are hindered by the high computational costs required to\nperform discrete adversarial attacks at each training iteration. We address\nthis problem by instead calculating adversarial attacks in the continuous\nembedding space of the LLM, which is orders of magnitudes more efficient. We\npropose a fast adversarial training algorithm (C-AdvUL) composed of two losses:\nthe first makes the model robust on continuous embedding attacks computed on an\nadversarial behaviour dataset; the second ensures the usefulness of the final\nmodel by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an\nadversarial variant of IPO that does not require utility data for adversarially\nrobust alignment. Our empirical evaluation on five models from different\nfamilies (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B,\n3.8B, 7B) shows that both algorithms substantially enhance LLM robustness\nagainst discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our\nresults demonstrate that robustness to continuous perturbations can extrapolate\nto discrete threat models. Thereby, we present a path toward scalable\nadversarial training algorithms for robustly aligning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are vulnerable to adversarial attacks that can\nbypass their safety guardrails. In many domains, adversarial training has\nproven to be one of the most promising methods to reliably improve robustness\nagainst such attacks. Yet, in the context of LLMs, current methods for\nadversarial training are hindered by the high computational costs required to\nperform discrete adversarial attacks at each training iteration. We address\nthis problem by instead calculating adversarial attacks in the continuous\nembedding space of the LLM, which is orders of magnitudes more efficient. We\npropose a fast adversarial training algorithm (C-AdvUL) composed of two losses:\nthe first makes the model robust on continuous embedding attacks computed on an\nadversarial behaviour dataset; the second ensures the usefulness of the final\nmodel by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an\nadversarial variant of IPO that does not require utility data for adversarially\nrobust alignment. Our empirical evaluation on five models from different\nfamilies (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B,\n3.8B, 7B) shows that both algorithms substantially enhance LLM robustness\nagainst discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our\nresults demonstrate that robustness to continuous perturbations can extrapolate\nto discrete threat models. Thereby, we present a path toward scalable\nadversarial training algorithms for robustly aligning LLMs."
                },
                "authors": [
                    {
                        "name": "Sophie Xhonneux"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04559v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04559v4",
                "updated": "2024-11-01T16:10:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    10,
                    41,
                    4,
                    306,
                    0
                ],
                "published": "2024-02-07T03:37:19Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    3,
                    37,
                    19,
                    2,
                    38,
                    0
                ],
                "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Model Agents Simulate Human Trust Behavior?"
                },
                "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount."
                },
                "authors": [
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Feiran Jia"
                    },
                    {
                        "name": "Ziyu Ye"
                    },
                    {
                        "name": "Shiyang Lai"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Adel Bibi"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "David Jurgens"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Guohao Li"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Li"
                },
                "author": "Guohao Li",
                "arxiv_comment": "Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04559v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04559v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24198v2",
                "updated": "2024-11-01T16:06:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    16,
                    6,
                    10,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-31T17:55:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "SelfCodeAlign: Self-Alignment for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfCodeAlign: Self-Alignment for Code Generation"
                },
                "summary": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Yifeng Ding"
                    },
                    {
                        "name": "Naman Jain"
                    },
                    {
                        "name": "Zachary Mueller"
                    },
                    {
                        "name": "Harm de Vries"
                    },
                    {
                        "name": "Leandro von Werra"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08964v2",
                "updated": "2024-11-01T15:53:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    53,
                    8,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-11T16:32:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    32,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Imbalance Driven Rewarding for Multilingual Self-improving"
                },
                "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18050v2",
                "updated": "2024-11-01T15:36:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    36,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-23T17:24:58Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    24,
                    58,
                    2,
                    297,
                    0
                ],
                "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering"
                },
                "summary": "Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG."
                },
                "authors": [
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Yukuo Cen"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Shicheng Tan"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "EMNLP 2024 Main, Final",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19381v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19381v3",
                "updated": "2024-11-01T14:51:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    51,
                    38,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-28T15:12:55Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    12,
                    55,
                    5,
                    272,
                    0
                ],
                "title": "INC-Math: Integrating Natural Language and Code for Enhanced\n  Mathematical Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INC-Math: Integrating Natural Language and Code for Enhanced\n  Mathematical Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are commonly used to generate solutions for\nmathematical reasoning problems in the following formats: natural language,\ncode, or a combination of both. In this paper, we explore fundamental questions\nrelated to solving mathematical reasoning problems using natural language and\ncode with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.\nOur findings show that LLMs are better at reasoning in natural language\ncompared to code. Additionally, although natural language and code serve as\ncomplementary forms of reasoning, they can affect each other in a negative way\nin certain scenarios. These insights motivate our development of a new\nprompting method, INC-Math, which leverages an LLM to dynamically select the\nmost appropriate reasoning form, resulting in improved performance over\ncomparable baselines with GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are commonly used to generate solutions for\nmathematical reasoning problems in the following formats: natural language,\ncode, or a combination of both. In this paper, we explore fundamental questions\nrelated to solving mathematical reasoning problems using natural language and\ncode with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.\nOur findings show that LLMs are better at reasoning in natural language\ncompared to code. Additionally, although natural language and code serve as\ncomplementary forms of reasoning, they can affect each other in a negative way\nin certain scenarios. These insights motivate our development of a new\nprompting method, INC-Math, which leverages an LLM to dynamically select the\nmost appropriate reasoning form, resulting in improved performance over\ncomparable baselines with GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Xuyuan Xiong"
                    },
                    {
                        "name": "Simeng Han"
                    },
                    {
                        "name": "Ziyue Zhou"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19381v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19381v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18760v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18760v4",
                "updated": "2024-11-01T14:37:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    37,
                    37,
                    4,
                    306,
                    0
                ],
                "published": "2023-11-30T18:02:44Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    18,
                    2,
                    44,
                    3,
                    334,
                    0
                ],
                "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaskBench: Benchmarking Large Language Models for Task Automation"
                },
                "summary": "In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents."
                },
                "authors": [
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18760v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18760v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.00352v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.00352v7",
                "updated": "2024-11-01T14:36:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    36,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2023-08-01T07:49:10Z",
                "published_parsed": [
                    2023,
                    8,
                    1,
                    7,
                    49,
                    10,
                    1,
                    213,
                    0
                ],
                "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework"
                },
                "summary": "Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT"
                },
                "authors": [
                    {
                        "name": "Sirui Hong"
                    },
                    {
                        "name": "Mingchen Zhuge"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Yuheng Cheng"
                    },
                    {
                        "name": "Ceyao Zhang"
                    },
                    {
                        "name": "Jinlin Wang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Steven Ka Shing Yau"
                    },
                    {
                        "name": "Zijuan Lin"
                    },
                    {
                        "name": "Liyang Zhou"
                    },
                    {
                        "name": "Chenyu Ran"
                    },
                    {
                        "name": "Lingfeng Xiao"
                    },
                    {
                        "name": "Chenglin Wu"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.00352v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.00352v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00132v2",
                "updated": "2024-11-01T14:36:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    36,
                    49,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-31T18:47:30Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    18,
                    47,
                    30,
                    4,
                    152,
                    0
                ],
                "title": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed\n  Tensor Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed\n  Tensor Adaptation"
                },
                "summary": "We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,\neasy-to-implement, fine-tuning method with no inference overhead for\nlarge-scale pre-trained language models. By leveraging quantum-inspired methods\nderived from quantum circuit structures, QuanTA enables efficient high-rank\nfine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank\napproximation may fail for complicated downstream tasks. Our approach is\ntheoretically supported by the universality theorem and the rank representation\ntheorem to achieve efficient high-rank adaptations. Experiments demonstrate\nthat QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,\nand scalability compared to traditional methods. Furthermore, QuanTA shows\nsuperior performance with fewer trainable parameters compared to other\napproaches and can be designed to integrate with existing fine-tuning\nalgorithms for further improvement, providing a scalable and efficient solution\nfor fine-tuning large language models and advancing state-of-the-art in natural\nlanguage processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,\neasy-to-implement, fine-tuning method with no inference overhead for\nlarge-scale pre-trained language models. By leveraging quantum-inspired methods\nderived from quantum circuit structures, QuanTA enables efficient high-rank\nfine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank\napproximation may fail for complicated downstream tasks. Our approach is\ntheoretically supported by the universality theorem and the rank representation\ntheorem to achieve efficient high-rank adaptations. Experiments demonstrate\nthat QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,\nand scalability compared to traditional methods. Furthermore, QuanTA shows\nsuperior performance with fewer trainable parameters compared to other\napproaches and can be designed to integrate with existing fine-tuning\nalgorithms for further improvement, providing a scalable and efficient solution\nfor fine-tuning large language models and advancing state-of-the-art in natural\nlanguage processing."
                },
                "authors": [
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Rumen Dangovski"
                    },
                    {
                        "name": "Charlotte Loh"
                    },
                    {
                        "name": "Owen Dugan"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "Marin Soljačić"
                    }
                ],
                "author_detail": {
                    "name": "Marin Soljačić"
                },
                "author": "Marin Soljačić",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23528v2",
                "updated": "2024-11-01T14:27:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    27,
                    42,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-31T00:29:52Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    29,
                    52,
                    3,
                    305,
                    0
                ],
                "title": "Large Language Models for Patient Comments Multi-Label Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Patient Comments Multi-Label Classification"
                },
                "summary": "Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4 Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4 Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4 Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4 Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses."
                },
                "authors": [
                    {
                        "name": "Hajar Sakai"
                    },
                    {
                        "name": "Sarah S. Lam"
                    },
                    {
                        "name": "Mohammadsadegh Mikaeili"
                    },
                    {
                        "name": "Joshua Bosire"
                    },
                    {
                        "name": "Franziska Jovin"
                    }
                ],
                "author_detail": {
                    "name": "Franziska Jovin"
                },
                "author": "Franziska Jovin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10691v2",
                "updated": "2024-11-01T14:08:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    8,
                    31,
                    4,
                    306,
                    0
                ],
                "published": "2024-07-15T13:04:09Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    13,
                    4,
                    9,
                    0,
                    197,
                    0
                ],
                "title": "$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity"
                },
                "summary": "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based\nretrievers, respectively, averaged on queries containing multiple subqueries\nfrom five scientific retrieval datasets. Moreover, the efficacy of two\ndownstream scientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The\ncode and experimental datasets are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based\nretrievers, respectively, averaged on queries containing multiple subqueries\nfrom five scientific retrieval datasets. Moreover, the efficacy of two\ndownstream scientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The\ncode and experimental datasets are available."
                },
                "authors": [
                    {
                        "name": "Fengyu Cai"
                    },
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Sihao Chen"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Heinz Koeppl"
                    }
                ],
                "author_detail": {
                    "name": "Heinz Koeppl"
                },
                "author": "Heinz Koeppl",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13845v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13845v3",
                "updated": "2024-11-01T13:25:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    13,
                    25,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-22T17:13:49Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    17,
                    13,
                    49,
                    2,
                    143,
                    0
                ],
                "title": "Semantic Density: Uncertainty Quantification for Large Language Models\n  through Confidence Measurement in Semantic Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Density: Uncertainty Quantification for Large Language Models\n  through Confidence Measurement in Semantic Space"
                },
                "summary": "With the widespread application of Large Language Models (LLMs) to various\ndomains, concerns regarding the trustworthiness of LLMs in safety-critical\nscenarios have been raised, due to their unpredictable tendency to hallucinate\nand generate misinformation. Existing LLMs do not have an inherent\nfunctionality to provide the users with an uncertainty/confidence metric for\neach response it generates, making it difficult to evaluate trustworthiness.\nAlthough several studies aim to develop uncertainty quantification methods for\nLLMs, they have fundamental limitations, such as being restricted to\nclassification tasks, requiring additional training and data, considering only\nlexical instead of semantic information, and being prompt-wise but not\nresponse-wise. A new framework is proposed in this paper to address these\nissues. Semantic density extracts uncertainty/confidence information for each\nresponse from a probability distribution perspective in semantic space. It has\nno restriction on task types and is \"off-the-shelf\" for new models and tasks.\nExperiments on seven state-of-the-art LLMs, including the latest Llama 3 and\nMixtral-8x22B models, on four free-form question-answering benchmarks\ndemonstrate the superior performance and robustness of semantic density\ncompared to prior approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Large Language Models (LLMs) to various\ndomains, concerns regarding the trustworthiness of LLMs in safety-critical\nscenarios have been raised, due to their unpredictable tendency to hallucinate\nand generate misinformation. Existing LLMs do not have an inherent\nfunctionality to provide the users with an uncertainty/confidence metric for\neach response it generates, making it difficult to evaluate trustworthiness.\nAlthough several studies aim to develop uncertainty quantification methods for\nLLMs, they have fundamental limitations, such as being restricted to\nclassification tasks, requiring additional training and data, considering only\nlexical instead of semantic information, and being prompt-wise but not\nresponse-wise. A new framework is proposed in this paper to address these\nissues. Semantic density extracts uncertainty/confidence information for each\nresponse from a probability distribution perspective in semantic space. It has\nno restriction on task types and is \"off-the-shelf\" for new models and tasks.\nExperiments on seven state-of-the-art LLMs, including the latest Llama 3 and\nMixtral-8x22B models, on four free-form question-answering benchmarks\ndemonstrate the superior performance and robustness of semantic density\ncompared to prior approaches."
                },
                "authors": [
                    {
                        "name": "Xin Qiu"
                    },
                    {
                        "name": "Risto Miikkulainen"
                    }
                ],
                "author_detail": {
                    "name": "Risto Miikkulainen"
                },
                "author": "Risto Miikkulainen",
                "arxiv_comment": "Accepted to Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13845v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13845v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15782v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15782v3",
                "updated": "2024-11-01T12:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    53,
                    4,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-24T10:04:53Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    10,
                    4,
                    53,
                    2,
                    115,
                    0
                ],
                "title": "Risk or Chance? Large Language Models and Reproducibility in HCI\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk or Chance? Large Language Models and Reproducibility in HCI\n  Research"
                },
                "summary": "Reproducibility is a major concern across scientific fields. Human-Computer\nInteraction (HCI), in particular, is subject to diverse reproducibility\nchallenges due to the wide range of research methodologies employed. In this\narticle, we explore how the increasing adoption of Large Language Models (LLMs)\nacross all user experience (UX) design and research activities impacts\nreproducibility in HCI. In particular, we review upcoming reproducibility\nchallenges through the lenses of analogies from past to future (mis)practices\nlike p-hacking and prompt-hacking, general bias, support in data analysis,\ndocumentation and education requirements, and possible pressure on the\ncommunity. We discuss the risks and chances for each of these lenses with the\nexpectation that a more comprehensive discussion will help shape best practices\nand contribute to valid and reproducible practices around using LLMs in HCI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducibility is a major concern across scientific fields. Human-Computer\nInteraction (HCI), in particular, is subject to diverse reproducibility\nchallenges due to the wide range of research methodologies employed. In this\narticle, we explore how the increasing adoption of Large Language Models (LLMs)\nacross all user experience (UX) design and research activities impacts\nreproducibility in HCI. In particular, we review upcoming reproducibility\nchallenges through the lenses of analogies from past to future (mis)practices\nlike p-hacking and prompt-hacking, general bias, support in data analysis,\ndocumentation and education requirements, and possible pressure on the\ncommunity. We discuss the risks and chances for each of these lenses with the\nexpectation that a more comprehensive discussion will help shape best practices\nand contribute to valid and reproducible practices around using LLMs in HCI\nresearch."
                },
                "authors": [
                    {
                        "name": "Thomas Kosch"
                    },
                    {
                        "name": "Sebastian Feger"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Feger"
                },
                "author": "Sebastian Feger",
                "arxiv_doi": "10.1145/3695765",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695765",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.15782v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15782v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in ACM Interactions",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15246v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15246v3",
                "updated": "2024-11-01T12:49:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    49,
                    19,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-23T17:42:05Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    42,
                    5,
                    0,
                    267,
                    0
                ],
                "title": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks"
                },
                "summary": "Earth Observation (EO) systems are crucial for cartography, disaster\nsurveillance, and resource administration. Nonetheless, they encounter\nconsiderable obstacles in the processing and transmission of extensive data,\nespecially in specialized domains such as precision agriculture and real-time\ndisaster response. Earth observation satellites, outfitted with remote sensing\ntechnology, gather data from onboard sensors and IoT-enabled terrestrial\nobjects, delivering important information remotely. Domain-adapted Large\nLanguage Models (LLMs) provide a solution by enabling the integration of raw\nand processed EO data. Through domain adaptation, LLMs improve the assimilation\nand analysis of many data sources, tackling the intricacies of specialized\ndatasets in agriculture and disaster response. This data synthesis, directed by\nLLMs, enhances the precision and pertinence of conveyed information. This study\nprovides a thorough examination of using semantic inference and deep learning\nfor sophisticated EO systems. It presents an innovative architecture for\nsemantic communication in EO satellite networks, designed to improve data\ntransmission efficiency using semantic processing methodologies. Recent\nadvancements in onboard processing technologies enable dependable, adaptable,\nand energy-efficient data management in orbit. These improvements guarantee\nreliable performance in adverse space circumstances using radiation-hardened\nand reconfigurable technology. Collectively, these advancements enable\nnext-generation satellite missions with improved processing capabilities,\ncrucial for operational flexibility and real-time decision-making in 6G\nsatellite communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth Observation (EO) systems are crucial for cartography, disaster\nsurveillance, and resource administration. Nonetheless, they encounter\nconsiderable obstacles in the processing and transmission of extensive data,\nespecially in specialized domains such as precision agriculture and real-time\ndisaster response. Earth observation satellites, outfitted with remote sensing\ntechnology, gather data from onboard sensors and IoT-enabled terrestrial\nobjects, delivering important information remotely. Domain-adapted Large\nLanguage Models (LLMs) provide a solution by enabling the integration of raw\nand processed EO data. Through domain adaptation, LLMs improve the assimilation\nand analysis of many data sources, tackling the intricacies of specialized\ndatasets in agriculture and disaster response. This data synthesis, directed by\nLLMs, enhances the precision and pertinence of conveyed information. This study\nprovides a thorough examination of using semantic inference and deep learning\nfor sophisticated EO systems. It presents an innovative architecture for\nsemantic communication in EO satellite networks, designed to improve data\ntransmission efficiency using semantic processing methodologies. Recent\nadvancements in onboard processing technologies enable dependable, adaptable,\nand energy-efficient data management in orbit. These improvements guarantee\nreliable performance in adverse space circumstances using radiation-hardened\nand reconfigurable technology. Collectively, these advancements enable\nnext-generation satellite missions with improved processing capabilities,\ncrucial for operational flexibility and real-time decision-making in 6G\nsatellite communication."
                },
                "authors": [
                    {
                        "name": "Hong-fu Chou"
                    },
                    {
                        "name": "Vu Nguyen Ha"
                    },
                    {
                        "name": "Prabhu Thiruvasagam"
                    },
                    {
                        "name": "Thanh-Dung Le"
                    },
                    {
                        "name": "Geoffrey Eappen"
                    },
                    {
                        "name": "Ti Ti Nguyen"
                    },
                    {
                        "name": "Luis M. Garces-Socarras"
                    },
                    {
                        "name": "Jorge L. Gonzalez-Rios"
                    },
                    {
                        "name": "Juan Carlos Merlano-Duncan"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas",
                "arxiv_comment": "17 pages, 7 figures, Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15246v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15246v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22932v2",
                "updated": "2024-11-01T12:37:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    37,
                    10,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-30T11:38:13Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    38,
                    13,
                    2,
                    304,
                    0
                ],
                "title": "Multi-Agent Large Language Models for Conversational Task-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Large Language Models for Conversational Task-Solving"
                },
                "summary": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs."
                },
                "authors": [
                    {
                        "name": "Jonas Becker"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Becker"
                },
                "author": "Jonas Becker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16405v2",
                "updated": "2024-11-01T12:15:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    15,
                    36,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-26T02:12:02Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    12,
                    2,
                    6,
                    147,
                    0
                ],
                "title": "Intruding with Words: Towards Understanding Graph Injection Attacks at\n  the Text Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intruding with Words: Towards Understanding Graph Injection Attacks at\n  the Text Level"
                },
                "summary": "Graph Neural Networks (GNNs) excel across various applications but remain\nvulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs),\nwhich inject malicious nodes into the original graph and pose realistic\nthreats. Text-attributed graphs (TAGs), where nodes are associated with textual\nfeatures, are crucial due to their prevalence in real-world applications and\nare commonly used to evaluate these vulnerabilities. However, existing research\nonly focuses on embedding-level GIAs, which inject node embeddings rather than\nactual textual content, limiting their applicability and simplifying detection.\nIn this paper, we pioneer the exploration of GIAs at the text level, presenting\nthree novel attack designs that inject textual content into the graph. Through\ntheoretical and empirical analysis, we demonstrate that text interpretability,\na factor previously overlooked at the embedding level, plays a crucial role in\nattack strength. Among the designs we investigate, the Word-frequency-based\nText-level GIA (WTGIA) is particularly notable for its balance between\nperformance and interpretability. Despite the success of WTGIA, we discover\nthat defenders can easily enhance their defenses with customized text embedding\nmethods or large language model (LLM)--based predictors. These insights\nunderscore the necessity for further research into the potential and practical\nsignificance of text-level GIAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) excel across various applications but remain\nvulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs),\nwhich inject malicious nodes into the original graph and pose realistic\nthreats. Text-attributed graphs (TAGs), where nodes are associated with textual\nfeatures, are crucial due to their prevalence in real-world applications and\nare commonly used to evaluate these vulnerabilities. However, existing research\nonly focuses on embedding-level GIAs, which inject node embeddings rather than\nactual textual content, limiting their applicability and simplifying detection.\nIn this paper, we pioneer the exploration of GIAs at the text level, presenting\nthree novel attack designs that inject textual content into the graph. Through\ntheoretical and empirical analysis, we demonstrate that text interpretability,\na factor previously overlooked at the embedding level, plays a crucial role in\nattack strength. Among the designs we investigate, the Word-frequency-based\nText-level GIA (WTGIA) is particularly notable for its balance between\nperformance and interpretability. Despite the success of WTGIA, we discover\nthat defenders can easily enhance their defenses with customized text embedding\nmethods or large language model (LLM)--based predictors. These insights\nunderscore the necessity for further research into the potential and practical\nsignificance of text-level GIAs."
                },
                "authors": [
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Yuchen Ren"
                    },
                    {
                        "name": "Zhewei Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhewei Wei"
                },
                "author": "Zhewei Wei",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.08385v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.08385v4",
                "updated": "2024-11-01T10:28:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    10,
                    28,
                    12,
                    4,
                    306,
                    0
                ],
                "published": "2023-11-14T18:48:27Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    18,
                    48,
                    27,
                    1,
                    318,
                    0
                ],
                "title": "Aligning Large Language Models with Human Opinions through Persona\n  Selection and Value--Belief--Norm Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models with Human Opinions through Persona\n  Selection and Value--Belief--Norm Reasoning"
                },
                "summary": "Reasoning and predicting human opinions with large language models (LLMs) is\nessential yet challenging. Current methods employ role-playing with personae\nbut face two major issues: LLMs are sensitive to even a single irrelevant\npersona, skewing predictions by up to 30%, and LLMs fail to reason\nstrategically over personae. We propose Chain-of-Opinion (COO), a simple\nfour-step solution modeling which and how to reason with personae, inspired by\nthe Value--Belief--Norm (VBN) theory. COO differentiates between explicit\npersonae (demographics and ideology) and implicit personae (historical\nopinions), involves: (1) filtering irrelevant attributes from explicit\npersonae, (2) ranking implicit personae into a preferential list for selecting\ntop-k, (3) applying novel VBN reasoning to extract user environmental and\npersonal value, belief, and norm variables for accurate and reliable\npredictions, and (4) iterating VBN reasoning with progressively larger lists of\nimplicit personae to handle potential persona insufficiency. COO efficiently\nachieves new state-of-the-art opinion prediction via prompting with only 5\ninference calls, improving prior techniques by up to 4%. Notably, fine-tuning\nLMs with COO data results in significantly better opinion-aligned models, by up\nto 23%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and predicting human opinions with large language models (LLMs) is\nessential yet challenging. Current methods employ role-playing with personae\nbut face two major issues: LLMs are sensitive to even a single irrelevant\npersona, skewing predictions by up to 30%, and LLMs fail to reason\nstrategically over personae. We propose Chain-of-Opinion (COO), a simple\nfour-step solution modeling which and how to reason with personae, inspired by\nthe Value--Belief--Norm (VBN) theory. COO differentiates between explicit\npersonae (demographics and ideology) and implicit personae (historical\nopinions), involves: (1) filtering irrelevant attributes from explicit\npersonae, (2) ranking implicit personae into a preferential list for selecting\ntop-k, (3) applying novel VBN reasoning to extract user environmental and\npersonal value, belief, and norm variables for accurate and reliable\npredictions, and (4) iterating VBN reasoning with progressively larger lists of\nimplicit personae to handle potential persona insufficiency. COO efficiently\nachieves new state-of-the-art opinion prediction via prompting with only 5\ninference calls, improving prior techniques by up to 4%. Notably, fine-tuning\nLMs with COO data results in significantly better opinion-aligned models, by up\nto 23%."
                },
                "authors": [
                    {
                        "name": "Do Xuan Long"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Min-Yen Kan"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.08385v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.08385v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20778v2",
                "updated": "2024-11-01T09:53:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    53,
                    53,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-28T06:10:12Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    6,
                    10,
                    12,
                    1,
                    149,
                    0
                ],
                "title": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs"
                },
                "summary": "Adversarial prompts generated using gradient-based methods exhibit\noutstanding performance in performing automatic jailbreak attacks against\nsafety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the\ninput gradient of LLMs struggles to precisely reflect the magnitude of loss\nchange that results from token replacements in the prompt, leading to limited\nattack success rates against safety-aligned LLMs, even in the white-box\nsetting. In this paper, we explore a new perspective on this problem,\nsuggesting that it can be alleviated by leveraging innovations inspired in\ntransfer-based attacks that were originally proposed for attacking black-box\nimage classification models. For the first time, we appropriate the ideologies\nof effective methods among these transfer-based attacks, i.e., Skip Gradient\nMethod and Intermediate Level Attack, into gradient-based adversarial prompt\ngeneration and achieve significant performance gains without introducing\nobvious computational cost. Meanwhile, by discussing mechanisms behind the\ngains, new insights are drawn, and proper combinations of these methods are\nalso developed. Our empirical results show that 87% of the query-specific\nadversarial suffixes generated by the developed combination can induce\nLlama-2-7B-Chat to produce the output that exactly matches the target string on\nAdvBench. This match rate is 33% higher than that of a very strong baseline\nknown as GCG, demonstrating advanced discrete optimization for adversarial\nprompt generation against LLMs. In addition, without introducing obvious cost,\nthe combination achieves >30% absolute increase in attack success rates\ncompared with GCG when generating both query-specific (38% -> 68%) and\nuniversal adversarial prompts (26.68% -> 60.32%) for attacking the\nLlama-2-7B-Chat model on AdvBench. Code at:\nhttps://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial prompts generated using gradient-based methods exhibit\noutstanding performance in performing automatic jailbreak attacks against\nsafety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the\ninput gradient of LLMs struggles to precisely reflect the magnitude of loss\nchange that results from token replacements in the prompt, leading to limited\nattack success rates against safety-aligned LLMs, even in the white-box\nsetting. In this paper, we explore a new perspective on this problem,\nsuggesting that it can be alleviated by leveraging innovations inspired in\ntransfer-based attacks that were originally proposed for attacking black-box\nimage classification models. For the first time, we appropriate the ideologies\nof effective methods among these transfer-based attacks, i.e., Skip Gradient\nMethod and Intermediate Level Attack, into gradient-based adversarial prompt\ngeneration and achieve significant performance gains without introducing\nobvious computational cost. Meanwhile, by discussing mechanisms behind the\ngains, new insights are drawn, and proper combinations of these methods are\nalso developed. Our empirical results show that 87% of the query-specific\nadversarial suffixes generated by the developed combination can induce\nLlama-2-7B-Chat to produce the output that exactly matches the target string on\nAdvBench. This match rate is 33% higher than that of a very strong baseline\nknown as GCG, demonstrating advanced discrete optimization for adversarial\nprompt generation against LLMs. In addition, without introducing obvious cost,\nthe combination achieves >30% absolute increase in attack success rates\ncompared with GCG when generating both query-specific (38% -> 68%) and\nuniversal adversarial prompts (26.68% -> 60.32%) for attacking the\nLlama-2-7B-Chat model on AdvBench. Code at:\nhttps://github.com/qizhangli/Gradient-based-Jailbreak-Attacks."
                },
                "authors": [
                    {
                        "name": "Qizhang Li"
                    },
                    {
                        "name": "Yiwen Guo"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14716v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14716v3",
                "updated": "2024-11-01T09:38:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    38,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-11T13:17:19Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    17,
                    19,
                    4,
                    285,
                    0
                ],
                "title": "A Systematic Survey on Large Language Models for Algorithm Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Survey on Large Language Models for Algorithm Design"
                },
                "summary": "Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has seen substantial progress, with applications spanning\noptimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid advancements and expanding scope of this field, a\nsystematic review is both timely and necessary. This paper provides a\nsystematic review of LLM4AD. First, we offer an overview and summary of\nexisting studies. Then, we introduce a taxonomy and review the literature\nacross four dimensions: the roles of LLMs, search methods, prompt methods, and\napplication domains with a discussion of potential and achievements of LLMs in\nAD. Finally, we identify current challenges and highlight several promising\ndirections for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has seen substantial progress, with applications spanning\noptimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid advancements and expanding scope of this field, a\nsystematic review is both timely and necessary. This paper provides a\nsystematic review of LLM4AD. First, we offer an overview and summary of\nexisting studies. Then, we introduce a taxonomy and review the literature\nacross four dimensions: the roles of LLMs, search methods, prompt methods, and\napplication domains with a discussion of potential and achievements of LLMs in\nAD. Finally, we identify current challenges and highlight several promising\ndirections for future research."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yiming Yao"
                    },
                    {
                        "name": "Ping Guo"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14716v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14716v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.08648v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.08648v4",
                "updated": "2024-11-01T08:06:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    6,
                    25,
                    4,
                    306,
                    0
                ],
                "published": "2023-09-15T13:15:54Z",
                "published_parsed": [
                    2023,
                    9,
                    15,
                    13,
                    15,
                    54,
                    4,
                    258,
                    0
                ],
                "title": "MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings"
                },
                "summary": "In recent years, predicting mobile app usage has become increasingly\nimportant for areas like app recommendation, user behaviour analysis, and\nmobile resource management. Existing models, however, struggle with the\nheterogeneous nature of contextual data and the user cold start problem. This\nstudy introduces a novel prediction model, Mobile App Prediction Leveraging\nLarge Language Model Embeddings (MAPLE), which employs Large Language Models\n(LLMs) and installed app similarity to overcome these challenges. MAPLE\nutilises the power of LLMs to process contextual data and discern intricate\nrelationships within it effectively. Additionally, we explore the use of\ninstalled app similarity to address the cold start problem, facilitating the\nmodelling of user preferences and habits, even for new users with limited\nhistorical data. In essence, our research presents MAPLE as a novel, potent,\nand practical approach to app usage prediction, making significant strides in\nresolving issues faced by existing models. MAPLE stands out as a comprehensive\nand effective solution, setting a new benchmark for more precise and\npersonalised app usage predictions. In tests on two real-world datasets, MAPLE\nsurpasses contemporary models in both standard and cold start scenarios. These\noutcomes validate MAPLE's capacity for precise app usage predictions and its\nresilience against the cold start problem. This enhanced performance stems from\nthe model's proficiency in capturing complex temporal patterns and leveraging\ncontextual information. As a result, MAPLE can potentially improve personalised\nmobile app usage predictions and user experiences markedly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, predicting mobile app usage has become increasingly\nimportant for areas like app recommendation, user behaviour analysis, and\nmobile resource management. Existing models, however, struggle with the\nheterogeneous nature of contextual data and the user cold start problem. This\nstudy introduces a novel prediction model, Mobile App Prediction Leveraging\nLarge Language Model Embeddings (MAPLE), which employs Large Language Models\n(LLMs) and installed app similarity to overcome these challenges. MAPLE\nutilises the power of LLMs to process contextual data and discern intricate\nrelationships within it effectively. Additionally, we explore the use of\ninstalled app similarity to address the cold start problem, facilitating the\nmodelling of user preferences and habits, even for new users with limited\nhistorical data. In essence, our research presents MAPLE as a novel, potent,\nand practical approach to app usage prediction, making significant strides in\nresolving issues faced by existing models. MAPLE stands out as a comprehensive\nand effective solution, setting a new benchmark for more precise and\npersonalised app usage predictions. In tests on two real-world datasets, MAPLE\nsurpasses contemporary models in both standard and cold start scenarios. These\noutcomes validate MAPLE's capacity for precise app usage predictions and its\nresilience against the cold start problem. This enhanced performance stems from\nthe model's proficiency in capturing complex temporal patterns and leveraging\ncontextual information. As a result, MAPLE can potentially improve personalised\nmobile app usage predictions and user experiences markedly."
                },
                "authors": [
                    {
                        "name": "Yonchanok Khaokaew"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_doi": "10.1145/3643514",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3643514",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.08648v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.08648v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13752v3",
                "updated": "2024-11-01T07:51:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    51,
                    36,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-21T19:24:15Z",
                "published_parsed": [
                    2024,
                    4,
                    21,
                    19,
                    24,
                    15,
                    6,
                    112,
                    0
                ],
                "title": "Adversarial Representation Engineering: A General Model Editing\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Representation Engineering: A General Model Editing\n  Framework for Large Language Models"
                },
                "summary": "Since the rapid development of Large Language Models (LLMs) has achieved\nremarkable success, understanding and rectifying their internal complex\nmechanisms has become an urgent issue. Recent research has attempted to\ninterpret their behaviors through the lens of inner representation. However,\ndeveloping practical and efficient methods for applying these representations\nfor general and flexible model editing remains challenging. In this work, we\nexplore how to leverage insights from representation engineering to guide the\nediting of LLMs by deploying a representation sensor as an editing oracle. We\nfirst identify the importance of a robust and reliable sensor during editing,\nthen propose an Adversarial Representation Engineering (ARE) framework to\nprovide a unified and interpretable approach for conceptual model editing\nwithout compromising baseline performance. Experiments on multiple tasks\ndemonstrate the effectiveness of ARE in various model editing scenarios. Our\ncode and data are available at\nhttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the rapid development of Large Language Models (LLMs) has achieved\nremarkable success, understanding and rectifying their internal complex\nmechanisms has become an urgent issue. Recent research has attempted to\ninterpret their behaviors through the lens of inner representation. However,\ndeveloping practical and efficient methods for applying these representations\nfor general and flexible model editing remains challenging. In this work, we\nexplore how to leverage insights from representation engineering to guide the\nediting of LLMs by deploying a representation sensor as an editing oracle. We\nfirst identify the importance of a robust and reliable sensor during editing,\nthen propose an Adversarial Representation Engineering (ARE) framework to\nprovide a unified and interpretable approach for conceptual model editing\nwithout compromising baseline performance. Experiments on multiple tasks\ndemonstrate the effectiveness of ARE in various model editing scenarios. Our\ncode and data are available at\nhttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering."
                },
                "authors": [
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Meng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Meng Sun"
                },
                "author": "Meng Sun",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v4",
                "updated": "2024-11-01T07:41:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    41,
                    4,
                    4,
                    306,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder)."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01475v2",
                "updated": "2024-11-01T07:05:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    5,
                    33,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-01T20:56:25Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    20,
                    56,
                    25,
                    0,
                    92,
                    0
                ],
                "title": "Are large language models superhuman chemists?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are large language models superhuman chemists?"
                },
                "summary": "Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained.\n  However, we possess only a limited systematic understanding of the chemical\ncapabilities of LLMs, which would be required to improve models and mitigate\npotential harm. Here, we introduce \"ChemBench,\" an automated framework for\nevaluating the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of chemists.\n  We curated more than 2,700 question-answer pairs, evaluated leading open- and\nclosed-source LLMs, and found that the best models outperformed the best human\nchemists in our study on average. However, the models struggle with some basic\ntasks and provide overconfident predictions.\n  These findings reveal LLMs' impressive chemical capabilities while\nemphasizing the need for further research to improve their safety and\nusefulness. They also suggest adapting chemistry education and show the value\nof benchmarking frameworks for evaluating LLMs in specific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained.\n  However, we possess only a limited systematic understanding of the chemical\ncapabilities of LLMs, which would be required to improve models and mitigate\npotential harm. Here, we introduce \"ChemBench,\" an automated framework for\nevaluating the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of chemists.\n  We curated more than 2,700 question-answer pairs, evaluated leading open- and\nclosed-source LLMs, and found that the best models outperformed the best human\nchemists in our study on average. However, the models struggle with some basic\ntasks and provide overconfident predictions.\n  These findings reveal LLMs' impressive chemical capabilities while\nemphasizing the need for further research to improve their safety and\nusefulness. They also suggest adapting chemistry education and show the value\nof benchmarking frameworks for evaluating LLMs in specific domains."
                },
                "authors": [
                    {
                        "name": "Adrian Mirza"
                    },
                    {
                        "name": "Nawaf Alampara"
                    },
                    {
                        "name": "Sreekanth Kunchapu"
                    },
                    {
                        "name": "Martiño Ríos-García"
                    },
                    {
                        "name": "Benedict Emoekabu"
                    },
                    {
                        "name": "Aswanth Krishnan"
                    },
                    {
                        "name": "Tanya Gupta"
                    },
                    {
                        "name": "Mara Schilling-Wilhelmi"
                    },
                    {
                        "name": "Macjonathan Okereke"
                    },
                    {
                        "name": "Anagha Aneesh"
                    },
                    {
                        "name": "Amir Mohammad Elahi"
                    },
                    {
                        "name": "Mehrdad Asgari"
                    },
                    {
                        "name": "Juliane Eberhardt"
                    },
                    {
                        "name": "Hani M. Elbeheiry"
                    },
                    {
                        "name": "María Victoria Gil"
                    },
                    {
                        "name": "Maximilian Greiner"
                    },
                    {
                        "name": "Caroline T. Holick"
                    },
                    {
                        "name": "Christina Glaubitz"
                    },
                    {
                        "name": "Tim Hoffmann"
                    },
                    {
                        "name": "Abdelrahman Ibrahim"
                    },
                    {
                        "name": "Lea C. Klepsch"
                    },
                    {
                        "name": "Yannik Köster"
                    },
                    {
                        "name": "Fabian Alexander Kreth"
                    },
                    {
                        "name": "Jakob Meyer"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Jan Matthias Peschel"
                    },
                    {
                        "name": "Michael Ringleb"
                    },
                    {
                        "name": "Nicole Roesner"
                    },
                    {
                        "name": "Johanna Schreiber"
                    },
                    {
                        "name": "Ulrich S. Schubert"
                    },
                    {
                        "name": "Leanne M. Stafast"
                    },
                    {
                        "name": "Dinga Wonanke"
                    },
                    {
                        "name": "Michael Pieler"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Kevin Maik Jablonka"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Maik Jablonka"
                },
                "author": "Kevin Maik Jablonka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15665v2",
                "updated": "2024-11-01T06:57:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    57,
                    43,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-21T06:09:30Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    6,
                    9,
                    30,
                    0,
                    295,
                    0
                ],
                "title": "Long Term Memory: The Foundation of AI Self-Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Term Memory: The Foundation of AI Self-Evolution"
                },
                "summary": "Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications."
                },
                "authors": [
                    {
                        "name": "Xun Jiang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Jiaying Wang"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shu Zhang"
                    },
                    {
                        "name": "Weiling Chen"
                    },
                    {
                        "name": "Xavier Tang"
                    },
                    {
                        "name": "Yize Chen"
                    },
                    {
                        "name": "Mengyue Wu"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Tianqiao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqiao Chen"
                },
                "author": "Tianqiao Chen",
                "arxiv_comment": "56 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11406v3",
                "updated": "2024-11-01T06:25:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    25,
                    15,
                    4,
                    306,
                    0
                ],
                "published": "2024-07-16T05:48:24Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    5,
                    48,
                    24,
                    1,
                    198,
                    0
                ],
                "title": "Revisiting the Impact of Pursuing Modularity for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Impact of Pursuing Modularity for Code Generation"
                },
                "summary": "Modular programming, which aims to construct the final program by integrating\nsmaller, independent building blocks, has been regarded as a desirable practice\nin software development. However, with the rise of recent code generation\nagents built upon large language models (LLMs), a question emerges: is this\ntraditional practice equally effective for these new tools? In this work, we\nassess the impact of modularity in code generation by introducing a novel\nmetric for its quantitative measurement. Surprisingly, unlike conventional\nwisdom on the topic, we find that modularity is not a core factor for improving\nthe performance of code generation models. We also explore potential\nexplanations for why LLMs do not exhibit a preference for modular code compared\nto non-modular code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular programming, which aims to construct the final program by integrating\nsmaller, independent building blocks, has been regarded as a desirable practice\nin software development. However, with the rise of recent code generation\nagents built upon large language models (LLMs), a question emerges: is this\ntraditional practice equally effective for these new tools? In this work, we\nassess the impact of modularity in code generation by introducing a novel\nmetric for its quantitative measurement. Surprisingly, unlike conventional\nwisdom on the topic, we find that modularity is not a core factor for improving\nthe performance of code generation models. We also explore potential\nexplanations for why LLMs do not exhibit a preference for modular code compared\nto non-modular code."
                },
                "authors": [
                    {
                        "name": "Deokyeong Kang"
                    },
                    {
                        "name": "Ki Jung Seo"
                    },
                    {
                        "name": "Taeuk Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taeuk Kim"
                },
                "author": "Taeuk Kim",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20468v2",
                "updated": "2024-11-01T06:21:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    21,
                    26,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-27T15:08:54Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    15,
                    8,
                    54,
                    6,
                    301,
                    0
                ],
                "title": "Understanding Communication Preferences of Information Workers in\n  Engagement with Text-Based Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Communication Preferences of Information Workers in\n  Engagement with Text-Based Conversational Agents"
                },
                "summary": "Communication traits in text-based human-AI conversations play pivotal roles\nin shaping user experiences and perceptions of systems. With the advancement of\nlarge language models (LLMs), it is now feasible to analyze these traits at a\nmore granular level. In this study, we explore the preferences of information\nworkers regarding chatbot communication traits across seven applications.\nParticipants were invited to participate in an interactive survey, which\nfeatured adjustable sliders, allowing them to adjust and express their\npreferences for five key communication traits: formality, personification,\nempathy, sociability, and humor. Our findings reveal distinct communication\npreferences across different applications; for instance, there was a preference\nfor relatively high empathy in wellbeing contexts and relatively low\npersonification in coding. Similarities in preferences were also noted between\napplications such as chatbots for customer service and scheduling. These\ninsights offer crucial design guidelines for future chatbots, emphasizing the\nneed for nuanced trait adjustments for each application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication traits in text-based human-AI conversations play pivotal roles\nin shaping user experiences and perceptions of systems. With the advancement of\nlarge language models (LLMs), it is now feasible to analyze these traits at a\nmore granular level. In this study, we explore the preferences of information\nworkers regarding chatbot communication traits across seven applications.\nParticipants were invited to participate in an interactive survey, which\nfeatured adjustable sliders, allowing them to adjust and express their\npreferences for five key communication traits: formality, personification,\nempathy, sociability, and humor. Our findings reveal distinct communication\npreferences across different applications; for instance, there was a preference\nfor relatively high empathy in wellbeing contexts and relatively low\npersonification in coding. Similarities in preferences were also noted between\napplications such as chatbots for customer service and scheduling. These\ninsights offer crucial design guidelines for future chatbots, emphasizing the\nneed for nuanced trait adjustments for each application."
                },
                "authors": [
                    {
                        "name": "Ananya Bhattacharjee"
                    },
                    {
                        "name": "Jina Suh"
                    },
                    {
                        "name": "Mahsa Ershadi"
                    },
                    {
                        "name": "Shamsi T. Iqbal"
                    },
                    {
                        "name": "Andrew D. Wilson"
                    },
                    {
                        "name": "Javier Hernandez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Hernandez"
                },
                "author": "Javier Hernandez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14155v2",
                "updated": "2024-11-01T06:19:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    19,
                    47,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-18T03:45:42Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    45,
                    42,
                    4,
                    292,
                    0
                ],
                "title": "Towards Faithful Natural Language Explanations: A Study Using Activation\n  Patching in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Faithful Natural Language Explanations: A Study Using Activation\n  Patching in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}"
                },
                "authors": [
                    {
                        "name": "Wei Jie Yeo"
                    },
                    {
                        "name": "Ranjan Satapathy"
                    },
                    {
                        "name": "Erik Cambria"
                    }
                ],
                "author_detail": {
                    "name": "Erik Cambria"
                },
                "author": "Erik Cambria",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16247v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16247v3",
                "updated": "2024-11-01T06:13:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    13,
                    12,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-25T14:11:44Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    14,
                    11,
                    44,
                    5,
                    146,
                    0
                ],
                "title": "AutoManual: Generating Instruction Manuals by LLM Agents via Interactive\n  Environmental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoManual: Generating Instruction Manuals by LLM Agents via Interactive\n  Environmental Learning"
                },
                "summary": "Large Language Models (LLM) based agents have shown promise in autonomously\ncompleting tasks across various domains, e.g., robotics, games, and web\nnavigation. However, these agents typically require elaborate design and expert\nprompts to solve tasks in specific domains, which limits their adaptability. We\nintroduce AutoManual, a framework enabling LLM agents to autonomously build\ntheir understanding through interaction and adapt to new environments.\nAutoManual categorizes environmental knowledge into diverse rules and optimizes\nthem in an online fashion by two agents: 1) The Planner codes actionable plans\nbased on current rules for interacting with the environment. 2) The Builder\nupdates the rules through a well-structured rule system that facilitates online\nrule management and essential detail retention. To mitigate hallucinations in\nmanaging rules, we introduce a *case-conditioned prompting* strategy for the\nBuilder. Finally, the Formulator agent compiles these rules into a\ncomprehensive manual. The self-generated manual can not only improve the\nadaptability but also guide the planning of smaller LLMs while being\nhuman-readable. Given only one simple demonstration, AutoManual significantly\nimproves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with\nGPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at\nhttps://github.com/minghchen/automanual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) based agents have shown promise in autonomously\ncompleting tasks across various domains, e.g., robotics, games, and web\nnavigation. However, these agents typically require elaborate design and expert\nprompts to solve tasks in specific domains, which limits their adaptability. We\nintroduce AutoManual, a framework enabling LLM agents to autonomously build\ntheir understanding through interaction and adapt to new environments.\nAutoManual categorizes environmental knowledge into diverse rules and optimizes\nthem in an online fashion by two agents: 1) The Planner codes actionable plans\nbased on current rules for interacting with the environment. 2) The Builder\nupdates the rules through a well-structured rule system that facilitates online\nrule management and essential detail retention. To mitigate hallucinations in\nmanaging rules, we introduce a *case-conditioned prompting* strategy for the\nBuilder. Finally, the Formulator agent compiles these rules into a\ncomprehensive manual. The self-generated manual can not only improve the\nadaptability but also guide the planning of smaller LLMs while being\nhuman-readable. Given only one simple demonstration, AutoManual significantly\nimproves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with\nGPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at\nhttps://github.com/minghchen/automanual."
                },
                "authors": [
                    {
                        "name": "Minghao Chen"
                    },
                    {
                        "name": "Yihang Li"
                    },
                    {
                        "name": "Yanting Yang"
                    },
                    {
                        "name": "Shiyu Yu"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Xiaofei He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofei He"
                },
                "author": "Xiaofei He",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16247v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01548v2",
                "updated": "2024-11-01T06:12:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    6,
                    12,
                    33,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-02T13:37:54Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    37,
                    54,
                    2,
                    276,
                    0
                ],
                "title": "In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks"
                },
                "summary": "In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Dingzirui Wang"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Xiao Xu"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04744v2",
                "updated": "2024-11-01T05:30:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    5,
                    30,
                    17,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-07T08:43:07Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    8,
                    43,
                    7,
                    4,
                    159,
                    0
                ],
                "title": "CRAG -- Comprehensive RAG Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRAG -- Comprehensive RAG Benchmark"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution to alleviate Large Language Model (LLM)'s deficiency in lack of\nknowledge. Existing RAG datasets, however, do not adequately represent the\ndiverse and dynamic nature of real-world Question Answering (QA) tasks. To\nbridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual\nquestion answering benchmark of 4,409 question-answer pairs and mock APIs to\nsimulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a\ndiverse array of questions across five domains and eight question categories,\nreflecting varied entity popularity from popular to long-tail, and temporal\ndynamisms ranging from years to seconds. Our evaluation of this benchmark\nhighlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve\n<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the\naccuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%\nof questions without any hallucination. CRAG also reveals much lower accuracy\nin answering questions regarding facts with higher dynamism, lower popularity,\nor higher complexity, suggesting future research directions. The CRAG benchmark\nlaid the groundwork for a KDD Cup 2024 challenge and attracted thousands of\nparticipants and submissions. We commit to maintaining CRAG to serve research\ncommunities in advancing RAG solutions and general QA solutions. CRAG is\navailable at https://github.com/facebookresearch/CRAG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution to alleviate Large Language Model (LLM)'s deficiency in lack of\nknowledge. Existing RAG datasets, however, do not adequately represent the\ndiverse and dynamic nature of real-world Question Answering (QA) tasks. To\nbridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual\nquestion answering benchmark of 4,409 question-answer pairs and mock APIs to\nsimulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a\ndiverse array of questions across five domains and eight question categories,\nreflecting varied entity popularity from popular to long-tail, and temporal\ndynamisms ranging from years to seconds. Our evaluation of this benchmark\nhighlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve\n<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the\naccuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%\nof questions without any hallucination. CRAG also reveals much lower accuracy\nin answering questions regarding facts with higher dynamism, lower popularity,\nor higher complexity, suggesting future research directions. The CRAG benchmark\nlaid the groundwork for a KDD Cup 2024 challenge and attracted thousands of\nparticipants and submissions. We commit to maintaining CRAG to serve research\ncommunities in advancing RAG solutions and general QA solutions. CRAG is\navailable at https://github.com/facebookresearch/CRAG/."
                },
                "authors": [
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Hao Xin"
                    },
                    {
                        "name": "Yushi Sun"
                    },
                    {
                        "name": "Nikita Bhalla"
                    },
                    {
                        "name": "Xiangsen Chen"
                    },
                    {
                        "name": "Sajal Choudhary"
                    },
                    {
                        "name": "Rongze Daniel Gui"
                    },
                    {
                        "name": "Ziran Will Jiang"
                    },
                    {
                        "name": "Ziyu Jiang"
                    },
                    {
                        "name": "Lingkun Kong"
                    },
                    {
                        "name": "Brian Moran"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yifan Ethan Xu"
                    },
                    {
                        "name": "An Yan"
                    },
                    {
                        "name": "Chenyu Yang"
                    },
                    {
                        "name": "Eting Yuan"
                    },
                    {
                        "name": "Hanwen Zha"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Nicolas Scheffer"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Nirav Shah"
                    },
                    {
                        "name": "Rakesh Wanga"
                    },
                    {
                        "name": "Anuj Kumar"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Xin Luna Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xin Luna Dong"
                },
                "author": "Xin Luna Dong",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11724v2",
                "updated": "2024-11-01T04:19:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    4,
                    19,
                    21,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-18T06:19:59Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    6,
                    19,
                    59,
                    2,
                    262,
                    0
                ],
                "title": "TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning"
                },
                "summary": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lu"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01763v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01763v3",
                "updated": "2024-11-01T03:49:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    49,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-01-30T23:35:28Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    23,
                    35,
                    28,
                    1,
                    30,
                    0
                ],
                "title": "When Large Language Models Meet Vector Databases: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Large Language Models Meet Vector Databases: A Survey"
                },
                "summary": "This survey explores the synergistic potential of Large Language Models\n(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving\nresearch area. With the proliferation of LLMs comes a host of challenges,\nincluding hallucinations, outdated knowledge, prohibitive commercial\napplication costs, and memory issues. VecDBs emerge as a compelling solution to\nthese issues by offering an efficient means to store, retrieve, and manage the\nhigh-dimensional vector representations intrinsic to LLM operations. Through\nthis nuanced review, we delineate the foundational principles of LLMs and\nVecDBs and critically analyze their integration's impact on enhancing LLM\nfunctionalities. This discourse extends into a discussion on the speculative\nfuture developments in this domain, aiming to catalyze further research into\noptimizing the confluence of LLMs and VecDBs for advanced data handling and\nknowledge extraction capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the synergistic potential of Large Language Models\n(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving\nresearch area. With the proliferation of LLMs comes a host of challenges,\nincluding hallucinations, outdated knowledge, prohibitive commercial\napplication costs, and memory issues. VecDBs emerge as a compelling solution to\nthese issues by offering an efficient means to store, retrieve, and manage the\nhigh-dimensional vector representations intrinsic to LLM operations. Through\nthis nuanced review, we delineate the foundational principles of LLMs and\nVecDBs and critically analyze their integration's impact on enhancing LLM\nfunctionalities. This discourse extends into a discussion on the speculative\nfuture developments in this domain, aiming to catalyze further research into\noptimizing the confluence of LLMs and VecDBs for advanced data handling and\nknowledge extraction capabilities."
                },
                "authors": [
                    {
                        "name": "Zhi Jing"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Yikun Han"
                    }
                ],
                "author_detail": {
                    "name": "Yikun Han"
                },
                "author": "Yikun Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01763v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01763v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10159v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10159v3",
                "updated": "2024-11-01T03:47:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    47,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-08-19T17:09:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    9,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10159v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10159v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02428v3",
                "updated": "2024-11-01T03:47:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    47,
                    51,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-04T04:15:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    4,
                    15,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models as Efficient Reward Function Searchers for\n  Custom-Environment Multi-Objective Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Efficient Reward Function Searchers for\n  Custom-Environment Multi-Objective Reinforcement Learning"
                },
                "summary": "Achieving the effective design and improvement of reward functions in\nreinforcement learning (RL) tasks with complex custom environments and multiple\nrequirements presents considerable challenges. In this paper, we propose ERFSL,\nan efficient reward function searcher using LLMs, which enables LLMs to be\neffective white-box searchers and highlights their advanced semantic\nunderstanding capabilities. Specifically, we generate reward components for\neach numerically explicit user requirement and employ a reward critic to\nidentify the correct code form. Then, LLMs assign weights to the reward\ncomponents to balance their values and iteratively adjust the weights without\nambiguity and redundant adjustments by flexibly adopting directional mutation\nand crossover strategies, similar to genetic algorithms, based on the context\nprovided by the training log analyzer. We applied the framework to an\nunderwater data collection RL task without direct human feedback or reward\nexamples (zero-shot learning). The reward critic successfully corrects the\nreward code with only one feedback instance for each requirement, effectively\npreventing unrectifiable errors. The initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout the need for weight search. Even in cases where a weight is 500 times\noff, on average, only 5.2 iterations are needed to meet user requirements. The\nERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose\nthe weight searching process to reduce the requirement for numerical and\nlong-context understanding capabilities",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving the effective design and improvement of reward functions in\nreinforcement learning (RL) tasks with complex custom environments and multiple\nrequirements presents considerable challenges. In this paper, we propose ERFSL,\nan efficient reward function searcher using LLMs, which enables LLMs to be\neffective white-box searchers and highlights their advanced semantic\nunderstanding capabilities. Specifically, we generate reward components for\neach numerically explicit user requirement and employ a reward critic to\nidentify the correct code form. Then, LLMs assign weights to the reward\ncomponents to balance their values and iteratively adjust the weights without\nambiguity and redundant adjustments by flexibly adopting directional mutation\nand crossover strategies, similar to genetic algorithms, based on the context\nprovided by the training log analyzer. We applied the framework to an\nunderwater data collection RL task without direct human feedback or reward\nexamples (zero-shot learning). The reward critic successfully corrects the\nreward code with only one feedback instance for each requirement, effectively\npreventing unrectifiable errors. The initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout the need for weight search. Even in cases where a weight is 500 times\noff, on average, only 5.2 iterations are needed to meet user requirements. The\nERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose\nthe weight searching process to reduce the requirement for numerical and\nlong-context understanding capabilities"
                },
                "authors": [
                    {
                        "name": "Guanwen Xie"
                    },
                    {
                        "name": "Jingzehua Xu"
                    },
                    {
                        "name": "Yiyuan Yang"
                    },
                    {
                        "name": "Yimian Ding"
                    },
                    {
                        "name": "Shuai Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Zhang"
                },
                "author": "Shuai Zhang",
                "arxiv_journal_ref": "AAAI (Student) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04501v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04501v3",
                "updated": "2024-11-01T03:42:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    42,
                    37,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-06T14:45:01Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    14,
                    45,
                    1,
                    6,
                    280,
                    0
                ],
                "title": "Leveraging Large Language Models for Suicide Detection on Social Media\n  with Limited Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Suicide Detection on Social Media\n  with Limited Labels"
                },
                "summary": "The increasing frequency of suicidal thoughts highlights the importance of\nearly detection and intervention. Social media platforms, where users often\nshare personal experiences and seek help, could be utilized to identify\nindividuals at risk. However, the large volume of daily posts makes manual\nreview impractical. This paper explores the use of Large Language Models (LLMs)\nto automatically detect suicidal content in text-based social media posts. We\npropose a novel method for generating pseudo-labels for unlabeled data by\nprompting LLMs, along with traditional classification fine-tuning techniques to\nenhance label accuracy. To create a strong suicide detection model, we develop\nan ensemble approach involving prompting with Qwen2-72B-Instruct, and using\nfine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate\nour approach on the dataset of the Suicide Ideation Detection on Social Media\nChallenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we\nconduct a comprehensive analysis to assess the impact of different models and\nfine-tuning strategies on detection performance. Experimental results show that\nthe ensemble model significantly improves the detection accuracy, by 5% points\ncompared with the individual models. It achieves a weight F1 score of 0.770 on\nthe public test set, and 0.731 on the private test set, providing a promising\nsolution for identifying suicidal content in social media. Our analysis shows\nthat the choice of LLMs affects the prompting performance, with larger models\nproviding better accuracy. Our code and checkpoints are publicly available at\nhttps://github.com/khanhvynguyen/Suicide_Detection_LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing frequency of suicidal thoughts highlights the importance of\nearly detection and intervention. Social media platforms, where users often\nshare personal experiences and seek help, could be utilized to identify\nindividuals at risk. However, the large volume of daily posts makes manual\nreview impractical. This paper explores the use of Large Language Models (LLMs)\nto automatically detect suicidal content in text-based social media posts. We\npropose a novel method for generating pseudo-labels for unlabeled data by\nprompting LLMs, along with traditional classification fine-tuning techniques to\nenhance label accuracy. To create a strong suicide detection model, we develop\nan ensemble approach involving prompting with Qwen2-72B-Instruct, and using\nfine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate\nour approach on the dataset of the Suicide Ideation Detection on Social Media\nChallenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we\nconduct a comprehensive analysis to assess the impact of different models and\nfine-tuning strategies on detection performance. Experimental results show that\nthe ensemble model significantly improves the detection accuracy, by 5% points\ncompared with the individual models. It achieves a weight F1 score of 0.770 on\nthe public test set, and 0.731 on the private test set, providing a promising\nsolution for identifying suicidal content in social media. Our analysis shows\nthat the choice of LLMs affects the prompting performance, with larger models\nproviding better accuracy. Our code and checkpoints are publicly available at\nhttps://github.com/khanhvynguyen/Suicide_Detection_LLMs."
                },
                "authors": [
                    {
                        "name": "Vy Nguyen"
                    },
                    {
                        "name": "Chau Pham"
                    }
                ],
                "author_detail": {
                    "name": "Chau Pham"
                },
                "author": "Chau Pham",
                "arxiv_comment": "Accepted at IEEE International Conference on Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04501v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13056v2",
                "updated": "2024-11-01T03:16:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    16,
                    30,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-16T21:34:41Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    21,
                    34,
                    41,
                    2,
                    290,
                    0
                ],
                "title": "Channel-Wise Mixed-Precision Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel-Wise Mixed-Precision Quantization for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of language tasks, but their deployment on edge devices remains\nchallenging due to the substantial memory requirements imposed by their large\nparameter sizes. Weight-only quantization presents a promising solution to\nreduce the memory footprint of LLMs. However, existing approaches primarily\nfocus on integer-bit quantization, limiting their adaptability to\nfractional-bit quantization tasks and preventing the full utilization of\navailable storage space on devices. In this paper, we introduce Channel-Wise\nMixed-Precision Quantization (CMPQ), a novel mixed-precision quantization\nmethod that allocates quantization precision in a channel-wise pattern based on\nactivation distributions. By assigning different precision levels to different\nweight channels, CMPQ can adapt to any bit-width constraint. CMPQ employs a\nnon-uniform quantization strategy and incorporates two outlier extraction\ntechniques that collaboratively preserve the critical information, thereby\nminimizing the quantization loss. Experiments on different sizes of LLMs\ndemonstrate that CMPQ not only enhances performance in integer-bit quantization\ntasks but also achieves significant performance gains with a modest increase in\nmemory usage. CMPQ thus represents an adaptive and effective approach to LLM\nquantization, offering substantial benefits across diverse device capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of language tasks, but their deployment on edge devices remains\nchallenging due to the substantial memory requirements imposed by their large\nparameter sizes. Weight-only quantization presents a promising solution to\nreduce the memory footprint of LLMs. However, existing approaches primarily\nfocus on integer-bit quantization, limiting their adaptability to\nfractional-bit quantization tasks and preventing the full utilization of\navailable storage space on devices. In this paper, we introduce Channel-Wise\nMixed-Precision Quantization (CMPQ), a novel mixed-precision quantization\nmethod that allocates quantization precision in a channel-wise pattern based on\nactivation distributions. By assigning different precision levels to different\nweight channels, CMPQ can adapt to any bit-width constraint. CMPQ employs a\nnon-uniform quantization strategy and incorporates two outlier extraction\ntechniques that collaboratively preserve the critical information, thereby\nminimizing the quantization loss. Experiments on different sizes of LLMs\ndemonstrate that CMPQ not only enhances performance in integer-bit quantization\ntasks but also achieves significant performance gains with a modest increase in\nmemory usage. CMPQ thus represents an adaptive and effective approach to LLM\nquantization, offering substantial benefits across diverse device capabilities."
                },
                "authors": [
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Bike Xie"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Cong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Cong Shen"
                },
                "author": "Cong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20646v2",
                "updated": "2024-11-01T03:12:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    3,
                    12,
                    44,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-31T07:24:42Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    7,
                    24,
                    42,
                    4,
                    152,
                    0
                ],
                "title": "LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential\n  Recommendation"
                },
                "summary": "Sequential recommender systems (SRS) aim to predict users' subsequent choices\nbased on their historical interactions and have found applications in diverse\nfields such as e-commerce and social media. However, in real-world systems,\nmost users interact with only a handful of items, while the majority of items\nare seldom consumed. These two issues, known as the long-tail user and\nlong-tail item challenges, often pose difficulties for existing SRS. These\nchallenges can adversely affect user experience and seller benefits, making\nthem crucial to address. Though a few works have addressed the challenges, they\nstill struggle with the seesaw or noisy issues due to the intrinsic scarcity of\ninteractions. The advancements in large language models (LLMs) present a\npromising solution to these problems from a semantic perspective. As one of the\npioneers in this field, we propose the Large Language Models Enhancement\nframework for Sequential Recommendation (LLM-ESR). This framework utilizes\nsemantic embeddings derived from LLMs to enhance SRS without adding extra\ninference load from LLMs. To address the long-tail item challenge, we design a\ndual-view modeling framework that combines semantics from LLMs and\ncollaborative signals from conventional SRS. For the long-tail user challenge,\nwe propose a retrieval augmented self-distillation method to enhance user\npreference representation using more informative interactions from similar\nusers. To verify the effectiveness and versatility of our proposed enhancement\nframework, we conduct extensive experiments on three real-world datasets using\nthree popular SRS models. The results show that our method surpasses existing\nbaselines consistently, and benefits long-tail users and items especially. The\nimplementation code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/LLM-ESR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommender systems (SRS) aim to predict users' subsequent choices\nbased on their historical interactions and have found applications in diverse\nfields such as e-commerce and social media. However, in real-world systems,\nmost users interact with only a handful of items, while the majority of items\nare seldom consumed. These two issues, known as the long-tail user and\nlong-tail item challenges, often pose difficulties for existing SRS. These\nchallenges can adversely affect user experience and seller benefits, making\nthem crucial to address. Though a few works have addressed the challenges, they\nstill struggle with the seesaw or noisy issues due to the intrinsic scarcity of\ninteractions. The advancements in large language models (LLMs) present a\npromising solution to these problems from a semantic perspective. As one of the\npioneers in this field, we propose the Large Language Models Enhancement\nframework for Sequential Recommendation (LLM-ESR). This framework utilizes\nsemantic embeddings derived from LLMs to enhance SRS without adding extra\ninference load from LLMs. To address the long-tail item challenge, we design a\ndual-view modeling framework that combines semantics from LLMs and\ncollaborative signals from conventional SRS. For the long-tail user challenge,\nwe propose a retrieval augmented self-distillation method to enhance user\npreference representation using more informative interactions from similar\nusers. To verify the effectiveness and versatility of our proposed enhancement\nframework, we conduct extensive experiments on three real-world datasets using\nthree popular SRS models. The results show that our method surpasses existing\nbaselines consistently, and benefits long-tail users and items especially. The\nimplementation code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/LLM-ESR."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "accepted by NeruIPS'24 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21358v2",
                "updated": "2024-11-01T02:50:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    50,
                    16,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-28T17:35:59Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    35,
                    59,
                    0,
                    302,
                    0
                ],
                "title": "\"We do use it, but not how hearing people think\": How the Deaf and Hard\n  of Hearing Community Uses Large Language Model Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"We do use it, but not how hearing people think\": How the Deaf and Hard\n  of Hearing Community Uses Large Language Model Tools"
                },
                "summary": "Generative AI tools, particularly those utilizing large language models\n(LLMs), have become increasingly prevalent in both professional and personal\ncontexts, offering powerful capabilities for text generation and communication\nsupport. While these tools are widely used to enhance productivity and\naccessibility, there has been limited exploration of how Deaf and Hard of\nHearing (DHH) individuals engage with text-based generative AI tools, as well\nas the challenges they may encounter. This paper presents a mixed-method survey\nstudy investigating how the DHH community uses Text AI tools, such as ChatGPT,\nto reduce communication barriers, bridge Deaf and hearing cultures, and improve\naccess to information. Through a survey of 80 DHH participants and separate\ninterviews with 11 other participants, we found that while these tools provide\nsignificant benefits, including enhanced communication and mental health\nsupport, they also introduce barriers, such as a lack of American Sign Language\n(ASL) support and understanding of Deaf cultural nuances. Our findings\nhighlight unique usage patterns within the DHH community and underscore the\nneed for inclusive design improvements. We conclude by offering practical\nrecommendations to enhance the accessibility of Text AI for the DHH community\nand suggest directions for future research in AI and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI tools, particularly those utilizing large language models\n(LLMs), have become increasingly prevalent in both professional and personal\ncontexts, offering powerful capabilities for text generation and communication\nsupport. While these tools are widely used to enhance productivity and\naccessibility, there has been limited exploration of how Deaf and Hard of\nHearing (DHH) individuals engage with text-based generative AI tools, as well\nas the challenges they may encounter. This paper presents a mixed-method survey\nstudy investigating how the DHH community uses Text AI tools, such as ChatGPT,\nto reduce communication barriers, bridge Deaf and hearing cultures, and improve\naccess to information. Through a survey of 80 DHH participants and separate\ninterviews with 11 other participants, we found that while these tools provide\nsignificant benefits, including enhanced communication and mental health\nsupport, they also introduce barriers, such as a lack of American Sign Language\n(ASL) support and understanding of Deaf cultural nuances. Our findings\nhighlight unique usage patterns within the DHH community and underscore the\nneed for inclusive design improvements. We conclude by offering practical\nrecommendations to enhance the accessibility of Text AI for the DHH community\nand suggest directions for future research in AI and accessibility."
                },
                "authors": [
                    {
                        "name": "Shuxu Huffman"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Kelly Avery Mack"
                    },
                    {
                        "name": "Haotian Su"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Raja Kushalnagar"
                    }
                ],
                "author_detail": {
                    "name": "Raja Kushalnagar"
                },
                "author": "Raja Kushalnagar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15880v2",
                "updated": "2024-11-01T02:46:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    46,
                    3,
                    4,
                    306,
                    0
                ],
                "published": "2024-05-24T18:45:51Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    18,
                    45,
                    51,
                    4,
                    145,
                    0
                ],
                "title": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis"
                },
                "summary": "Many structured prediction and reasoning tasks can be framed as program\nsynthesis problems, where the goal is to generate a program in a\ndomain-specific language (DSL) that transforms input data into the desired\noutput. Unfortunately, purely neural approaches, such as large language models\n(LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while\npurely symbolic methods based on combinatorial search scale poorly to complex\nproblems. Motivated by these limitations, we introduce a hybrid approach, where\nLLM completions for a given task are used to learn a task-specific,\ncontext-free surrogate model, which is then used to guide program synthesis. We\nevaluate this hybrid approach on three domains, and show that it outperforms\nboth unguided search and direct sampling from LLMs, as well as existing program\nsynthesizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many structured prediction and reasoning tasks can be framed as program\nsynthesis problems, where the goal is to generate a program in a\ndomain-specific language (DSL) that transforms input data into the desired\noutput. Unfortunately, purely neural approaches, such as large language models\n(LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while\npurely symbolic methods based on combinatorial search scale poorly to complex\nproblems. Motivated by these limitations, we introduce a hybrid approach, where\nLLM completions for a given task are used to learn a task-specific,\ncontext-free surrogate model, which is then used to guide program synthesis. We\nevaluate this hybrid approach on three domains, and show that it outperforms\nboth unguided search and direct sampling from LLMs, as well as existing program\nsynthesizers."
                },
                "authors": [
                    {
                        "name": "Shraddha Barke"
                    },
                    {
                        "name": "Emmanuel Anaya Gonzalez"
                    },
                    {
                        "name": "Saketh Ram Kasibatla"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Nadia Polikarpova"
                    }
                ],
                "author_detail": {
                    "name": "Nadia Polikarpova"
                },
                "author": "Nadia Polikarpova",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09131v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09131v4",
                "updated": "2024-11-01T02:43:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    43,
                    34,
                    4,
                    306,
                    0
                ],
                "published": "2024-03-14T06:49:16Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    6,
                    49,
                    16,
                    3,
                    74,
                    0
                ],
                "title": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between\n  Professional and Non-Professional Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSwitch: Knowledge-Guided Instruction Tuning to Switch Between\n  Professional and Non-Professional Answers"
                },
                "summary": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\ninstruction tuning remain underexplored. This study concentrates on the\nstyle-switching abilities of LLMs and introduces a novel approach, named\nProSwitch, which enables a language model to switch between professional and\nnon-professional answers, by tuning and evaluating through the guidance of\ndomain and style knowledge. ProSwitch unfolds across three phases:\nLLM-augmented preparation to collect domain knowledge and QA pairs, instruction\ntuning to optimize LLMs with multiple levels of knowledge, and comprehensive\nevaluation to assess both style discrimination and reference-based quality of\ngenerated text. Comparative analysis of ProSwitch against general and\nspecialized LLMs reveals that our approach outperforms baselines in switching\nbetween professional and non-professional answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\ninstruction tuning remain underexplored. This study concentrates on the\nstyle-switching abilities of LLMs and introduces a novel approach, named\nProSwitch, which enables a language model to switch between professional and\nnon-professional answers, by tuning and evaluating through the guidance of\ndomain and style knowledge. ProSwitch unfolds across three phases:\nLLM-augmented preparation to collect domain knowledge and QA pairs, instruction\ntuning to optimize LLMs with multiple levels of knowledge, and comprehensive\nevaluation to assess both style discrimination and reference-based quality of\ngenerated text. Comparative analysis of ProSwitch against general and\nspecialized LLMs reveals that our approach outperforms baselines in switching\nbetween professional and non-professional answers."
                },
                "authors": [
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Yuyan Chen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "8 pages main body, 16 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09131v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09131v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13623v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13623v3",
                "updated": "2024-11-01T02:41:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    41,
                    36,
                    4,
                    306,
                    0
                ],
                "published": "2024-07-18T15:58:54Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    15,
                    58,
                    54,
                    3,
                    200,
                    0
                ],
                "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies"
                },
                "summary": "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe conclusion that the optimal vocabulary size depends on the compute budget,\nwith larger models requiring larger vocabularies. Most LLMs, however, use\ninsufficient vocabulary sizes. For example, we predict that the optimal\nvocabulary size of Llama2-70B should have been at least 216K, 7 times larger\nthan its vocabulary of 32K. We validate our predictions empirically by training\nmodels with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly\nconsidering tokenization and model scaling for efficient pre-training. The code\nand demo are available at https://github.com/sail-sg/scaling-with-vocab and\nhttps://hf.co/spaces/sail/scaling-with-vocab-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe conclusion that the optimal vocabulary size depends on the compute budget,\nwith larger models requiring larger vocabularies. Most LLMs, however, use\ninsufficient vocabulary sizes. For example, we predict that the optimal\nvocabulary size of Llama2-70B should have been at least 216K, 7 times larger\nthan its vocabulary of 32K. We validate our predictions empirically by training\nmodels with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly\nconsidering tokenization and model scaling for efficient pre-training. The code\nand demo are available at https://github.com/sail-sg/scaling-with-vocab and\nhttps://hf.co/spaces/sail/scaling-with-vocab-demo."
                },
                "authors": [
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13623v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13623v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17508v2",
                "updated": "2024-11-01T02:38:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    38,
                    53,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-26T03:33:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    33,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task\n  Learning Via Connector-MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task\n  Learning Via Connector-MoE"
                },
                "summary": "Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization in MLLMs, recent advances primarily focus on improving\nthe LLM components, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector in MLLMs. Extensive ablation\nexperiments validate the effectiveness of introducing CMoE under any\nconfiguration, with up to an average 8% performance gains. We further provide\ninterpretation analysis of the tug-of-war problem from the perspective of\ngradient optimization and parameter statistics. Compared to previous\nstate-of-the-art medical MLLMs, Uni-Med achieves competitive or superior\nevaluation metrics on diverse tasks. Code and resources are available at\nhttps://github.com/tsinghua-msiip/Uni-Med.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization in MLLMs, recent advances primarily focus on improving\nthe LLM components, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector in MLLMs. Extensive ablation\nexperiments validate the effectiveness of introducing CMoE under any\nconfiguration, with up to an average 8% performance gains. We further provide\ninterpretation analysis of the tug-of-war problem from the perspective of\ngradient optimization and parameter statistics. Compared to previous\nstate-of-the-art medical MLLMs, Uni-Med achieves competitive or superior\nevaluation metrics on diverse tasks. Code and resources are available at\nhttps://github.com/tsinghua-msiip/Uni-Med."
                },
                "authors": [
                    {
                        "name": "Xun Zhu"
                    },
                    {
                        "name": "Ying Hu"
                    },
                    {
                        "name": "Fanbin Mo"
                    },
                    {
                        "name": "Miao Li"
                    },
                    {
                        "name": "Ji Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ji Wu"
                },
                "author": "Ji Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14909v2",
                "updated": "2024-11-01T02:26:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    26,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-21T06:58:37Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    6,
                    58,
                    37,
                    4,
                    173,
                    0
                ],
                "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoA: Mixture of Sparse Attention for Automatic Large Language Model\n  Compression"
                },
                "summary": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by $3.9\\times$ with the same\naverage attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the\nuniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$\nacross two long-context understanding benchmarks. MoA achieves a\n$1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by\n$6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with\nminimal impact on performance. Our code is available at\n\\url{https://github.com/thu-nics/MoA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by $3.9\\times$ with the same\naverage attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the\nuniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$\nacross two long-context understanding benchmarks. MoA achieves a\n$1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by\n$6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with\nminimal impact on performance. Our code is available at\n\\url{https://github.com/thu-nics/MoA}."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Genghan Zhang"
                    },
                    {
                        "name": "Boju Chen"
                    },
                    {
                        "name": "Tianqi Wu"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Zixiao Huang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02834v3",
                "updated": "2024-11-01T02:21:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    21,
                    13,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-04T16:00:21Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    0,
                    21,
                    2,
                    248,
                    0
                ],
                "title": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models"
                },
                "summary": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets."
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Ji Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18451v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18451v3",
                "updated": "2024-11-01T02:13:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    13,
                    59,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-26T16:00:35Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    16,
                    0,
                    35,
                    2,
                    178,
                    0
                ],
                "title": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in\n  Deep Robust Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in\n  Deep Robust Classifiers"
                },
                "summary": "Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate high margin consistency with\na strong correlation between their input space margins and the logit margins.\nThen, we show that we can effectively and confidently use the logit margin to\ndetect brittle decisions with such models. Finally, we address cases where the\nmodel is not sufficiently margin-consistent by learning a pseudo-margin from\nthe feature representation. Our findings highlight the potential of leveraging\ndeep representations to assess adversarial vulnerability in deployment\nscenarios efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate high margin consistency with\na strong correlation between their input space margins and the logit margins.\nThen, we show that we can effectively and confidently use the logit margin to\ndetect brittle decisions with such models. Finally, we address cases where the\nmodel is not sufficiently margin-consistent by learning a pseudo-margin from\nthe feature representation. Our findings highlight the potential of leveraging\ndeep representations to assess adversarial vulnerability in deployment\nscenarios efficiently."
                },
                "authors": [
                    {
                        "name": "Jonas Ngnawé"
                    },
                    {
                        "name": "Sabyasachi Sahoo"
                    },
                    {
                        "name": "Yann Pequignot"
                    },
                    {
                        "name": "Frédéric Precioso"
                    },
                    {
                        "name": "Christian Gagné"
                    }
                ],
                "author_detail": {
                    "name": "Christian Gagné"
                },
                "author": "Christian Gagné",
                "arxiv_comment": "10 pages, 6 figures, 2 tables. Version Update: Neurips Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18451v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18451v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17213v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17213v4",
                "updated": "2024-11-01T02:08:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    8,
                    3,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-25T17:38:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    38,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles"
                },
                "summary": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by democratic deliberation theory, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by democratic deliberation theory, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated."
                },
                "authors": [
                    {
                        "name": "Joshua Ashkinaze"
                    },
                    {
                        "name": "Emily Fry"
                    },
                    {
                        "name": "Narendra Edara"
                    },
                    {
                        "name": "Eric Gilbert"
                    },
                    {
                        "name": "Ceren Budak"
                    }
                ],
                "author_detail": {
                    "name": "Ceren Budak"
                },
                "author": "Ceren Budak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17213v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17213v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15235v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15235v3",
                "updated": "2024-11-01T02:00:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    2,
                    0,
                    49,
                    4,
                    306,
                    0
                ],
                "published": "2024-02-23T09:57:20Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    57,
                    20,
                    4,
                    54,
                    0
                ],
                "title": "MACRec: a Multi-Agent Collaboration Framework for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACRec: a Multi-Agent Collaboration Framework for Recommendation"
                },
                "summary": "LLM-based agents have gained considerable attention for their decision-making\nskills and ability to handle complex tasks. Recognizing the current gap in\nleveraging agent capabilities for multi-agent collaboration in recommendation\nsystems, we introduce MACRec, a novel framework designed to enhance\nrecommendation systems through multi-agent collaboration. Unlike existing work\non using agents for user/item simulation, we aim to deploy multi-agents to\ntackle recommendation tasks directly. In our framework, recommendation tasks\nare addressed through the collaborative efforts of various specialized agents,\nincluding Manager, User/Item Analyst, Reflector, Searcher, and Task\nInterpreter, with different working flows. Furthermore, we provide application\nexamples of how developers can easily use MACRec on various recommendation\ntasks, including rating prediction, sequential recommendation, conversational\nrecommendation, and explanation generation of recommendation results. The\nframework and demonstration video are publicly available at\nhttps://github.com/wzf2000/MACRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have gained considerable attention for their decision-making\nskills and ability to handle complex tasks. Recognizing the current gap in\nleveraging agent capabilities for multi-agent collaboration in recommendation\nsystems, we introduce MACRec, a novel framework designed to enhance\nrecommendation systems through multi-agent collaboration. Unlike existing work\non using agents for user/item simulation, we aim to deploy multi-agents to\ntackle recommendation tasks directly. In our framework, recommendation tasks\nare addressed through the collaborative efforts of various specialized agents,\nincluding Manager, User/Item Analyst, Reflector, Searcher, and Task\nInterpreter, with different working flows. Furthermore, we provide application\nexamples of how developers can easily use MACRec on various recommendation\ntasks, including rating prediction, sequential recommendation, conversational\nrecommendation, and explanation generation of recommendation results. The\nframework and demonstration video are publicly available at\nhttps://github.com/wzf2000/MACRec."
                },
                "authors": [
                    {
                        "name": "Zhefan Wang"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Wendi Zheng"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_doi": "10.1145/3626772.3657669",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626772.3657669",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.15235v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15235v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by SIGIR2024",
                "arxiv_journal_ref": "ACM SIGIR Conference on Research and Development in Information\n  Retrieval 47 (2024) 2760-2764",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08357v2",
                "updated": "2024-11-01T01:45:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    1,
                    45,
                    2,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-12T18:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    18,
                    50,
                    13,
                    3,
                    256,
                    0
                ],
                "title": "An Experimental Study of Competitive Market Behavior Through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Experimental Study of Competitive Market Behavior Through LLMs"
                },
                "summary": "This study explores the potential of large language models (LLMs) to conduct\nmarket experiments, aiming to understand their capability to comprehend\ncompetitive market dynamics. We model the behavior of market agents in a\ncontrolled experimental setting, assessing their ability to converge toward\ncompetitive equilibria. The results reveal the challenges current LLMs face in\nreplicating the dynamic decision-making processes characteristic of human\ntrading behavior. Unlike humans, LLMs lacked the capacity to achieve market\nequilibrium. The research demonstrates that while LLMs provide a valuable tool\nfor scalable and reproducible market simulations, their current limitations\nnecessitate further advancements to fully capture the complexities of market\nbehavior. Future work that enhances dynamic learning capabilities and\nincorporates elements of behavioral economics could improve the effectiveness\nof LLMs in the economic domain, providing new insights into market dynamics and\naiding in the refinement of economic policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the potential of large language models (LLMs) to conduct\nmarket experiments, aiming to understand their capability to comprehend\ncompetitive market dynamics. We model the behavior of market agents in a\ncontrolled experimental setting, assessing their ability to converge toward\ncompetitive equilibria. The results reveal the challenges current LLMs face in\nreplicating the dynamic decision-making processes characteristic of human\ntrading behavior. Unlike humans, LLMs lacked the capacity to achieve market\nequilibrium. The research demonstrates that while LLMs provide a valuable tool\nfor scalable and reproducible market simulations, their current limitations\nnecessitate further advancements to fully capture the complexities of market\nbehavior. Future work that enhances dynamic learning capabilities and\nincorporates elements of behavioral economics could improve the effectiveness\nof LLMs in the economic domain, providing new insights into market dynamics and\naiding in the refinement of economic policies."
                },
                "authors": [
                    {
                        "name": "Jingru Jia"
                    },
                    {
                        "name": "Zehua Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zehua Yuan"
                },
                "author": "Zehua Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23683v2",
                "updated": "2024-11-01T01:21:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    1,
                    21,
                    4,
                    4,
                    306,
                    0
                ],
                "published": "2024-10-31T07:19:22Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    7,
                    19,
                    22,
                    3,
                    305,
                    0
                ],
                "title": "Unveiling User Satisfaction and Creator Productivity Trade-Offs in\n  Recommendation Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling User Satisfaction and Creator Productivity Trade-Offs in\n  Recommendation Platforms"
                },
                "summary": "On User-Generated Content (UGC) platforms, recommendation algorithms\nsignificantly impact creators' motivation to produce content as they compete\nfor algorithmically allocated user traffic. This phenomenon subtly shapes the\nvolume and diversity of the content pool, which is crucial for the platform's\nsustainability. In this work, we demonstrate, both theoretically and\nempirically, that a purely relevance-driven policy with low exploration\nstrength boosts short-term user satisfaction but undermines the long-term\nrichness of the content pool. In contrast, a more aggressive exploration policy\nmay slightly compromise user satisfaction but promote higher content creation\nvolume. Our findings reveal a fundamental trade-off between immediate user\nsatisfaction and overall content production on UGC platforms. Building on this\nfinding, we propose an efficient optimization method to identify the optimal\nexploration strength, balancing user and creator engagement. Our model can\nserve as a pre-deployment audit tool for recommendation algorithms on UGC\nplatforms, helping to align their immediate objectives with sustainable,\nlong-term goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On User-Generated Content (UGC) platforms, recommendation algorithms\nsignificantly impact creators' motivation to produce content as they compete\nfor algorithmically allocated user traffic. This phenomenon subtly shapes the\nvolume and diversity of the content pool, which is crucial for the platform's\nsustainability. In this work, we demonstrate, both theoretically and\nempirically, that a purely relevance-driven policy with low exploration\nstrength boosts short-term user satisfaction but undermines the long-term\nrichness of the content pool. In contrast, a more aggressive exploration policy\nmay slightly compromise user satisfaction but promote higher content creation\nvolume. Our findings reveal a fundamental trade-off between immediate user\nsatisfaction and overall content production on UGC platforms. Building on this\nfinding, we propose an efficient optimization method to identify the optimal\nexploration strength, balancing user and creator engagement. Our model can\nserve as a pre-deployment audit tool for recommendation algorithms on UGC\nplatforms, helping to align their immediate objectives with sustainable,\nlong-term goals."
                },
                "authors": [
                    {
                        "name": "Fan Yao"
                    },
                    {
                        "name": "Yiming Liao"
                    },
                    {
                        "name": "Jingzhou Liu"
                    },
                    {
                        "name": "Shaoliang Nie"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Haifeng Xu"
                    },
                    {
                        "name": "Hongning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongning Wang"
                },
                "author": "Hongning Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14556v2",
                "updated": "2024-11-01T01:15:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    1,
                    15,
                    51,
                    4,
                    306,
                    0
                ],
                "published": "2024-09-22T18:39:27Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    18,
                    39,
                    27,
                    6,
                    266,
                    0
                ],
                "title": "RACOON: An LLM-based Framework for Retrieval-Augmented Column Type\n  Annotation with a Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RACOON: An LLM-based Framework for Retrieval-Augmented Column Type\n  Annotation with a Knowledge Graph"
                },
                "summary": "As an important component of data exploration and integration, Column Type\nAnnotation (CTA) aims to label columns of a table with one or more semantic\ntypes. With the recent development of Large Language Models (LLMs), researchers\nhave started to explore the possibility of using LLMs for CTA, leveraging their\nstrong zero-shot capabilities. In this paper, we build on this promising work\nand improve on LLM-based methods for CTA by showing how to use a Knowledge\nGraph (KG) to augment the context information provided to the LLM. Our\napproach, called RACOON, combines both pre-trained parametric and\nnon-parametric knowledge during generation to improve LLMs' performance on CTA.\nOur experiments show that RACOON achieves up to a 0.21 micro F-1 improvement\ncompared against vanilla LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an important component of data exploration and integration, Column Type\nAnnotation (CTA) aims to label columns of a table with one or more semantic\ntypes. With the recent development of Large Language Models (LLMs), researchers\nhave started to explore the possibility of using LLMs for CTA, leveraging their\nstrong zero-shot capabilities. In this paper, we build on this promising work\nand improve on LLM-based methods for CTA by showing how to use a Knowledge\nGraph (KG) to augment the context information provided to the LLM. Our\napproach, called RACOON, combines both pre-trained parametric and\nnon-parametric knowledge during generation to improve LLMs' performance on CTA.\nOur experiments show that RACOON achieves up to a 0.21 micro F-1 improvement\ncompared against vanilla LLM inference."
                },
                "authors": [
                    {
                        "name": "Lindsey Linxi Wei"
                    },
                    {
                        "name": "Guorui Xiao"
                    },
                    {
                        "name": "Magdalena Balazinska"
                    }
                ],
                "author_detail": {
                    "name": "Magdalena Balazinska"
                },
                "author": "Magdalena Balazinska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05972v2",
                "updated": "2024-11-01T00:50:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    0,
                    50,
                    56,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-10T02:14:19Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    2,
                    14,
                    19,
                    0,
                    162,
                    0
                ],
                "title": "Decision-Making Behavior Evaluation Framework for LLMs under Uncertain\n  Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Making Behavior Evaluation Framework for LLMs under Uncertain\n  Context"
                },
                "summary": "When making decisions under uncertainty, individuals often deviate from\nrational behavior, which can be evaluated across three dimensions: risk\npreference, probability weighting, and loss aversion. Given the widespread use\nof large language models (LLMs) in decision-making processes, it is crucial to\nassess whether their behavior aligns with human norms and ethical expectations\nor exhibits potential biases. Several empirical studies have investigated the\nrationality and social behavior performance of LLMs, yet their internal\ndecision-making tendencies and capabilities remain inadequately understood.\nThis paper proposes a framework, grounded in behavioral economics, to evaluate\nthe decision-making behaviors of LLMs. Through a multiple-choice-list\nexperiment, we estimate the degree of risk preference, probability weighting,\nand loss aversion in a context-free setting for three commercial LLMs:\nChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that\nLLMs generally exhibit patterns similar to humans, such as risk aversion and\nloss aversion, with a tendency to overweight small probabilities. However,\nthere are significant variations in the degree to which these behaviors are\nexpressed across different LLMs. We also explore their behavior when embedded\nwith socio-demographic features, uncovering significant disparities. For\ninstance, when modeled with attributes of sexual minority groups or physical\ndisabilities, Claude-3-Opus displays increased risk aversion, leading to more\nconservative choices. These findings underscore the need for careful\nconsideration of the ethical implications and potential biases in deploying\nLLMs in decision-making scenarios. Therefore, this study advocates for\ndeveloping standards and guidelines to ensure that LLMs operate within ethical\nboundaries while enhancing their utility in complex decision-making\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When making decisions under uncertainty, individuals often deviate from\nrational behavior, which can be evaluated across three dimensions: risk\npreference, probability weighting, and loss aversion. Given the widespread use\nof large language models (LLMs) in decision-making processes, it is crucial to\nassess whether their behavior aligns with human norms and ethical expectations\nor exhibits potential biases. Several empirical studies have investigated the\nrationality and social behavior performance of LLMs, yet their internal\ndecision-making tendencies and capabilities remain inadequately understood.\nThis paper proposes a framework, grounded in behavioral economics, to evaluate\nthe decision-making behaviors of LLMs. Through a multiple-choice-list\nexperiment, we estimate the degree of risk preference, probability weighting,\nand loss aversion in a context-free setting for three commercial LLMs:\nChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that\nLLMs generally exhibit patterns similar to humans, such as risk aversion and\nloss aversion, with a tendency to overweight small probabilities. However,\nthere are significant variations in the degree to which these behaviors are\nexpressed across different LLMs. We also explore their behavior when embedded\nwith socio-demographic features, uncovering significant disparities. For\ninstance, when modeled with attributes of sexual minority groups or physical\ndisabilities, Claude-3-Opus displays increased risk aversion, leading to more\nconservative choices. These findings underscore the need for careful\nconsideration of the ethical implications and potential biases in deploying\nLLMs in decision-making scenarios. Therefore, this study advocates for\ndeveloping standards and guidelines to ensure that LLMs operate within ethical\nboundaries while enhancing their utility in complex decision-making\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jingru Jia"
                    },
                    {
                        "name": "Zehua Yuan"
                    },
                    {
                        "name": "Junhao Pan"
                    },
                    {
                        "name": "Paul E. McNamara"
                    },
                    {
                        "name": "Deming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Deming Chen"
                },
                "author": "Deming Chen",
                "arxiv_comment": "Jingru Jia and Zehua Yuan have equal contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16218v2",
                "updated": "2024-11-01T00:01:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    0,
                    1,
                    1,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-23T21:05:31Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    21,
                    5,
                    31,
                    6,
                    175,
                    0
                ],
                "title": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback,\n  Execution Traces, and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback,\n  Execution Traces, and LLMs"
                },
                "summary": "We study a class of optimization problems motivated by automating the design\nand update of AI systems like coding assistants, robots, and copilots. AutoDiff\nframeworks, like PyTorch, enable efficient end-to-end optimization of\ndifferentiable systems. However, general computational workflows can be\nnon-differentiable and involve rich feedback (e.g. console output or user's\nresponses), heterogeneous parameters (e.g. prompts, codes), and intricate\nobjectives (beyond maximizing a score). We investigate end-to-end generative\noptimization -- using generative models such as LLMs within the optimizer for\nautomatic updating of general computational workflows. We discover that\nworkflow execution traces are akin to back-propagated gradients in AutoDiff and\ncan provide key information to interpret feedback for efficient optimization.\nFormally, we frame a new mathematical setup, Optimization with Trace Oracle\n(OPTO). In OPTO, an optimizer receives an execution trace along with feedback\non the computed output and updates parameters iteratively. We provide a Python\nlibrary, Trace, that efficiently converts a workflow optimization problem into\nan OPTO instance using PyTorch-like syntax. Using Trace, we develop a general\nLLM-based generative optimizer called OptoPrime. In empirical studies, we find\nthat OptoPrime is capable of first-order numerical optimization, prompt\noptimization, hyper-parameter tuning, robot controller design, code debugging,\netc., and is often competitive with specialized optimizers for each domain. We\nenvision Trace as an open research platform for devising novel generative\noptimizers and developing the next generation of interactive learning agents.\nWebsite: https://microsoft.github.io/Trace/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study a class of optimization problems motivated by automating the design\nand update of AI systems like coding assistants, robots, and copilots. AutoDiff\nframeworks, like PyTorch, enable efficient end-to-end optimization of\ndifferentiable systems. However, general computational workflows can be\nnon-differentiable and involve rich feedback (e.g. console output or user's\nresponses), heterogeneous parameters (e.g. prompts, codes), and intricate\nobjectives (beyond maximizing a score). We investigate end-to-end generative\noptimization -- using generative models such as LLMs within the optimizer for\nautomatic updating of general computational workflows. We discover that\nworkflow execution traces are akin to back-propagated gradients in AutoDiff and\ncan provide key information to interpret feedback for efficient optimization.\nFormally, we frame a new mathematical setup, Optimization with Trace Oracle\n(OPTO). In OPTO, an optimizer receives an execution trace along with feedback\non the computed output and updates parameters iteratively. We provide a Python\nlibrary, Trace, that efficiently converts a workflow optimization problem into\nan OPTO instance using PyTorch-like syntax. Using Trace, we develop a general\nLLM-based generative optimizer called OptoPrime. In empirical studies, we find\nthat OptoPrime is capable of first-order numerical optimization, prompt\noptimization, hyper-parameter tuning, robot controller design, code debugging,\netc., and is often competitive with specialized optimizers for each domain. We\nenvision Trace as an open research platform for devising novel generative\noptimizers and developing the next generation of interactive learning agents.\nWebsite: https://microsoft.github.io/Trace/."
                },
                "authors": [
                    {
                        "name": "Ching-An Cheng"
                    },
                    {
                        "name": "Allen Nie"
                    },
                    {
                        "name": "Adith Swaminathan"
                    }
                ],
                "author_detail": {
                    "name": "Adith Swaminathan"
                },
                "author": "Adith Swaminathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01006v2",
                "updated": "2024-10-31T23:44:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    23,
                    44,
                    31,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-03T05:36:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    5,
                    36,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "SemCoder: Training Code Language Models with Comprehensive Semantics\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemCoder: Training Code Language Models with Comprehensive Semantics\n  Reasoning"
                },
                "summary": "Code Large Language Models (Code LLMs) have excelled at tasks like code\ncompletion but often miss deeper semantics such as execution effects and\ndynamic states. This paper aims to bridge the gap between Code LLMs' reliance\non static text data and the need for semantic understanding for complex tasks\nlike debugging and program repair. We introduce a novel strategy, monologue\nreasoning, to train Code LLMs to reason comprehensive semantics, encompassing\nhigh-level functional descriptions, local execution effects of individual\nstatements, and overall input/output behavior, thereby linking static code text\nwith dynamic execution states. We begin by collecting PyX, a clean Python\ncorpus of fully executable code samples with functional descriptions and test\ncases. We propose training Code LLMs not only to write code but also to\nunderstand code semantics by reasoning about key properties, constraints, and\nexecution behaviors using natural language, mimicking human verbal debugging,\ni.e., rubber-duck debugging. This approach led to the development of SemCoder,\na Code LLM with only 6.7B parameters, which shows competitive performance with\nGPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder\nachieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I\n(GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also\nstudy the effectiveness of SemCoder's monologue-style execution reasoning\ncompared to concrete scratchpad reasoning, showing that our approach integrates\nsemantics from multiple dimensions more smoothly. Finally, we demonstrate the\npotential of applying learned semantics to improve Code LLMs' debugging and\nself-refining capabilities. Our data, code, and models are available at:\nhttps://github.com/ARiSE-Lab/SemCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (Code LLMs) have excelled at tasks like code\ncompletion but often miss deeper semantics such as execution effects and\ndynamic states. This paper aims to bridge the gap between Code LLMs' reliance\non static text data and the need for semantic understanding for complex tasks\nlike debugging and program repair. We introduce a novel strategy, monologue\nreasoning, to train Code LLMs to reason comprehensive semantics, encompassing\nhigh-level functional descriptions, local execution effects of individual\nstatements, and overall input/output behavior, thereby linking static code text\nwith dynamic execution states. We begin by collecting PyX, a clean Python\ncorpus of fully executable code samples with functional descriptions and test\ncases. We propose training Code LLMs not only to write code but also to\nunderstand code semantics by reasoning about key properties, constraints, and\nexecution behaviors using natural language, mimicking human verbal debugging,\ni.e., rubber-duck debugging. This approach led to the development of SemCoder,\na Code LLM with only 6.7B parameters, which shows competitive performance with\nGPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder\nachieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I\n(GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also\nstudy the effectiveness of SemCoder's monologue-style execution reasoning\ncompared to concrete scratchpad reasoning, showing that our approach integrates\nsemantics from multiple dimensions more smoothly. Finally, we demonstrate the\npotential of applying learned semantics to improve Code LLMs' debugging and\nself-refining capabilities. Our data, code, and models are available at:\nhttps://github.com/ARiSE-Lab/SemCoder."
                },
                "authors": [
                    {
                        "name": "Yangruibo Ding"
                    },
                    {
                        "name": "Jinjun Peng"
                    },
                    {
                        "name": "Marcus J. Min"
                    },
                    {
                        "name": "Gail Kaiser"
                    },
                    {
                        "name": "Junfeng Yang"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "arxiv_comment": "NeurIPS 2024 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07791v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07791v6",
                "updated": "2024-10-31T23:10:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    23,
                    10,
                    55,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-12T01:12:28Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    1,
                    12,
                    28,
                    2,
                    164,
                    0
                ],
                "title": "Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs"
                },
                "summary": "LLM-as-a-Judge presents a promising alternative to human evaluators across\nvarious tasks, but inherent biases, especially position bias - a tendency to\nfavor solutions based on their position in the prompt - have compromised its\neffectiveness. Our study introduces a systematic framework to examine position\nbias in pairwise comparisons, focusing on repetition stability, position\nconsistency, and preference fairness. This research significantly contributes\nto the field by introducing new concepts for understanding position bias and\nproviding a multi-dimensional framework for evaluations. We conducted\nexperiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks\nand approximately 40 solution-generating models - candidates, resulting in over\n100,000 evaluation instances. Our findings confirm that position bias in\ncapable LLM judges is not due to random chances, along with notable variations\nobserved across judges and tasks. Moreover, position bias is weakly influenced\nby the length of prompt components but significantly impacted by the quality\ngap between solutions. These insights can help optimize judge model selections,\nimprove benchmark design, and inform future research on debiasing strategies,\nultimately enhancing the reliability of LLM judges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge presents a promising alternative to human evaluators across\nvarious tasks, but inherent biases, especially position bias - a tendency to\nfavor solutions based on their position in the prompt - have compromised its\neffectiveness. Our study introduces a systematic framework to examine position\nbias in pairwise comparisons, focusing on repetition stability, position\nconsistency, and preference fairness. This research significantly contributes\nto the field by introducing new concepts for understanding position bias and\nproviding a multi-dimensional framework for evaluations. We conducted\nexperiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks\nand approximately 40 solution-generating models - candidates, resulting in over\n100,000 evaluation instances. Our findings confirm that position bias in\ncapable LLM judges is not due to random chances, along with notable variations\nobserved across judges and tasks. Moreover, position bias is weakly influenced\nby the length of prompt components but significantly impacted by the quality\ngap between solutions. These insights can help optimize judge model selections,\nimprove benchmark design, and inform future research on debiasing strategies,\nultimately enhancing the reliability of LLM judges."
                },
                "authors": [
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Chiyu Ma"
                    },
                    {
                        "name": "Wenhua Liang"
                    },
                    {
                        "name": "Weicheng Ma"
                    },
                    {
                        "name": "Soroush Vosoughi"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Vosoughi"
                },
                "author": "Soroush Vosoughi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07791v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07791v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01318v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01318v5",
                "updated": "2024-10-31T22:26:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    22,
                    26,
                    40,
                    3,
                    305,
                    0
                ],
                "published": "2024-03-28T02:44:02Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    2,
                    44,
                    2,
                    3,
                    88,
                    0
                ],
                "title": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large\n  Language Models"
                },
                "summary": "Jailbreak attacks cause large language models (LLMs) to generate harmful,\nunethical, or otherwise objectionable content. Evaluating these attacks\npresents a number of challenges, which the current collection of benchmarks and\nevaluation techniques do not adequately address. First, there is no clear\nstandard of practice regarding jailbreaking evaluation. Second, existing works\ncompute costs and success rates in incomparable ways. And third, numerous works\nare not reproducible, as they withhold adversarial prompts, involve\nclosed-source code, or rely on evolving proprietary APIs. To address these\nchallenges, we introduce JailbreakBench, an open-sourced benchmark with the\nfollowing components: (1) an evolving repository of state-of-the-art\nadversarial prompts, which we refer to as jailbreak artifacts; (2) a\njailbreaking dataset comprising 100 behaviors -- both original and sourced from\nprior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with\nOpenAI's usage policies; (3) a standardized evaluation framework at\nhttps://github.com/JailbreakBench/jailbreakbench that includes a clearly\ndefined threat model, system prompts, chat templates, and scoring functions;\nand (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the\nperformance of attacks and defenses for various LLMs. We have carefully\nconsidered the potential ethical implications of releasing this benchmark, and\nbelieve that it will be a net positive for the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks cause large language models (LLMs) to generate harmful,\nunethical, or otherwise objectionable content. Evaluating these attacks\npresents a number of challenges, which the current collection of benchmarks and\nevaluation techniques do not adequately address. First, there is no clear\nstandard of practice regarding jailbreaking evaluation. Second, existing works\ncompute costs and success rates in incomparable ways. And third, numerous works\nare not reproducible, as they withhold adversarial prompts, involve\nclosed-source code, or rely on evolving proprietary APIs. To address these\nchallenges, we introduce JailbreakBench, an open-sourced benchmark with the\nfollowing components: (1) an evolving repository of state-of-the-art\nadversarial prompts, which we refer to as jailbreak artifacts; (2) a\njailbreaking dataset comprising 100 behaviors -- both original and sourced from\nprior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with\nOpenAI's usage policies; (3) a standardized evaluation framework at\nhttps://github.com/JailbreakBench/jailbreakbench that includes a clearly\ndefined threat model, system prompts, chat templates, and scoring functions;\nand (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the\nperformance of attacks and defenses for various LLMs. We have carefully\nconsidered the potential ethical implications of releasing this benchmark, and\nbelieve that it will be a net positive for the community."
                },
                "authors": [
                    {
                        "name": "Patrick Chao"
                    },
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Alexander Robey"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Francesco Croce"
                    },
                    {
                        "name": "Vikash Sehwag"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Florian Tramer"
                    },
                    {
                        "name": "Hamed Hassani"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "arxiv_comment": "The camera-ready version of JailbreakBench v1.0 (accepted at NeurIPS\n  2024 Datasets and Benchmarks Track): more attack artifacts, more test-time\n  defenses, a more accurate jailbreak judge (Llama-3-70B with a custom prompt),\n  a larger dataset of human preferences for selecting a jailbreak judge (300\n  examples), an over-refusal evaluation dataset, a semantic refusal judge based\n  on Llama-3-8B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01318v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01318v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22296v2",
                "updated": "2024-10-31T21:46:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    46,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-29T17:45:57Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    45,
                    57,
                    1,
                    303,
                    0
                ],
                "title": "LLMs are Highly-Constrained Biophysical Sequence Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Highly-Constrained Biophysical Sequence Optimizers"
                },
                "summary": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Samuel D. Stanton"
                    },
                    {
                        "name": "Robert G. Alberstein"
                    },
                    {
                        "name": "Andrew M. Watkins"
                    },
                    {
                        "name": "Richard Bonneau"
                    },
                    {
                        "name": "Vladimir Gligorijevi"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Nathan C. Frey"
                    }
                ],
                "author_detail": {
                    "name": "Nathan C. Frey"
                },
                "author": "Nathan C. Frey",
                "arxiv_comment": "Supercedes arXiv:2407.00236v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14755v3",
                "updated": "2024-10-31T21:33:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    33,
                    37,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-23T16:21:57Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    21,
                    57,
                    3,
                    144,
                    0
                ],
                "title": "Large language models can be zero-shot anomaly detectors for time\n  series?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be zero-shot anomaly detectors for time\n  series?"
                },
                "summary": "Recent studies have shown the ability of large language models to perform a\nvariety of tasks, including time series forecasting. The flexible nature of\nthese models allows them to be used for many applications. In this paper, we\npresent a novel study of large language models used for the challenging task of\ntime series anomaly detection. This problem entails two aspects novel for LLMs:\nthe need for the model to identify part of the input sequence (or multiple\nparts) as anomalous; and the need for it to work with time series data rather\nthan the traditional text input. We introduce sigllm, a framework for time\nseries anomaly detection using large language models. Our framework includes a\ntime-series-to-text conversion module, as well as end-to-end pipelines that\nprompt language models to perform time series anomaly detection. We investigate\ntwo paradigms for testing the abilities of large language models to perform the\ndetection task. First, we present a prompt-based detection method that directly\nasks a language model to indicate which elements of the input are anomalies.\nSecond, we leverage the forecasting capability of a large language model to\nguide the anomaly detection process. We evaluated our framework on 11 datasets\nspanning various sources and 10 pipelines. We show that the forecasting method\nsignificantly outperformed the prompting method in all 11 datasets with respect\nto the F1 score. Moreover, while large language models are capable of finding\nanomalies, state-of-the-art deep learning models are still superior in\nperformance, achieving results 30% better than large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown the ability of large language models to perform a\nvariety of tasks, including time series forecasting. The flexible nature of\nthese models allows them to be used for many applications. In this paper, we\npresent a novel study of large language models used for the challenging task of\ntime series anomaly detection. This problem entails two aspects novel for LLMs:\nthe need for the model to identify part of the input sequence (or multiple\nparts) as anomalous; and the need for it to work with time series data rather\nthan the traditional text input. We introduce sigllm, a framework for time\nseries anomaly detection using large language models. Our framework includes a\ntime-series-to-text conversion module, as well as end-to-end pipelines that\nprompt language models to perform time series anomaly detection. We investigate\ntwo paradigms for testing the abilities of large language models to perform the\ndetection task. First, we present a prompt-based detection method that directly\nasks a language model to indicate which elements of the input are anomalies.\nSecond, we leverage the forecasting capability of a large language model to\nguide the anomaly detection process. We evaluated our framework on 11 datasets\nspanning various sources and 10 pipelines. We show that the forecasting method\nsignificantly outperformed the prompting method in all 11 datasets with respect\nto the F1 score. Moreover, while large language models are capable of finding\nanomalies, state-of-the-art deep learning models are still superior in\nperformance, achieving results 30% better than large language models."
                },
                "authors": [
                    {
                        "name": "Sarah Alnegheimish"
                    },
                    {
                        "name": "Linh Nguyen"
                    },
                    {
                        "name": "Laure Berti-Equille"
                    },
                    {
                        "name": "Kalyan Veeramachaneni"
                    }
                ],
                "author_detail": {
                    "name": "Kalyan Veeramachaneni"
                },
                "author": "Kalyan Veeramachaneni",
                "arxiv_comment": "This work is accepted by IEEE International Conference on Data\n  Science and Advanced Analytics (DSAA 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18923v2",
                "updated": "2024-10-31T19:44:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    44,
                    5,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-24T17:11:52Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    11,
                    52,
                    3,
                    298,
                    0
                ],
                "title": "SegLLM: Multi-round Reasoning Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SegLLM: Multi-round Reasoning Segmentation"
                },
                "summary": "We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization."
                },
                "authors": [
                    {
                        "name": "XuDong Wang"
                    },
                    {
                        "name": "Shaolun Zhang"
                    },
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Trevor Darrell"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Darrell"
                },
                "author": "Trevor Darrell",
                "arxiv_comment": "22 pages, 10 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17946v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17946v4",
                "updated": "2024-10-31T19:38:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    38,
                    15,
                    3,
                    305,
                    0
                ],
                "published": "2024-02-28T00:09:07Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    0,
                    9,
                    7,
                    2,
                    59,
                    0
                ],
                "title": "SparseLLM: Towards Global Pruning for Pre-trained Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseLLM: Towards Global Pruning for Pre-trained Language Models"
                },
                "summary": "The transformative impact of large language models (LLMs) like LLaMA and GPT\non natural language processing is countered by their prohibitive computational\ndemands. Pruning has emerged as a pivotal compression strategy, introducing\nsparsity to enhance both memory and computational efficiency. Yet, traditional\nglobal pruning is impractical for LLMs due to scalability issues, while local\npruning, despite its efficiency, leads to suboptimal solutions. Addressing\nthese challenges, we propose SparseLLM, a novel framework that redefines the\nglobal pruning process into manageable, coordinated subproblems, allowing for\nresource-efficient optimization with global optimality. SparseLLM's approach,\nwhich conceptualizes LLMs as a chain of modular functions and leverages\nauxiliary variables for problem decomposition, not only facilitates a pragmatic\napplication on LLMs but also demonstrates significant performance improvements,\nparticularly in high-sparsity regimes where it surpasses current\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformative impact of large language models (LLMs) like LLaMA and GPT\non natural language processing is countered by their prohibitive computational\ndemands. Pruning has emerged as a pivotal compression strategy, introducing\nsparsity to enhance both memory and computational efficiency. Yet, traditional\nglobal pruning is impractical for LLMs due to scalability issues, while local\npruning, despite its efficiency, leads to suboptimal solutions. Addressing\nthese challenges, we propose SparseLLM, a novel framework that redefines the\nglobal pruning process into manageable, coordinated subproblems, allowing for\nresource-efficient optimization with global optimality. SparseLLM's approach,\nwhich conceptualizes LLMs as a chain of modular functions and leverages\nauxiliary variables for problem decomposition, not only facilitates a pragmatic\napplication on LLMs but also demonstrates significant performance improvements,\nparticularly in high-sparsity regimes where it surpasses current\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Guangji Bai"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Chen Ling"
                    },
                    {
                        "name": "Kibaek Kim"
                    },
                    {
                        "name": "Liang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhao"
                },
                "author": "Liang Zhao",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17946v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17946v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19302v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19302v3",
                "updated": "2024-10-31T19:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    37,
                    7,
                    3,
                    305,
                    0
                ],
                "published": "2024-03-28T10:40:22Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    10,
                    40,
                    22,
                    3,
                    88,
                    0
                ],
                "title": "Generating Multi-Aspect Queries for Conversational Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Multi-Aspect Queries for Conversational Search"
                },
                "summary": "Conversational information seeking (CIS) systems aim to model the user's\ninformation need within the conversational context and retrieve the relevant\ninformation. One major approach to modeling the conversational context aims to\nrewrite the user utterance in the conversation to represent the information\nneed independently. Recent work has shown the benefit of expanding the\nrewritten utterance with relevant terms. In this work, we hypothesize that\nbreaking down the information of an utterance into multi-aspect rewritten\nqueries can lead to more effective retrieval performance. This is more evident\nin more complex utterances that require gathering evidence from various\ninformation sources, where a single query rewrite or query representation\ncannot capture the complexity of the utterance. To test this hypothesis, we\nconduct extensive experiments on five widely used CIS datasets where we\nleverage LLMs to generate multi-aspect queries to represent the information\nneed for each utterance in multiple query rewrites. We show that, for most of\nthe utterances, the same retrieval model would perform better with more than\none rewritten query by 85% in terms of nDCG@3. We further propose a\nmulti-aspect query generation and retrieval framework, called MQ4CS. Our\nextensive experiments show that MQ4CS outperforms the state-of-the-art query\nrewriting methods. We make our code and our new dataset of generated\nmulti-aspect queries publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational information seeking (CIS) systems aim to model the user's\ninformation need within the conversational context and retrieve the relevant\ninformation. One major approach to modeling the conversational context aims to\nrewrite the user utterance in the conversation to represent the information\nneed independently. Recent work has shown the benefit of expanding the\nrewritten utterance with relevant terms. In this work, we hypothesize that\nbreaking down the information of an utterance into multi-aspect rewritten\nqueries can lead to more effective retrieval performance. This is more evident\nin more complex utterances that require gathering evidence from various\ninformation sources, where a single query rewrite or query representation\ncannot capture the complexity of the utterance. To test this hypothesis, we\nconduct extensive experiments on five widely used CIS datasets where we\nleverage LLMs to generate multi-aspect queries to represent the information\nneed for each utterance in multiple query rewrites. We show that, for most of\nthe utterances, the same retrieval model would perform better with more than\none rewritten query by 85% in terms of nDCG@3. We further propose a\nmulti-aspect query generation and retrieval framework, called MQ4CS. Our\nextensive experiments show that MQ4CS outperforms the state-of-the-art query\nrewriting methods. We make our code and our new dataset of generated\nmulti-aspect queries publicly available."
                },
                "authors": [
                    {
                        "name": "Zahra Abbasiantaeb"
                    },
                    {
                        "name": "Simon Lupart"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Aliannejadi"
                },
                "author": "Mohammad Aliannejadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19302v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19302v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10648v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10648v3",
                "updated": "2024-10-31T19:26:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    26,
                    43,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-14T15:59:16Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    59,
                    16,
                    0,
                    288,
                    0
                ],
                "title": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers"
                },
                "summary": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences."
                },
                "authors": [
                    {
                        "name": "Alex Stein"
                    },
                    {
                        "name": "Samuel Sharpe"
                    },
                    {
                        "name": "Doron Bergman"
                    },
                    {
                        "name": "Senthil Kumar"
                    },
                    {
                        "name": "C. Bayan Bruss"
                    },
                    {
                        "name": "John Dickerson"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum",
                "arxiv_comment": "10 pages, 6 pages of references+appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10648v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10648v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09359v2",
                "updated": "2024-10-31T19:02:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    2,
                    17,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-14T08:17:30Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    8,
                    17,
                    30,
                    5,
                    258,
                    0
                ],
                "title": "Symbolic Regression with a Learned Concept Library",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Regression with a Learned Concept Library"
                },
                "summary": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs."
                },
                "authors": [
                    {
                        "name": "Arya Grayeli"
                    },
                    {
                        "name": "Atharva Sehgal"
                    },
                    {
                        "name": "Omar Costilla-Reyes"
                    },
                    {
                        "name": "Miles Cranmer"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Swarat Chaudhuri"
                },
                "author": "Swarat Chaudhuri",
                "arxiv_comment": "NeurIPS version; 10 pages; no checklist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06423v3",
                "updated": "2024-10-31T18:43:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    43,
                    1,
                    3,
                    305,
                    0
                ],
                "published": "2024-08-12T18:01:50Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    1,
                    50,
                    0,
                    225,
                    0
                ],
                "title": "Evaluating LLMs on Entity Disambiguation in Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs on Entity Disambiguation in Tables"
                },
                "summary": "Tables are crucial containers of information, but understanding their meaning\nmay be challenging. Over the years, there has been a surge in interest in\ndata-driven approaches based on deep learning that have increasingly been\ncombined with heuristic-based ones. In the last period, the advent of\n\\acf{llms} has led to a new category of approaches for table annotation.\nHowever, these approaches have not been consistently evaluated on a common\nground, making evaluation and comparison difficult. This work proposes an\nextensive evaluation of four STI SOTA approaches: Alligator (formerly s-elbat),\nDagobah, TURL, and TableLlama; the first two belong to the family of\nheuristic-based algorithms, while the others are respectively encoder-only and\ndecoder-only Large Language Models (LLMs). We also include in the evaluation\nboth GPT-4o and GPT-4o-mini, since they excel in various public benchmarks. The\nprimary objective is to measure the ability of these approaches to solve the\nentity disambiguation task with respect to both the performance achieved on a\ncommon-ground evaluation setting and the computational and cost requirements\ninvolved, with the ultimate aim of charting new research paths in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables are crucial containers of information, but understanding their meaning\nmay be challenging. Over the years, there has been a surge in interest in\ndata-driven approaches based on deep learning that have increasingly been\ncombined with heuristic-based ones. In the last period, the advent of\n\\acf{llms} has led to a new category of approaches for table annotation.\nHowever, these approaches have not been consistently evaluated on a common\nground, making evaluation and comparison difficult. This work proposes an\nextensive evaluation of four STI SOTA approaches: Alligator (formerly s-elbat),\nDagobah, TURL, and TableLlama; the first two belong to the family of\nheuristic-based algorithms, while the others are respectively encoder-only and\ndecoder-only Large Language Models (LLMs). We also include in the evaluation\nboth GPT-4o and GPT-4o-mini, since they excel in various public benchmarks. The\nprimary objective is to measure the ability of these approaches to solve the\nentity disambiguation task with respect to both the performance achieved on a\ncommon-ground evaluation setting and the computational and cost requirements\ninvolved, with the ultimate aim of charting new research paths in the field."
                },
                "authors": [
                    {
                        "name": "Federico Belotti"
                    },
                    {
                        "name": "Fabio Dadda"
                    },
                    {
                        "name": "Marco Cremaschi"
                    },
                    {
                        "name": "Roberto Avogadro"
                    },
                    {
                        "name": "Matteo Palmonari"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Palmonari"
                },
                "author": "Matteo Palmonari",
                "arxiv_comment": "13 pages, 6 figures; fixed avg. accuracy-over-price plot for GPT\n  families, fixed typos in table referencing, added evaluation and inference\n  subsubsection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13959v2",
                "updated": "2024-10-31T18:38:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    38,
                    37,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-17T18:34:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    18,
                    34,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven\n  Question Answering Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven\n  Question Answering Pipeline"
                },
                "summary": "Financial decision-making hinges on the analysis of relevant information\nembedded in the enormous volume of documents in the financial domain. To\naddress this challenge, we developed FinQAPT, an end-to-end pipeline that\nstreamlines the identification of relevant financial reports based on a query,\nextracts pertinent context, and leverages Large Language Models (LLMs) to\nperform downstream tasks. To evaluate the pipeline, we experimented with\nvarious techniques to optimize the performance of each module using the FinQA\ndataset. We introduced a novel clustering-based negative sampling technique to\nenhance context extraction and a novel prompting method called Dynamic N-shot\nPrompting to boost the numerical question-answering capabilities of LLMs. At\nthe module level, we achieved state-of-the-art accuracy on FinQA, attaining an\naccuracy of 80.6%. However, at the pipeline level, we observed decreased\nperformance due to challenges in extracting relevant context from financial\nreports. We conducted a detailed error analysis of each module and the\nend-to-end pipeline, pinpointing specific challenges that must be addressed to\ndevelop a robust solution for handling complex financial tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial decision-making hinges on the analysis of relevant information\nembedded in the enormous volume of documents in the financial domain. To\naddress this challenge, we developed FinQAPT, an end-to-end pipeline that\nstreamlines the identification of relevant financial reports based on a query,\nextracts pertinent context, and leverages Large Language Models (LLMs) to\nperform downstream tasks. To evaluate the pipeline, we experimented with\nvarious techniques to optimize the performance of each module using the FinQA\ndataset. We introduced a novel clustering-based negative sampling technique to\nenhance context extraction and a novel prompting method called Dynamic N-shot\nPrompting to boost the numerical question-answering capabilities of LLMs. At\nthe module level, we achieved state-of-the-art accuracy on FinQA, attaining an\naccuracy of 80.6%. However, at the pipeline level, we observed decreased\nperformance due to challenges in extracting relevant context from financial\nreports. We conducted a detailed error analysis of each module and the\nend-to-end pipeline, pinpointing specific challenges that must be addressed to\ndevelop a robust solution for handling complex financial tasks."
                },
                "authors": [
                    {
                        "name": "Kuldeep Singh"
                    },
                    {
                        "name": "Simerjot Kaur"
                    },
                    {
                        "name": "Charese Smiley"
                    }
                ],
                "author_detail": {
                    "name": "Charese Smiley"
                },
                "author": "Charese Smiley",
                "arxiv_doi": "10.1145/3677052.3698682",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3677052.3698682",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.13959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in ICAIF 2024, 8 pages, 5 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.2.6; I.5.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20290v2",
                "updated": "2024-10-31T18:27:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    27,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-26T23:20:48Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    23,
                    20,
                    48,
                    5,
                    300,
                    0
                ],
                "title": "Fast Best-of-N Decoding via Speculative Rejection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Best-of-N Decoding via Speculative Rejection"
                },
                "summary": "The safe and effective deployment of Large Language Models (LLMs) involves a\ncritical step called alignment, which ensures that the model's responses are in\naccordance with human preferences. Prevalent alignment techniques, such as DPO,\nPPO and their variants, align LLMs by changing the pre-trained model weights\nduring a phase called post-training. While predominant, these post-training\nmethods add substantial complexity before LLMs can be deployed. Inference-time\nalignment methods avoid the complex post-training step and instead bias the\ngeneration towards responses that are aligned with human preferences. The\nbest-known inference-time alignment method, called Best-of-N, is as effective\nas the state-of-the-art post-training procedures. Unfortunately, Best-of-N\nrequires vastly more resources at inference time than standard decoding\nstrategies, which makes it computationally not viable. In this work, we\nintroduce Speculative Rejection, a computationally-viable inference-time\nalignment algorithm. It generates high-scoring responses according to a given\nreward model, like Best-of-N does, while being between 16 to 32 times more\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safe and effective deployment of Large Language Models (LLMs) involves a\ncritical step called alignment, which ensures that the model's responses are in\naccordance with human preferences. Prevalent alignment techniques, such as DPO,\nPPO and their variants, align LLMs by changing the pre-trained model weights\nduring a phase called post-training. While predominant, these post-training\nmethods add substantial complexity before LLMs can be deployed. Inference-time\nalignment methods avoid the complex post-training step and instead bias the\ngeneration towards responses that are aligned with human preferences. The\nbest-known inference-time alignment method, called Best-of-N, is as effective\nas the state-of-the-art post-training procedures. Unfortunately, Best-of-N\nrequires vastly more resources at inference time than standard decoding\nstrategies, which makes it computationally not viable. In this work, we\nintroduce Speculative Rejection, a computationally-viable inference-time\nalignment algorithm. It generates high-scoring responses according to a given\nreward model, like Best-of-N does, while being between 16 to 32 times more\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Momin Haider"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Huitao Yang"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Andrea Zanette"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanette"
                },
                "author": "Andrea Zanette",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05817v2",
                "updated": "2024-10-31T18:20:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    20,
                    32,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-08T08:47:11Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    47,
                    11,
                    1,
                    282,
                    0
                ],
                "title": "Probing Language Models on Their Knowledge Source",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Language Models on Their Knowledge Source"
                },
                "summary": "Large Language Models (LLMs) often encounter conflicts between their learned,\ninternal (parametric knowledge, PK) and external knowledge provided during\ninference (contextual knowledge, CK). Understanding how LLMs models prioritize\none knowledge source over the other remains a challenge. In this paper, we\npropose a novel probing framework to explore the mechanisms governing the\nselection between PK and CK in LLMs. Using controlled prompts designed to\ncontradict the model's PK, we demonstrate that specific model activations are\nindicative of the knowledge source employed. We evaluate this framework on\nvarious LLMs of different sizes and demonstrate that mid-layer activations,\nparticularly those related to relations in the input, are crucial in predicting\nknowledge source selection, paving the way for more reliable models capable of\nhandling knowledge conflicts effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often encounter conflicts between their learned,\ninternal (parametric knowledge, PK) and external knowledge provided during\ninference (contextual knowledge, CK). Understanding how LLMs models prioritize\none knowledge source over the other remains a challenge. In this paper, we\npropose a novel probing framework to explore the mechanisms governing the\nselection between PK and CK in LLMs. Using controlled prompts designed to\ncontradict the model's PK, we demonstrate that specific model activations are\nindicative of the knowledge source employed. We evaluate this framework on\nvarious LLMs of different sizes and demonstrate that mid-layer activations,\nparticularly those related to relations in the input, are crucial in predicting\nknowledge source selection, paving the way for more reliable models capable of\nhandling knowledge conflicts effectively."
                },
                "authors": [
                    {
                        "name": "Zineddine Tighidet"
                    },
                    {
                        "name": "Andrea Mogini"
                    },
                    {
                        "name": "Jiali Mei"
                    },
                    {
                        "name": "Benjamin Piwowarski"
                    },
                    {
                        "name": "Patrick Gallinari"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Gallinari"
                },
                "author": "Patrick Gallinari",
                "arxiv_comment": "Accepted at BlackBoxNLP@EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24190v1",
                "updated": "2024-10-31T17:51:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    51,
                    0,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:51:00Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    51,
                    0,
                    3,
                    305,
                    0
                ],
                "title": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters"
                },
                "summary": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions."
                },
                "authors": [
                    {
                        "name": "Yujin Potter"
                    },
                    {
                        "name": "Shiyang Lai"
                    },
                    {
                        "name": "Junsol Kim"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06711v2",
                "updated": "2024-10-31T17:48:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    48,
                    6,
                    3,
                    305,
                    0
                ],
                "published": "2024-08-25T13:14:59Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    13,
                    14,
                    59,
                    6,
                    238,
                    0
                ],
                "title": "Quantized neural network for complex hologram generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized neural network for complex hologram generation"
                },
                "summary": "Computer-generated holography (CGH) is a promising technology for augmented\nreality displays, such as head-mounted or head-up displays. However, its high\ncomputational demand makes it impractical for implementation. Recent efforts to\nintegrate neural networks into CGH have successfully accelerated computing\nspeed, demonstrating the potential to overcome the trade-off between\ncomputational cost and image quality. Nevertheless, deploying neural\nnetwork-based CGH algorithms on computationally limited embedded systems\nrequires more efficient models with lower computational cost, memory footprint,\nand power consumption. In this study, we developed a lightweight model for\ncomplex hologram generation by introducing neural network quantization.\nSpecifically, we built a model based on tensor holography and quantized it from\n32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Our\nperformance evaluation shows that the proposed INT8 model achieves hologram\nquality comparable to that of the FP32 model while reducing the model size by\napproximately 70% and increasing the speed fourfold. Additionally, we\nimplemented the INT8 model on a system-on-module to demonstrate its\ndeployability on embedded platforms and high power efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-generated holography (CGH) is a promising technology for augmented\nreality displays, such as head-mounted or head-up displays. However, its high\ncomputational demand makes it impractical for implementation. Recent efforts to\nintegrate neural networks into CGH have successfully accelerated computing\nspeed, demonstrating the potential to overcome the trade-off between\ncomputational cost and image quality. Nevertheless, deploying neural\nnetwork-based CGH algorithms on computationally limited embedded systems\nrequires more efficient models with lower computational cost, memory footprint,\nand power consumption. In this study, we developed a lightweight model for\ncomplex hologram generation by introducing neural network quantization.\nSpecifically, we built a model based on tensor holography and quantized it from\n32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Our\nperformance evaluation shows that the proposed INT8 model achieves hologram\nquality comparable to that of the FP32 model while reducing the model size by\napproximately 70% and increasing the speed fourfold. Additionally, we\nimplemented the INT8 model on a system-on-module to demonstrate its\ndeployability on embedded platforms and high power efficiency."
                },
                "authors": [
                    {
                        "name": "Yutaka Endo"
                    },
                    {
                        "name": "Minoru Oikawa"
                    },
                    {
                        "name": "Timothy D. Wilkinson"
                    },
                    {
                        "name": "Tomoyoshi Shimobaba"
                    },
                    {
                        "name": "Tomoyoshi Ito"
                    }
                ],
                "author_detail": {
                    "name": "Tomoyoshi Ito"
                },
                "author": "Tomoyoshi Ito",
                "arxiv_doi": "10.1364/AO.538096",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1364/AO.538096",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 4 figures",
                "arxiv_journal_ref": "Appl. Opt. 64, A12-A18 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24175v1",
                "updated": "2024-10-31T17:42:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    42,
                    26,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:42:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    42,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13046v2",
                "updated": "2024-10-31T17:39:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    39,
                    34,
                    3,
                    305,
                    0
                ],
                "published": "2024-04-19T17:59:48Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    17,
                    59,
                    48,
                    4,
                    110,
                    0
                ],
                "title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context"
                },
                "summary": "As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM). In the fine-grained stage, we\nelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to\nextract and fuse task-specific knowledge from various experts. This\ncoarse-to-fine paradigm effectively leverages representations from experts\nbased on multimodal context and model expertise, further enhancing the\ngeneralization ability. We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach. Without any bells and whistles, MoVA\ncan achieve significant performance gains over current state-of-the-art methods\nin a wide range of challenging multimodal benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM). In the fine-grained stage, we\nelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to\nextract and fuse task-specific knowledge from various experts. This\ncoarse-to-fine paradigm effectively leverages representations from experts\nbased on multimodal context and model expertise, further enhancing the\ngeneralization ability. We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach. Without any bells and whistles, MoVA\ncan achieve significant performance gains over current state-of-the-art methods\nin a wide range of challenging multimodal benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhuofan Zong"
                    },
                    {
                        "name": "Bingqi Ma"
                    },
                    {
                        "name": "Dazhong Shen"
                    },
                    {
                        "name": "Guanglu Song"
                    },
                    {
                        "name": "Hao Shao"
                    },
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03555v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03555v4",
                "updated": "2024-10-31T17:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    36,
                    55,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-06T15:25:48Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    15,
                    25,
                    48,
                    0,
                    127,
                    0
                ],
                "title": "A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware\n  Architecture, Deployment Options, Use Cases, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware\n  Architecture, Deployment Options, Use Cases, and Challenges"
                },
                "summary": "Open-radio access network (O-RAN) seeks to establish principles of openness,\nprogrammability, automation, intelligence, and hardware-software disaggregation\nwith interoperable interfaces. It advocates for multi-vendorism and\nmulti-stakeholderism within a cloudified and virtualized wireless\ninfrastructure, aimed at enhancing the deployment, operation, and maintenance\nof RAN architecture. This enhancement promises increased flexibility,\nperformance optimization, service innovation, energy efficiency, and cost\nefficiency in fifth-generation (5G), sixth-generation (6G), and future\nnetworks. One of the key features of the O-RAN architecture is its support for\nnetwork slicing, which entails interaction with other slicing domains within a\nmobile network, notably the transport network (TN) domain and the core network\n(CN) domain, to realize end-to-end (E2E) network slicing. The study of this\nfeature requires exploring the stances and contributions of diverse standards\ndevelopment organizations (SDOs). In this context, we note that despite the\nongoing industrial deployments and standardization efforts, the research and\nstandardization communities have yet to comprehensively address network slicing\nin O-RAN. To address this gap, this survey paper provides a comprehensive\nexploration of network slicing in O-RAN through an in-depth review of\nspecification documents from O-RAN Alliance and research papers from leading\nindustry and academic institutions. The paper commences with an overview of the\nongoing standardization efforts and open-source contributions associated with\nO-RAN, subsequently delving into the latest O-RAN architecture with an emphasis\non its slicing aspects. Further, the paper explores deployment scenarios for\nnetwork slicing within O-RAN, examining options for the deployment and\norchestration of O-RAN and TN network slice subnets...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-radio access network (O-RAN) seeks to establish principles of openness,\nprogrammability, automation, intelligence, and hardware-software disaggregation\nwith interoperable interfaces. It advocates for multi-vendorism and\nmulti-stakeholderism within a cloudified and virtualized wireless\ninfrastructure, aimed at enhancing the deployment, operation, and maintenance\nof RAN architecture. This enhancement promises increased flexibility,\nperformance optimization, service innovation, energy efficiency, and cost\nefficiency in fifth-generation (5G), sixth-generation (6G), and future\nnetworks. One of the key features of the O-RAN architecture is its support for\nnetwork slicing, which entails interaction with other slicing domains within a\nmobile network, notably the transport network (TN) domain and the core network\n(CN) domain, to realize end-to-end (E2E) network slicing. The study of this\nfeature requires exploring the stances and contributions of diverse standards\ndevelopment organizations (SDOs). In this context, we note that despite the\nongoing industrial deployments and standardization efforts, the research and\nstandardization communities have yet to comprehensively address network slicing\nin O-RAN. To address this gap, this survey paper provides a comprehensive\nexploration of network slicing in O-RAN through an in-depth review of\nspecification documents from O-RAN Alliance and research papers from leading\nindustry and academic institutions. The paper commences with an overview of the\nongoing standardization efforts and open-source contributions associated with\nO-RAN, subsequently delving into the latest O-RAN architecture with an emphasis\non its slicing aspects. Further, the paper explores deployment scenarios for\nnetwork slicing within O-RAN, examining options for the deployment and\norchestration of O-RAN and TN network slice subnets..."
                },
                "authors": [
                    {
                        "name": "Khurshid Alam"
                    },
                    {
                        "name": "Mohammad Asif Habibi"
                    },
                    {
                        "name": "Matthias Tammen"
                    },
                    {
                        "name": "Dennis Krummacker"
                    },
                    {
                        "name": "Walid Saad"
                    },
                    {
                        "name": "Marco Di Renzo"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Xavier Costa-Pérez"
                    },
                    {
                        "name": "Mérouane Debbah"
                    },
                    {
                        "name": "Ashutosh Dutta"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "46 pages, 12 figures, 4 tables, submitted to the IEEE for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03555v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03555v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24155v1",
                "updated": "2024-10-31T17:12:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    12,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:12:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    12,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Thought Space Explorer: Navigating and Expanding Thought Space for Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought Space Explorer: Navigating and Expanding Thought Space for Large\n  Language Model Reasoning"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Xiting Wang"
                    },
                    {
                        "name": "Kunpeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kunpeng Liu"
                },
                "author": "Kunpeng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24152v1",
                "updated": "2024-10-31T17:10:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    10,
                    1,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:10:01Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    10,
                    1,
                    3,
                    305,
                    0
                ],
                "title": "Language-Driven Policy Distillation for Cooperative Driving in\n  Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Driven Policy Distillation for Cooperative Driving in\n  Multi-Agent Reinforcement Learning"
                },
                "summary": "The cooperative driving technology of Connected and Autonomous Vehicles\n(CAVs) is crucial for improving the efficiency and safety of transportation\nsystems. Learning-based methods, such as Multi-Agent Reinforcement Learning\n(MARL), have demonstrated strong capabilities in cooperative decision-making\ntasks. However, existing MARL approaches still face challenges in terms of\nlearning efficiency and performance. In recent years, Large Language Models\n(LLMs) have rapidly advanced and shown remarkable abilities in various\nsequential decision-making tasks. To enhance the learning capabilities of\ncooperative agents while ensuring decision-making efficiency and\ncost-effectiveness, we propose LDPD, a language-driven policy distillation\nmethod for guiding MARL exploration. In this framework, a teacher agent based\non LLM trains smaller student agents to achieve cooperative decision-making\nthrough its own decision-making demonstrations. The teacher agent enhances the\nobservation information of CAVs and utilizes LLMs to perform complex\ncooperative decision-making reasoning, which also leverages carefully designed\ndecision-making tools to achieve expert-level decisions, providing high-quality\nteaching experiences. The student agent then refines the teacher's prior\nknowledge into its own model through gradient policy updates. The experiments\ndemonstrate that the students can rapidly improve their capabilities with\nminimal guidance from the teacher and eventually surpass the teacher's\nperformance. Extensive experiments show that our approach demonstrates better\nperformance and learning efficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cooperative driving technology of Connected and Autonomous Vehicles\n(CAVs) is crucial for improving the efficiency and safety of transportation\nsystems. Learning-based methods, such as Multi-Agent Reinforcement Learning\n(MARL), have demonstrated strong capabilities in cooperative decision-making\ntasks. However, existing MARL approaches still face challenges in terms of\nlearning efficiency and performance. In recent years, Large Language Models\n(LLMs) have rapidly advanced and shown remarkable abilities in various\nsequential decision-making tasks. To enhance the learning capabilities of\ncooperative agents while ensuring decision-making efficiency and\ncost-effectiveness, we propose LDPD, a language-driven policy distillation\nmethod for guiding MARL exploration. In this framework, a teacher agent based\non LLM trains smaller student agents to achieve cooperative decision-making\nthrough its own decision-making demonstrations. The teacher agent enhances the\nobservation information of CAVs and utilizes LLMs to perform complex\ncooperative decision-making reasoning, which also leverages carefully designed\ndecision-making tools to achieve expert-level decisions, providing high-quality\nteaching experiences. The student agent then refines the teacher's prior\nknowledge into its own model through gradient policy updates. The experiments\ndemonstrate that the students can rapidly improve their capabilities with\nminimal guidance from the teacher and eventually surpass the teacher's\nperformance. Extensive experiments show that our approach demonstrates better\nperformance and learning efficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Chengkai Xu"
                    },
                    {
                        "name": "Peng Hang"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Mingyu Ding"
                    },
                    {
                        "name": "Wei Zhan"
                    },
                    {
                        "name": "Masayoshi Tomizuka"
                    }
                ],
                "author_detail": {
                    "name": "Masayoshi Tomizuka"
                },
                "author": "Masayoshi Tomizuka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17947v2",
                "updated": "2024-10-31T17:08:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    8,
                    0,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-25T21:47:53Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    21,
                    47,
                    53,
                    1,
                    177,
                    0
                ],
                "title": "Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias"
                },
                "summary": "The variations between in-group and out-group speech (intergroup bias) are\nsubtle and could underlie many social phenomena like stereotype perpetuation\nand implicit bias. In this paper, we model the intergroup bias as a tagging\ntask on English sports comments from forums dedicated to fandom for NFL teams.\nWe curate a unique dataset of over 6 million game-time comments from opposing\nperspectives (the teams in the game), each comment grounded in a non-linguistic\ndescription of the events that precipitated these comments (live win\nprobabilities for each team). Expert and crowd annotations justify modeling the\nbias through tagging of implicit and explicit referring expressions and reveal\nthe rich, contextual understanding of language and the world required for this\ntask. For large-scale analysis of intergroup variation, we use LLMs for\nautomated tagging, and discover that some LLMs perform best when prompted with\nlinguistic descriptions of the win probability at the time of the comment,\nrather than numerical probability. Further, large-scale tagging of comments\nusing LLMs uncovers linear variations in the form of referent across win\nprobabilities that distinguish in-group and out-group utterances. Code and data\nare available at https://github.com/venkatasg/intergroup-nfl .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The variations between in-group and out-group speech (intergroup bias) are\nsubtle and could underlie many social phenomena like stereotype perpetuation\nand implicit bias. In this paper, we model the intergroup bias as a tagging\ntask on English sports comments from forums dedicated to fandom for NFL teams.\nWe curate a unique dataset of over 6 million game-time comments from opposing\nperspectives (the teams in the game), each comment grounded in a non-linguistic\ndescription of the events that precipitated these comments (live win\nprobabilities for each team). Expert and crowd annotations justify modeling the\nbias through tagging of implicit and explicit referring expressions and reveal\nthe rich, contextual understanding of language and the world required for this\ntask. For large-scale analysis of intergroup variation, we use LLMs for\nautomated tagging, and discover that some LLMs perform best when prompted with\nlinguistic descriptions of the win probability at the time of the comment,\nrather than numerical probability. Further, large-scale tagging of comments\nusing LLMs uncovers linear variations in the form of referent across win\nprobabilities that distinguish in-group and out-group utterances. Code and data\nare available at https://github.com/venkatasg/intergroup-nfl ."
                },
                "authors": [
                    {
                        "name": "Venkata S Govindarajan"
                    },
                    {
                        "name": "Matianyu Zang"
                    },
                    {
                        "name": "Kyle Mahowald"
                    },
                    {
                        "name": "David Beaver"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Jessy Li"
                },
                "author": "Junyi Jessy Li",
                "arxiv_comment": "Accepted to Findings@EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12794v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12794v3",
                "updated": "2024-10-31T16:58:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    58,
                    51,
                    3,
                    305,
                    0
                ],
                "published": "2024-01-23T14:29:17Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    14,
                    29,
                    17,
                    1,
                    23,
                    0
                ],
                "title": "Benchmarking LLMs via Uncertainty Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs via Uncertainty Quantification"
                },
                "summary": "The proliferation of open-source Large Language Models (LLMs) from various\ninstitutions has highlighted the urgent need for comprehensive evaluation\nmethods. However, current evaluation platforms, such as the widely recognized\nHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,\nwhich is vital for thoroughly assessing LLMs. To bridge this gap, we introduce\na new benchmarking approach for LLMs that integrates uncertainty\nquantification. Our examination involves nine LLMs (LLM series) spanning five\nrepresentative natural language processing tasks. Our findings reveal that: I)\nLLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs\nmay display greater uncertainty compared to their smaller counterparts; and\nIII) Instruction-finetuning tends to increase the uncertainty of LLMs. These\nresults underscore the significance of incorporating uncertainty in the\nevaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of open-source Large Language Models (LLMs) from various\ninstitutions has highlighted the urgent need for comprehensive evaluation\nmethods. However, current evaluation platforms, such as the widely recognized\nHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,\nwhich is vital for thoroughly assessing LLMs. To bridge this gap, we introduce\na new benchmarking approach for LLMs that integrates uncertainty\nquantification. Our examination involves nine LLMs (LLM series) spanning five\nrepresentative natural language processing tasks. Our findings reveal that: I)\nLLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs\nmay display greater uncertainty compared to their smaller counterparts; and\nIII) Instruction-finetuning tends to increase the uncertainty of LLMs. These\nresults underscore the significance of incorporating uncertainty in the\nevaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Mingming Yang"
                    },
                    {
                        "name": "Jianhui Pang"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Shuming Shi"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "30 pages, accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12794v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12794v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03636v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03636v4",
                "updated": "2024-10-31T16:54:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    54,
                    30,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-05T22:16:19Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    22,
                    16,
                    19,
                    2,
                    157,
                    0
                ],
                "title": "Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource\n  Programming and Formal Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource\n  Programming and Formal Languages"
                },
                "summary": "Recent advances in large language models (LLMs) for code applications have\ndemonstrated remarkable zero-shot fluency and instruction following on\nchallenging code related tasks ranging from test case generation to\nself-repair. Unsurprisingly, however, models struggle to compose syntactically\nvalid programs in programming languages unrepresented in pre-training, referred\nto as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial\nsettings, including domain-specific languages for internal tools, tool-chains\nfor legacy languages, and formal verification frameworks. Inspired by a\ntechnique called natural programming elicitation, we propose designing an\nintermediate language that LLMs \"naturally\" know how to use and which can be\nautomatically compiled to a target VLPL. When LLMs generate code that lies\noutside of this intermediate language, we use compiler techniques to repair the\ncode into programs in the intermediate language. Overall, we introduce\n\\emph{synthetic programming elicitation and compilation} (SPEAC), an approach\nthat enables LLMs to generate syntactically valid code even for VLPLs. We\nempirically evaluate the performance of SPEAC in a case study for the UCLID5\nformal verification language and find that, compared to existing retrieval and\nfine-tuning baselines, SPEAC produces syntactically correct programs more\nfrequently and without sacrificing semantic correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) for code applications have\ndemonstrated remarkable zero-shot fluency and instruction following on\nchallenging code related tasks ranging from test case generation to\nself-repair. Unsurprisingly, however, models struggle to compose syntactically\nvalid programs in programming languages unrepresented in pre-training, referred\nto as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial\nsettings, including domain-specific languages for internal tools, tool-chains\nfor legacy languages, and formal verification frameworks. Inspired by a\ntechnique called natural programming elicitation, we propose designing an\nintermediate language that LLMs \"naturally\" know how to use and which can be\nautomatically compiled to a target VLPL. When LLMs generate code that lies\noutside of this intermediate language, we use compiler techniques to repair the\ncode into programs in the intermediate language. Overall, we introduce\n\\emph{synthetic programming elicitation and compilation} (SPEAC), an approach\nthat enables LLMs to generate syntactically valid code even for VLPLs. We\nempirically evaluate the performance of SPEAC in a case study for the UCLID5\nformal verification language and find that, compared to existing retrieval and\nfine-tuning baselines, SPEAC produces syntactically correct programs more\nfrequently and without sacrificing semantic correctness."
                },
                "authors": [
                    {
                        "name": "Federico Mora"
                    },
                    {
                        "name": "Justin Wong"
                    },
                    {
                        "name": "Haley Lepe"
                    },
                    {
                        "name": "Sahil Bhatia"
                    },
                    {
                        "name": "Karim Elmaaroufi"
                    },
                    {
                        "name": "George Varghese"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Elizabeth Polgreen"
                    },
                    {
                        "name": "Sanjit A. Seshia"
                    }
                ],
                "author_detail": {
                    "name": "Sanjit A. Seshia"
                },
                "author": "Sanjit A. Seshia",
                "arxiv_comment": "14 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03636v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03636v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24117v1",
                "updated": "2024-10-31T16:46:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    46,
                    52,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T16:46:52Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    46,
                    52,
                    3,
                    305,
                    0
                ],
                "title": "Repository-Level Compositional Code Translation and Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-Level Compositional Code Translation and Validation"
                },
                "summary": "Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans translated the entire repository of\nthese projects consisting of 6899 source code fragments. 99.1% of the\ntranslated code fragments are syntactically correct, and AlphaTrans validates\nthe translations' runtime behavior and functional correctness for 25.8%. On\naverage, the integrated translation and validation take 36 hours to translate a\nproject, showing its scalability in practice. For the syntactically or\nsemantically incorrect translations, AlphaTrans generates a report including\nexisting translation, stack trace, test errors, or assertion failures. We\nprovided these artifacts to two developers to fix the translation bugs in four\nprojects. They were able to fix the issues in 20.1 hours on average and achieve\nall passing tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans translated the entire repository of\nthese projects consisting of 6899 source code fragments. 99.1% of the\ntranslated code fragments are syntactically correct, and AlphaTrans validates\nthe translations' runtime behavior and functional correctness for 25.8%. On\naverage, the integrated translation and validation take 36 hours to translate a\nproject, showing its scalability in practice. For the syntactically or\nsemantically incorrect translations, AlphaTrans generates a report including\nexisting translation, stack trace, test errors, or assertion failures. We\nprovided these artifacts to two developers to fix the translation bugs in four\nprojects. They were able to fix the issues in 20.1 hours on average and achieve\nall passing tests."
                },
                "authors": [
                    {
                        "name": "Ali Reza Ibrahimzada"
                    },
                    {
                        "name": "Kaiyao Ke"
                    },
                    {
                        "name": "Mrigank Pawagi"
                    },
                    {
                        "name": "Muhammad Salman Abid"
                    },
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15892v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15892v3",
                "updated": "2024-10-31T16:36:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    36,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-07-22T01:52:30Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    52,
                    30,
                    0,
                    204,
                    0
                ],
                "title": "Mini-Sequence Transformer: Optimizing Intermediate Memory for Long\n  Sequences Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mini-Sequence Transformer: Optimizing Intermediate Memory for Long\n  Sequences Training"
                },
                "summary": "We introduce Mini-Sequence Transformer (MsT), a simple and effective\nmethodology for highly efficient and accurate LLM training with extremely long\nsequences. MsT partitions input sequences and iteratively processes\nmini-sequences to reduce intermediate memory usage. Integrated with activation\nrecomputation, it enables significant memory savings in both forward and\nbackward passes. In experiments with the Llama3-8B model, with MsT, we measure\nno degradation in throughput or convergence even with 12x longer sequences than\nstandard implementations. MsT is fully general, implementation-agnostic, and\nrequires minimal code changes to integrate with existing LLM training\nframeworks. Integrated with the huggingface library, MsT successfully extends\nthe maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Mini-Sequence Transformer (MsT), a simple and effective\nmethodology for highly efficient and accurate LLM training with extremely long\nsequences. MsT partitions input sequences and iteratively processes\nmini-sequences to reduce intermediate memory usage. Integrated with activation\nrecomputation, it enables significant memory savings in both forward and\nbackward passes. In experiments with the Llama3-8B model, with MsT, we measure\nno degradation in throughput or convergence even with 12x longer sequences than\nstandard implementations. MsT is fully general, implementation-agnostic, and\nrequires minimal code changes to integrate with existing LLM training\nframeworks. Integrated with the huggingface library, MsT successfully extends\nthe maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15892v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15892v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24096v1",
                "updated": "2024-10-31T16:28:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    28,
                    33,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T16:28:33Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    28,
                    33,
                    3,
                    305,
                    0
                ],
                "title": "Progressive Safeguards for Safe and Model-Agnostic Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Safeguards for Safe and Model-Agnostic Reinforcement\n  Learning"
                },
                "summary": "In this paper we propose a formal, model-agnostic meta-learning framework for\nsafe reinforcement learning. Our framework is inspired by how parents safeguard\ntheir children across a progression of increasingly riskier tasks, imparting a\nsense of safety that is carried over from task to task. We model this as a\nmeta-learning process where each task is synchronized with a safeguard that\nmonitors safety and provides a reward signal to the agent. The safeguard is\nimplemented as a finite-state machine based on a safety specification; the\nreward signal is formally shaped around this specification. The safety\nspecification and its corresponding safeguard can be arbitrarily complex and\nnon-Markovian, which adds flexibility to the training process and\nexplainability to the learned policy. The design of the safeguard is manual but\nit is high-level and model-agnostic, which gives rise to an end-to-end safe\nlearning approach with wide applicability, from pixel-level game control to\nlanguage model fine-tuning. Starting from a given set of safety specifications\n(tasks), we train a model such that it can adapt to new specifications using\nonly a small number of training samples. This is made possible by our method\nfor efficiently transferring safety bias between tasks, which effectively\nminimizes the number of safety violations. We evaluate our framework in a\nMinecraft-inspired Gridworld, a VizDoom game environment, and an LLM\nfine-tuning application. Agents trained with our approach achieve near-minimal\nsafety violations, while baselines are shown to underperform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we propose a formal, model-agnostic meta-learning framework for\nsafe reinforcement learning. Our framework is inspired by how parents safeguard\ntheir children across a progression of increasingly riskier tasks, imparting a\nsense of safety that is carried over from task to task. We model this as a\nmeta-learning process where each task is synchronized with a safeguard that\nmonitors safety and provides a reward signal to the agent. The safeguard is\nimplemented as a finite-state machine based on a safety specification; the\nreward signal is formally shaped around this specification. The safety\nspecification and its corresponding safeguard can be arbitrarily complex and\nnon-Markovian, which adds flexibility to the training process and\nexplainability to the learned policy. The design of the safeguard is manual but\nit is high-level and model-agnostic, which gives rise to an end-to-end safe\nlearning approach with wide applicability, from pixel-level game control to\nlanguage model fine-tuning. Starting from a given set of safety specifications\n(tasks), we train a model such that it can adapt to new specifications using\nonly a small number of training samples. This is made possible by our method\nfor efficiently transferring safety bias between tasks, which effectively\nminimizes the number of safety violations. We evaluate our framework in a\nMinecraft-inspired Gridworld, a VizDoom game environment, and an LLM\nfine-tuning application. Agents trained with our approach achieve near-minimal\nsafety violations, while baselines are shown to underperform."
                },
                "authors": [
                    {
                        "name": "Nabil Omi"
                    },
                    {
                        "name": "Hosein Hasanbeig"
                    },
                    {
                        "name": "Hiteshi Sharma"
                    },
                    {
                        "name": "Sriram K. Rajamani"
                    },
                    {
                        "name": "Siddhartha Sen"
                    }
                ],
                "author_detail": {
                    "name": "Siddhartha Sen"
                },
                "author": "Siddhartha Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17446v2",
                "updated": "2024-10-31T16:16:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    16,
                    0,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-26T00:38:18Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    38,
                    18,
                    3,
                    270,
                    0
                ],
                "title": "Efficient Federated Learning against Heterogeneous and Non-stationary\n  Client Unavailability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Federated Learning against Heterogeneous and Non-stationary\n  Client Unavailability"
                },
                "summary": "Addressing intermittent client availability is critical for the real-world\ndeployment of federated learning algorithms. Most prior work either overlooks\nthe potential non-stationarity in the dynamics of client unavailability or\nrequires substantial memory/computation overhead. We study federated learning\nin the presence of heterogeneous and non-stationary client availability, which\nmay occur when the deployment environments are uncertain, or the clients are\nmobile. The impacts of heterogeneity and non-stationarity on client\nunavailability can be significant, as we illustrate using FedAvg, the most\nwidely adopted federated learning algorithm. We propose FedAPM, which includes\nnovel algorithmic structures that (i) compensate for missed computations due to\nunavailability with only $O(1)$ additional memory and computation with respect\nto standard FedAvg, and (ii) evenly diffuse local updates within the federated\nlearning system through implicit gossiping, despite being agnostic to\nnon-stationary dynamics. We show that FedAPM converges to a stationary point of\neven non-convex objectives while achieving the desired linear speedup property.\nWe corroborate our analysis with numerical experiments over diversified client\nunavailability dynamics on real-world data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing intermittent client availability is critical for the real-world\ndeployment of federated learning algorithms. Most prior work either overlooks\nthe potential non-stationarity in the dynamics of client unavailability or\nrequires substantial memory/computation overhead. We study federated learning\nin the presence of heterogeneous and non-stationary client availability, which\nmay occur when the deployment environments are uncertain, or the clients are\nmobile. The impacts of heterogeneity and non-stationarity on client\nunavailability can be significant, as we illustrate using FedAvg, the most\nwidely adopted federated learning algorithm. We propose FedAPM, which includes\nnovel algorithmic structures that (i) compensate for missed computations due to\nunavailability with only $O(1)$ additional memory and computation with respect\nto standard FedAvg, and (ii) evenly diffuse local updates within the federated\nlearning system through implicit gossiping, despite being agnostic to\nnon-stationary dynamics. We show that FedAPM converges to a stationary point of\neven non-convex objectives while achieving the desired linear speedup property.\nWe corroborate our analysis with numerical experiments over diversified client\nunavailability dynamics on real-world data sets."
                },
                "authors": [
                    {
                        "name": "Ming Xiang"
                    },
                    {
                        "name": "Stratis Ioannidis"
                    },
                    {
                        "name": "Edmund Yeh"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "Lili Su"
                    }
                ],
                "author_detail": {
                    "name": "Lili Su"
                },
                "author": "Lili Su",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19238v2",
                "updated": "2024-10-31T16:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    6,
                    22,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-27T15:01:53Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    15,
                    1,
                    53,
                    3,
                    179,
                    0
                ],
                "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Fine-Grained Values and Opinions in Large Language Models"
                },
                "summary": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances."
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Nadav Borenstein"
                    },
                    {
                        "name": "Srishti Yadav"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05977v2",
                "updated": "2024-10-31T16:01:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    1,
                    59,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-09T18:21:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    18,
                    21,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4"
                },
                "summary": "Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra."
                },
                "authors": [
                    {
                        "name": "Xichen Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xichen Tang"
                },
                "author": "Xichen Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02119v3",
                "updated": "2024-10-31T15:57:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    57,
                    42,
                    3,
                    305,
                    0
                ],
                "published": "2023-12-04T18:49:23Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    18,
                    49,
                    23,
                    0,
                    338,
                    0
                ],
                "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"
                },
                "summary": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard."
                },
                "authors": [
                    {
                        "name": "Anay Mehrotra"
                    },
                    {
                        "name": "Manolis Zampetakis"
                    },
                    {
                        "name": "Paul Kassianik"
                    },
                    {
                        "name": "Blaine Nelson"
                    },
                    {
                        "name": "Hyrum Anderson"
                    },
                    {
                        "name": "Yaron Singer"
                    },
                    {
                        "name": "Amin Karbasi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Karbasi"
                },
                "author": "Amin Karbasi",
                "arxiv_comment": "Accepted for presentation at NeurIPS 2024. Code:\n  https://github.com/RICommunity/TAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24049v1",
                "updated": "2024-10-31T15:45:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    45,
                    23,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:45:23Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    45,
                    23,
                    3,
                    305,
                    0
                ],
                "title": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs"
                },
                "summary": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs."
                },
                "authors": [
                    {
                        "name": "Muhammed Saeed"
                    },
                    {
                        "name": "Elgizouli Mohamed"
                    },
                    {
                        "name": "Mukhtar Mohamed"
                    },
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Shady Shehata"
                    },
                    {
                        "name": "Muhammad Abdul-Mageed"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Abdul-Mageed"
                },
                "author": "Muhammad Abdul-Mageed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24039v1",
                "updated": "2024-10-31T15:35:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    35,
                    41,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:35:41Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    35,
                    41,
                    3,
                    305,
                    0
                ],
                "title": "Efficient Satellite-Ground Interconnection Design for Low-orbit\n  Mega-Constellation Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Satellite-Ground Interconnection Design for Low-orbit\n  Mega-Constellation Topology"
                },
                "summary": "The low-orbit mega-constellation network (LMCN) is an important part of the\nspace-air-ground integrated network system. An effective satellite-ground\ninterconnection design can result in a stable constellation topology for LMCNs.\nA naive solution is accessing the satellite with the longest remaining service\ntime (LRST), which is widely used in previous designs. The Coordinated\nSatellite-Ground Interconnecting (CSGI), the state-of-the-art algorithm,\ncoordinates the establishment of ground-satellite links (GSLs). Compared with\nexisting solutions, it reduces latency by 19% and jitter by 70% on average.\nHowever, CSGI only supports the scenario where terminals access only one\nsatellite and cannot fully utilize the multi-access capabilities of terminals.\nAdditionally, CSGI's high computational complexity poses deployment challenges.\nTo overcome these problems, we propose the Classification-based Longest\nRemaining Service Time (C-LRST) algorithm. C-LRST supports the actual scenario\nwith multi-access capabilities. It adds optional paths during routing with low\ncomputational complexity, improving end-to-end communications quality. We\nconduct our 1000s simulation from Brazil to Lithuania on the open-source\nplatform Hypatia. Experiment results show that compared with CSGI, C-LRST\nreduces the latency and increases the throughput by approximately 60% and 40%,\nrespectively. In addition, C-LRST's GSL switching number is 14, whereas CSGI is\n23. C-LRST has better link stability than CSGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The low-orbit mega-constellation network (LMCN) is an important part of the\nspace-air-ground integrated network system. An effective satellite-ground\ninterconnection design can result in a stable constellation topology for LMCNs.\nA naive solution is accessing the satellite with the longest remaining service\ntime (LRST), which is widely used in previous designs. The Coordinated\nSatellite-Ground Interconnecting (CSGI), the state-of-the-art algorithm,\ncoordinates the establishment of ground-satellite links (GSLs). Compared with\nexisting solutions, it reduces latency by 19% and jitter by 70% on average.\nHowever, CSGI only supports the scenario where terminals access only one\nsatellite and cannot fully utilize the multi-access capabilities of terminals.\nAdditionally, CSGI's high computational complexity poses deployment challenges.\nTo overcome these problems, we propose the Classification-based Longest\nRemaining Service Time (C-LRST) algorithm. C-LRST supports the actual scenario\nwith multi-access capabilities. It adds optional paths during routing with low\ncomputational complexity, improving end-to-end communications quality. We\nconduct our 1000s simulation from Brazil to Lithuania on the open-source\nplatform Hypatia. Experiment results show that compared with CSGI, C-LRST\nreduces the latency and increases the throughput by approximately 60% and 40%,\nrespectively. In addition, C-LRST's GSL switching number is 14, whereas CSGI is\n23. C-LRST has better link stability than CSGI."
                },
                "authors": [
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Jiazhi Wu"
                    },
                    {
                        "name": "Quanwei Lin"
                    },
                    {
                        "name": "Handong Luo"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Kun Qiu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Yue Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Gao"
                },
                "author": "Yue Gao",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24034v1",
                "updated": "2024-10-31T15:32:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    32,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:32:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    32,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Handwriting Recognition in Historical Documents with Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handwriting Recognition in Historical Documents with Multimodal LLM"
                },
                "summary": "There is an immense quantity of historical and cultural documentation that\nexists only as handwritten manuscripts. At the same time, performing OCR across\nscripts and different handwriting styles has proven to be an enormously\ndifficult problem relative to the process of digitizing print. While recent\nTransformer based models have achieved relatively strong performance, they rely\nheavily on manually transcribed training data and have difficulty generalizing\nacross writers. Multimodal LLM, such as GPT-4v and Gemini, have demonstrated\neffectiveness in performing OCR and computer vision tasks with few shot\nprompting. In this paper, I evaluate the accuracy of handwritten document\ntranscriptions generated by Gemini against the current state of the art\nTransformer based methods.\n  Keywords: Optical Character Recognition, Multimodal Language Models, Cultural\nPreservation, Mass digitization, Handwriting Recognitio",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an immense quantity of historical and cultural documentation that\nexists only as handwritten manuscripts. At the same time, performing OCR across\nscripts and different handwriting styles has proven to be an enormously\ndifficult problem relative to the process of digitizing print. While recent\nTransformer based models have achieved relatively strong performance, they rely\nheavily on manually transcribed training data and have difficulty generalizing\nacross writers. Multimodal LLM, such as GPT-4v and Gemini, have demonstrated\neffectiveness in performing OCR and computer vision tasks with few shot\nprompting. In this paper, I evaluate the accuracy of handwritten document\ntranscriptions generated by Gemini against the current state of the art\nTransformer based methods.\n  Keywords: Optical Character Recognition, Multimodal Language Models, Cultural\nPreservation, Mass digitization, Handwriting Recognitio"
                },
                "authors": [
                    {
                        "name": "Lucian Li"
                    }
                ],
                "author_detail": {
                    "name": "Lucian Li"
                },
                "author": "Lucian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24032v1",
                "updated": "2024-10-31T15:30:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    30,
                    55,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:30:55Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    30,
                    55,
                    3,
                    305,
                    0
                ],
                "title": "Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks"
                },
                "summary": "The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration."
                },
                "authors": [
                    {
                        "name": "Yingzhe Peng"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24024v1",
                "updated": "2024-10-31T15:25:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    25,
                    20,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:25:20Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    25,
                    20,
                    3,
                    305,
                    0
                ],
                "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents"
                },
                "summary": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59\\% to 21.50\\% for LLMs and\nfrom 1.93\\% to 13.28\\% for LMMs. AndroidLab is open-sourced and publicly\navailable at \\url{https://github.com/THUDM/Android-Lab}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59\\% to 21.50\\% for LLMs and\nfrom 1.93\\% to 13.28\\% for LMMs. AndroidLab is open-sourced and publicly\navailable at \\url{https://github.com/THUDM/Android-Lab}."
                },
                "authors": [
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Siyi Cheng"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Shudan Zhang"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24021v1",
                "updated": "2024-10-31T15:21:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    21,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:21:27Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    21,
                    27,
                    3,
                    305,
                    0
                ],
                "title": "Detecting text level intellectual influence with knowledge graph\n  embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting text level intellectual influence with knowledge graph\n  embeddings"
                },
                "summary": "Introduction: Tracing the spread of ideas and the presence of influence is a\nquestion of special importance across a wide range of disciplines, ranging from\nintellectual history to cultural analytics, computational social science, and\nthe science of science.\n  Method: We collect a corpus of open source journal articles, generate\nKnowledge Graph representations using the Gemini LLM, and attempt to predict\nthe existence of citations between sampled pairs of articles using previously\npublished methods and a novel Graph Neural Network based embedding model.\n  Results: We demonstrate that our knowledge graph embedding method is superior\nat distinguishing pairs of articles with and without citation. Once trained, it\nruns efficiently and can be fine-tuned on specific corpora to suit individual\nresearcher needs.\n  Conclusion(s): This experiment demonstrates that the relationships encoded in\na knowledge graph, especially the types of concepts brought together by\nspecific relations can encode information capable of revealing intellectual\ninfluence. This suggests that further work in analyzing document level\nknowledge graphs to understand latent structures could provide valuable\ninsights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Tracing the spread of ideas and the presence of influence is a\nquestion of special importance across a wide range of disciplines, ranging from\nintellectual history to cultural analytics, computational social science, and\nthe science of science.\n  Method: We collect a corpus of open source journal articles, generate\nKnowledge Graph representations using the Gemini LLM, and attempt to predict\nthe existence of citations between sampled pairs of articles using previously\npublished methods and a novel Graph Neural Network based embedding model.\n  Results: We demonstrate that our knowledge graph embedding method is superior\nat distinguishing pairs of articles with and without citation. Once trained, it\nruns efficiently and can be fine-tuned on specific corpora to suit individual\nresearcher needs.\n  Conclusion(s): This experiment demonstrates that the relationships encoded in\na knowledge graph, especially the types of concepts brought together by\nspecific relations can encode information capable of revealing intellectual\ninfluence. This suggests that further work in analyzing document level\nknowledge graphs to understand latent structures could provide valuable\ninsights."
                },
                "authors": [
                    {
                        "name": "Lucian Li"
                    },
                    {
                        "name": "Eryclis Silva"
                    }
                ],
                "author_detail": {
                    "name": "Eryclis Silva"
                },
                "author": "Eryclis Silva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24013v1",
                "updated": "2024-10-31T15:14:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    14,
                    15,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T15:14:15Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    14,
                    15,
                    3,
                    305,
                    0
                ],
                "title": "Distributing Intelligence in 6G Programmable Data Planes for Effective\n  In-Network Deployment of an Active Intrusion Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributing Intelligence in 6G Programmable Data Planes for Effective\n  In-Network Deployment of an Active Intrusion Detection System"
                },
                "summary": "The problem of attacks on new generation network infrastructures is becoming\nincreasingly relevant, given the widening of the attack surface of these\nnetworks resulting from the greater number of devices that will access them in\nthe future (sensors, actuators, vehicles, household appliances, etc.).\nApproaches to the design of intrusion detection systems must evolve and go\nbeyond the traditional concept of perimeter control to build on new paradigms\nthat exploit the typical characteristics of future 5G and 6G networks, such as\nin-network computing and intelligent programmable data planes. The aim of this\nresearch is to propose a disruptive paradigm in which devices in a typical data\nplane of a future programmable network have %classification and anomaly\ndetection capabilities and cooperate in a fully distributed fashion to act as\nan ML-enabled Active Intrusion Detection System \"embedded\" into the network.\nThe reported proof-of-concept experiments demonstrate that the proposed\nparadigm allows working effectively and with a good level of precision while\noccupying overall less CPU and RAM resources of the devices involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of attacks on new generation network infrastructures is becoming\nincreasingly relevant, given the widening of the attack surface of these\nnetworks resulting from the greater number of devices that will access them in\nthe future (sensors, actuators, vehicles, household appliances, etc.).\nApproaches to the design of intrusion detection systems must evolve and go\nbeyond the traditional concept of perimeter control to build on new paradigms\nthat exploit the typical characteristics of future 5G and 6G networks, such as\nin-network computing and intelligent programmable data planes. The aim of this\nresearch is to propose a disruptive paradigm in which devices in a typical data\nplane of a future programmable network have %classification and anomaly\ndetection capabilities and cooperate in a fully distributed fashion to act as\nan ML-enabled Active Intrusion Detection System \"embedded\" into the network.\nThe reported proof-of-concept experiments demonstrate that the proposed\nparadigm allows working effectively and with a good level of precision while\noccupying overall less CPU and RAM resources of the devices involved."
                },
                "authors": [
                    {
                        "name": "Mattia G. Spina"
                    },
                    {
                        "name": "Floriano De Rango"
                    },
                    {
                        "name": "Edoardo Scalzo"
                    },
                    {
                        "name": "Francesca Guerriero"
                    },
                    {
                        "name": "Antonio Iera"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Iera"
                },
                "author": "Antonio Iera",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.15656v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.15656v3",
                "updated": "2024-10-31T15:01:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    1,
                    41,
                    3,
                    305,
                    0
                ],
                "published": "2022-11-28T18:59:02Z",
                "published_parsed": [
                    2022,
                    11,
                    28,
                    18,
                    59,
                    2,
                    0,
                    332,
                    0
                ],
                "title": "SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation"
                },
                "summary": "High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion."
                },
                "authors": [
                    {
                        "name": "Hao Dong"
                    },
                    {
                        "name": "Weihao Gu"
                    },
                    {
                        "name": "Xianjing Zhang"
                    },
                    {
                        "name": "Jintao Xu"
                    },
                    {
                        "name": "Rui Ai"
                    },
                    {
                        "name": "Huimin Lu"
                    },
                    {
                        "name": "Juho Kannala"
                    },
                    {
                        "name": "Xieyuanli Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xieyuanli Chen"
                },
                "author": "Xieyuanli Chen",
                "arxiv_comment": "ICRA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.15656v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.15656v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15852v2",
                "updated": "2024-10-31T14:43:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    43,
                    58,
                    3,
                    305,
                    0
                ],
                "published": "2024-03-23T14:04:48Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    14,
                    4,
                    48,
                    5,
                    83,
                    0
                ],
                "title": "SOEN-101: Code Generation by Emulating Software Process Models Using\n  Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOEN-101: Code Generation by Emulating Software Process Models Using\n  Large Language Model Agents"
                },
                "summary": "Software process models are essential to facilitate collaboration and\ncommunication among software teams to solve complex development tasks. Inspired\nby these software engineering practices, we present FlowGen - a code generation\nframework that emulates software process models based on multiple Large\nLanguage Model (LLM) agents. We emulate three process models, FlowGenWaterfall,\nFlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e.,\nrequirement engineer, architect, developer, tester, and scrum master) that\ncorrespond to everyday development activities and organize their communication\npatterns. The agents work collaboratively using chain-of-thought and prompt\ncomposition with continuous self-refinement to improve the code quality. We use\nGPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion)\nto evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP,\nand MBPP-ET. Our findings show that FlowGenScrum excels compared to other\nprocess models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval,\nHumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement\nover RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum\nachieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming\nReflexion. Notably, integrating CodeT into FlowGenScrum resulted in\nstatistically significant improvements, achieving the highest Pass@1 scores.\nOur analysis also reveals that the development activities impacted code smell\nand exception handling differently, with design and code review adding more\nexception handling and reducing code smells. Finally, FlowGen models maintain\nstable Pass@1 scores across GPT3.5 versions and temperature values,\nhighlighting the effectiveness of software process models in enhancing the\nquality and stability of LLM-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software process models are essential to facilitate collaboration and\ncommunication among software teams to solve complex development tasks. Inspired\nby these software engineering practices, we present FlowGen - a code generation\nframework that emulates software process models based on multiple Large\nLanguage Model (LLM) agents. We emulate three process models, FlowGenWaterfall,\nFlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e.,\nrequirement engineer, architect, developer, tester, and scrum master) that\ncorrespond to everyday development activities and organize their communication\npatterns. The agents work collaboratively using chain-of-thought and prompt\ncomposition with continuous self-refinement to improve the code quality. We use\nGPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion)\nto evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP,\nand MBPP-ET. Our findings show that FlowGenScrum excels compared to other\nprocess models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval,\nHumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement\nover RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum\nachieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming\nReflexion. Notably, integrating CodeT into FlowGenScrum resulted in\nstatistically significant improvements, achieving the highest Pass@1 scores.\nOur analysis also reveals that the development activities impacted code smell\nand exception handling differently, with design and code review adding more\nexception handling and reducing code smells. Finally, FlowGen models maintain\nstable Pass@1 scores across GPT3.5 versions and temperature values,\nhighlighting the effectiveness of software process models in enhancing the\nquality and stability of LLM-generated code."
                },
                "authors": [
                    {
                        "name": "Feng Lin"
                    },
                    {
                        "name": "Dong Jae Kim"
                    },
                    {
                        "name": "Tse-Husn"
                    },
                    {
                        "name": "Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen"
                },
                "arxiv_affiliation": "Peter",
                "author": "Chen",
                "arxiv_comment": "ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17692v2",
                "updated": "2024-10-31T14:38:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    38,
                    27,
                    3,
                    305,
                    0
                ],
                "published": "2024-09-26T09:57:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    57,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "MIO: A Foundation Model on Multimodal Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIO: A Foundation Model on Multimodal Tokens"
                },
                "summary": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Ning Shi"
                    },
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "arxiv_comment": "Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22637v2",
                "updated": "2024-10-31T14:35:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    35,
                    31,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-30T02:04:23Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    4,
                    23,
                    2,
                    304,
                    0
                ],
                "title": "Consistency Diffusion Bridge Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Diffusion Bridge Models"
                },
                "summary": "Diffusion models (DMs) have become the dominant paradigm of generative\nmodeling in a variety of domains by learning stochastic processes from noise to\ndata. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\ngenerative modeling that builds stochastic processes between fixed data\nendpoints based on a reference diffusion process, have achieved empirical\nsuccess across tasks with coupled data distribution, such as image-to-image\ntranslation. However, DDBM's sampling process typically requires hundreds of\nnetwork evaluations to achieve decent performance, which may impede their\npractical deployment due to high computational demands. In this work, inspired\nby the recent advance of consistency models in DMs, we tackle this problem by\nlearning the consistency function of the probability-flow ordinary differential\nequation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\nstep given any point on the ODE trajectory. Based on a dedicated general-form\nODE solver, we propose two paradigms: consistency bridge distillation and\nconsistency bridge training, which is flexible to apply on DDBMs with broad\ndesign choices. Experimental results show that our proposed method could sample\n$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\nquality given the same step in various tasks with pixel resolution ranging from\n$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\nas semantic interpolation in the data space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the dominant paradigm of generative\nmodeling in a variety of domains by learning stochastic processes from noise to\ndata. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\ngenerative modeling that builds stochastic processes between fixed data\nendpoints based on a reference diffusion process, have achieved empirical\nsuccess across tasks with coupled data distribution, such as image-to-image\ntranslation. However, DDBM's sampling process typically requires hundreds of\nnetwork evaluations to achieve decent performance, which may impede their\npractical deployment due to high computational demands. In this work, inspired\nby the recent advance of consistency models in DMs, we tackle this problem by\nlearning the consistency function of the probability-flow ordinary differential\nequation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\nstep given any point on the ODE trajectory. Based on a dedicated general-form\nODE solver, we propose two paradigms: consistency bridge distillation and\nconsistency bridge training, which is flexible to apply on DDBMs with broad\ndesign choices. Experimental results show that our proposed method could sample\n$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\nquality given the same step in various tasks with pixel resolution ranging from\n$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\nas semantic interpolation in the data space."
                },
                "authors": [
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Fan Bao"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19534v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19534v4",
                "updated": "2024-10-31T14:32:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    32,
                    28,
                    3,
                    305,
                    0
                ],
                "published": "2024-05-29T21:29:44Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    21,
                    29,
                    44,
                    2,
                    150,
                    0
                ],
                "title": "Preference Learning Algorithms Do Not Learn Preference Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Learning Algorithms Do Not Learn Preference Rankings"
                },
                "summary": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Sadhika Malladi"
                    },
                    {
                        "name": "Lily H. Zhang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Qiuyi Zhang"
                    },
                    {
                        "name": "Rajesh Ranganath"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghyun Cho"
                },
                "author": "Kyunghyun Cho",
                "arxiv_comment": "NeurIPS 2024 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19534v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19534v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06246v2",
                "updated": "2024-10-31T14:24:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    24,
                    45,
                    3,
                    305,
                    0
                ],
                "published": "2024-06-10T13:23:00Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    13,
                    23,
                    0,
                    0,
                    162,
                    0
                ],
                "title": "Data-Efficient Learning with Neural Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Learning with Neural Programs"
                },
                "summary": "Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymbolic learning literature. Our evaluation shows\nthat for the latter benchmarks, ISED has comparable performance to\nstate-of-the-art neurosymbolic frameworks. For the former, we use adaptations\nof prior work on gradient approximations of black-box components as a baseline,\nand show that ISED achieves comparable accuracy but in a more data- and\nsample-efficient manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymbolic learning literature. Our evaluation shows\nthat for the latter benchmarks, ISED has comparable performance to\nstate-of-the-art neurosymbolic frameworks. For the former, we use adaptations\nof prior work on gradient approximations of black-box components as a baseline,\nand show that ISED achieves comparable accuracy but in a more data- and\nsample-efficient manner."
                },
                "authors": [
                    {
                        "name": "Alaia Solko-Breslin"
                    },
                    {
                        "name": "Seewon Choi"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Neelay Velingker"
                    },
                    {
                        "name": "Rajeev Alur"
                    },
                    {
                        "name": "Mayur Naik"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]